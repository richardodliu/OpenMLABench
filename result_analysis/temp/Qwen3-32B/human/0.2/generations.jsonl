{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    # Check if the number of columns in the matrix matches the length of the vector\n    if len(a[0]) != len(b):\n        return -1\n\n    result = []\n    # Compute the dot product for each row of the matrix with the vector\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    vector = [7, 8, 9]\n    print(matrix_dot_vector(matrix, vector))  # Output: [50, 122]\n```"}
{"task_id": 2, "completion_id": 0, "completion": "Sure! Here's a self-contained Python function that computes the **transpose of a matrix**. The transpose of a matrix is obtained by converting rows into columns and columns into rows.\n\n```python\ndef transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    # Use zip(*a) to transpose the matrix and convert tuples back to lists\n    return [list(row) for row in zip(*a)]\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6]\n    ]\n    transposed = transpose_matrix(matrix)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed:\n        print(row)\n```\n\n### Explanation:\n- `zip(*a)` unpacks the rows of the matrix and zips them together, effectively transposing the matrix.\n- The result of `zip(*a)` is a list of tuples, so we convert each tuple to a list using a list comprehension.\n\n### Example Output:\nFor the input matrix:\n```\n[1, 2, 3]\n[4, 5, 6]\n```\nThe output will be:\n```\n[1, 4]\n[2, 5]\n[3, 6]\n```\n\nLet me know if you'd like a version that avoids using `zip`."}
{"task_id": 3, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that defines the `reshape_matrix` function as described. It uses NumPy to reshape the matrix and returns the result as a Python list. If the reshape is not possible due to incompatible dimensions, it returns an empty list `[]`.\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    # Flatten the input matrix to a 1D array\n    flat = [item for sublist in a for item in sublist]\n    \n    # Check if the total number of elements matches the new shape\n    if len(flat) != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Convert to NumPy array and reshape\n    reshaped = np.array(flat).reshape(new_shape)\n    \n    # Convert back to Python list\n    return reshaped.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    original_matrix = [[1, 2, 3], [4, 5, 6]]\n    new_shape = (3, 2)\n    result = reshape_matrix(original_matrix, new_shape)\n    print(\"Reshaped Matrix:\", result)\n```\n\n### Explanation:\n- The function first flattens the input matrix into a 1D list.\n- It checks if the total number of elements matches the product of the new shape dimensions.\n- If the reshape is valid, it uses `np.reshape()` and converts the result to a Python list using `tolist()`.\n- If the reshape is not possible, it returns an empty list `[]`.\n\nYou can modify the `original_matrix` and `new_shape` in the `if __name__ == \"__main__\":` block to test different inputs."}
{"task_id": 4, "completion_id": 0, "completion": "Here's a self-contained Python function that calculates the mean of a matrix either by row or by column, based on the given mode:\n\n```python\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        # Transpose the matrix to iterate over columns\n        transposed = list(zip(*matrix))\n        return [sum(col) / len(col) for col in transposed]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1.0, 2.0, 3.0],\n        [4.0, 5.0, 6.0],\n        [7.0, 8.0, 9.0]\n    ]\n    \n    print(\"Row means:\", calculate_matrix_mean(matrix, 'row'))\n    print(\"Column means:\", calculate_matrix_mean(matrix, 'column'))\n```\n\n### Explanation:\n- **Row mode**: For each row in the matrix, compute the average of its elements.\n- **Column mode**: Transpose the matrix using `zip(*matrix)` to iterate over columns, then compute the average for each column.\n- The function raises a `ValueError` if an invalid mode is provided.\n\nYou can run this script directly to see the output for the example matrix."}
{"task_id": 5, "completion_id": 0, "completion": "Here's a self-contained Python script that performs scalar multiplication of a matrix:\n\n```python\ndef scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the resulting matrix.\n    \n    Parameters:\n    matrix (list of list of int/float): The input matrix.\n    scalar (int/float): The scalar to multiply with the matrix.\n    \n    Returns:\n    list of list of int/float: The resulting matrix after scalar multiplication.\n    \"\"\"\n    # Use list comprehension to multiply each element by the scalar\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 2\n\n    result = scalar_multiply(matrix, scalar)\n    print(\"Result of scalar multiplication:\")\n    for row in result:\n        print(row)\n```\n\n### Explanation:\n- The function `scalar_multiply` takes a matrix (a list of lists) and a scalar value.\n- It uses nested list comprehensions to iterate over each row and each element in the row, multiplying each element by the scalar.\n- The example usage demonstrates how to call the function and print the resulting matrix."}
{"task_id": 6, "completion_id": 0, "completion": "Here's a self-contained Python function that calculates the eigenvalues of a 2x2 matrix and returns them sorted in descending order:\n\n```python\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    Parameters:\n    matrix (list[list[float | int]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[float]: A list of eigenvalues sorted in descending order.\n    \"\"\"\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Trace of the matrix\n    trace = a + d\n    \n    # Determinant of the matrix\n    determinant = a * d - b * c\n    \n    # Discriminant for the quadratic equation\n    discriminant = (trace ** 2) - 4 * determinant\n    \n    # Calculate the two eigenvalues\n    eigenvalue1 = (trace + discriminant ** 0.5) / 2\n    eigenvalue2 = (trace - discriminant ** 0.5) / 2\n    \n    # Return sorted eigenvalues (highest to lowest)\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [[2, 1], [1, 2]]\n    eigenvalues = calculate_eigenvalues(matrix)\n    print(\"Eigenvalues:\", eigenvalues)\n```\n\n### Explanation:\n- The function calculates the **trace** (`a + d`) and **determinant** (`ad - bc`) of the 2x2 matrix.\n- It uses the quadratic formula to find the eigenvalues:  \n  $$\n  \\lambda = \\frac{\\text{trace} \\pm \\sqrt{\\text{trace}^2 - 4 \\cdot \\text{determinant}}}{2}\n  $$\n- The resulting eigenvalues are sorted in descending order using Python\u2019s built-in `sorted()` function.\n\n### Example:\nFor the matrix `[[2, 1], [1, 2]]`, the eigenvalues are `[3.0, 1.0]`."}
{"task_id": 7, "completion_id": 0, "completion": "Here's a self-contained Python script that performs the matrix transformation $ T^{-1}AS $, validates the invertibility of matrices $ T $ and $ S $, and returns the result rounded to 4 decimal places:\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A, dtype=float)\n        T_np = np.array(T, dtype=float)\n        S_np = np.array(S, dtype=float)\n        \n        # Check if T and S are square matrices\n        if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n            return -1\n        \n        # Check if T and S are invertible by computing determinant\n        if np.isclose(np.linalg.det(T_np), 0) or np.isclose(np.linalg.det(S_np), 0):\n            return -1\n        \n        # Perform transformation: T^{-1} @ A @ S\n        result = np.linalg.inv(T_np) @ A_np @ S_np\n        \n        # Round to 4 decimal places and convert to list\n        return np.round(result, 4).tolist()\n    \n    except np.linalg.LinAlgError:\n        # In case of any linear algebra error (e.g., singular matrix)\n        return -1\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[1, 2], [3, 4]]\n    T = [[2, 0], [0, 2]]\n    S = [[1, 1], [0, 1]]\n    \n    result = transform_matrix(A, T, S)\n    print(\"Transformed Matrix:\")\n    for row in result:\n        print(row)\n```\n\n### Explanation:\n- **Matrix Invertibility Check**: Uses `np.linalg.det()` to check if the determinant is non-zero (within a small tolerance using `np.isclose`).\n- **Matrix Multiplication**: Uses `@` for matrix multiplication.\n- **Rounding**: Uses `np.round(..., 4)` to round the result to 4 decimal places.\n- **Output**: Converts the result to a Python list using `.tolist()`.\n\nYou can test this function with different matrices to see how it handles valid and invalid inputs."}
{"task_id": 8, "completion_id": 0, "completion": "Here's a self-contained Python function that calculates the inverse of a 2x2 matrix. If the matrix is not invertible (i.e., determinant is zero), it returns `None`.\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Extract elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    determinant = a * d - b * c\n    \n    # If determinant is zero, the matrix is not invertible\n    if determinant == 0:\n        return None\n    \n    # Calculate the inverse using the formula for 2x2 matrices\n    inv = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    return inv\n\n# Example usage:\nif __name__ == \"__main__\":\n    mat = [[1, 2], [3, 4]]\n    inv = inverse_2x2(mat)\n    if inv:\n        print(\"Inverse matrix:\")\n        for row in inv:\n            print(row)\n    else:\n        print(\"Matrix is not invertible.\")\n```"}
{"task_id": 9, "completion_id": 0, "completion": "Sure! Here's a self-contained Python function that multiplies two matrices. It checks if the matrices can be multiplied (i.e., the number of columns in the first matrix must equal the number of rows in the second matrix). If not, it returns `-1`. Otherwise, it returns the resulting matrix.\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    # Check if matrices can be multiplied: columns of a must equal rows of b\n    if len(a[0]) != len(b):\n        return -1\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n\n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n\n    return result\n```\n\n### Example usage:\n```python\na = [[1, 2], [3, 4]]\nb = [[2, 0], [1, 2]]\nprint(matrixmul(a, b))  # Output: [[4, 4], [10, 8]]\n```\n\nThis function works for both integer and float matrix elements."}
{"task_id": 10, "completion_id": 0, "completion": "Here's a self-contained Python script that defines the `calculate_covariance_matrix` function and includes test cases to verify its correctness:\n\n```python\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given list of feature vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix.\n    \"\"\"\n    n = len(vectors[0])  # Number of observations\n    m = len(vectors)     # Number of features\n    \n    # Step 1: Calculate the mean of each feature\n    means = [sum(feature) / n for feature in vectors]\n    \n    # Step 2: Calculate the covariance matrix\n    covariance_matrix = [[0.0 for _ in range(m)] for _ in range(m)]\n    \n    for i in range(m):\n        for j in range(m):\n            covariance = 0.0\n            for k in range(n):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= (n - 1)  # Use sample covariance (n-1)\n            covariance_matrix[i][j] = covariance\n    \n    return covariance_matrix\n\n# Test cases\ndef test_calculate_covariance_matrix():\n    # Test case 1: Simple 2D data\n    data1 = [\n        [1, 2, 3, 4],  # Feature 1\n        [2, 4, 6, 8]   # Feature 2\n    ]\n    expected1 = [\n        [1.6666666666666667, 3.3333333333333335],\n        [3.3333333333333335, 6.666666666666667]\n    ]\n    result1 = calculate_covariance_matrix(data1)\n    print(\"Test 1 Result:\", result1)\n    print(\"Test 1 Passed:\", all(abs(r[i] - e[i]) < 1e-6 for i, e in enumerate(row)) for row, r in zip(expected1, result1))\n\n    # Test case 2: 3D data\n    data2 = [\n        [1, 2, 3],  # Feature 1\n        [4, 5, 6],  # Feature 2\n        [7, 8, 9]   # Feature 3\n    ]\n    expected2 = [\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0]\n    ]\n    result2 = calculate_covariance_matrix(data2)\n    print(\"Test 2 Result:\", result2)\n    print(\"Test 2 Passed:\", all(abs(r[i] - e[i]) < 1e-6 for i, e in enumerate(row)) for row, r in zip(expected2, result2))\n\n    # Test case 3: Random data\n    import random\n    data3 = [\n        [random.gauss(0, 1) for _ in range(100)],  # Feature 1\n        [random.gauss(0, 1) for _ in range(100)],  # Feature 2\n        [random.gauss(0, 1) for _ in range(100)]   # Feature 3\n    ]\n    result3 = calculate_covariance_matrix(data3)\n    print(\"Test 3 Result (first 3x3):\", [row[:3] for row in result3[:3]])\n    print(\"Test 3 Passed: All diagonal values should be close to 1 (variance of unit normal distribution)\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\n### Explanation:\n- **Function `calculate_covariance_matrix`**:\n  - Computes the covariance matrix using the formula for sample covariance.\n  - Assumes all feature vectors have the same number of observations.\n- **Test Cases**:\n  - **Test 1**: Simple linear relationship between two features.\n  - **Test 2**: Perfect linear relationship in 3D.\n  - **Test 3**: Random data from a normal distribution to test variance and covariance behavior.\n\nYou can run this script as-is in a Python environment to see the results and verify the correctness of the implementation."}
{"task_id": 11, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Jacobi method** to solve a system of linear equations `Ax = b`, iterating `n` times and rounding the solution to four decimal places at each step.\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi method.\n    \n    Parameters:\n    - A: Coefficient matrix (n x n)\n    - b: Right-hand side vector (n x 1)\n    - n: Number of iterations\n    \n    Returns:\n    - x: Approximate solution vector as a list, rounded to 4 decimal places\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"Matrix A and vector b must have the same number of rows.\")\n    \n    x = np.zeros_like(b, dtype=np.float64)  # Initial guess\n    D = np.diag(np.diag(A))  # Diagonal component\n    R = A - D  # Remaining components (off-diagonal)\n    \n    for _ in range(n):\n        x = np.round((np.linalg.inv(D) @ (b - R @ x)), 4)\n    \n    return x.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([\n        [5, 2, 1],\n        [2, 8, -1],\n        [1, -1, 4]\n    ], dtype=np.float64)\n    \n    b = np.array([10, 12, 4], dtype=np.float64)\n    n = 10  # Number of iterations\n    \n    solution = solve_jacobi(A, b, n)\n    print(\"Approximate solution x:\", solution)\n```\n\n### Explanation:\n- **Jacobi Method Formula**:  \n  $ x^{(k+1)} = D^{-1}(b - R x^{(k)}) $  \n  where:\n  - $ D $ is the diagonal matrix of $ A $\n  - $ R = A - D $ is the remaining matrix (off-diagonal elements)\n\n- The solution is updated iteratively for `n` times.\n- Each intermediate solution is rounded to **4 decimal places** using `np.round(..., 4)`.\n- The final solution is returned as a **Python list** using `tolist()`.\n\n### Notes:\n- The method assumes that the matrix `A` is diagonally dominant for convergence.\n- You can test with different matrices and vectors to see how the method behaves."}
{"task_id": 12, "completion_id": 0, "completion": "Here's a self-contained Python script that computes the singular values of a 2x2 matrix using the **Jacobian method** (also known as the **Jacobi eigenvalue algorithm**) applied to the matrix $ A^T A $. This avoids using `numpy.linalg.svd` and instead computes the singular values from the square roots of the eigenvalues of $ A^T A $.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix using the Jacobian method.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 real matrix.\n    \n    Returns:\n        tuple: A tuple of two singular values, rounded to 4 decimal places.\n    \"\"\"\n    # Step 1: Compute A^T * A\n    A_T_A = A.T @ A\n\n    # Step 2: Extract the symmetric matrix elements\n    a, b = A_T_A[0, 0], A_T_A[0, 1]\n    c, d = A_T_A[1, 0], A_T_A[1, 1]\n\n    # Step 3: Apply the Jacobian method to diagonalize A^T * A\n    # We'll perform a rotation to zero out the off-diagonal elements\n    max_iterations = 1000\n    tolerance = 1e-10\n\n    for _ in range(max_iterations):\n        if abs(b) < tolerance:\n            break\n        # Compute the rotation angle\n        theta = 0.5 * np.arctan2(2 * b, (a - d))\n        c_theta = np.cos(theta)\n        s_theta = np.sin(theta)\n\n        # Apply the rotation\n        a_new = a * c_theta**2 - 2 * b * c_theta * s_theta + d * s_theta**2\n        d_new = a * s_theta**2 + 2 * b * c_theta * s_theta + d * c_theta**2\n        b_new = (a - d) * c_theta * s_theta + b * (c_theta**2 - s_theta**2)\n\n        a, d, b = a_new, d_new, b_new\n\n    # Step 4: The diagonal entries are the eigenvalues of A^T * A\n    # Singular values are the square roots of the eigenvalues\n    sigma1 = np.sqrt(max(a, d))\n    sigma2 = np.sqrt(min(a, d))\n\n    # Round to 4 decimal places\n    return (round(sigma1, 4), round(sigma2, 4))\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[3, 4], [1, 2]])\n    singular_values = svd_2x2_singular_values(A)\n    print(\"Singular values:\", singular_values)\n```\n\n### Explanation:\n- The function computes the matrix $ A^T A $, which is symmetric.\n- It then applies the **Jacobian method** to diagonalize this matrix by rotating the off-diagonal elements to zero.\n- The resulting diagonal elements are the eigenvalues of $ A^T A $.\n- The singular values are the square roots of these eigenvalues.\n- The result is returned as a tuple of two values, rounded to 4 decimal places.\n\nYou can test this function with any 2x2 real matrix."}
{"task_id": 13, "completion_id": 0, "completion": "Here's a self-contained Python script that calculates the determinant of a 4x4 matrix using Laplace's Expansion method recursively:\n\n```python\ndef determinant_4x4(matrix: list[list[int | float]]) -> float:\n    def determinant_3x3(mat: list[list[int | float]]) -> float:\n        # Calculate determinant of a 3x3 matrix using the rule of Sarrus or cofactor expansion\n        return (\n            mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) -\n            mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) +\n            mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0])\n        )\n\n    det = 0\n    for col in range(4):\n        # Create the minor matrix by removing the first row and current column\n        minor = [row[:col] + row[col+1:] for row in matrix[1:]]\n        # Apply Laplace expansion along the first row\n        sign = (-1) ** col\n        det += sign * matrix[0][col] * determinant_3x3(minor)\n    \n    return det\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    print(\"Determinant of the 4x4 matrix:\", determinant_4x4(matrix))\n```\n\n### Explanation:\n- The `determinant_4x4` function uses **Laplace's Expansion** along the **first row**.\n- For each element in the first row, it computes the corresponding **3x3 minor** and recursively calculates its determinant.\n- The `determinant_3x3` helper function computes the determinant of a 3x3 matrix using cofactor expansion.\n- The final determinant is the sum of the signed products of the first row elements and their respective 3x3 minors.\n\nYou can test this with any 4x4 matrix."}
{"task_id": 14, "completion_id": 0, "completion": "Here's a self-contained Python script that performs linear regression using the normal equation. The function takes a matrix `X` and a vector `y` as input and returns the coefficients rounded to four decimal places.\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert input lists to NumPy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Compute the normal equation: theta = (X^T * X)^(-1) * X^T * y\n    X_T = X_np.T\n    theta = np.linalg.inv(X_T @ X_np) @ X_T @ y_np\n    \n    # Round the result to four decimal places\n    theta_rounded = [round(val, 4) for val in theta]\n    \n    return theta_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample input data\n    X = [[1, 2], [1, 3], [1, 4], [1, 5]]  # Including bias term (1s)\n    y = [3, 5, 7, 9]\n    \n    # Get the coefficients\n    coefficients = linear_regression_normal_equation(X, y)\n    \n    # Print the result\n    print(\"Coefficients:\", coefficients)\n```\n\n### Explanation:\n- The **normal equation** is given by:  \n  $$\n  \\theta = (X^T X)^{-1} X^T y\n  $$\n- This method directly computes the optimal parameters without iteration.\n- The function uses `numpy` for matrix operations and ensures the result is rounded to **4 decimal places** as required.\n\nYou can replace the `X` and `y` in the `if __name__ == \"__main__\"` block with your own data to test the function."}
{"task_id": 15, "completion_id": 0, "completion": "Here's a self-contained Python script that performs linear regression using gradient descent as per your task description:\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    # Number of training examples\n    m = len(y)\n    \n    # Initialize coefficients (theta) to zeros\n    theta = np.zeros(X.shape[1])\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Compute prediction\n        predictions = X @ theta\n        \n        # Compute error\n        errors = predictions - y\n        \n        # Compute gradient\n        gradient = (1/m) * X.T @ errors\n        \n        # Update theta\n        theta -= alpha * gradient\n    \n    # Round the coefficients to four decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert to list and return\n    return theta_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example dataset\n    X = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])\n    y = np.array([2, 4, 6, 8])\n    alpha = 0.01\n    iterations = 1000\n    \n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```\n\n### Explanation:\n- `X` is a NumPy array with a column of ones for the intercept (bias).\n- `y` is the target variable.\n- The function performs `iterations` of gradient descent to minimize the cost function.\n- The learning rate is `alpha`.\n- The resulting coefficients are rounded to four decimal places and returned as a list.\n\nYou can replace the example `X`, `y`, `alpha`, and `iterations` with your own data to fit different models."}
{"task_id": 16, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the feature scaling task as described:\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization: (x - mean) / std\n    standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Min-Max Normalization: (x - min) / (max - min)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    min_max_scaled = (data - min_vals) / (max_vals - min_vals)\n    \n    # Round to 4 decimal places and convert to list\n    standardized_list = np.round(standardized, 4).tolist()\n    min_max_scaled_list = np.round(min_max_scaled, 4).tolist()\n    \n    return standardized_list, min_max_scaled_list\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    standardized, min_max = feature_scaling(data)\n    print(\"Standardized:\")\n    print(standardized)\n    print(\"\\nMin-Max Normalized:\")\n    print(min_max)\n```\n\n### Explanation:\n- **Standardization**: Each feature is centered around 0 with a standard deviation of 1.\n- **Min-Max Normalization**: Each feature is scaled to the range [0, 1].\n- The results are rounded to 4 decimal places and converted to Python lists using `tolist()`.\n\nYou can run this script directly to see the output for the example dataset."}
{"task_id": 17, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **k-Means clustering algorithm** using the **Normal Equation** for centroid updates. The function adheres to the specified inputs and outputs, and ensures all results are rounded to the **nearest fourth decimal**.\n\n```python\nimport numpy as np\nfrom typing import List, Tuple\n\ndef k_means_clustering(\n    points: List[Tuple[float, float]], \n    k: int, \n    initial_centroids: List[Tuple[float, float]], \n    max_iterations: int\n) -> List[Tuple[float, float]]:\n    # Convert points and centroids to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Step 1: Assign each point to the nearest centroid\n        distances = np.sqrt(((points[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=1)\n        \n        # Step 2: Update centroids by computing the mean of all points assigned to each cluster\n        new_centroids = np.array([\n            points[labels == i].mean(axis=0) for i in range(k)\n        ])\n        \n        # If centroids do not change, break early\n        if np.allclose(centroids, new_centroids):\n            break\n        \n        centroids = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\n```\n\n### \u2705 Example Usage:\n\n```python\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.1, 1.0), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nresult = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(result)\n```\n\n### \ud83d\udccc Notes:\n- The function uses `numpy` for vectorized operations to improve performance.\n- The `np.allclose` check helps in early stopping if centroids stop changing.\n- The final centroids are rounded to **4 decimal places** as required.\n\nLet me know if you'd like a version that supports higher-dimensional points or uses random initialization!"}
{"task_id": 18, "completion_id": 0, "completion": "Here's a self-contained Python script that implements K-Fold Cross-Validation. It defines a function `k_fold_cross_validation` that returns a list of train-test index splits for each fold.\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate indices to split data into k folds for cross-validation.\n\n    Parameters:\n    - X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n    - y (np.ndarray): Target vector of shape (n_samples,)\n    - k (int): Number of folds\n    - shuffle (bool): Whether to shuffle the data before splitting\n    - random_seed (int): Seed for random number generator\n\n    Returns:\n    - List of tuples: Each tuple contains (train_indices, test_indices)\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n\n    if shuffle:\n        if random_seed is not None:\n            np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    folds = []\n    fold_size = n_samples // k\n    remainder = n_samples % k\n\n    start = 0\n    for i in range(k):\n        # Determine the size of the current fold\n        fold_end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:fold_end]\n        train_indices = np.concatenate((indices[:start], indices[fold_end:]))\n        folds.append((train_indices, test_indices))\n        start = fold_end\n\n    return folds\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 0, 1, 0])\n\n    # Generate K-Fold splits\n    k = 3\n    splits = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=42)\n\n    # Print the splits\n    for i, (train_idx, test_idx) in enumerate(splits):\n        print(f\"Fold {i+1}:\")\n        print(\"  Train indices:\", train_idx)\n        print(\"  Test indices: \", test_idx)\n        print()\n```\n\n### Explanation:\n- The function first generates an array of indices from `0` to `n_samples - 1`.\n- If `shuffle` is `True`, it shuffles the indices using `np.random.shuffle`.\n- It calculates the size of each fold and distributes any remainder samples across the first few folds.\n- For each fold, it creates a test set and the remaining indices form the training set.\n- The function returns a list of tuples, where each tuple contains the training and test indices for a fold.\n\nYou can run this script directly to see how the function works with sample data."}
{"task_id": 19, "completion_id": 0, "completion": "Here's a self-contained Python script that implements Principal Component Analysis (PCA) from scratch, following the given task description and output constraints:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data\n    mean = np.mean(data, axis=0)\n    standardized_data = data - mean\n\n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Step 3: Compute eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # Step 4: Sort eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Step 5: Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n\n    # Step 6: Round the principal components to 4 decimal places\n    principal_components_rounded = np.round(principal_components, 4)\n\n    # Convert NumPy array to list of lists\n    return principal_components_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data (4 samples, 2 features)\n    data = np.array([\n        [2.5, 2.4],\n        [0.5, 0.7],\n        [2.2, 2.9],\n        [1.9, 2.2]\n    ])\n\n    k = 1  # Number of principal components to return\n    components = pca(data, k)\n    print(\"Principal Components:\")\n    for pc in components:\n        print(pc)\n```\n\n### Explanation:\n- **Standardization**: The data is centered by subtracting the mean of each feature.\n- **Covariance Matrix**: Computed using `np.cov` with `rowvar=False` to treat columns as variables.\n- **Eigen Decomposition**: Eigenvalues and eigenvectors are computed using `np.linalg.eig`.\n- **Sorting**: Eigenvectors are sorted by their corresponding eigenvalues in descending order.\n- **Selection**: The top `k` eigenvectors are selected as the principal components.\n- **Output**: The result is rounded to 4 decimal places and returned as a list of lists.\n\nThis script is self-contained and can be run directly."}
{"task_id": 20, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **decision tree learning algorithm** using **entropy** and **information gain** for classification. The function `learn_decision_tree` recursively builds a binary decision tree using the ID3-like approach.\n\n```python\nimport math\nfrom collections import Counter\nfrom typing import Dict, List, Any\n\ndef entropy(class_counts: Dict[Any, int]) -> float:\n    \"\"\"Calculate the entropy of a set of class counts.\"\"\"\n    total = sum(class_counts.values())\n    if total == 0:\n        return 0.0\n    return -sum((count / total) * math.log2(count / total) for count in class_counts.values())\n\ndef information_gain(parent_entropy: float, subsets: List[tuple[float, Dict[Any, int]]]) -> float:\n    \"\"\"\n    Calculate the information gain from splitting into subsets.\n    Each subset is a tuple (weight, class_counts), where weight is the proportion of examples in the subset.\n    \"\"\"\n    return parent_entropy - sum(weight * entropy(class_counts) for weight, class_counts in subsets)\n\ndef most_common_class(examples: List[Dict[str, Any]], target_attr: str) -> Any:\n    \"\"\"Return the most common class in the examples.\"\"\"\n    return Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n\ndef split_examples(examples: List[Dict[str, Any]], attribute: str, value: Any) -> List[Dict[str, Any]]:\n    \"\"\"Return a list of examples where the attribute has the given value.\"\"\"\n    return [example for example in examples if example[attribute] == value]\n\ndef learn_decision_tree(examples: List[Dict[str, Any]], attributes: List[str], target_attr: str) -> Dict[str, Any]:\n    \"\"\"\n    Learn a decision tree from examples using entropy and information gain.\n    Returns a nested dictionary representing the decision tree.\n    \"\"\"\n    if not examples:\n        return most_common_class(examples, target_attr)  # Return default class if no examples\n\n    # Check if all examples have the same class\n    class_values = [example[target_attr] for example in examples]\n    if len(set(class_values)) == 1:\n        return {\"class\": class_values[0]}\n\n    if not attributes:\n        return {\"class\": most_common_class(examples, target_attr)}\n\n    # Choose the best attribute to split on\n    best_attr = None\n    max_gain = -1.0\n    parent_entropy = entropy(Counter(class_values))\n\n    for attr in attributes:\n        if attr == target_attr:\n            continue\n\n        # Get all possible values for this attribute\n        values = set(example[attr] for example in examples)\n        subsets = []\n        for value in values:\n            subset = split_examples(examples, attr, value)\n            class_counts = Counter(example[target_attr] for example in subset)\n            weight = len(subset) / len(examples)\n            subsets.append((weight, class_counts))\n\n        gain = information_gain(parent_entropy, subsets)\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n\n    # Remove the best attribute from the list of attributes to avoid reusing it\n    remaining_attributes = [attr for attr in attributes if attr != best_attr]\n\n    # Build the subtree for each value of the best attribute\n    tree = {best_attr: {}}\n    for value in set(example[best_attr] for example in examples):\n        subset = split_examples(examples, best_attr, value)\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n\n    return tree\n```\n\n### Example Usage:\n\n```python\n# Example dataset\nexamples = [\n    {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Play\": \"No\"},\n    {\"Outlook\": \"Sunny\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Play\": \"No\"},\n    {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"High\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Rain\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Play\": \"No\"},\n    {\"Outlook\": \"Overcast\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Play\": \"No\"},\n    {\"Outlook\": \"Sunny\", \"Temperature\": \"Cool\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Sunny\", \"Temperature\": \"Mild\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Overcast\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Overcast\", \"Temperature\": \"Hot\", \"Humidity\": \"Normal\", \"Play\": \"Yes\"},\n    {\"Outlook\": \"Rain\", \"Temperature\": \"Mild\", \"Humidity\": \"High\", \"Play\": \"No\"},\n]\n\nattributes = [\"Outlook\", \"Temperature\", \"Humidity\", \"Play\"]\ntarget_attr = \"Play\"\n\n# Learn the decision tree\ntree = learn_decision_tree(examples, attributes, target_attr)\n\n# Print the tree\nimport pprint\npprint.pprint(tree)\n```\n\nThis implementation is a simplified version of the **ID3 algorithm** using **entropy** and **information gain** for attribute selection. It builds a **nested dictionary** representing the decision tree, which can be used for classification."}
{"task_id": 21, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a **deterministic version of the Pegasos algorithm** for a **kernel SVM**. It supports both **linear** and **RBF (Radial Basis Function)** kernels and returns the **alpha coefficients** and **bias** after training.\n\n```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Deterministic Pegasos Kernel SVM implementation.\n    \n    Parameters:\n    - data: 2D NumPy array of shape (n_samples, n_features)\n    - labels: 1D NumPy array of shape (n_samples,)\n    - kernel: 'linear' or 'rbf'\n    - lambda_val: Regularization parameter\n    - iterations: Number of training iterations\n    - sigma: Kernel width for RBF kernel\n    \n    Returns:\n    - alphas: List of alpha coefficients (rounded to 4 decimals)\n    - bias: Bias term (rounded to 4 decimals)\n    \"\"\"\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)  # Learning rate\n\n        # Compute kernel matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    K[i, j] = np.dot(data[i], data[j])\n                elif kernel == 'rbf':\n                    K[i, j] = np.exp(-np.linalg.norm(data[i] - data[j]) ** 2 / (2 * sigma ** 2))\n\n        # Compute the decision function\n        y_pred = np.dot(alphas * labels, K) + bias\n\n        # Compute the gradient\n        grad = np.zeros(n_samples)\n        for i in range(n_samples):\n            if labels[i] * y_pred[i] < 1:\n                grad[i] = -labels[i] * K[:, i]\n            else:\n                grad[i] = 0\n\n        # Update alphas and bias\n        alphas = alphas - eta * (lambda_val * alphas + grad)\n        bias = bias - eta * (-labels * (labels * y_pred < 1)).sum()\n\n    # Apply the constraint: alphas >= 0\n    alphas = np.clip(alphas, a_min=0, a_max=None)\n\n    return {\n        'alphas': np.round(alphas, 4).tolist(),\n        'bias': round(bias, 4)\n    }\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example dataset\n    data = np.array([[1, 2], [2, 3], [3, 3], [6, 7], [7, 8], [8, 8]])\n    labels = np.array([1, 1, 1, -1, -1, -1])\n\n    # Train the model\n    model = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.01, iterations=100, sigma=1.0)\n\n    print(\"Alpha coefficients:\", model['alphas'])\n    print(\"Bias:\", model['bias'])\n```\n\n### Key Features:\n- **Deterministic**: Uses all samples in each iteration.\n- **Kernel Support**: Implements both **linear** and **RBF** kernels.\n- **Regularization**: Uses L2 regularization with parameter `lambda_val`.\n- **Output**: Returns `alphas` and `bias` rounded to 4 decimal places as per the requirement.\n\nYou can modify the `data` and `labels` in the `__main__` block to test with your own dataset."}
{"task_id": 22, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the sigmoid activation function and returns the result rounded to four decimal places:\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): Input value\n    \n    Returns:\n    float: Sigmoid output rounded to four decimal places\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    z = 0.5\n    result = sigmoid(z)\n    print(f\"Sigmoid({z}) = {result}\")\n```\n\n### Explanation:\n- The **sigmoid function** is defined as:  \n  $$\n  \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n  $$\n- We use `math.exp(-z)` to compute $ e^{-z} $.\n- The result is rounded to **four decimal places** using Python's `round()` function.\n\nYou can test it with different values of `z` by changing the `z` variable in the `if __name__ == \"__main__\":` block."}
{"task_id": 23, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the **softmax activation function** for a given list of scores. The output values are rounded to **four decimal places** as required.\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Compute the exponent of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Compute the sum of all exponentials\n    sum_exp = sum(exp_scores)\n    \n    # Compute the softmax for each score\n    softmax_values = [round(exp / sum_exp, 4) for exp in exp_scores]\n    \n    return softmax_values\n\n# Example usage\nif __name__ == \"__main__\":\n    example_scores = [2.0, 1.0, 0.1]\n    result = softmax(example_scores)\n    print(\"Softmax values:\", result)\n```\n\n### Explanation:\n- `math.exp(score)` computes the exponential of each score.\n- The sum of all exponentials is used to normalize the values.\n- Each exponential is divided by the sum to get the softmax value.\n- The result is rounded to 4 decimal places using `round()`.\n\nYou can replace `example_scores` with any list of floats to compute the softmax for different inputs."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features: List of feature vectors (each vector is a list of floats)\n    - labels: List of true binary labels (0 or 1)\n    - weights: List of weights (one for each feature)\n    - bias: Bias term\n    \n    Returns:\n    - predicted_probabilities: List of predicted probabilities (after sigmoid activation)\n    - mean_squared_error: Mean squared error between predicted probabilities and true labels\n    \"\"\"\n    predicted_probabilities = []\n    \n    # Ensure features and labels are numpy arrays for vectorized operations\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    \n    # Compute the linear combination (z = W\u00b7X + b)\n    linear_combination = np.dot(features_array, weights_array) + bias\n    \n    # Apply the sigmoid activation function: \u03c3(z) = 1 / (1 + exp(-z))\n    for z in linear_combination:\n        probability = 1 / (1 + math.exp(-z))\n        predicted_probabilities.append(round(probability, 4))\n    \n    # Compute mean squared error\n    predicted_probabilities_array = np.array(predicted_probabilities)\n    labels_array = np.array(labels)\n    mean_squared_error = np.mean((predicted_probabilities_array - labels_array) ** 2)\n    mean_squared_error = round(float(mean_squared_error), 4)\n    \n    return predicted_probabilities, mean_squared_error\n```"}
{"task_id": 25, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the task of training a single neuron with a sigmoid activation function using backpropagation and gradient descent to minimize the Mean Squared Error (MSE) loss:\n\n```python\nimport numpy as np\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    # Initialize weights and bias\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_history = []\n\n    # Sigmoid activation function\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    # Derivative of sigmoid\n    def sigmoid_derivative(x):\n        return x * (1 - x)\n\n    # Training loop\n    for epoch in range(epochs):\n        # Forward pass\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n\n        # Compute MSE loss\n        mse = np.mean((predictions - labels) ** 2)\n        mse_history.append(round(mse, 4))\n\n        # Backward pass\n        error = predictions - labels\n        d_loss = error  # Derivative of MSE w.r.t. predictions\n        d_pred = d_loss * sigmoid_derivative(predictions)  # Derivative of predictions w.r.t. linear output\n\n        # Gradients\n        d_weights = np.dot(features.T, d_pred) / features.shape[0]\n        d_bias = np.mean(d_pred)\n\n        # Update weights and bias\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    # Round and convert to list\n    weights_rounded = np.round(weights, 4)\n    bias_rounded = round(bias, 4)\n    mse_history = [round(mse, 4) for mse in mse_history]\n\n    return weights_rounded, bias_rounded, mse_history\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    features = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n    labels = np.array([0, 1, 1, 0])\n    initial_weights = np.array([0.5, 0.5])\n    initial_bias = 0.0\n    learning_rate = 0.1\n    epochs = 1000\n\n    weights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n\n    print(\"Final Weights:\", weights)\n    print(\"Final Bias:\", bias)\n    print(\"MSE History (first 10):\", mse_values[:10])\n```\n\n### Explanation:\n- **Sigmoid Function**: Used as the activation function to introduce non-linearity.\n- **MSE Loss**: The mean squared error is used as the loss function.\n- **Backpropagation**: Gradients are computed using the chain rule and weights are updated using gradient descent.\n- **Rounding**: All outputs are rounded to 4 decimal places as required.\n- **Output**: The function returns the updated weights, bias, and a list of MSE values for each epoch.\n\nYou can run this script directly in a Python environment."}
{"task_id": 26, "completion_id": 0, "completion": "Here's a self-contained Python script implementing the `Value` class with basic autograd operations: addition, multiplication, and ReLU activation. The class supports scalar values and computes gradients using reverse-mode automatic differentiation.\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            self.grad += out.grad * 1\n            other.grad += out.grad * 1\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += out.grad * other.data\n            other.grad += out.grad * self.data\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='relu')\n\n        def _backward():\n            self.grad += out.grad * (1 if self.data > 0 else 0)\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        # Topological sort\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        # Reverse-mode autograd\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __neg__(self):\n        return self * -1\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __rsub__(self, other):\n        return other + (-self)\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        return self * other ** -1\n\n    def __rtruediv__(self, other):\n        return other * self ** -1\n\n    def __pow__(self, other):\n        assert isinstance(other, (int, float)), \"Only int/float exponents are supported\"\n        out = Value(self.data ** other, _children=(self,), _op=f'**{other}')\n\n        def _backward():\n            self.grad += out.grad * other * (self.data ** (other - 1))\n        out._backward = _backward\n\n        return out\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad})\"\n```\n\n### Example Usage:\n\n```python\n# Example usage\na = Value(2.0)\nb = Value(3.0)\nc = a * b  # 6\nd = c + b  # 9\ne = d.relu()  # 9\ne.backward()\n\nprint(f\"a.grad = {a.grad}\")  # 3.0\nprint(f\"b.grad = {b.grad}\")  # 2.0 + 1.0 = 3.0\nprint(f\"c.grad = {c.grad}\")  # 1.0\nprint(f\"d.grad = {d.grad}\")  # 1.0\nprint(f\"e.grad = {e.grad}\")  # 1.0\n```\n\nThis script provides a minimal but functional autograd system for scalar operations. You can extend it further to support more operations or vectorized inputs."}
{"task_id": 27, "completion_id": 0, "completion": "Here's a self-contained Python script that computes the transformation matrix from basis `B` to basis `C` in $\\mathbb{R}^3$. The transformation matrix $ P $ is computed as $ P = C^{-1}B $, where $ B $ and $ C $ are matrices whose columns are the basis vectors of the respective bases.\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    B (list[list[int]]): A list of 3 basis vectors in basis B.\n    C (list[list[int]]): A list of 3 basis vectors in basis C.\n    \n    Returns:\n    list[list[float]]: The transformation matrix P from B to C, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input lists to NumPy arrays\n    B_matrix = np.array(B, dtype=float)\n    C_matrix = np.array(C, dtype=float)\n    \n    # Compute the inverse of matrix C\n    C_inv = np.linalg.inv(C_matrix)\n    \n    # Compute the transformation matrix P = C^{-1} * B\n    P = np.dot(C_inv, B_matrix)\n    \n    # Round the result to 4 decimal places and convert to list\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    B = [[1, 0, 0],\n         [0, 1, 0],\n         [0, 0, 1]]  # Standard basis\n\n    C = [[1, 1, 0],\n         [0, 1, 1],\n         [1, 0, 1]]  # Another basis\n\n    result = transform_basis(B, C)\n    print(\"Transformation Matrix P (B to C):\")\n    for row in result:\n        print(row)\n```\n\n### Explanation:\n- `B` and `C` are 3x3 matrices where each column is a basis vector.\n- The transformation matrix from `B` to `C` is given by $ P = C^{-1}B $.\n- The result is rounded to 4 decimal places using `np.round`.\n- The final matrix is returned as a Python list of lists using `tolist()`.\n\nYou can replace the example `B` and `C` with any valid 3x3 basis matrices to compute the transformation matrix."}
{"task_id": 28, "completion_id": 0, "completion": "Here's a self-contained Python script that performs the SVD of a 2x2 matrix using the method based on eigendecomposition of $A^T A$:\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Step 1: Compute A^T * A\n    A_T_A = A.T @ A\n\n    # Step 2: Compute eigenvalues and eigenvectors of A^T * A\n    eigenvalues, eigenvectors = np.linalg.eig(A_T_A)\n\n    # Step 3: Sort eigenvalues and eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Step 4: Compute singular values as square roots of eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n\n    # Step 5: Construct V from eigenvectors of A^T * A\n    V = eigenvectors\n\n    # Step 6: Construct S as a diagonal matrix of singular values\n    S = np.diag(singular_values)\n\n    # Step 7: Compute U by normalizing A * v_i / sigma_i\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] > 1e-10:  # Avoid division by zero\n            U[:, i] = (A @ V[:, i]) / singular_values[i]\n        else:\n            U[:, i] = np.zeros(2)  # Handle zero singular value\n\n    # Step 8: Round all results to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n\n    # Step 9: Convert to Python lists using tolist()\n    return U.tolist(), S.tolist(), V.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]], dtype=float)\n    U, S, V = svd_2x2(A)\n    print(\"U:\")\n    print(U)\n    print(\"S:\")\n    print(S)\n    print(\"V:\")\n    print(V)\n```\n\n### Explanation:\n- **A^T A** is computed to find the symmetric matrix needed for eigendecomposition.\n- **Eigenvalues and eigenvectors** are computed using `np.linalg.eig`.\n- **Singular values** are the square roots of the eigenvalues.\n- **V** is formed from the eigenvectors.\n- **U** is computed using the relation $ u_i = \\frac{A v_i}{\\sigma_i} $.\n- All results are rounded to 4 decimal places and converted to Python lists using `.tolist()`.\n\nThis script is suitable for 2x2 matrices and follows the geometric construction of SVD using eigendecomposition."}
{"task_id": 29, "completion_id": 0, "completion": "Here's a self-contained Python script that performs a random shuffle of two NumPy arrays `X` and `y` while maintaining their correspondence. It includes an optional `seed` parameter for reproducibility and returns the shuffled arrays as Python lists using `tolist()`.\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in X and y while maintaining their correspondence.\n    \n    Parameters:\n    - X (np.ndarray): Features array.\n    - y (np.ndarray): Labels array.\n    - seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    - X_shuffled (list): Shuffled features as a Python list.\n    - y_shuffled (list): Shuffled labels as a Python list.\n    \"\"\"\n    # Ensure X and y are NumPy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Check if they have the same number of samples\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples.\")\n    \n    # Set the random seed if provided\n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate random indices\n    indices = np.random.permutation(X.shape[0])\n    \n    # Shuffle both arrays using the same indices\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Convert to Python lists using tolist()\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    \n    X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n    \n    print(\"Shuffled X:\", X_shuffled)\n    print(\"Shuffled y:\", y_shuffled)\n```\n\n### Explanation:\n- `np.random.permutation(X.shape[0])` generates a random permutation of indices.\n- `X[indices]` and `y[indices]` shuffle both arrays using the same indices to maintain correspondence.\n- `tolist()` is used to convert the NumPy arrays to Python lists as required."}
{"task_id": 30, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `batch_iterator` function as described. This function yields batches of data from the input arrays `X` and optionally `y`, using NumPy and converting the final output to a Python list using `tolist()`.\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of data from the input arrays X and optionally y.\n    \n    Parameters:\n        X (np.ndarray): Input data array.\n        y (np.ndarray, optional): Target data array. Defaults to None.\n        batch_size (int): Number of samples per batch. Defaults to 64.\n    \n    Yields:\n        tuple or np.ndarray: Batches of (X_batch, y_batch) if y is provided, else X_batch.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_end = min(i + batch_size, n_samples)\n        X_batch = X[i:batch_end]\n        if y is not None:\n            y_batch = y[i:batch_end]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 0, 1, 0])\n\n    batch_size = 2\n\n    # With y\n    print(\"Batches with y:\")\n    for batch in batch_iterator(X, y, batch_size):\n        print(batch)\n\n    # Without y\n    print(\"\\nBatches without y:\")\n    for batch in batch_iterator(X, batch_size=batch_size):\n        print(batch)\n```\n\n### Explanation:\n- The function `batch_iterator` takes in `X`, `y` (optional), and `batch_size`.\n- It loops over the data in steps of `batch_size`.\n- If `y` is provided, it yields tuples of `(X_batch, y_batch)` as Python lists.\n- If `y` is not provided, it yields only `X_batch` as a Python list.\n- The `tolist()` method is used to convert NumPy arrays to Python lists as required.\n\nYou can run this script directly to see how it works with sample data."}
{"task_id": 31, "completion_id": 0, "completion": "Here's a self-contained Python script that defines the `divide_on_feature` function as described. It uses NumPy to handle the dataset and returns the subsets as Python lists after reshaping using `tolist()`.\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n\n    Parameters:\n    - X: numpy.ndarray, the dataset (2D array).\n    - feature_i: int, the index of the feature to split on.\n    - threshold: float, the threshold value for splitting.\n\n    Returns:\n    - left: list, subset of samples where feature_i >= threshold.\n    - right: list, subset of samples where feature_i < threshold.\n    \"\"\"\n    # Extract the specified feature column\n    feature_values = X[:, feature_i]\n\n    # Create boolean mask for the condition\n    mask = feature_values >= threshold\n\n    # Split the dataset using the mask\n    left = X[mask].tolist()\n    right = X[~mask].tolist()\n\n    return left, right\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample dataset (2D array)\n    X = np.array([\n        [1.2, 3.5],\n        [2.8, 1.1],\n        [3.4, 4.0],\n        [0.5, 2.2],\n        [4.1, 3.9]\n    ])\n\n    # Feature index and threshold\n    feature_i = 0  # Using the first feature\n    threshold = 2.0\n\n    # Divide the dataset\n    left, right = divide_on_feature(X, feature_i, threshold)\n\n    print(\"Left subset (feature >= threshold):\")\n    print(left)\n    print(\"\\nRight subset (feature < threshold):\")\n    print(right)\n```\n\n### Explanation:\n- The function `divide_on_feature` takes a 2D NumPy array `X`, an integer `feature_i` (the column index), and a `threshold` value.\n- It creates a boolean mask to identify rows where the feature value is greater than or equal to the threshold.\n- It then splits the dataset into two subsets using this mask.\n- Finally, it converts the NumPy arrays to Python lists using `tolist()` and returns them."}
{"task_id": 32, "completion_id": 0, "completion": "Here's a self-contained Python script that generates polynomial features up to a specified degree for a given 2D numpy array `X`. The function uses `itertools.combinations_with_replacement` to generate all possible combinations of features up to the given degree.\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    features = []\n\n    for deg in range(1, degree + 1):\n        for combo in combinations_with_replacement(range(n_features), deg):\n            feature = np.prod(X[:, combo], axis=1)\n            features.append(feature)\n\n    return np.column_stack(features).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[2, 3],\n                  [4, 5]])\n    degree = 2\n    result = polynomial_features(X, degree)\n    print(result)\n```\n\n### Explanation:\n- `combinations_with_replacement(range(n_features), deg)` generates all possible combinations of feature indices with replacement for the given degree.\n- `np.prod(X[:, combo], axis=1)` computes the product of the selected features for each sample.\n- `np.column_stack(features)` stacks all generated features into a single 2D array.\n- `.tolist()` converts the final result to a Python list as required.\n\n### Example Output:\nFor the input:\n```python\nX = np.array([[2, 3],\n              [4, 5]])\ndegree = 2\n```\nThe output will be:\n```python\n[[2, 3, 4, 6, 9],\n [4, 5, 16, 20, 25]]\n```\nWhere the columns correspond to:\n- Original features: `[x1, x2]`\n- Degree 2 features: `[x1^2, x1*x2, x2^2]`"}
{"task_id": 33, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `get_random_subsets` function as described:\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - y: 1D numpy array of shape (n_samples,)\n    - n_subsets: Number of subsets to generate\n    - replacements: If True, samples are selected with replacement; otherwise without replacement\n    - seed: Random seed for reproducibility\n\n    Returns:\n    - List of tuples, each tuple is (X_subset, y_subset)\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        else:\n            indices = np.random.permutation(n_samples)\n        \n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset, y_subset))\n    \n    return [(X_sub.tolist(), y_sub.tolist()) for X_sub, y_sub in subsets]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample dataset\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n\n    # Generate 3 random subsets with replacement\n    subsets = get_random_subsets(X, y, n_subsets=3, replacements=True)\n\n    # Print the result\n    for i, (X_sub, y_sub) in enumerate(subsets):\n        print(f\"Subset {i+1}:\")\n        print(\"X:\", X_sub)\n        print(\"y:\", y_sub)\n        print()\n```"}
{"task_id": 34, "completion_id": 0, "completion": "Here's a self-contained Python script that performs one-hot encoding of nominal values using NumPy:\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of a 1D numpy array of integer values.\n\n    Parameters:\n    x (np.ndarray): 1D array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. \n                           If not provided, it is determined from the max value in x.\n\n    Returns:\n    list: One-hot encoded array as a Python list.\n    \"\"\"\n    x = np.array(x)\n    if n_col is None:\n        n_col = x.max() + 1\n    n_row = x.shape[0]\n    one_hot = np.zeros((n_row, n_col))\n    one_hot[np.arange(n_row), x] = 1\n    return one_hot.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([0, 1, 2, 1, 0, 3])\n    result = to_categorical(x)\n    print(result)\n```\n\n### Explanation:\n- The function `to_categorical` takes a 1D NumPy array `x` and an optional number of columns `n_col`.\n- If `n_col` is not provided, it is inferred as `max(x) + 1` to cover all possible classes.\n- A zero matrix of shape `(n_row, n_col)` is created.\n- The appropriate indices are set to 1 to create the one-hot encoding.\n- The result is converted to a Python list using `tolist()` as required.\n\n### Example Output:\nFor the input `[0, 1, 2, 1, 0, 3]`, the output will be:\n```python\n[\n    [1.0, 0.0, 0.0, 0.0],\n    [0.0, 1.0, 0.0, 0.0],\n    [0.0, 0.0, 1.0, 0.0],\n    [0.0, 1.0, 0.0, 0.0],\n    [1.0, 0.0, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 1.0]\n]\n```"}
{"task_id": 35, "completion_id": 0, "completion": "Here's a self-contained Python script that converts a 1D NumPy array into a diagonal matrix and returns it as a Python list using NumPy's `tolist()` method:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix and returns it as a list.\n    \n    Parameters:\n    x (np.ndarray): A 1D numpy array.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Create a diagonal matrix from the input array\n    diagonal_matrix = np.diag(x)\n    # Convert the result to a Python list\n    return diagonal_matrix.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([1, 2, 3])\n    result = make_diagonal(x)\n    print(result)\n```\n\n### Example Output:\nFor input `x = [1, 2, 3]`, the output will be:\n```\n[[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```\n\nThis script uses `np.diag()` to create the diagonal matrix and `tolist()` to convert it to a Python list as required."}
{"task_id": 36, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that defines the `accuracy_score` function as described. It calculates the accuracy by comparing the true labels (`y_true`) with the predicted labels (`y_pred`) and returns the result rounded to 4 decimal places.\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score between true and predicted labels.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true labels.\n    y_pred (np.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 1, 0, 1, 0, 1])\n    y_pred = np.array([0, 1, 0, 0, 1, 1, 1])\n    \n    acc = accuracy_score(y_true, y_pred)\n    print(f\"Accuracy: {acc}\")\n```\n\n### Explanation:\n- `np.sum(y_true == y_pred)` counts the number of correct predictions.\n- The accuracy is calculated as the ratio of correct predictions to the total number of samples.\n- The result is rounded to 4 decimal places using `round(accuracy, 4)`."}
{"task_id": 37, "completion_id": 0, "completion": "Here's a self-contained Python script that defines the `calculate_correlation_matrix` function as described:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between X and Y (or X with itself if Y is None).\n    \n    Parameters:\n    X (np.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    Y (np.ndarray, optional): A 2D numpy array of shape (n_samples, m_features). Defaults to None.\n    \n    Returns:\n    list: Correlation matrix as a 2D list, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Center the data (subtract the mean)\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    \n    # Calculate standard deviations\n    std_X = np.std(X, axis=0, ddof=0)  # Population standard deviation\n    std_Y = np.std(Y, axis=0, ddof=0)\n    \n    # Avoid division by zero\n    std_X[std_X == 0] = 1.0\n    std_Y[std_Y == 0] = 1.0\n    \n    # Normalize the centered data\n    X_normalized = X_centered / std_X\n    Y_normalized = Y_centered / std_Y\n    \n    # Compute the correlation matrix\n    correlation_matrix = np.dot(X_normalized.T, Y_normalized) / X.shape[0]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(correlation_matrix, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example dataset\n    X = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    \n    Y = np.array([\n        [9, 8, 7],\n        [6, 5, 4],\n        [3, 2, 1]\n    ])\n    \n    # Calculate correlation matrix\n    corr_matrix = calculate_correlation_matrix(X, Y)\n    \n    # Print result\n    print(\"Correlation Matrix:\")\n    for row in corr_matrix:\n        print(row)\n```"}
{"task_id": 38, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `adaboost_fit` function as described. The implementation includes finding the best thresholds for each feature, computing errors, updating weights, and returning a list of classifiers with their parameters.\n\n```python\nimport numpy as np\nimport math\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    D = np.ones(n_samples) / n_samples  # Initialize weights\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_alpha = 0\n        best_rule = None  # (feature_index, threshold, direction)\n\n        # Try all features\n        for feature_index in range(n_features):\n            feature_values = X[:, feature_index]\n            unique_values = np.unique(feature_values)\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2  # Midpoints between unique values\n\n            for threshold in thresholds:\n                for direction in [1, -1]:  # 1 for >= threshold, -1 for < threshold\n                    predictions = np.where(X[:, feature_index] * direction >= threshold * direction, 1, -1)\n                    error = np.sum(D[y != predictions])\n\n                    if error > 0.5:\n                        error = 1 - error\n                        predictions = -predictions\n\n                    if error < best_error:\n                        best_error = error\n                        best_alpha = 0.5 * np.log((1 - error) / error)\n                        best_rule = (feature_index, threshold, direction)\n\n        # Update classifier list\n        classifiers.append({\n            'feature_index': best_rule[0],\n            'threshold': best_rule[1],\n            'direction': best_rule[2],\n            'alpha': round(best_alpha, 4)\n        })\n\n        # Update weights\n        predictions = np.where(\n            X[:, best_rule[0]] * best_rule[2] >= best_rule[1] * best_rule[2], 1, -1\n        )\n        D *= np.exp(-best_alpha * y * predictions)\n        D /= np.sum(D)  # Normalize\n\n    return classifiers\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([\n        [1, 2],\n        [2, 3],\n        [3, 4],\n        [4, 5],\n        [5, 6]\n    ])\n    y = np.array([1, 1, -1, -1, -1])\n    n_clf = 2\n    result = adaboost_fit(X, y, n_clf)\n    print(result)\n```\n\n### Explanation:\n- **Initialization**: Weights `D` are initialized uniformly.\n- **For each classifier**:\n  - We try all features and thresholds.\n  - For each threshold, we compute the error and update the best rule.\n  - We calculate the classifier weight `alpha`.\n  - We update the weights `D` using the exponential loss.\n- **Output**: A list of classifiers with their parameters, rounded to 4 decimal places.\n\nThis script is self-contained and can be run directly."}
{"task_id": 39, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **log-softmax** function for a 1D numpy array of scores, with numerical stability and output rounded to 4 decimal places:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    scores = np.array(scores, dtype=np.float64)\n    # Subtract the max for numerical stability\n    shifted_scores = scores - np.max(scores)\n    # Compute exp of shifted scores\n    exp_scores = np.exp(shifted_scores)\n    # Compute log of sum of exps\n    log_sum_exp = np.log(np.sum(exp_scores))\n    # Compute log-softmax\n    log_softmax_values = shifted_scores - log_sum_exp\n    # Round to 4 decimal places and convert to list\n    return np.round(log_softmax_values, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    example_scores = [2.0, 1.0, 0.1]\n    result = log_softmax(example_scores)\n    print(\"Log-Softmax:\", result)\n```\n\n### Explanation:\n- **Numerical Stability**: Subtracting the max value from the input scores before applying the exponential function prevents overflow when dealing with large numbers.\n- **Log-Sum-Exp Trick**: This is used to compute the log of the sum of exponentials in a numerically stable way.\n- **Rounding**: The final result is rounded to 4 decimal places using `np.round`.\n- **Output Format**: The result is converted to a Python list using `tolist()`.\n\nYou can test this function with any 1D list of numerical scores."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport copy\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        # Initialize weights and biases\n        input_dim = self.input_shape[0]\n        limit = 1 / math.sqrt(input_dim)\n        self.W = np.random.uniform(-limit, limit, size=(input_dim, self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        \n        # Initialize optimizers for weights and biases\n        self.optimizer = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        # Return total number of trainable parameters\n        return self.W.size + self.w0.size\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return output\n\n    def backward_pass(self, accum_grad):\n        # Save the input for backward pass\n        X = self.layer_input\n        \n        # Gradients of weights\n        grad_W = np.dot(X.T, accum_grad)\n        grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n        \n        # Gradient with respect to input\n        grad_input = np.dot(accum_grad, self.W.T)\n        \n        # If the layer is trainable, update weights and biases\n        if self.trainable:\n            self.W = self.optimizer.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        \n        return grad_input\n\n    def output_shape(self):\n        return (self.n_units,)\n```"}
{"task_id": 41, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a simple 2D convolutional layer with the specified parameters:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a 2D convolution on the input matrix using the given kernel, padding, and stride.\n\n    Parameters:\n    - input_matrix (np.ndarray): Input 2D matrix (image).\n    - kernel (np.ndarray): Convolution kernel (filter).\n    - padding (int): Number of zero-padding layers to add around the input.\n    - stride (int): Step size for moving the kernel over the input.\n\n    Returns:\n    - output (list): Resulting 2D matrix after convolution, rounded to 4 decimal places.\n    \"\"\"\n    # Get input and kernel dimensions\n    in_height, in_width = input_matrix.shape\n    k_height, k_width = kernel.shape\n\n    # Apply padding to the input matrix\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')\n\n    # Calculate output dimensions\n    out_height = (in_height + 2 * padding - k_height) // stride + 1\n    out_width = (in_width + 2 * padding - k_width) // stride + 1\n\n    # Initialize output matrix\n    output = np.zeros((out_height, out_width))\n\n    # Perform convolution\n    for i in range(out_height):\n        for j in range(out_width):\n            # Get the region of interest in the padded input\n            start_i = i * stride\n            start_j = j * stride\n            region = padded_input[start_i:start_i + k_height, start_j:start_j + k_width]\n            # Compute the dot product between the region and the kernel\n            output[i, j] = np.sum(region * kernel)\n\n    # Round to 4 decimal places and convert to list\n    output = np.round(output, 4)\n    return output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_matrix = np.array([[1, 2, 3],\n                             [4, 5, 6],\n                             [7, 8, 9]])\n\n    kernel = np.array([[1, 0],\n                       [-1, 1]])\n\n    padding = 0\n    stride = 1\n\n    result = simple_conv2d(input_matrix, kernel, padding, stride)\n    print(\"Convolution Result:\")\n    print(result)\n```\n\n### Explanation:\n- **Padding**: Adds zeros around the input matrix to preserve spatial dimensions or allow the kernel to reach the edges.\n- **Stride**: Determines how much the kernel moves at each step.\n- **Convolution**: For each position in the output, the kernel is applied to the corresponding region in the padded input, and the result is the sum of element-wise multiplication.\n\n### Output:\nThe result is a 2D list with values rounded to 4 decimal places, as required."}
{"task_id": 42, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the ReLU activation function as described:\n\n```python\ndef relu(z: float) -> float:\n    return max(0, z)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [-2.0, -0.5, 0.0, 1.2, 3.7]\n    for val in test_values:\n        print(f\"relu({val}) = {relu(val)}\")\n```\n\n### Explanation:\n- The `relu` function uses Python\u2019s built-in `max` function to return `z` if it is greater than 0, otherwise it returns 0.\n- The example usage demonstrates how the function behaves with a variety of input values.\n\nYou can copy and run this script in any Python environment."}
{"task_id": 43, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Ridge Regression loss function** as described:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Compute the Ridge Regression loss function.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features) representing the feature matrix.\n    - w: 1D numpy array of shape (n_features,) representing the coefficients.\n    - y_true: 1D numpy array of shape (n_samples,) representing the true labels.\n    - alpha: float, the regularization parameter.\n\n    Returns:\n    - float: The Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    # Compute the predicted values: y_pred = X @ w\n    y_pred = X @ w\n\n    # Compute the Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n\n    # Compute the regularization term: alpha * ||w||^2\n    regularization = alpha * np.sum(w ** 2)\n\n    # Total Ridge loss = MSE + regularization\n    loss = mse + regularization\n\n    return round(loss, 4)\n```\n\n### Example Usage:\n```python\n# Example data\nX = np.array([[1, 2], [3, 4], [5, 6]])\nw = np.array([0.5, -0.5])\ny_true = np.array([1, 2, 3])\nalpha = 0.1\n\n# Compute Ridge loss\nloss = ridge_loss(X, w, y_true, alpha)\nprint(\"Ridge Loss:\", loss)\n```\n\nThis function computes the **Ridge loss** as:\n\n$$\n\\text{Loss} = \\text{MSE} + \\alpha \\cdot ||w||^2\n$$\n\nWhere:\n- $ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n- $ ||w||^2 = w_1^2 + w_2^2 + \\dots + w_p^2 $ is the L2 norm of the weight vector."}
{"task_id": 44, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the Leaky ReLU activation function as described:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Applies the Leaky ReLU activation function.\n\n    Parameters:\n    z (float): Input value.\n    alpha (float): Slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: Output after applying Leaky ReLU.\n    \"\"\"\n    return z if z > 0 else alpha * z\n\n# Example usage:\nif __name__ == \"__main__\":\n    print(leaky_relu(5.0))       # Output: 5.0\n    print(leaky_relu(-3.0))      # Output: -0.03\n    print(leaky_relu(0.0))       # Output: 0.0\n    print(leaky_relu(-10.0, 0.2))# Output: -2.0\n``` \n\n### Explanation:\n- If the input `z` is positive, it is returned as-is.\n- If `z` is zero or negative, it is multiplied by `alpha` (default is 0.01).\n- The function is flexible and allows the user to specify a custom `alpha` value."}
{"task_id": 45, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that defines the `kernel_function` to compute the linear kernel (dot product) between two input vectors using NumPy:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel (dot product) between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n    result = kernel_function(x1, x2)\n    print(\"Linear kernel result:\", result)\n```"}
{"task_id": 46, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `precision` function as described:\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n    \n    Returns:\n    float: Precision score.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Calculate true positives and false positives\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Avoid division by zero\n    if (true_positives + false_positives) == 0:\n        return 0.0\n    \n    # Calculate and return precision\n    return true_positives / (true_positives + false_positives)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n    y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n    \n    print(\"Precision:\", precision(y_true, y_pred))\n```\n\n### Explanation:\n- **True Positives (TP)**: Cases where the true label is 1 and the predicted label is also 1.\n- **False Positives (FP)**: Cases where the true label is 0 but the predicted label is 1.\n- **Precision** is calculated as:  \n  $$\n  \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n  $$\n- The function handles the case where both TP and FP are zero to avoid division by zero."}
{"task_id": 47, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the three variants of gradient descent (Batch, Stochastic, and Mini-Batch) using the Mean Squared Error (MSE) loss function:\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using the specified method.\n    \n    Parameters:\n    - X: numpy array of shape (n_samples, n_features)\n    - y: numpy array of shape (n_samples,)\n    - weights: numpy array of shape (n_features,)\n    - learning_rate: float\n    - n_iterations: int\n    - batch_size: int (default=1)\n    - method: str, one of 'batch', 'stochastic', 'mini-batch' (default='batch')\n    \n    Returns:\n    - weights: numpy array of optimized weights\n    - loss_history: list of MSE loss values at each iteration\n    \"\"\"\n    n_samples, n_features = X.shape\n    loss_history = []\n\n    for _ in range(n_iterations):\n        if method == 'batch':\n            # Use the entire dataset\n            indices = np.arange(n_samples)\n        elif method == 'stochastic':\n            # Use one random sample\n            indices = np.random.choice(n_samples, size=1, replace=False)\n        elif method == 'mini-batch':\n            # Use a random mini-batch\n            indices = np.random.choice(n_samples, size=batch_size, replace=False)\n        else:\n            raise ValueError(\"Method must be 'batch', 'stochastic', or 'mini-batch'\")\n\n        X_batch = X[indices]\n        y_batch = y[indices]\n\n        # Compute predictions and error\n        y_pred = X_batch @ weights\n        error = y_pred - y_batch\n\n        # Compute gradient\n        gradient = (2 / len(indices)) * X_batch.T @ error\n\n        # Update weights\n        weights -= learning_rate * gradient\n\n        # Compute and record MSE loss\n        y_pred_full = X @ weights\n        loss = np.mean((y_pred_full - y) ** 2)\n        loss_history.append(round(loss, 4))\n\n    # Round weights to 4 decimal places and convert to list\n    weights = np.round(weights, 4).tolist()\n    loss_history = [round(loss, 4) for loss in loss_history]\n\n    return weights, loss_history\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n    y = np.array([3, 5, 7, 9])\n    weights = np.array([0.0, 0.0])\n    learning_rate = 0.01\n    n_iterations = 100\n    batch_size = 2\n    method = 'mini-batch'  # Can be 'batch', 'stochastic', or 'mini-batch'\n\n    optimized_weights, loss_history = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method)\n\n    print(\"Optimized Weights:\", optimized_weights)\n    print(\"Loss History:\", loss_history)\n```\n\n### Explanation:\n- **Batch Gradient Descent**: Uses the full dataset to compute the gradient at each iteration.\n- **Stochastic Gradient Descent (SGD)**: Uses one randomly selected sample per iteration.\n- **Mini-Batch Gradient Descent**: Uses a small random subset (mini-batch) of the data per iteration.\n- **MSE Loss**: Computed as the mean of squared errors.\n- **Rounding**: All results are rounded to 4 decimal places as required.\n- **Output**: Returns weights and loss history as Python lists.\n\nYou can change the `method` parameter to test different gradient descent variants."}
{"task_id": 48, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the Reduced Row Echelon Form (RREF) of a matrix using NumPy. The function `rref(matrix)` takes a 2D list or NumPy array as input and returns the RREF of the matrix as a Python list.\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert a matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists or np.ndarray): The input matrix.\n    \n    Returns:\n    list: The RREF of the matrix as a Python list.\n    \"\"\"\n    matrix = np.array(matrix, dtype=float)\n    rows, cols = matrix.shape\n    lead = 0  # Column index for pivot\n\n    for r in range(rows):\n        if lead >= cols:\n            return matrix.tolist()\n\n        # Find the pivot row\n        i = r\n        while matrix[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if lead == cols:\n                    return matrix.tolist()\n\n        # Swap rows\n        matrix[[i, r]] = matrix[[r, i]]\n\n        # Normalize the pivot row\n        matrix[r] = matrix[r] / matrix[r, lead]\n\n        # Eliminate other rows\n        for i in range(rows):\n            if i != r and matrix[i, lead] != 0:\n                matrix[i] = matrix[i] - matrix[i, lead] * matrix[r]\n\n        lead += 1\n\n    return matrix.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    result = rref(matrix)\n    print(\"RREF of the matrix:\")\n    for row in result:\n        print(row)\n```\n\n### Explanation:\n- The function uses **Gaussian elimination** to transform the matrix into RREF.\n- It handles **zero rows**, **zero columns**, and **non-square matrices**.\n- The result is returned as a **Python list** using `tolist()`.\n\n### Sample Output for the example:\n```\nRREF of the matrix:\n[1.0, 0.0, -1.0]\n[0.0, 1.0, 2.0]\n[0.0, 0.0, 0.0]\n```\n\nThis script is fully self-contained and ready to run."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):\n    \"\"\"\n    Implements the Adam optimization algorithm to minimize a given function.\n    \n    Parameters:\n    - f: Objective function to be optimized\n    - grad: Function that computes the gradient of f\n    - x0: Initial parameter values (numpy array)\n    - learning_rate: Step size (default: 0.001)\n    - beta1: Exponential decay rate for the first moment estimates (default: 0.9)\n    - beta2: Exponential decay rate for the second moment estimates (default: 0.999)\n    - epsilon: Small constant for numerical stability (default: 1e-8)\n    - num_iterations: Number of iterations to run the optimizer (default: 10)\n    \n    Returns:\n    - Optimized parameters as a list (rounded to 4 decimal places)\n    \"\"\"\n    # Initialize parameters\n    x = np.array(x0, dtype=np.float64)\n    m = np.zeros_like(x)  # First moment vector\n    v = np.zeros_like(x)  # Second moment vector\n    t = 0  # Time step\n    \n    for _ in range(num_iterations):\n        t += 1\n        grad_x = grad(x)  # Compute gradient\n        \n        # Update biased first and second moment estimates\n        m = beta1 * m + (1 - beta1) * grad_x\n        v = beta2 * v + (1 - beta2) * (grad_x ** 2)\n        \n        # Compute bias-corrected first and second moment estimates\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage (uncomment to test)\n# def f(x):\n#     return x[0]**2 + x[1]**2\n\n# def grad(x):\n#     return np.array([2*x[0], 2*x[1]])\n\n# x0 = [1.0, 1.0]\n# result = adam_optimizer(f, grad, x0)\n# print(result)\n```"}
{"task_id": 50, "completion_id": 0, "completion": "Here's a self-contained Python script that implements **Lasso Regression using Gradient Descent** with L1 regularization. The function updates the weights and bias iteratively using the gradient of the loss function and the L1 penalty.\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent with L1 regularization.\n\n    Parameters:\n    - X: np.array, shape (n_samples, n_features)\n    - y: np.array, shape (n_samples,)\n    - alpha: float, regularization parameter\n    - learning_rate: float, step size for gradient descent\n    - max_iter: int, maximum number of iterations\n    - tol: float, tolerance for stopping condition\n\n    Returns:\n    - w: np.array, shape (n_features,), optimized weights\n    - b: float, optimized bias\n    \"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = (-1 / n_samples) * np.dot(X.T, (y - y_pred)) + (alpha / n_samples) * np.sign(w)\n        db = (-1 / n_samples) * np.sum(y - y_pred)\n\n        w_new = w - learning_rate * dw\n        b_new = b - learning_rate * db\n\n        # Check for convergence\n        if np.linalg.norm(w_new - w) < tol and abs(b_new - b) < tol:\n            break\n\n        w = w_new\n        b = b_new\n\n    # Round to 4 decimal places and convert to list\n    w = np.round(w, 4)\n    b = round(b, 4)\n\n    return w.tolist(), b\n```\n\n### Example Usage:\n```python\n# Example dataset\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([2, 3, 4, 5])\n\n# Run Lasso Regression\nweights, bias = l1_regularization_gradient_descent(X, y, alpha=0.1, learning_rate=0.01, max_iter=1000)\n\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias)\n```\n\n### Notes:\n- The L1 regularization term is added to the gradient of the weights (`dw`).\n- The `np.sign(w)` function is used to compute the subgradient of the L1 penalty.\n- The function stops early if the change in weights and bias is below a certain tolerance (`tol`).\n- The final weights and bias are rounded to 4 decimal places and returned as a Python list and float, respectively."}
{"task_id": 51, "completion_id": 0, "completion": "Here's a self-contained Python script that computes the **Optimal String Alignment (OSA) distance** using dynamic programming. The implementation includes the four allowed operations: insert, delete, substitute, and transpose.\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    m = len(source)\n    n = len(target)\n    \n    # Create a (m+1) x (n+1) matrix initialized with zeros\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    # Initialize the first row and column\n    for i in range(m + 1):\n        dp[i][0] = i  # deletions\n    for j in range(n + 1):\n        dp[0][j] = j  # insertions\n    \n    # Fill the matrix\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            # If characters are the same, no operation is needed\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                # Minimum of insert, delete, substitute\n                dp[i][j] = 1 + min(\n                    dp[i][j - 1],    # Insert\n                    dp[i - 1][j],    # Delete\n                    dp[i - 1][j - 1] # Substitute\n                )\n                # Check for transposition (only if previous characters match)\n                if i > 1 and j > 1 and source[i - 2] == target[j - 1] and source[i - 1] == target[j - 2]:\n                    dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    \n    return dp[m][n]\n\n# Example usage\nif __name__ == \"__main__\":\n    s1 = \"caper\"\n    s2 = \"acer\"\n    print(f\"OSA distance between '{s1}' and '{s2}' is {OSA(s1, s2)}\")\n```\n\n### Explanation:\n- The function `OSA` uses a dynamic programming table `dp` where `dp[i][j]` represents the minimum number of operations needed to convert the first `i` characters of `source` to the first `j` characters of `target`.\n- The transpose operation is only allowed if the characters are adjacent and swapped, and it's checked by comparing `source[i-2]` with `target[j-1]` and `source[i-1]` with `target[j-2]`.\n\n### Example:\nFor `source = \"caper\"` and `target = \"acer\"`, the OSA distance is `2`:\n1. Delete `'p'`\n2. Transpose `'c'` and `'a'` (i.e., swap them)\n\nThis script is efficient and handles all edge cases."}
{"task_id": 52, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `recall` function for binary classification as described:\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List of true binary labels (0 or 1)\n    - y_pred: List of predicted binary labels (0 or 1)\n    \n    Returns:\n    - Recall value rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # True Positives: actual 1s predicted as 1s\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # False Negatives: actual 1s predicted as 0s\n    false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    denominator = true_positives + false_negatives\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(true_positives / denominator, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n    y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n    \n    print(\"Recall:\", recall(y_true, y_pred))\n```\n\n### Explanation:\n- **True Positives (TP)**: Instances where the true label is 1 and the predicted label is also 1.\n- **False Negatives (FN)**: Instances where the true label is 1 but the predicted label is 0.\n- **Recall Formula**: `Recall = TP / (TP + FN)`\n- If there are no actual positive cases (i.e., TP + FN = 0), the function returns `0.0` to avoid division by zero.\n\nYou can test this function with different `y_true` and `y_pred` lists to evaluate recall for various binary classification scenarios."}
{"task_id": 53, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the **self-attention mechanism** using NumPy. The function takes an input matrix `X` and three weight matrices `W_q`, `W_k`, and `W_v` for computing queries, keys, and values, respectively. The output is the self-attention result as a Python list (after reshaping and rounding to 4 decimal places).\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention mechanism.\n    \n    Parameters:\n    - X: Input matrix of shape (seq_len, d_model)\n    - W_q: Query weight matrix of shape (d_model, d_k)\n    - W_k: Key weight matrix of shape (d_model, d_k)\n    - W_v: Value weight matrix of shape (d_model, d_v)\n    \n    Returns:\n    - Attention output as a Python list (after rounding and reshaping)\n    \"\"\"\n    # Compute queries, keys, and values\n    Q = X @ W_q  # (seq_len, d_k)\n    K = X @ W_k  # (seq_len, d_k)\n    V = X @ W_v  # (seq_len, d_v)\n    \n    # Compute attention scores\n    scores = Q @ K.T  # (seq_len, seq_len)\n    \n    # Apply softmax to get attention weights\n    exp_scores = np.exp(scores)\n    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)  # (seq_len, seq_len)\n    \n    # Compute the final output\n    output = attention_weights @ V  # (seq_len, d_v)\n    \n    # Round to 4 decimal places and convert to Python list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input and weights\n    X = np.array([[1, 0, 1],\n                  [0, 2, 1],\n                  [1, 1, 0]], dtype=np.float32)  # (3, 3)\n    \n    W_q = np.array([[0.1, 0.2],\n                    [0.3, 0.4],\n                    [0.5, 0.6]], dtype=np.float32)  # (3, 2)\n    \n    W_k = np.array([[0.7, 0.8],\n                    [0.9, 1.0],\n                    [1.1, 1.2]], dtype=np.float32)  # (3, 2)\n    \n    W_v = np.array([[1.3, 1.4],\n                    [1.5, 1.6],\n                    [1.7, 1.8]], dtype=np.float32)  # (3, 2)\n    \n    # Compute self-attention\n    attention_output = self_attention(X, W_q, W_k, W_v)\n    \n    # Print the result\n    print(\"Self-Attention Output:\")\n    print(attention_output)\n```\n\n### Explanation:\n- **Queries (Q)**, **Keys (K)**, and **Values (V)** are derived from the input `X` using the respective weight matrices.\n- The **attention scores** are computed as the dot product of `Q` and `K.T`.\n- **Softmax** is applied to normalize the scores into attention weights.\n- The final output is the weighted sum of the values using the attention weights.\n\nThis script is fully self-contained and will run as-is with NumPy installed."}
{"task_id": 54, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a simple RNN cell as described in the task:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN forward pass over a sequence of input vectors.\n    \n    Parameters:\n    - input_sequence: List of input vectors (each is a list of floats)\n    - initial_hidden_state: Initial hidden state (list of floats)\n    - Wx: Weight matrix for input-to-hidden connections\n    - Wh: Weight matrix for hidden-to-hidden connections\n    - b: Bias vector\n    \n    Returns:\n    - final_hidden_state: Final hidden state after processing the entire sequence (rounded to 4 decimals)\n    \"\"\"\n    # Convert all inputs to numpy arrays for matrix operations\n    h_prev = np.array(initial_hidden_state)\n    \n    for x in input_sequence:\n        x_t = np.array(x)\n        h_prev = np.tanh(np.dot(Wx, x_t) + np.dot(Wh, h_prev) + np.array(b))\n    \n    # Round the final hidden state to 4 decimal places and convert to list\n    return np.round(h_prev, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example inputs\n    input_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n    initial_hidden_state = [0.0, 0.0]\n    Wx = [[0.1, 0.2], [0.3, 0.4]]\n    Wh = [[0.5, 0.6], [0.7, 0.8]]\n    b = [0.1, 0.2]\n\n    # Run the RNN forward pass\n    final_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\n    print(\"Final Hidden State:\", final_state)\n```\n\n### Explanation:\n- The function `rnn_forward` processes each input vector in the sequence using the RNN update rule:\n  $$\n  h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n  $$\n- It uses NumPy for efficient matrix operations.\n- The final hidden state is rounded to four decimal places and returned as a Python list.\n\nYou can modify the example inputs in the `if __name__ == \"__main__\":` block to test with different sequences and weights."}
{"task_id": 55, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `translate_object` function using NumPy to apply a 2D translation matrix to a set of points:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of [x, y] points.\n    \n    Parameters:\n        points (list of lists): List of [x, y] coordinates.\n        tx (float): Translation in x-direction.\n        ty (float): Translation in y-direction.\n        \n    Returns:\n        list of lists: Translated points as a Python list.\n    \"\"\"\n    # Convert the list of points to a NumPy array\n    points_array = np.array(points, dtype=np.float64)\n    \n    # Create a translation vector\n    translation = np.array([tx, ty], dtype=np.float64)\n    \n    # Apply the translation\n    translated_array = points_array + translation\n    \n    # Convert back to a Python list\n    return translated_array.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    points = [[1, 2], [3, 4], [5, 6]]\n    tx = 2.5\n    ty = -1.0\n    translated_points = translate_object(points, tx, ty)\n    print(\"Translated Points:\", translated_points)\n```\n\n### Explanation:\n- The function takes a list of `[x, y]` points.\n- It converts the list to a NumPy array for vectorized operations.\n- It adds the translation vector `[tx, ty]` to each point.\n- The result is converted back to a Python list using `tolist()` as required.\n\nYou can run this script directly to see the output of the translation."}
{"task_id": 56, "completion_id": 0, "completion": "Here's a self-contained Python script that computes the Kullback-Leibler (KL) divergence between two univariate normal distributions:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the KL divergence between two univariate normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of distribution P\n    sigma_p (float): Standard deviation of distribution P\n    mu_q (float): Mean of distribution Q\n    sigma_q (float): Standard deviation of distribution Q\n    \n    Returns:\n    float: KL divergence D_KL(P || Q)\n    \"\"\"\n    # KL divergence formula for two univariate normal distributions\n    # D_KL(P || Q) = ln(\u03c3_q / \u03c3_p) + (\u03c3_p^2 + (\u03bc_p - \u03bc_q)^2) / (2 * \u03c3_q^2) - 0.5\n    return (\n        np.log(sigma_q / sigma_p)\n        + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n        - 0.5\n    )\n\n# Example usage\nif __name__ == \"__main__\":\n    mu_p = 0.0\n    sigma_p = 1.0\n    mu_q = 1.0\n    sigma_q = 2.0\n\n    kl = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n    print(f\"KL Divergence D_KL(P || Q) = {kl:.4f}\")\n```\n\n### Explanation:\n- The KL divergence between two univariate normal distributions $ P \\sim N(\\mu_p, \\sigma_p^2) $ and $ Q \\sim N(\\mu_q, \\sigma_q^2) $ is given by:\n  $$\n  D_{KL}(P \\parallel Q) = \\ln\\left(\\frac{\\sigma_q}{\\sigma_p}\\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}\n  $$\n- The function uses this formula to compute the divergence.\n- The example computes the KL divergence between $ P \\sim N(0, 1) $ and $ Q \\sim N(1, 4) $.\n\nYou can modify the `mu_p`, `sigma_p`, `mu_q`, and `sigma_q` values to test with different distributions."}
{"task_id": 57, "completion_id": 0, "completion": "Here's a self-contained Python script implementing the **Gauss-Seidel method** for solving a system of linear equations `Ax = b`:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves the linear system Ax = b using the Gauss-Seidel iterative method.\n    \n    Parameters:\n    - A: Square matrix of coefficients (numpy array)\n    - b: Right-hand side vector (numpy array)\n    - n: Number of iterations\n    - x_ini: Optional initial guess for x (default is zero vector)\n    \n    Returns:\n    - x: Approximated solution vector after n iterations (rounded to 4 decimals)\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    m = len(b)\n    \n    if x_ini is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    for _ in range(n):\n        for i in range(m):\n            s1 = np.dot(A[i, :i], x[:i])  # Sum of x[0] to x[i-1] multiplied by A[i][0] to A[i][i-1]\n            s2 = np.dot(A[i, i+1:], x[i+1:])  # Sum of x[i+1] to x[m-1] multiplied by A[i][i+1] to A[i][m-1]\n            x[i] = (b[i] - s1 - s2) / A[i, i]  # Update x[i] using the latest values\n    \n    return np.round(x, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[4, 1, 2],\n         [3, 5, 1],\n         [1, 1, 3]]\n    b = [4, 7, 3]\n    n = 100  # Number of iterations\n    x = gauss_seidel(A, b, n)\n    print(\"Solution x:\", x)\n```\n\n### Explanation:\n- The function uses the **Gauss-Seidel update rule**, which computes each component of the solution vector using the most recent values from the current iteration.\n- The matrix `A` and vector `b` are converted to NumPy arrays for easier manipulation.\n- If no initial guess is provided, it defaults to a zero vector.\n- After `n` iterations, the solution vector is rounded to **4 decimal places** and returned as a Python list using `tolist()`.\n\n### Example Output:\nFor the given example matrix and vector, the function will print the approximated solution vector `x` after 100 iterations."}
{"task_id": 58, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Gaussian Elimination with Partial Pivoting** to solve a system of linear equations $Ax = b$:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (list of lists or np.ndarray): Coefficient matrix.\n    b (list or np.ndarray): Right-hand side vector.\n    \n    Returns:\n    list: Solution vector x, rounded to 4 decimal places.\n    \"\"\"\n    n = len(b)\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n\n    for i in range(n):\n        # Partial pivoting: find the pivot row\n        max_row = i + np.argmax(np.abs(A[i:, i]))\n        if A[max_row, i] == 0:\n            raise ValueError(\"Matrix is singular or nearly singular.\")\n\n        # Swap with the max row\n        A[[i, max_row]] = A[[max_row, i]]\n        b[[i, max_row]] = b[[max_row, i]]\n\n        # Eliminate all rows below the pivot\n        for j in range(i + 1, n):\n            factor = A[j, i] / A[i, i]\n            A[j, i:] = A[j, i:] - factor * A[i, i:]\n            b[j] = b[j] - factor * b[i]\n\n    # Back substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (b[i] - np.dot(A[i, i + 1:], x[i + 1:])) / A[i, i]\n\n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = [[2, 1, -1],\n         [-3, -1, 2],\n         [-2, 1, 2]]\n    b = [8, -11, -3]\n    solution = gaussian_elimination(A, b)\n    print(\"Solution vector x:\", solution)\n```\n\n### Explanation:\n- **Partial Pivoting**: Ensures numerical stability by swapping rows to use the largest pivot in the current column.\n- **Forward Elimination**: Transforms the matrix into an upper triangular form.\n- **Back Substitution**: Solves for the unknowns starting from the last variable.\n- **Rounding**: The final solution is rounded to 4 decimal places as required.\n\n### Output:\nFor the example provided, the output will be:\n```\nSolution vector x: [2.0, 3.0, -1.0]\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        # Initialize hidden states and cell states\n        h = [initial_hidden_state]\n        c = [initial_cell_state]\n        \n        # Process each time step\n        for t in range(x.shape[1]):\n            # Get current input\n            xt = x[:, t].reshape(-1, 1)\n            \n            # Concatenate input and previous hidden state\n            concat = np.concatenate((xt, h[-1]), axis=0)\n            \n            # Calculate gates\n            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)\n            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)\n            cct = np.tanh(np.dot(self.Wc, concat) + self.bc)\n            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)\n            \n            # Update cell state\n            ct = ft * c[-1] + it * cct\n            # Update hidden state\n            ht = ot * np.tanh(ct)\n            \n            # Append to sequences\n            c.append(ct)\n            h.append(ht)\n        \n        # Round results to 4 decimal places\n        h = [np.round(ht, 4).tolist() for ht in h]\n        final_hidden_state = np.round(h[-1], 4).tolist()\n        final_cell_state = np.round(c[-1], 4).tolist()\n        \n        return h, final_hidden_state, final_cell_state\n    \n    def sigmoid(self, x):\n        \"\"\"Sigmoid activation function\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create an LSTM with input size 3 and hidden size 5\n    lstm = LSTM(input_size=3, hidden_size=5)\n    \n    # Create a random input sequence (3 features, 4 time steps)\n    x = np.random.randn(3, 4)\n    \n    # Create random initial hidden and cell states\n    initial_hidden_state = np.random.randn(5, 1)\n    initial_cell_state = np.random.randn(5, 1)\n    \n    # Run forward pass\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n    \n    print(\"Hidden states at each time step:\")\n    for i, hs in enumerate(hidden_states):\n        print(f\"Time step {i}: {hs}\")\n    print(\"\\nFinal hidden state:\", final_hidden_state)\n    print(\"Final cell state:\", final_cell_state)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `compute_tf_idf` function as described in your task. It includes handling for edge cases, smoothing for IDF, and proper rounding of results.\n\n```python\nimport numpy as np\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus is empty. Cannot compute TF-IDF.\")\n    \n    # Flatten the corpus to get the vocabulary\n    vocabulary = set(word for doc in corpus for word in doc)\n    query_words = set(query)\n    \n    # Filter only the query words that exist in the vocabulary\n    valid_query_words = [word for word in query if word in vocabulary]\n    if not valid_query_words:\n        return [[0.0 for _ in query]] * len(corpus)\n    \n    # Initialize TF and IDF dictionaries\n    tf = np.zeros((len(corpus), len(valid_query_words)))\n    idf = np.zeros(len(valid_query_words))\n    \n    # Compute document frequency (df)\n    df = np.zeros(len(valid_query_words))\n    for i, word in enumerate(valid_query_words):\n        count = 0\n        for doc in corpus:\n            if word in doc:\n                count += 1\n        df[i] = count\n    \n    # Compute IDF with smoothing\n    num_docs = len(corpus)\n    for i in range(len(valid_query_words)):\n        idf[i] = np.log((num_docs + 1) / (df[i] + 1) + 1e-8)  # Smoothing and small epsilon to avoid log(0)\n    \n    # Compute TF (term frequency)\n    for doc_idx, doc in enumerate(corpus):\n        for word_idx, word in enumerate(valid_query_words):\n            tf_count = doc.count(word)\n            tf_total = len(doc) if len(doc) > 0 else 1  # Avoid division by zero\n            tf[doc_idx, word_idx] = tf_count / tf_total\n    \n    # Compute TF-IDF\n    tf_idf = tf * idf\n    \n    # Round to 4 decimal places and convert to list\n    tf_idf_rounded = np.round(tf_idf, 4)\n    return tf_idf_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    corpus = [\n        [\"this\", \"is\", \"a\", \"sample\", \"document\"],\n        [\"another\", \"example\", \"document\", \"with\", \"more\", \"words\"],\n        [\"yet\", \"another\", \"document\", \"here\"]\n    ]\n    query = [\"document\", \"example\", \"test\"]\n\n    result = compute_tf_idf(corpus, query)\n    print(\"TF-IDF Scores:\")\n    for i, doc_scores in enumerate(result):\n        print(f\"Document {i+1}: {doc_scores}\")\n```\n\n### Explanation:\n- **TF (Term Frequency)**: For each query word in each document, it's the count of the word divided by the total number of words in the document.\n- **IDF (Inverse Document Frequency)**: It's computed using smoothing to avoid division by zero: `log((num_docs + 1) / (df + 1))`.\n- **TF-IDF**: The product of TF and IDF.\n- **Rounding**: All results are rounded to 4 decimal places.\n- **Edge Cases**:\n  - Empty corpus raises a `ValueError`.\n  - Query words not in the corpus are ignored.\n  - Documents with zero words are handled safely.\n\nThis implementation is robust and suitable for real-world use."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives, False Positives, False Negatives\n    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n\n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # Calculate F-Score\n    f_score = ( (1 + beta**2) * precision * recall ) / ( beta**2 * precision + recall ) if (beta**2 * precision + recall) > 0 else 0\n\n    return round(f_score, 3)\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            \n        Returns:\n            outputs: Outputs at each time step of shape (output_size, sequence_length)\n            hiddens: Hidden states at each time step of shape (hidden_size, sequence_length)\n            inputs: Input sequence of shape (input_size, sequence_length)\n        \"\"\"\n        sequence_length = x.shape[1]\n        h = np.zeros((self.hidden_size, 1))  # Initial hidden state\n        \n        # Initialize arrays to store outputs and hidden states\n        outputs = np.zeros((self.W_hy.shape[0], sequence_length))\n        hiddens = np.zeros((self.hidden_size, sequence_length))\n        \n        for t in range(sequence_length):\n            h = np.tanh(self.W_xh @ x[:, t].reshape(-1, 1) + self.W_hh @ h + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            outputs[:, t] = y.flatten()\n            hiddens[:, t] = h.flatten()\n        \n        return outputs, hiddens, x\n\n    def backward(self, x, y, outputs, hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT.\n        \n        Args:\n            x: Input sequence of shape (input_size, sequence_length)\n            y: Expected output sequence of shape (output_size, sequence_length)\n            outputs: Outputs from the forward pass of shape (output_size, sequence_length)\n            hiddens: Hidden states from the forward pass of shape (hidden_size, sequence_length)\n            learning_rate: Learning rate for gradient descent\n            \n        Returns:\n            loss: Total loss across all time steps\n        \"\"\"\n        sequence_length = x.shape[1]\n        output_size = y.shape[0]\n        \n        # Initialize gradients\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        \n        # Initialize previous gradients\n        dh_next = np.zeros_like(hiddens[:, 0])\n        \n        loss = 0\n        \n        # Compute gradients using BPTT\n        for t in reversed(range(sequence_length)):\n            dy = outputs[:, t] - y[:, t]\n            loss += 0.5 * np.sum(dy**2)  # 1/2 * MSE\n            \n            dW_hy += dy.reshape(-1, 1) @ hiddens[:, t].reshape(1, -1)\n            db_y += dy.reshape(-1, 1)\n            \n            dh = self.W_hy.T @ dy.reshape(-1, 1) + dh_next\n            dh_raw = (1 - hiddens[:, t].reshape(-1, 1)**2) * dh  # tanh derivative\n            \n            db_h += dh_raw\n            dW_xh += dh_raw @ x[:, t].reshape(1, -1)\n            dW_hh += dh_raw @ hiddens[:, t].reshape(1, -1)\n            \n            dh_next = self.W_hh.T @ dh_raw\n        \n        # Update parameters\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        \n        return loss\n\n    def train(self, input_sequence, expected_output, learning_rate=0.01):\n        \"\"\"\n        Train the RNN on a sequence.\n        \n        Args:\n            input_sequence: Input sequence of shape (input_size, sequence_length)\n            expected_output: Expected output sequence of shape (output_size, sequence_length)\n            learning_rate: Learning rate for gradient descent\n            \n        Returns:\n            loss: Total loss for this training step\n        \"\"\"\n        outputs, hiddens, inputs = self.forward(input_sequence)\n        loss = self.backward(input_sequence, expected_output, outputs, hiddens, learning_rate)\n        return loss\n```"}
{"task_id": 63, "completion_id": 0, "completion": "Here's a self-contained Python script implementing the **Conjugate Gradient (CG) method** for solving symmetric, positive-definite linear systems:\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array = None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to list)\n    \"\"\"\n    # Ensure A is a numpy array\n    A = np.array(A)\n    b = np.array(b)\n\n    # Set initial guess\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = np.array(x0)\n\n    r = b - A @ x  # Residual\n    p = r.copy()   # Search direction\n    rs_old = r @ r  # Dot product of residual with itself\n\n    for i in range(n):\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rs_new = r @ r\n\n        # Check for convergence\n        if np.sqrt(rs_new) < tol:\n            break\n\n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    return np.round(x, 8).tolist()\n```\n\n### Example Usage:\n```python\n# Example matrix A and vector b\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\n\n# Solve using Conjugate Gradient\nx = conjugate_gradient(A, b, n=100)\nprint(\"Solution x:\", x)\n```\n\n### Notes:\n- The method assumes that `A` is **symmetric and positive-definite**.\n- The algorithm stops early if the residual norm is below the tolerance `tol`.\n- The solution is returned as a **list of floats**, rounded to **8 decimal places** as required."}
{"task_id": 64, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that calculates the **Gini Impurity** for a list of class labels:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0  # Gini impurity of an empty set is 0\n\n    total_samples = len(y)\n    class_counts = Counter(y)\n    probabilities = [count / total_samples for count in class_counts.values()]\n\n    gini = 1.0 - sum(p ** 2 for p in probabilities)\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    labels = [0, 0, 1, 1, 1, 2, 2]\n    impurity = gini_impurity(labels)\n    print(f\"Gini Impurity: {impurity}\")\n```\n\n### Explanation:\n- **Gini Impurity** is calculated as:  \n  $$\n  G = 1 - \\sum_{i=1}^{n} p_i^2\n  $$\n  where $ p_i $ is the probability of an object being classified to a particular class.\n- The function uses `collections.Counter` to count occurrences of each class label.\n- The result is rounded to **three decimal places** as required.\n\nYou can modify the `labels` list in the `if __name__ == \"__main__\"` block to test with different class distributions."}
{"task_id": 65, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `compressed_row_sparse_matrix` function as described:\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]  # Start with 0 to indicate the beginning of the first row\n\n    for row in dense_matrix:\n        count = 0  # Count of non-zero elements in the current row\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                count += 1\n        row_pointer.append(row_pointer[-1] + count)  # Append cumulative count\n\n    return values, column_indices, row_pointer\n\n# Example usage:\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [0, 0, 3, 0],\n        [4, 0, 0, 0],\n        [0, 5, 0, 6],\n        [0, 0, 0, 0]\n    ]\n\n    values, column_indices, row_pointer = compressed_row_sparse_matrix(dense_matrix)\n    print(\"Values array:\", values)\n    print(\"Column indices array:\", column_indices)\n    print(\"Row pointer array:\", row_pointer)\n```\n\n### Explanation:\n- **Values array**: Contains all non-zero elements in row-major order.\n- **Column indices array**: Contains the column index for each non-zero value.\n- **Row pointer array**: Contains the cumulative count of non-zero elements up to and including each row. It has `len(dense_matrix) + 1` elements.\n\n### Example Output for the given matrix:\n```\nValues array: [3, 4, 5, 6]\nColumn indices array: [2, 0, 1, 3]\nRow pointer array: [0, 1, 2, 4, 4]\n```\n\nThis script is self-contained and ready to run."}
{"task_id": 66, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the orthogonal projection of a vector `v` onto a line defined by vector `L`:\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    import numpy as np\n\n    # Convert input lists to numpy arrays for vector operations\n    v_array = np.array(v)\n    L_array = np.array(L)\n\n    # Compute the dot product of v and L\n    dot_vL = np.dot(v_array, L_array)\n\n    # Compute the dot product of L with itself\n    dot_LL = np.dot(L_array, L_array)\n\n    # Avoid division by zero if L is the zero vector\n    if dot_LL == 0:\n        raise ValueError(\"The line vector L cannot be the zero vector.\")\n\n    # Compute the scalar projection factor\n    scalar_proj = dot_vL / dot_LL\n\n    # Compute the projection vector\n    proj_vector = scalar_proj * L_array\n\n    # Round each component to three decimal places and convert back to list\n    return [round(x, 3) for x in proj_vector.tolist()]\n\n# Example usage:\nif __name__ == \"__main__\":\n    v = [3, 4]\n    L = [1, 0]\n    result = orthogonal_projection(v, L)\n    print(\"Orthogonal projection of v onto L:\", result)\n```\n\n### Explanation:\n- The function uses the formula for orthogonal projection:\n  $$\n  \\text{proj}_L(v) = \\left( \\frac{v \\cdot L}{L \\cdot L} \\right) L\n  $$\n- It handles the edge case where `L` is the zero vector by raising an error.\n- The result is rounded to three decimal places as required.\n\nYou can test this function with different vectors `v` and `L` to compute their projections."}
{"task_id": 67, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the Compressed Column Sparse (CSC) matrix format as described:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [0])\n\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n\n    values = []\n    row_indices = []\n    col_ptr = [0]  # Start with the first column starting at index 0\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            val = dense_matrix[row][col]\n            if val != 0:\n                values.append(val)\n                row_indices.append(row)\n        col_ptr.append(len(values))  # Mark the end of the current column\n\n    return (values, row_indices, col_ptr)\n\n# Example usage:\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [0, 3, 0],\n        [4, 0, 0],\n        [0, 5, 6]\n    ]\n\n    values, row_indices, col_ptr = compressed_col_sparse_matrix(dense_matrix)\n    print(\"Values:\", values)\n    print(\"Row Indices:\", row_indices)\n    print(\"Column Pointer:\", col_ptr)\n```\n\n### Explanation:\n- **`values`**: Contains all non-zero elements in column-major order.\n- **`row_indices`**: Contains the corresponding row indices for each non-zero value.\n- **`col_ptr`**: A list where each entry `col_ptr[i]` gives the index in `values` where column `i` starts. The last entry is the total number of non-zero elements.\n\n### Example Output for the given matrix:\n```\nValues: [3, 4, 5, 6]\nRow Indices: [0, 1, 2, 2]\nColumn Pointer: [0, 2, 3, 4]\n```\n\nThis output corresponds to the non-zero elements in column-major order and their respective row indices and column pointers."}
{"task_id": 68, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `matrix_image(A)` function to find the column space (image) of a matrix using the **Row Echelon Form** approach. The function identifies the **independent columns** and returns them as a list of vectors, rounded to 8 decimal places.\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Compute the column space (image) of a matrix A.\n    \n    Parameters:\n    A (np.ndarray): A 2D NumPy array representing the matrix.\n    \n    Returns:\n    list: A list of column vectors (as lists) that form a basis for the column space of A.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    rref, pivots = np.linalg.qr(A, mode='reduced')  # QR decomposition to find independent columns\n    independent_columns = A[:, pivots != 0]  # Select columns corresponding to non-zero pivots\n    return np.round(independent_columns, 8).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    A = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    print(\"Matrix A:\")\n    print(A)\n    print(\"\\nBasis for the image (column space) of A:\")\n    print(matrix_image(A))\n```\n\n### Explanation:\n- **QR Decomposition** is used here as a stable and efficient method to determine the rank and independent columns of the matrix.\n- The `pivots` from the decomposition help identify the independent columns.\n- The function returns the **independent columns** of the original matrix `A` as a list of vectors, rounded to 8 decimal places.\n\n### Output Example:\nFor the matrix:\n```\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n```\nThe output might be:\n```\n[[1.0, 2.0], [4.0, 5.0]]\n```\ndepending on the rank and independence of the columns."}
{"task_id": 69, "completion_id": 0, "completion": "Here's a self-contained Python script that defines the `r_squared` function to calculate the R-squared value given true and predicted values:\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Calculate the mean of the true values\n    mean_y_true = np.mean(y_true)\n    \n    # Total sum of squares (SST)\n    sst = np.sum((y_true - mean_y_true) ** 2)\n    \n    # Residual sum of squares (SSR)\n    ssr = np.sum((y_pred - y_true) ** 2)\n    \n    # R-squared formula\n    r2 = 1 - (ssr / sst)\n    \n    # Return the result rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    print(\"R-squared:\", r_squared(y_true, y_pred))\n```\n\n### Explanation:\n- **SST (Total Sum of Squares)**: Measures the total variance in the true values.\n- **SSR (Residual Sum of Squares)**: Measures the unexplained variance after the model makes predictions.\n- **R-squared**: $ R^2 = 1 - \\frac{SSR}{SST} $, which tells us the proportion of variance in the dependent variable that is predictable from the independent variable(s).\n\nThis script will output the R-squared value rounded to three decimal places."}
{"task_id": 70, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `calculate_brightness(img)` function as described:\n\n```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img or not img[0]:\n        return -1\n\n    # Determine the number of rows and columns\n    rows = len(img)\n    cols = len(img[0])\n\n    # Check for inconsistent row lengths\n    for row in img:\n        if len(row) != cols:\n            return -1\n\n    total_brightness = 0\n    total_pixels = 0\n\n    for row in img:\n        for pixel in row:\n            # Check if pixel value is valid\n            if not (0 <= pixel <= 255):\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n\n    # Calculate average brightness\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example 1: Valid image\n    img1 = [\n        [100, 150, 200],\n        [50, 75, 125]\n    ]\n    print(calculate_brightness(img1))  # Output: 116.67\n\n    # Example 2: Inconsistent row lengths\n    img2 = [\n        [100, 150],\n        [50, 75, 125]\n    ]\n    print(calculate_brightness(img2))  # Output: -1\n\n    # Example 3: Empty image\n    img3 = []\n    print(calculate_brightness(img3))  # Output: -1\n\n    # Example 4: Invalid pixel value\n    img4 = [\n        [100, 300],\n        [50, 75]\n    ]\n    print(calculate_brightness(img4))  # Output: -1\n```\n\n### Explanation:\n- The function first checks if the image is empty.\n- It then ensures all rows have the same length.\n- It sums up all pixel values and checks if they are within the valid range (0\u2013255).\n- If any condition fails, it returns `-1`.\n- Otherwise, it calculates the average brightness and rounds it to two decimal places."}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between y_true and y_pred.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays are not of the same length or are empty.\n    TypeError: If inputs are not array-like or cannot be converted to NumPy arrays.\n    \"\"\"\n    try:\n        # Convert inputs to NumPy arrays\n        y_true = np.asarray(y_true)\n        y_pred = np.asarray(y_pred)\n    except Exception as e:\n        raise TypeError(\"Input values must be array-like and convertible to NumPy arrays.\") from e\n\n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n\n    # Check if arrays have the same length\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n\n    # Calculate RMSE\n    squared_errors = (y_true - y_pred) ** 2\n    mean_squared_error = np.mean(squared_errors)\n    root_mean_squared_error = np.sqrt(mean_squared_error)\n\n    # Return RMSE rounded to three decimal places\n    return round(root_mean_squared_error, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [3, -0.5, 2, 7]\n    y_pred = [2.5, 0.0, 2, 8]\n    print(\"RMSE:\", rmse(y_true, y_pred))\n```"}
{"task_id": 72, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `jaccard_index` function as described:\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    # Compute intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle the edge case where both arrays are all zeros or no overlap\n    if union == 0:\n        return 0.0\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    return round(jaccard, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0]\n    y_pred = [1, 0, 0, 1, 0]\n    print(\"Jaccard Index:\", jaccard_index(y_true, y_pred))\n```\n\n### Explanation:\n- The function uses `np.logical_and` and `np.logical_or` to compute the intersection and union of the binary arrays.\n- It handles edge cases where the union is zero (e.g., both arrays are all zeros) by returning a Jaccard Index of 0.0.\n- The result is rounded to three decimal places as required.\n\nYou can test it with different binary arrays to verify its correctness."}
{"task_id": 73, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `dice_score` function as described:\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) between two binary arrays.\n    \n    Parameters:\n    y_true (array-like): Ground truth (correct) labels.\n    y_pred (array-like): Predicted labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    \n    # Calculate the sum of true and predicted positives\n    sum_true_pred = np.sum(y_true) + np.sum(y_pred)\n    \n    # Handle the case where both are zero to avoid division by zero\n    if sum_true_pred == 0:\n        return 1.0  # Perfect overlap if both are empty\n    \n    # Calculate the Dice Score\n    dice = (2.0 * intersection) / sum_true_pred\n    \n    # Return the result rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = [1, 0, 1, 1, 0]\n    y_pred = [1, 0, 0, 1, 0]\n    score = dice_score(y_true, y_pred)\n    print(f\"Dice Score: {score}\")\n```\n\n### Explanation:\n- The function calculates the intersection of the two binary arrays using `np.logical_and`.\n- It computes the sum of true and predicted positives.\n- It handles the edge case where both arrays are empty (i.e., no 1s), returning a score of 1.0.\n- The result is rounded to 3 decimal places as required.\n\nYou can test this function with different binary arrays to evaluate the Dice Score."}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary of seeds for reproducibility.\n    \n    Returns:\n    - list: Composite hypervector representing the entire row.\n    \"\"\"\n    # Initialize a zero vector for bundling\n    composite_hv = np.zeros(dim)\n    \n    for feature_name, feature_value in row.items():\n        # Get the seed for the feature from the random_seeds dictionary\n        seed = random_seeds.get(feature_name, 0)\n        \n        # Set the seed for reproducibility\n        np.random.seed(seed)\n        \n        # Generate a random hypervector for the feature value\n        value_hv = np.random.choice([-1, 1], size=dim)\n        \n        # Generate a random hypervector for the feature name (using a fixed seed for consistency)\n        np.random.seed(hash(feature_name) % 2**32)\n        name_hv = np.random.choice([-1, 1], size=dim)\n        \n        # Bind the feature name and value hypervectors (element-wise multiplication)\n        bound_hv = name_hv * value_hv\n        \n        # Bundle the bound hypervector into the composite hypervector (element-wise addition)\n        composite_hv += bound_hv\n    \n    # Normalize the composite hypervector to the range [-1, 1]\n    composite_hv = np.sign(composite_hv)\n    \n    # Convert to list and return\n    return composite_hv.tolist()\n```"}
{"task_id": 75, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `confusion_matrix(data)` function as described:\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for binary classification.\n    \n    Parameters:\n        data (list of lists): Each inner list is [y_true, y_pred]\n        \n    Returns:\n        list of lists: A 2x2 confusion matrix in the format:\n                       [[TN, FP],\n                        [FN, TP]]\n    \"\"\"\n    # Initialize the confusion matrix with zeros\n    matrix = [[0, 0],  # [TN, FP]\n              [0, 0]]  # [FN, TP]\n    \n    for y_true, y_pred in data:\n        if y_true == 0 and y_pred == 0:\n            matrix[0][0] += 1  # True Negative\n        elif y_true == 0 and y_pred == 1:\n            matrix[0][1] += 1  # False Positive\n        elif y_true == 1 and y_pred == 0:\n            matrix[1][0] += 1  # False Negative\n        elif y_true == 1 and y_pred == 1:\n            matrix[1][1] += 1  # True Positive\n    \n    return matrix\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data: [y_true, y_pred]\n    data = [\n        [0, 0],\n        [0, 1],\n        [1, 1],\n        [1, 0],\n        [0, 0],\n        [1, 1],\n        [1, 1],\n        [0, 1]\n    ]\n    \n    result = confusion_matrix(data)\n    print(\"Confusion Matrix:\")\n    for row in result:\n        print(row)\n```\n\n### Explanation:\n- **TN (True Negative)**: Actual 0, Predicted 0\n- **FP (False Positive)**: Actual 0, Predicted 1\n- **FN (False Negative)**: Actual 1, Predicted 0\n- **TP (True Positive)**: Actual 1, Predicted 1\n\nThis script will output the 2x2 confusion matrix based on the provided data."}
{"task_id": 76, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `cosine_similarity` function as described:\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Ensure the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Ensure the vectors are not empty\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError(\"Input vectors cannot be empty.\")\n    \n    # Compute the dot product\n    dot_product = np.dot(v1, v2)\n    \n    # Compute the magnitudes (Euclidean norms) of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Avoid division by zero\n    if magnitude_v1 == 0 or magnitude_v2 == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    # Compute cosine similarity\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the result rounded to three decimal places\n    return round(similarity, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    v1 = np.array([1, 2, 3])\n    v2 = np.array([4, 5, 6])\n    print(\"Cosine similarity:\", cosine_similarity(v1, v2))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    # Ensure inputs are valid\n    if len(actual) != len(predicted):\n        raise ValueError(\"The lengths of 'actual' and 'predicted' must be the same.\")\n    \n    for label in actual + predicted:\n        if label not in {0, 1}:\n            raise ValueError(\"All elements in 'actual' and 'predicted' must be either 0 or 1.\")\n    \n    # Count TP, TN, FP, FN\n    tp = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 1)\n    tn = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 0)\n    fp = sum(1 for a, p in zip(actual, predicted) if a == 0 and p == 1)\n    fn = sum(1 for a, p in zip(actual, predicted) if a == 1 and p == 0)\n    \n    # Confusion Matrix: [[TN, FP], [FN, TP]]\n    confusion_matrix = [[tn, fp], [fn, tp]]\n    \n    # Accuracy\n    total = len(actual)\n    accuracy = (tp + tn) / total if total > 0 else 0.0\n    \n    # Precision\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    \n    # Recall (Sensitivity)\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    # F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    # Specificity\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    \n    # Negative Predictive Value\n    negative_predictive_value = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n    \n    # Round the metrics to 3 decimal places\n    accuracy = round(accuracy, 3)\n    f1_score = round(f1_score, 3)\n    specificity = round(specificity, 3)\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n\n# Example usage:\n# actual = [1, 0, 1, 1, 0, 0, 1]\n# predicted = [1, 0, 1, 0, 0, 1, 1]\n# print(performance_metrics(actual, predicted))\n```"}
{"task_id": 78, "completion_id": 0, "completion": "Here's a self-contained Python script that fulfills the task of computing descriptive statistics for a given dataset:\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n\n    Parameters:\n    data (list or np.ndarray): A list or NumPy array of numerical values.\n\n    Returns:\n    dict: A dictionary containing the following statistics:\n        - mean\n        - median\n        - mode\n        - variance\n        - standard_deviation\n        - 25th_percentile\n        - 50th_percentile\n        - 75th_percentile\n        - interquartile_range\n    \"\"\"\n    data = np.array(data)\n    \n    # Calculate statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data, keepdims=True).mode[0]\n    variance = np.var(data, ddof=1)  # Sample variance\n    std_dev = np.std(data, ddof=1)   # Sample standard deviation\n    q25 = np.percentile(data, 25)\n    q50 = np.percentile(data, 50)\n    q75 = np.percentile(data, 75)\n    iqr = q75 - q25\n\n    # Round the statistics to 4 decimal places\n    stats_dict = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(std_dev, 4),\n        '25th_percentile': q25,\n        '50th_percentile': q50,\n        '75th_percentile': q75,\n        'interquartile_range': iqr\n    }\n\n    return stats_dict\n\n# Example usage:\nif __name__ == \"__main__\":\n    sample_data = [12, 15, 14, 10, 12, 16, 13, 11, 12, 18, 17, 14, 12, 15, 13]\n    result = descriptive_statistics(sample_data)\n    print(result)\n```\n\n### Notes:\n- The `scipy.stats.mode` function is used to compute the mode. It returns the most frequent value.\n- Variance and standard deviation are calculated using `ddof=1` to compute the **sample** variance and standard deviation.\n- Percentiles are calculated using `np.percentile`.\n- The output dictionary includes the statistics with appropriate rounding as per the task description.\n\nYou can run this script directly in a Python environment."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Binomial coefficient: n choose k\n    combination = math.comb(n, k)\n    \n    # Binomial probability formula\n    probability = combination * (p ** k) * ((1 - p) ** (n - k))\n    \n    # Return the result rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# print(binomial_probability(10, 3, 0.5))  # Example: 10 trials, 3 successes, 50% success probability\n```"}
{"task_id": 80, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that calculates the probability density function (PDF) of the normal distribution using the mathematical formula:\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value at x, rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\n\n# Example usage:\nif __name__ == \"__main__\":\n    x = 0\n    mean = 0\n    std_dev = 1\n    result = normal_pdf(x, mean, std_dev)\n    print(f\"PDF at x={x}, mean={mean}, std_dev={std_dev}: {result}\")\n```\n\n### Explanation:\n- The formula for the normal distribution PDF is:\n  $$\n  f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n  $$\n- The function computes this formula and rounds the result to **5 decimal places**.\n- The `math` module is used for mathematical operations like `sqrt`, `pi`, and `exp`.\n\nYou can test it with different values of `x`, `mean`, and `std_dev`."}
{"task_id": 81, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that calculates the Poisson distribution probability for observing exactly `k` events given a mean rate `\u03bb` (lambda), using the Poisson probability formula:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage\nif __name__ == \"__main__\":\n    k = 3\n    lam = 2.5\n    prob = poisson_probability(k, lam)\n    print(f\"P({k} events | \u03bb={lam}) = {prob}\")\n```\n\n### Explanation:\n- The Poisson probability formula is:\n  $$\n  P(k; \\lambda) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\n  $$\n- `math.exp(-lam)` computes $ e^{-\\lambda} $\n- `lam ** k` computes $ \\lambda^k $\n- `math.factorial(k)` computes $ k! $\n- The result is rounded to 5 decimal places using `round()`.\n\nYou can change the values of `k` and `lam` in the `if __name__ == \"__main__\":` block to test different scenarios."}
{"task_id": 82, "completion_id": 0, "completion": "Here's a self-contained Python script that calculates the contrast of a grayscale image using the difference between the maximum and minimum pixel values:\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast value, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D array (grayscale).\")\n    \n    if img.dtype != np.uint8:\n        raise ValueError(\"Input image must have pixel values of type uint8 (0-255).\")\n    \n    max_pixel = np.max(img)\n    min_pixel = np.min(img)\n    contrast = max_pixel - min_pixel\n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create a sample grayscale image (2D array)\n    sample_img = np.array([\n        [50, 75, 100],\n        [150, 200, 255],\n        [0, 25, 50]\n    ], dtype=np.uint8)\n\n    # Calculate and print the contrast\n    contrast_value = calculate_contrast(sample_img)\n    print(f\"Contrast of the image: {contrast_value}\")\n```\n\n### Explanation:\n- The function `calculate_contrast` takes a 2D NumPy array as input.\n- It checks if the input is a valid grayscale image (2D and `uint8` type).\n- It computes the contrast as the difference between the maximum and minimum pixel values.\n- An example usage is provided to demonstrate how to use the function."}
{"task_id": 83, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that calculates the dot product of two 1D NumPy arrays:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)\n\n# Example usage\nif __name__ == \"__main__\":\n    vector1 = np.array([1, 2, 3])\n    vector2 = np.array([4, 5, 6])\n    \n    result = calculate_dot_product(vector1, vector2)\n    print(\"Dot Product:\", result)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the `phi_transform` function as described:\n\n```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    transformed_data = []\n    for x in data:\n        features = [x**d for d in range(1, degree + 1)]\n        transformed_data.append([round(f, 8) for f in features])\n    \n    return transformed_data\n\n# Example usage:\nif __name__ == \"__main__\":\n    data = [1.0, 2.0, 3.0]\n    degree = 3\n    result = phi_transform(data, degree)\n    print(\"Phi Transformation Result:\")\n    print(result)\n```\n\n### Explanation:\n- The function `phi_transform` takes a list of numerical data and a degree.\n- For each data point `x`, it generates polynomial features from `x^1` to `x^degree`.\n- The result is a nested list where each inner list corresponds to a data point's polynomial features.\n- If the degree is less than 0, it returns an empty list.\n- The output is rounded to 8 decimal places as required.\n\n### Example Output:\nFor `data = [1.0, 2.0, 3.0]` and `degree = 3`, the output will be:\n```\nPhi Transformation Result:\n[[1.0, 1.0, 1.0], [2.0, 4.0, 8.0], [3.0, 9.0, 27.0]]\n```"}
{"task_id": 85, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the Positional Encoding layer for Transformers as described:\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n\n    # Initialize an array of size `position` x `d_model`\n    pe = np.zeros((position, d_model), dtype=np.float16)\n\n    # Generate position indices\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            if i + 1 <= d_model:\n                # Compute the division factor\n                div_term = np.power(10000, (2 * (i // 2)) / d_model).astype(np.float16)\n                # Apply sine to even indices\n                pe[pos, i] = np.sin(pos / div_term).astype(np.float16)\n                # Apply cosine to odd indices\n                if i + 1 < d_model:\n                    pe[pos, i + 1] = np.cos(pos / div_term).astype(np.float16)\n\n    return pe.tolist()\n```\n\n### Explanation:\n- **Input Validation**: If `position` is 0 or `d_model` is less than or equal to 0, the function returns `-1`.\n- **Positional Encoding Matrix**: A matrix of shape `(position, d_model)` is initialized.\n- **Sine and Cosine Application**: For each position and each dimension, sine is applied to even indices and cosine to odd indices using the formula from the original Transformer paper.\n- **Data Type**: The output is cast to `np.float16` and converted to a Python list using `.tolist()` before returning.\n\n### Example Usage:\n```python\nprint(pos_encoding(5, 4))\n```\n\nThis will return a list of 5 position vectors, each of size 4, using sine and cosine positional encodings."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting: training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    # Check for underfitting: both training and test accuracy are below 0.7\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    else:\n        return 0  # Good fit\n\n# Example usage:\n# print(model_fit_quality(0.95, 0.70))  # Output: 1 (Overfitting)\n# print(model_fit_quality(0.60, 0.55))  # Output: -1 (Underfitting)\n# print(model_fit_quality(0.80, 0.78))  # Output: 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Compute bias-corrected first moment estimate\n    m_hat = m / (1 - beta1 ** t)\n    # Compute bias-corrected second raw moment estimate\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round and convert to list\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    # Load encoder, hyperparameters, and model parameters\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    \n    # Encode the prompt\n    token_ids = encoder.encode(prompt)\n    \n    # Initialize the sequence length\n    seq_len = len(token_ids)\n    \n    # Generate new tokens\n    for _ in range(n_tokens_to_generate):\n        # Token and Positional Embeddings\n        token_embeddings = params[\"wte\"][token_ids]\n        positional_embeddings = params[\"wpe\"][:seq_len]\n        x = token_embeddings + positional_embeddings\n        \n        # For simplicity, we're skipping the transformer blocks (multi-head attention and feed-forward)\n        # In a real implementation, we would process x through the transformer blocks here\n        \n        # Final Layer Normalization\n        x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        \n        # Predict the next token (simplified: take the token with the highest embedding similarity)\n        # In a real implementation, we would use the output of the transformer blocks to get logits\n        # and then apply softmax to get probabilities and sample from them\n        last_token_embedding = x[-1]\n        similarities = np.dot(last_token_embedding, params[\"wte\"].T)\n        next_token_id = np.argmax(similarities)\n        \n        # Add the predicted token to the sequence\n        token_ids.append(next_token_id)\n        seq_len += 1\n    \n    # Decode and return the generated text\n    return encoder.decode(token_ids)\n\ndef layer_norm(x, gamma, beta, eps=1e-5):\n    \"\"\"Layer normalization.\"\"\"\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    std = np.sqrt(variance + eps)\n    x_norm = (x - mean) / std\n    return gamma * x_norm + beta\n\n# Example usage\nif __name__ == \"__main__\":\n    generated_text = gen_text(\"hello\", n_tokens_to_generate=5)\n    print(generated_text)\n```"}
{"task_id": 89, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the simplified self-attention mechanism as described in the task. It uses NumPy for numerical operations and softmax computation.\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Subtract max for numerical stability\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / exp_values.sum(axis=-1, keepdims=True)\n\n    # Convert crystal_values to a NumPy array\n    crystal_values = np.array(crystal_values)\n\n    # Step 1: Linear projection to reduce dimensionality\n    W_q = np.random.randn(dimension, n)\n    W_k = np.random.randn(dimension, n)\n    W_v = np.random.randn(dimension, n)\n\n    # Query, Key, Value matrices\n    Q = np.dot(W_q, crystal_values)\n    K = np.dot(W_k, crystal_values)\n    V = np.dot(W_v, crystal_values)\n\n    # Step 2: Compute attention scores (Q \u00b7 K^T)\n    attention_scores = np.dot(Q, K.T)\n\n    # Step 3: Apply softmax to get attention weights\n    attention_weights = softmax(attention_scores)\n\n    # Step 4: Compute weighted sum of values\n    output = np.dot(attention_weights, V)\n\n    # Return the final pattern as a list of floats rounded to 4 decimal places\n    return [round(float(val), 4) for val in output]\n\n# Example usage\nif __name__ == \"__main__\":\n    n = 5\n    crystal_values = [1.0, 2.0, 3.0, 4.0, 5.0]\n    dimension = 3\n    result = pattern_weaver(n, crystal_values, dimension)\n    print(result)\n```\n\n### Explanation:\n- **Softmax Function**: Ensures the attention scores sum to 1, making them interpretable as probabilities.\n- **Query, Key, Value (Q, K, V)**: These are derived from the input using random projection matrices.\n- **Attention Scores**: Computed as the dot product of Q and K.\n- **Attention Weights**: Softmax applied to the attention scores.\n- **Output**: Weighted sum of values using attention weights.\n\nThis script is self-contained and can be run directly. The output will be a list of floats representing the final pattern for each crystal, rounded to 4 decimal places."}
{"task_id": 90, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the BM25 ranking function as described:\n\n```python\nimport numpy as np\nfrom collections import Counter\nfrom math import log\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Step 1: Preprocess the corpus and query\n    # Convert query to a Counter (term frequency)\n    query_terms = Counter(query.lower().split())\n    \n    # Step 2: Compute document frequencies and average document length\n    N = len(corpus)\n    doc_freq = {}\n    doc_lengths = []\n    doc_term_freqs = []\n\n    for doc in corpus:\n        doc_lower = doc.lower()\n        doc_terms = doc_lower.split()\n        doc_term_freq = Counter(doc_terms)\n        doc_term_freqs.append(doc_term_freq)\n        doc_lengths.append(len(doc_terms))\n        \n        for term in doc_term_freq:\n            if term in query_terms:\n                doc_freq[term] = doc_freq.get(term, 0) + 1\n\n    avgdl = np.mean(doc_lengths)\n\n    # Step 3: Compute BM25 scores for each document\n    scores = []\n\n    for i, doc_term_freq in enumerate(doc_term_freqs):\n        score = 0.0\n        dl = doc_lengths[i]\n        for term, q_freq in query_terms.items():\n            if term not in doc_term_freq:\n                continue\n            ni = doc_freq[term]\n            tf = doc_term_freq[term]\n            idf = log((N - ni + 0.5) / (ni + 0.5) + 1)\n            ctf = tf * (k1 + 1) / (tf + k1 * (1 - b + b * (dl / avgdl)))\n            score += idf * ctf\n        scores.append(round(score, 3))\n\n    return scores\n\n# Example usage:\ncorpus = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Never jump over the lazy dog quickly\",\n    \"The lazy dog is not quick\",\n    \"Foxes are quick and brown\"\n]\nquery = \"quick brown fox\"\n\n# Compute BM25 scores\nbm25_scores = calculate_bm25_scores(corpus, query)\nprint(\"BM25 Scores:\", bm25_scores)\n```\n\n### Explanation:\n- **`corpus`**: A list of document strings.\n- **`query`**: A string representing the search query.\n- **`k1` and `b`**: BM25 tuning parameters (default values are used).\n- The function calculates the BM25 score for each document in the corpus and returns a list of scores rounded to three decimal places.\n\nYou can modify the `corpus` and `query` variables to test with different inputs."}
{"task_id": 91, "completion_id": 0, "completion": "Sure! Here's a self-contained Python function that calculates the F1 score from true and predicted labels. The F1 score is the harmonic mean of precision and recall, and it is calculated as:\n\n$$\nF1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n$$\n\nWhere:\n- **Precision** = TP / (TP + FP)\n- **Recall** = TP / (TP + FN)\n- **TP** = True Positives\n- **FP** = False Positives\n- **FN** = False Negatives\n\nHere's the implementation:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Count TP, FP, FN\n    tp = sum((t == 1 and p == 1) for t, p in zip(y_true, y_pred))\n    fp = sum((t == 0 and p == 1) for t, p in zip(y_true, y_pred))\n    fn = sum((t == 1 and p == 0) for t, p in zip(y_true, y_pred))\n\n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # Calculate F1 score\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n\n    return round(f1, 3)\n```\n\n### Example usage:\n\n```python\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(calculate_f1_score(y_true, y_pred))  # Output: 0.833\n```\n\nThis function is efficient and handles edge cases like division by zero."}
{"task_id": 92, "completion_id": 0, "completion": "Here's a self-contained Python script that performs the described power grid optimization task using linear regression:\n\n```python\nimport math\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation from each data point\n    detrended_data = []\n    for i in range(1, 11):  # day numbers 1 through 10\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended = consumption_data[i - 1] - fluctuation\n        detrended_data.append((i, detrended))  # (day, detrended consumption)\n\n    # 2) Perform linear regression on the detrended data\n    n = len(detrended_data)\n    sum_x = sum(day for day, _ in detrended_data)\n    sum_y = sum(val for _, val in detrended_data)\n    sum_xy = sum(day * val for day, val in detrended_data)\n    sum_x2 = sum(day ** 2 for day, _ in detrended_data)\n\n    # Calculate slope (m) and intercept (b) of the regression line\n    m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)\n    b = (sum_y - m * sum_x) / n\n\n    # 3) Predict day 15's base consumption\n    day_15_base = m * 15 + b\n\n    # 4) Add the day 15 fluctuation back\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    total_forecast = day_15_base + fluctuation_day_15\n\n    # 5) Add a 5% safety margin (rounded up)\n    safety_margin = total_forecast * 0.05\n    final_forecast = total_forecast + safety_margin\n\n    # Round to nearest integer\n    final_forecast = round(final_forecast)\n\n    # 6) Return the final integer\n    return final_forecast\n```\n\n### Example usage:\n```python\n# Example consumption data for 10 days\nconsumption_data = [120, 130, 140, 150, 160, 170, 180, 190, 200, 210]\n\n# Forecast for day 15\nresult = power_grid_forecast(consumption_data)\nprint(\"Forecasted power consumption for day 15:\", result)\n```\n\nThis script:\n- Removes the daily fluctuation using the provided formula.\n- Fits a linear regression model to the detrended data.\n- Predicts the base consumption for day 15.\n- Adds back the fluctuation for day 15.\n- Includes a 5% safety margin.\n- Returns the final result as an integer."}
{"task_id": 93, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the Mean Absolute Error (MAE) calculation as described:\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate absolute differences\n    absolute_errors = np.abs(y_true - y_pred)\n    # Compute mean of absolute errors\n    mean_error = np.mean(absolute_errors)\n    # Return rounded to 3 decimal places\n    return round(mean_error, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    result = mae(y_true, y_pred)\n    print(f\"Mean Absolute Error: {result}\")\n```\n\n### Explanation:\n- `np.abs(y_true - y_pred)` computes the absolute differences between true and predicted values.\n- `np.mean(...)` calculates the average of those absolute differences.\n- `round(..., 3)` ensures the result is rounded to 3 decimal places as required.\n\nYou can run this script directly in a Python environment."}
{"task_id": 94, "completion_id": 0, "completion": "Sure! Below is a self-contained Python script that implements the **multi-head attention** mechanism using the three key functions: `compute_qkv`, `self_attention`, and `multi_head_attention`. The script adheres to the output constraints and uses NumPy for numerical operations.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Compute Query, Key, and Value matrices.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute scaled dot-product attention.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    output = attention_weights @ V\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute multi-head attention.\n    \"\"\"\n    batch_size, seq_len, d_model = X.shape\n    head_dim = d_model // n_heads\n\n    # Split weights for each head\n    W_q_heads = np.split(W_q, n_heads, axis=1)\n    W_k_heads = np.split(W_k, n_heads, axis=1)\n    W_v_heads = np.split(W_v, n_heads, axis=1)\n\n    outputs = []\n\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q_heads[i], W_k_heads[i], W_v_heads[i])\n        head_output = self_attention(Q, K, V)\n        outputs.append(head_output)\n\n    # Concatenate all head outputs\n    final_output = np.concatenate(outputs, axis=1)\n    final_output = np.round(final_output, 4)  # Round to 4 decimal places\n    return final_output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    X = np.array([[1, 2, 3, 4],\n                  [5, 6, 7, 8]])\n    W_q = np.random.randn(4, 8)\n    W_k = np.random.randn(4, 8)\n    W_v = np.random.randn(4, 8)\n    n_heads = 2\n\n    result = multi_head_attention(X, W_q, W_k, W_v, n_heads)\n    print(\"Multi-Head Attention Output:\")\n    print(result)\n```\n\n### Explanation:\n- **`compute_qkv`**: Computes the Query, Key, and Value matrices using the input and weight matrices.\n- **`self_attention`**: Computes the attention scores using the scaled dot-product attention mechanism.\n- **`multi_head_attention`**: Splits the weights into `n_heads`, computes attention for each head, and concatenates the results.\n\n### Notes:\n- The input `X` is a batch of sequences.\n- The weight matrices `W_q`, `W_k`, and `W_v` are assumed to be of shape `(d_model, d_model)`.\n- The output is a Python list after rounding and reshaping using `tolist()`.\n\nYou can adjust the input dimensions and weights to test different scenarios."}
{"task_id": 95, "completion_id": 0, "completion": "Here's a self-contained Python function that calculates the **Phi coefficient** between two binary variables:\n\n```python\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError(\"Input lists must be of the same length.\")\n    \n    # Count occurrences of each combination\n    n11 = sum(1 for a, b in zip(x, y) if a == 1 and b == 1)\n    n10 = sum(1 for a, b in zip(x, y) if a == 1 and b == 0)\n    n01 = sum(1 for a, b in zip(x, y) if a == 0 and b == 1)\n    n00 = sum(1 for a, b in zip(x, y) if a == 0 and b == 0)\n\n    # Calculate Phi coefficient\n    numerator = (n11 * n00) - (n10 * n01)\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n\n    if denominator == 0:\n        return 0.0  # Avoid division by zero\n\n    phi = numerator / denominator\n    return round(phi, 4)\n```\n\n### Example usage:\n```python\nx = [1, 0, 1, 1, 0, 0, 1]\ny = [0, 0, 1, 1, 0, 1, 1]\nprint(phi_corr(x, y))  # Output: 0.4286\n```\n\nThis function:\n- Validates that the input lists are of the same length.\n- Counts the occurrences of all four possible combinations of 0s and 1s.\n- Computes the Phi coefficient using the formula:\n  $$\n  \\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{11}+n_{10})(n_{11}+n_{01})(n_{00}+n_{10})(n_{00}+n_{01})}}\n  $$\n- Returns the result rounded to 4 decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Hard Sigmoid** activation function as described:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\n    for val in test_values:\n        print(f\"hard_sigmoid({val}) = {hard_sigmoid(val)}\")\n```\n\n### Explanation:\n- The **Hard Sigmoid** is a piecewise linear approximation of the standard sigmoid function.\n- It is defined as:\n  $$\n  \\text{HardSigmoid}(x) = \n  \\begin{cases} \n  0 & \\text{if } x \\leq -2.5 \\\\\n  0.2x + 0.5 & \\text{if } -2.5 < x < 2.5 \\\\\n  1 & \\text{if } x \\geq 2.5\n  \\end{cases}\n  $$\n- This function is computationally more efficient than the standard sigmoid and is often used in neural networks for faster computation."}
{"task_id": 97, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the **ELU (Exponential Linear Unit)** activation function as described:\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [2.0, -1.0, 0.0, -0.5]\n    for val in test_values:\n        print(f\"ELU({val}) = {elu(val)}\")\n```\n\n### Explanation:\n- For `x > 0`, ELU returns `x` (same as ReLU).\n- For `x <= 0`, ELU returns `alpha * (exp(x) - 1)`, which introduces a smooth negative slope.\n- The result is **rounded to 4 decimal places** as required.\n\n### Example Output:\n```\nELU(2.0) = 2.0\nELU(-1.0) = -0.6321\nELU(0.0) = 0.0\nELU(-0.5) = -0.3935\n```"}
{"task_id": 98, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the **PReLU (Parametric ReLU)** activation function as described:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage\nif __name__ == \"__main__\":\n    input_values = [2.0, -1.0, 0.0, -3.5]\n    alpha = 0.25\n\n    for x in input_values:\n        result = prelu(x, alpha)\n        print(f\"prelu({x}, {alpha}) = {result}\")\n```\n\n### Explanation:\n- For **positive inputs** (`x > 0`), PReLU behaves like the standard ReLU: returns `x`.\n- For **negative inputs** (`x <= 0`), it returns `alpha * x`, where `alpha` is a learnable parameter (default is `0.25`).\n\n### Example Output:\n```\nprelu(2.0, 0.25) = 2.0\nprelu(-1.0, 0.25) = -0.25\nprelu(0.0, 0.25) = 0.0\nprelu(-3.5, 0.25) = -0.875\n```\n\nThis script is ready to run and demonstrates the behavior of the PReLU function."}
{"task_id": 99, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Softplus activation function**, with proper handling of numerical stability and rounding to 4 decimal places:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handle numerical stability for large x\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-1000, -10, -1, 0, 1, 10, 1000]\n    for val in test_values:\n        print(f\"softplus({val}) = {softplus(val)}\")\n```\n\n### Explanation:\n- The **Softplus function** is defined as:  \n  $$\n  \\text{softplus}(x) = \\log(1 + e^x)\n  $$\n- For **very large positive values of `x`**, $ e^x $ becomes extremely large, and the function approximates `x`, so we return `x` directly to avoid overflow.\n- For **very large negative values of `x`**, $ e^x $ approaches 0, so the function approximates `0`, and we return `0.0` to avoid underflow.\n- The result is **rounded to 4 decimal places** as required.\n\nThis implementation is numerically stable and efficient."}
{"task_id": 100, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Softsign activation function** as described:\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    test_values = [0, 1, -1, 10, -10, 0.5, -0.5]\n    for val in test_values:\n        print(f\"softsign({val}) = {softsign(val)}\")\n```\n\n### Explanation:\n- The **Softsign function** is defined as:  \n  $$\n  \\text{softsign}(x) = \\frac{x}{1 + |x|}\n  $$\n- This function is smooth and bounded between -1 and 1.\n- The result is rounded to **4 decimal places** as required.\n\n### Example Output:\n```\nsoftsign(0) = 0.0\nsoftsign(1) = 0.5\nsoftsign(-1) = -0.5\nsoftsign(10) = 0.9091\nsoftsign(-10) = -0.9091\nsoftsign(0.5) = 0.3333\nsoftsign(-0.5) = -0.3333\n```"}
{"task_id": 101, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the GRPO (Group Relative Policy Optimization) objective function as described:\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Compute the clipped likelihood ratios\n    clipped_rhos = [min(rho, 1 + epsilon) for rho in rhos]\n    \n    # Compute the KL divergence between the old and reference policies\n    kl_divergence = sum(pi_old * np.log(pi_old / pi_ref) for pi_old, pi_ref in zip(pi_theta_old, pi_theta_ref))\n    \n    # Compute the GRPO objective\n    objective = sum(min(rho * A[i], clipped_rho * A[i]) for i, (rho, clipped_rho) in enumerate(zip(rhos, clipped_rhos))) - beta * kl_divergence\n    \n    return round(objective, 6)\n```\n\n### Explanation:\n- **Clipping Mechanism**: The likelihood ratios are clipped at `1 + epsilon` to prevent large policy updates.\n- **KL Divergence Penalty**: A penalty is added to the objective to ensure the new policy doesn't deviate too much from the reference policy.\n- **Advantage Weighting**: The clipped and unclipped versions of the advantage-weighted likelihood ratios are compared and the minimum is taken to ensure stable updates.\n\nThis function is suitable for use in policy gradient methods where stability and constraint satisfaction are important."}
{"task_id": 102, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Swish activation function** as described:\n\n```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x / (1 + math.exp(-x))\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for val in test_values:\n        result = swish(val)\n        print(f\"swish({val}) = {round(result, 4)}\")\n```\n\n### Explanation:\n- The **Swish function** is defined as:  \n  $$\n  \\text{Swish}(x) = \\frac{x}{1 + e^{-x}}\n  $$\n- This is also known as the **Sigmoid-weighted Linear Unit (SiLU)**.\n- The function is smooth, non-monotonic, and has been shown to outperform ReLU in certain deep learning architectures.\n\n### Sample Output:\n```\nswish(-2.0) = -0.1049\nswish(-1.0) = -0.2689\nswish(0.0) = 0.0\nswish(1.0) = 0.7311\nswish(2.0) = 1.7616\n```\n\nThis script is ready to run and will compute the Swish values for the given inputs, rounded to 4 decimal places."}
{"task_id": 103, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **SELU (Scaled Exponential Linear Unit)** activation function with numerical stability and rounding to 4 decimal places as required:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to 4 decimal places\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * (np.exp(x) - 1)\n    \n    return round(result, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for val in test_values:\n        print(f\"selu({val}) = {selu(val)}\")\n```\n\n### Explanation:\n- **SELU Formula**:\n  $$\n  \\text{SELU}(x) = \\text{scale} \\times \n  \\begin{cases}\n    x & \\text{if } x > 0 \\\\\n    \\alpha \\cdot (\\exp(x) - 1) & \\text{if } x \\leq 0\n  \\end{cases}\n  $$\n- **Constants**:\n  - `alpha = 1.6732632423543772`\n  - `scale = 1.0507009873554804`\n- **Numerical Stability**: Using `numpy.exp(x)` ensures stable computation of the exponential.\n- **Rounding**: The result is rounded to 4 decimal places as required.\n\nYou can run this script directly to test the SELU function with various inputs."}
{"task_id": 104, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the `predict_logistic` function for binary classification using Logistic Regression:\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination: z = X * weights + bias\n    z = np.dot(X, weights) + bias\n\n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-z))\n\n    # Apply threshold of 0.5 to get binary predictions\n    predictions = (probabilities >= 0.5).astype(int)\n\n    # Convert numpy array to Python list\n    return predictions.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input data\n    X = np.array([[1, 2], [2, 3], [3, 4]])\n    weights = np.array([0.5, 0.5])\n    bias = 0.1\n\n    # Get predictions\n    predictions = predict_logistic(X, weights, bias)\n    print(\"Predictions:\", predictions)\n```\n\n### Explanation:\n- **Sigmoid Function**: `1 / (1 + np.exp(-z))` maps the linear output `z` to a probability between 0 and 1.\n- **Thresholding**: If the probability is \u2265 0.5, the prediction is 1; otherwise, it's 0.\n- **Output**: The result is converted to a Python list using `.tolist()` as required.\n\nYou can modify the `X`, `weights`, and `bias` values to test with different inputs."}
{"task_id": 105, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a gradient descent-based training algorithm for Softmax regression:\n\n```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of samples and features\n    N, M = X.shape\n    # Number of classes\n    C = len(np.unique(y))\n\n    # One-hot encode the labels\n    y_one_hot = np.zeros((N, C))\n    y_one_hot[np.arange(N), y.astype(int)] = 1\n\n    # Initialize weights (B) and bias (b)\n    B = np.random.randn(M, C) * 0.01\n    b = np.zeros((1, C))\n\n    losses = []\n\n    for _ in range(iterations):\n        # Linear scores: Z = X * B + b\n        Z = X @ B + b\n\n        # Softmax function\n        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n        A = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n\n        # Cross-Entropy loss\n        loss = -np.mean(np.log(A[np.arange(N), y.astype(int)] + 1e-15))\n        losses.append(round(loss, 4))\n\n        # Compute gradients\n        dZ = A - y_one_hot\n        dB = X.T @ dZ / N\n        db = np.mean(dZ, axis=0)\n\n        # Update parameters\n        B -= learning_rate * dB\n        b -= learning_rate * db\n\n    # Round and convert to lists\n    B_rounded = np.round(B, 4).tolist()\n    losses_rounded = [round(loss, 4) for loss in losses]\n\n    return B_rounded, losses_rounded\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 1, 1, 0])  # 2 classes\n    learning_rate = 0.1\n    iterations = 100\n\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    print(\"Optimized coefficients (B):\", B)\n    print(\"Loss over iterations:\", losses)\n```\n\n### Explanation:\n- **Softmax Regression** is a generalization of logistic regression for multi-class classification.\n- **Cross-Entropy Loss** is used to measure the performance of the model.\n- **Gradient Descent** is used to minimize the loss by updating the weights and bias.\n- The output is rounded to 4 decimal places and returned as Python lists using `tolist()`.\n\nYou can replace the example `X` and `y` with your own dataset for training."}
{"task_id": 106, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a gradient descent-based logistic regression model using Binary Cross Entropy loss. The function returns the optimized coefficients and the loss values over iterations, all rounded to the 4th decimal.\n\n```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \n    Parameters:\n    - X: Input feature matrix (shape: [n_samples, n_features])\n    - y: Target vector (shape: [n_samples])\n    - learning_rate: Step size for gradient descent\n    - iterations: Number of iterations to run the optimization\n    \n    Returns:\n    - coefficients: List of optimized model coefficients (including intercept)\n    - losses: List of loss values over iterations (rounded to 4 decimal places)\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Initialize weights and bias to zero\n    weights = np.zeros(n_features)\n    bias = 0.0\n    losses = []\n\n    for _ in range(iterations):\n        # Linear prediction\n        linear_model = np.dot(X, weights) + bias\n        # Apply sigmoid function\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        \n        # Compute Binary Cross Entropy loss\n        epsilon = 1e-15  # To avoid log(0)\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n\n        # Compute gradients\n        dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n        db = (1 / n_samples) * np.sum(y_pred - y)\n\n        # Update parameters\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n\n    # Combine weights and bias into a single list\n    coefficients = np.concatenate((weights.reshape(-1), [bias])).tolist()\n    losses = [round(loss, 4) for loss in losses]\n\n    return coefficients, losses\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 0, 1, 1])\n    learning_rate = 0.1\n    iterations = 100\n\n    coefficients, losses = train_logreg(X, y, learning_rate, iterations)\n    print(\"Coefficients:\", coefficients)\n    print(\"Losses:\", losses)\n```\n\n### Key Points:\n- **Sigmoid Function**: Used to map predictions to probabilities.\n- **Binary Cross Entropy Loss**: Measures the performance of a classification model.\n- **Gradient Descent**: Updates the model parameters iteratively to minimize the loss.\n- **Rounding**: All outputs are rounded to the 4th decimal as required.\n\nYou can run this script directly in a Python environment."}
{"task_id": 107, "completion_id": 0, "completion": "Sure! Below is a self-contained Python script that implements the **masked self-attention** mechanism using NumPy. The script includes the `compute_qkv` function to compute the Q, K, V matrices and the `masked_attention` function to compute the attention output with a given mask.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[-1]  # Dimension of the key vectors\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Compute attention scores\n    scores = np.where(mask == 0, -1e9, scores)  # Apply the mask\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))  # Softmax\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)  # Normalize\n    output = np.dot(attention_weights, V)  # Compute the final output\n    return output.tolist()  # Return as a Python list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Input sequence (batch_size=1, sequence_length=3, embedding_dim=4)\n    X = np.array([[1, 2, 3, 4],\n                  [5, 6, 7, 8],\n                  [9, 10, 11, 12]])\n\n    # Q, K, V weight matrices (embedding_dim=4, head_dim=2)\n    W_q = np.random.randn(4, 2)\n    W_k = np.random.randn(4, 2)\n    W_v = np.random.randn(4, 2)\n\n    # Compute Q, K, V\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n    # Attention mask (sequence_length=3)\n    # 1 = visible, 0 = masked\n    mask = np.array([[1, 1, 0],\n                     [1, 1, 1],\n                     [1, 0, 0]])\n\n    # Compute masked attention\n    attention_output = masked_attention(Q, K, V, mask)\n\n    # Print the result\n    print(\"Masked Attention Output:\")\n    print(attention_output)\n```\n\n### Explanation:\n- **`compute_qkv`** computes the Query, Key, and Value matrices using matrix multiplication.\n- **`masked_attention`**:\n  - Computes attention scores using dot product and scaled by `sqrt(d_k)`.\n  - Applies the mask by setting masked positions to a very low value (`-1e9`) to suppress their influence.\n  - Applies softmax to normalize the attention weights.\n  - Computes the final output by multiplying the attention weights with the Value matrix.\n- The final output is returned as a Python list using `.tolist()`.\n\nYou can modify the input dimensions and weights to test different scenarios."}
{"task_id": 108, "completion_id": 0, "completion": "Here's a self-contained Python script that defines the `disorder` function using **Shannon Entropy**, a well-known measure of uncertainty or disorder. This satisfies all the given properties:\n\n```python\nfrom collections import Counter\nimport math\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n\n    counts = Counter(apples)\n    total = len(apples)\n    entropy = 0.0\n\n    for count in counts.values():\n        prob = count / total\n        entropy -= prob * math.log2(prob)\n\n    return round(entropy, 4)\n\n# Example test cases\nif __name__ == \"__main__\":\n    test_cases = [\n        [0, 0, 0, 0],\n        [1, 1, 0, 0],\n        [0, 1, 2, 3],\n        [0, 0, 1, 1, 2, 2, 3, 3],\n        [0, 0, 0, 0, 0, 1, 2, 3]\n    ]\n\n    for idx, test in enumerate(test_cases):\n        print(f\"Test case {idx + 1}: {test} -> disorder = {disorder(test)}\")\n```\n\n### Explanation:\n- **Shannon Entropy** is calculated as:\n  $$\n  H = -\\sum_{i} p_i \\log_2(p_i)\n  $$\n  where $ p_i $ is the probability of color $ i $.\n- The entropy is 0 when all apples are the same color.\n- It increases with more variety and uneven distribution of colors.\n- The result is rounded to 4 decimal places as required.\n\n### Output Example:\nRunning the script will produce results like:\n```\nTest case 1: [0, 0, 0, 0] -> disorder = 0.0\nTest case 2: [1, 1, 0, 0] -> disorder = 1.0\nTest case 3: [0, 1, 2, 3] -> disorder = 2.0\nTest case 4: [0, 0, 1, 1, 2, 2, 3, 3] -> disorder = 2.0\nTest case 5: [0, 0, 0, 0, 0, 1, 2, 3] -> disorder = 1.5436\n```\n\nThis satisfies the required ordering of disorder."}
{"task_id": 109, "completion_id": 0, "completion": "Here's a self-contained Python script that implements **Layer Normalization** for a 3D input tensor. It normalizes across the feature dimension for each sequence in the batch, then applies learnable parameters `gamma` (scale) and `beta` (shift).\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (batch_size, sequence_length, features)\n    - gamma (np.ndarray): Scale parameter of shape (features,)\n    - beta (np.ndarray): Shift parameter of shape (features,)\n    - epsilon (float): Small value to prevent division by zero\n\n    Returns:\n    - np.ndarray: Normalized tensor of the same shape as X, rounded to 5 decimal places\n    \"\"\"\n    # Compute mean and variance across the feature dimension (axis=2)\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n\n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n\n    # Apply scale and shift\n    X_scaled = X_normalized * gamma + beta\n\n    # Reshape gamma and beta to match the shape of X for broadcasting\n    # gamma and beta are of shape (features,), so we add two new axes\n    gamma = gamma[np.newaxis, np.newaxis, :]\n    beta = beta[np.newaxis, np.newaxis, :]\n\n    # Return the result rounded to 5 decimal places and converted to a list\n    return (X_scaled.round(5)).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input: batch_size=2, sequence_length=3, features=2\n    X = np.array([\n        [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n        [[-1.0, -2.0], [-3.0, -4.0], [-5.0, -6.0]]\n    ])\n\n    # Scale and shift parameters (should match the feature dimension)\n    gamma = np.array([1.0, 1.0])\n    beta = np.array([0.0, 0.0])\n\n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n\n    # Print the result\n    print(\"Normalized Output:\")\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "Here's a self-contained Python script that computes the **METEOR score** based on unigram matches, precision, recall, F-mean, and a penalty for word order fragmentation. The implementation is a simplified version of the original METEOR algorithm.\n\n```python\nimport numpy as np\nfrom collections import Counter\nimport math\nfrom itertools import combinations\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the sentences\n    ref_tokens = reference.lower().split()\n    can_tokens = candidate.lower().split()\n\n    # Step 1: Unigram Matching\n    matches = set()\n    for i, can_word in enumerate(can_tokens):\n        for j, ref_word in enumerate(ref_tokens):\n            if can_word == ref_word:\n                matches.add((i, j))\n                break  # Match the first occurrence\n\n    # Precision and Recall\n    precision = len(matches) / len(can_tokens) if can_tokens else 0\n    recall = len(matches) / len(ref_tokens) if ref_tokens else 0\n\n    # F-mean\n    f_mean = (precision * recall) / (alpha * precision + (1 - alpha) * recall) if (precision + recall) > 0 else 0\n\n    # Step 2: Length Penalty\n    len_ratio = len(can_tokens) / len(ref_tokens)\n    if len_ratio > 1:\n        len_ratio = 1 / len_ratio\n    penalty = 1 - math.exp(-gamma * len_ratio)\n\n    # Step 3: Fragmentation Penalty\n    # Count the number of chunks (consecutive matches)\n    chunks = 0\n    prev_match = -2\n    for i, j in sorted(matches):\n        if i == prev_match + 1:\n            chunks += 1\n        else:\n            chunks += 1\n        prev_match = i\n\n    # Fragmentation penalty\n    if len(matches) > 0:\n        frag = 1 - (chunks / len(matches))\n    else:\n        frag = 0\n\n    # Final METEOR score\n    score = (1 - penalty) * f_mean * (1 + frag)\n\n    return round(score, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    reference = \"The cat is on the mat.\"\n    candidate = \"The cat is sitting on the mat.\"\n    print(\"METEOR Score:\", meteor_score(reference, candidate))\n```\n\n### Explanation:\n- **Unigram Matching**: Matches words between the reference and candidate.\n- **Precision & Recall**: Measures how many words are matched and how many are recovered.\n- **F-mean**: Harmonic mean of precision and recall.\n- **Length Penalty**: Penalizes for large differences in sentence length.\n- **Fragmentation Penalty**: Penalizes for non-consecutive matches (word order mismatch).\n\n### Output:\nThe function returns a float value between 0 and 1, rounded to **3 decimal places**, representing the METEOR score."}
{"task_id": 111, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements the Pointwise Mutual Information (PMI) function as described:\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    # Compute the probabilities\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Compute PMI using the formula: PMI(x, y) = log(p(x, y) / (p(x) * p(y)))\n    pmi = np.log(p_xy / (p_x * p_y))\n    \n    # Return the result rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example values\n    joint_counts = 10\n    total_counts_x = 50\n    total_counts_y = 40\n    total_samples = 200\n\n    pmi_value = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\n    print(f\"PMI: {pmi_value}\")\n```\n\n### Explanation:\n- **PMI Formula**: $ \\text{PMI}(x, y) = \\log \\left( \\frac{P(x, y)}{P(x) \\cdot P(y)} \\right) $\n- The function calculates the probabilities from the given counts and the total number of samples.\n- It uses `numpy.log` for the natural logarithm.\n- The result is rounded to 3 decimal places as required.\n\nYou can replace the example values in the `if __name__ == \"__main__\":` block with your own data to test different scenarios."}
{"task_id": 112, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Min-Max Normalization** function as described:\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    if min_val == max_val:\n        # All values are the same, return list of 0.0\n        return [0.0 for _ in x]\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage\nif __name__ == \"__main__\":\n    data = [10, 20, 30, 40, 50]\n    result = min_max(data)\n    print(\"Min-Max Normalized Values:\", result)\n```\n\n### Explanation:\n- **Min-Max Normalization Formula**:  \n  $$\n  x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n  $$\n- The function handles edge cases:\n  - Empty list: returns an empty list.\n  - All values are the same: returns a list of `0.0`.\n- The result is rounded to **4 decimal places** as required.\n\n### Example Output:\nFor input `[10, 20, 30, 40, 50]`, the output will be:\n```\nMin-Max Normalized Values: [0.0, 0.25, 0.5, 0.75, 1.0]\n```"}
{"task_id": 113, "completion_id": 0, "completion": "Sure! Below is a self-contained Python script that implements a simple residual block using NumPy, as described in your task.\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    A simple residual block with two weight layers and shortcut connection.\n    \n    Parameters:\n    - x: Input 1D array (np.ndarray)\n    - w1: First weight matrix (np.ndarray)\n    - w2: Second weight matrix (np.ndarray)\n    \n    Returns:\n    - Output of the residual block as a list, rounded to 4 decimal places.\n    \"\"\"\n    # First linear transformation\n    z1 = np.dot(x, w1)\n    # First ReLU activation\n    a1 = np.maximum(0, z1)\n    # Second linear transformation\n    z2 = np.dot(a1, w2)\n    # Add shortcut connection (original input)\n    out = z2 + x\n    # Final ReLU activation\n    final_out = np.maximum(0, out)\n    # Round to 4 decimal places and convert to list\n    return np.round(final_out, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input and weights\n    x = np.array([1.0, 2.0, 3.0])\n    w1 = np.array([[0.1, 0.2, 0.3],\n                   [0.4, 0.5, 0.6]])\n    w2 = np.array([[0.7, 0.8],\n                   [0.9, 1.0],\n                   [1.1, 1.2]])\n    \n    result = residual_block(x, w1, w2)\n    print(\"Residual block output:\", result)\n```"}
{"task_id": 114, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that implements **Global Average Pooling** on a 3D NumPy array:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels)\n\n    Returns:\n    np.ndarray: Output array of shape (channels,)\n    \"\"\"\n    # Check if input is 3D\n    if x.ndim != 3:\n        raise ValueError(\"Input must be a 3D array (height, width, channels).\")\n    \n    # Compute the average over height and width for each channel\n    return np.mean(x, axis=(0, 1))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input: 3x3 feature maps with 2 channels\n    x = np.array([\n        [[1, 2], [3, 4], [5, 6]],\n        [[7, 8], [9, 10], [11, 12]],\n        [[13, 14], [15, 16], [17, 18]]\n    ])\n\n    # Perform Global Average Pooling\n    output = global_avg_pool(x)\n    print(\"Output after Global Average Pooling:\", output)\n```\n\n### Explanation:\n- The input `x` is a 3D array with shape `(height, width, channels)`.\n- `np.mean(x, axis=(0, 1))` computes the average over the height and width dimensions for each channel.\n- The result is a 1D array of shape `(channels,)`.\n\n### Example Output:\nFor the example input, the output will be:\n```\nOutput after Global Average Pooling: [11. 12.]\n```\n\nThis is because the average of each channel is:\n- Channel 0: (1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17) / 9 = 11\n- Channel 1: (2 + 4 + 6 + 8 + 10 + 12 + 14 + 16 + 18) / 9 = 12"}
{"task_id": 115, "completion_id": 0, "completion": "Here's a self-contained Python script that implements batch normalization for a 4D NumPy array in the BCHW format:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Batch Normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X: Input array of shape (batch, channels, height, width)\n    - gamma: Scale parameters of shape (channels,)\n    - beta: Shift parameters of shape (channels,)\n    - epsilon: Small value for numerical stability\n\n    Returns:\n    - Output array after batch normalization, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Get the shape of the input\n    batch, channels, height, width = X.shape\n\n    # Reshape X to (batch * height * width, channels) to compute mean and variance per channel\n    X_reshaped = X.transpose(1, 0, 2, 3).reshape(channels, -1)\n\n    # Compute mean and variance for each channel\n    mean = np.mean(X_reshaped, axis=1)\n    var = np.var(X_reshaped, axis=1)\n\n    # Normalize\n    X_normalized = (X_reshaped - mean[:, np.newaxis]) / np.sqrt(var[:, np.newaxis] + epsilon)\n\n    # Reshape back to (batch, channels, height, width)\n    X_normalized = X_normalized.reshape(channels, batch, height, width).transpose(1, 0, 2, 3)\n\n    # Apply scale (gamma) and shift (beta)\n    X_output = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n\n    # Round to 4 decimal places and convert to list\n    return np.round(X_output, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample input\n    X = np.random.randn(2, 3, 4, 4)  # (batch=2, channels=3, height=4, width=4)\n    gamma = np.random.randn(3)       # (channels=3,)\n    beta = np.random.randn(3)        # (channels=3,)\n    epsilon = 1e-5\n\n    # Run batch normalization\n    output = batch_normalization(X, gamma, beta, epsilon)\n\n    # Print output\n    print(\"Output after batch normalization:\")\n    print(output)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the function to compute the derivative of a polynomial term `c * x^n` at a given point `x`, using the power rule from calculus:\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of the polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n    \n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    result = poly_term_derivative(3.0, 2.0, 4.0)\n    print(f\"The derivative is: {result}\")\n```\n\n### Explanation:\n- The derivative of `c * x^n` is `c * n * x^(n - 1)` by the power rule.\n- The function rounds the result to 4 decimal places as required.\n\nYou can modify the parameters in the `if __name__ == \"__main__\":` block to test with different values."}
{"task_id": 117, "completion_id": 0, "completion": "Here's a self-contained Python script that computes an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process:\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    # Convert list of vectors to NumPy array\n    vectors = np.array(vectors, dtype=float)\n    n_vectors = vectors.shape[0]\n    \n    # Initialize an empty list to store orthonormal vectors\n    basis = []\n\n    for v in vectors:\n        # Subtract the projection of v onto each vector in the current basis\n        v = v.copy()\n        for u in basis:\n            proj = np.dot(v, u) / np.dot(u, u) * u\n            v = v - proj\n        \n        # Check if the resulting vector is non-zero (i.e., linearly independent)\n        if np.linalg.norm(v) > tol:\n            # Normalize the vector\n            u = v / np.linalg.norm(v)\n            # Round to 4 decimal places and convert to list\n            basis.append(np.round(u, 4).tolist())\n    \n    return basis\n\n# Example usage\nif __name__ == \"__main__\":\n    vectors = [[1, 1], [2, 2], [1, 0]]\n    result = orthonormal_basis(vectors)\n    print(\"Orthonormal Basis:\")\n    print(result)\n```\n\n### Explanation:\n- The function takes a list of 2D vectors and a tolerance `tol`.\n- It applies the **Gram-Schmidt process** to orthogonalize the vectors.\n- Each vector is normalized to unit length.\n- Vectors with norm less than `tol` are considered linearly dependent and are skipped.\n- The final orthonormal basis is returned as a list of lists, with each vector rounded to 4 decimal places.\n\n### Example Output:\nFor the input `[[1, 1], [2, 2], [1, 0]]`, the output might be:\n```\nOrthonormal Basis:\n[[0.7071, 0.7071], [1.0, 0.0]]\n```\n\nThis indicates that the orthonormal basis for the subspace spanned by the input vectors is `[ [0.7071, 0.7071], [1.0, 0.0] ]`."}
{"task_id": 118, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that computes the cross product of two 3D vectors using NumPy, rounds the result to 4 decimal places, and returns it as a list:\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Compute the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list or np.ndarray): First 3D vector.\n    b (list or np.ndarray): Second 3D vector.\n    \n    Returns:\n    list: Cross product of the two vectors, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input to numpy arrays if they are not already\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a, b)\n    \n    # Round the result to 4 decimal places and convert to list\n    return np.round(result, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    vector_a = [1, 2, 3]\n    vector_b = [4, 5, 6]\n    result = cross_product(vector_a, vector_b)\n    print(\"Cross Product:\", result)\n``` \n\n### Explanation:\n- `np.cross(a, b)` computes the cross product of the two 3D vectors.\n- `np.round(result, 4)` rounds the result to 4 decimal places.\n- `.tolist()` converts the NumPy array to a standard Python list.\n\nYou can replace `vector_a` and `vector_b` with any other 3D vectors to compute their cross product."}
{"task_id": 119, "completion_id": 0, "completion": "Here's a self-contained Python script that solves a system of linear equations using **Cramer's Rule**:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (list of lists or numpy array): Square coefficient matrix.\n    b (list or numpy array): Constant vector.\n    \n    Returns:\n    list: Solution vector x rounded to 4 decimal places, or -1 if no unique solution.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    n = A.shape[0]\n    \n    if A.shape[1] != n:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    if b.shape[0] != n:\n        raise ValueError(\"Vector b must have the same number of elements as rows in A.\")\n    \n    det_A = np.linalg.det(A)\n    \n    if abs(det_A) < 1e-10:\n        return -1  # No unique solution\n    \n    x = []\n    for i in range(n):\n        # Replace the i-th column of A with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_Ai = np.linalg.det(A_i)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    \n    return x\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = [[2, 1, -1],\n         [-3, -1, 2],\n         [-2, 1, 2]]\n    b = [8, -11, -3]\n    \n    solution = cramers_rule(A, b)\n    print(\"Solution vector x:\", solution)\n```\n\n### Explanation:\n- **Cramer's Rule** computes the solution of a system of linear equations by computing the ratio of determinants.\n- The function checks if the determinant of the coefficient matrix is zero (within a small tolerance) to determine if a unique solution exists.\n- If a solution exists, it computes each component of the solution vector by replacing the corresponding column of the matrix with the constant vector `b` and computing the determinant.\n- The result is rounded to **4 decimal places** and returned as a **Python list**.\n\n### Output Example:\nFor the given example:\n```\nSolution vector x: [2.0, 3.0, -1.0]\n```"}
{"task_id": 120, "completion_id": 0, "completion": "Here's a self-contained Python script that calculates the **Bhattacharyya distance** between two discrete probability distributions:\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if inputs are valid\n    if not p or not q or len(p) != len(q):\n        return 0.0\n\n    # Convert to numpy arrays for element-wise operations\n    p = np.array(p)\n    q = np.array(q)\n\n    # Ensure the inputs are non-negative and sum to 1 (probability distributions)\n    if np.any(p < 0) or np.any(q < 0) or not np.isclose(np.sum(p), 1.0) or not np.isclose(np.sum(q), 1.0):\n        return 0.0\n\n    # Compute the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p * q))\n\n    # Avoid log(0) by adding a small epsilon if bc is zero\n    epsilon = 1e-10\n    bc = max(bc, epsilon)\n\n    # Compute the Bhattacharyya distance\n    distance = -np.log(bc)\n\n    # Round to 4 decimal places\n    return round(distance, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    p = [0.1, 0.4, 0.5]\n    q = [0.2, 0.3, 0.5]\n    result = bhattacharyya_distance(p, q)\n    print(f\"Bhattacharyya Distance: {result}\")\n```\n\n### Explanation:\n- **Bhattacharyya Distance** is defined as:  \n  $$\n  D_B(p, q) = -\\ln\\left( \\sum_i \\sqrt{p_i q_i} \\right)\n  $$\n- The function includes checks for:\n  - Empty inputs\n  - Mismatched lengths\n  - Valid probability distributions (non-negative and sum to 1)\n- A small epsilon is added to avoid taking the logarithm of zero.\n- The result is rounded to 4 decimal places as required.\n\nYou can run this script directly in any Python environment."}
{"task_id": 121, "completion_id": 0, "completion": "Sure! Here's a self-contained Python script that defines the `vector_sum` function as described:\n\n```python\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    vector1 = [1, 2, 3]\n    vector2 = [4, 5, 6]\n    result = vector_sum(vector1, vector2)\n    print(\"Element-wise sum:\", result)\n\n    vector3 = [1, 2]\n    vector4 = [3, 4, 5]\n    result = vector_sum(vector3, vector4)\n    print(\"Incompatible vectors:\", result)\n```\n\n### Explanation:\n- The function `vector_sum` takes two lists of integers or floats.\n- It first checks if the lengths of the two vectors are equal.\n- If they are not, it returns `-1` to indicate an error.\n- If they are equal, it returns a new list containing the sum of corresponding elements.\n\nYou can run this script directly in a Python environment."}
{"task_id": 122, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the REINFORCE policy gradient estimator as described:\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        # Compute returns for each time step\n        returns = []\n        rewards = [r for s, a, r in episode]\n        G = 0\n        for r in reversed(rewards):\n            G = r + G  # No discount factor is mentioned, so assume gamma = 1\n            returns.insert(0, G)\n        \n        # Compute the gradient for this episode\n        for t, (state, action, _) in enumerate(episode):\n            G_t = returns[t]\n            \n            # Compute the softmax probabilities for the current state\n            exp_theta = np.exp(theta[state, :])\n            pi = exp_theta / exp_theta.sum()\n            \n            # Compute the gradient of the log-policy\n            grad_log_pi = np.zeros(num_actions)\n            grad_log_pi[action] = 1 - pi[action]\n            grad_log_pi -= pi\n            \n            # Accumulate the gradient\n            gradient[state, :] += G_t * grad_log_pi\n    \n    # Average the gradient over all episodes\n    gradient /= len(episodes)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(gradient, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example theta (2 states, 3 actions)\n    theta = np.array([\n        [1.0, 2.0, 3.0],\n        [0.5, -1.0, 2.0]\n    ])\n\n    # Example episodes: list of (state, action, reward) tuples\n    episodes = [\n        [(0, 0, 1.0), (0, 1, 2.0), (1, 2, 0.5)],\n        [(1, 0, -0.5), (1, 1, 1.0), (0, 2, 3.0)]\n    ]\n\n    # Compute policy gradient\n    gradient = compute_policy_gradient(theta, episodes)\n    print(\"Policy Gradient:\")\n    print(gradient)\n```\n\n### Explanation:\n- **Softmax Policy**: The policy is defined as a softmax over `theta[s, :]` for each state `s`.\n- **Return Calculation**: The return `G_t` is the cumulative sum of rewards from time `t` to the end of the episode.\n- **Gradient Calculation**: The gradient of the log-policy is computed using the derivative of the softmax function.\n- **Averaging**: The gradient is averaged over all episodes.\n- **Output**: The result is rounded to 4 decimal places and returned as a Python list using `tolist()`.\n\nYou can modify the `theta` and `episodes` to test with different inputs."}
{"task_id": 123, "completion_id": 0, "completion": "Here's a self-contained Python script that calculates the computational efficiency of a Mixture-of-Experts (MoE) layer compared to a dense layer, based on the number of experts, the number of active experts, and the input/output dimensions.\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    # FLOPs for a dense layer: (input_dim * output_dim)\n    flops_dense = d_in * d_out\n\n    # FLOPs for an MoE layer: (k_active / n_experts) * (input_dim * output_dim)\n    flops_moe = (k_active / n_experts) * d_in * d_out\n\n    # Savings in FLOPs\n    savings_flops = flops_dense - flops_moe\n\n    # Savings percentage\n    savings_percentage = (savings_flops / flops_dense) * 100\n\n    # Round to the nearest 1st decimal\n    flops_dense = round(flops_dense, 1)\n    flops_moe = round(flops_moe, 1)\n    savings_flops = round(savings_flops, 1)\n    savings_percentage = round(savings_percentage, 1)\n\n    return {\n        \"flops_dense\": flops_dense,\n        \"flops_moe\": flops_moe,\n        \"savings_flops\": savings_flops,\n        \"savings_percentage\": savings_percentage\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    n_experts = 8\n    k_active = 2\n    d_in = 128\n    d_out = 256\n\n    result = compute_efficiency(n_experts, k_active, d_in, d_out)\n    print(f\"Dense Layer FLOPs: {result['flops_dense']}\")\n    print(f\"MoE Layer FLOPs: {result['flops_moe']}\")\n    print(f\"FLOPs Savings: {result['savings_flops']}\")\n    print(f\"Savings Percentage: {result['savings_percentage']}%\")\n```\n\n### Explanation:\n- **Dense Layer FLOPs**: A fully connected layer computes `d_in * d_out` FLOPs.\n- **MoE Layer FLOPs**: Only `k_active` out of `n_experts` are active, so the FLOPs are reduced by a factor of `k_active / n_experts`.\n- **Savings**: The difference in FLOPs and the percentage of savings compared to the dense layer.\n\nYou can adjust the parameters `n_experts`, `k_active`, `d_in`, and `d_out` to test different scenarios."}
{"task_id": 124, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Noisy Top-K Gating Function** as described:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K Gating mechanism used in Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    - X: Input matrix of shape (batch_size, input_dim)\n    - W_g: Gating weight matrix of shape (input_dim, num_experts)\n    - W_noise: Noise weight matrix of shape (input_dim, num_experts)\n    - N: Pre-sampled noise matrix of shape (batch_size, num_experts)\n    - k: Sparsity constraint (number of top experts to select per input)\n    \n    Returns:\n    - gate_output: Final gating probabilities matrix of shape (batch_size, num_experts)\n    \"\"\"\n    batch_size, input_dim = X.shape\n    _, num_experts = W_g.shape\n\n    # Step 1: Compute raw logits using the gating weights\n    logits = X @ W_g  # Shape: (batch_size, num_experts)\n\n    # Step 2: Add noise using the noise weights and pre-sampled noise\n    noise_logits = X @ W_noise  # Shape: (batch_size, num_experts)\n    noisy_logits = logits + N * noise_logits  # Shape: (batch_size, num_experts)\n\n    # Step 3: Compute temperature-scaled softmax\n    temperatures = 1.0 / np.sqrt(W_g.shape[0])  # Temperature scaling\n    soft_logits = noisy_logits * temperatures  # Shape: (batch_size, num_experts)\n    soft_probs = np.softmax(soft_logits, axis=1)  # Shape: (batch_size, num_experts)\n\n    # Step 4: Compute top-k indices and values\n    topk_values, topk_indices = [], []\n    for i in range(batch_size):\n        values, indices = zip(*sorted(zip(soft_logits[i], np.arange(num_experts)), reverse=True)[:k])\n        topk_values.append(values)\n        topk_indices.append(indices)\n    topk_values = np.array(topk_values)  # Shape: (batch_size, k)\n    topk_indices = np.array(topk_indices)  # Shape: (batch_size, k)\n\n    # Step 5: Create a mask for top-k experts\n    gate_output = np.zeros((batch_size, num_experts))\n    for i in range(batch_size):\n        gate_output[i, topk_indices[i]] = topk_values[i]\n\n    # Normalize the gate output to sum to 1 across experts\n    gate_output /= np.sum(gate_output, axis=1, keepdims=True)\n\n    # Round to 4 decimal places and convert to list\n    gate_output = np.round(gate_output, 4)\n    return gate_output.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    W_g = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    W_noise = np.array([[0.05, 0.1, 0.15], [0.2, 0.25, 0.3]])\n    N = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    k = 2\n\n    result = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(\"Gating Output:\", result)\n```\n\n### Explanation of the Steps:\n1. **Logits Computation**: Compute the raw logits using the input and the gating weights.\n2. **Noise Addition**: Add noise scaled by the noise weights and pre-sampled noise matrix.\n3. **Softmax with Temperature**: Apply softmax after scaling logits with a temperature.\n4. **Top-k Selection**: Select the top-k experts for each input.\n5. **Output Normalization**: Normalize the selected top-k probabilities to sum to 1.\n6. **Rounding and Conversion**: Round to 4 decimal places and convert to a Python list.\n\nThis script is ready to use and test with your own inputs."}
{"task_id": 125, "completion_id": 0, "completion": "Here's a self-contained Python script that implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing:\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer with top-k routing.\n\n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weight matrices of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weight matrix of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to select per token\n\n    Returns:\n    - Output tensor of shape (batch_size, output_dim)\n    \"\"\"\n    batch_size, input_dim = x.shape\n    _, _, output_dim = We.shape\n\n    # Step 1: Compute gate scores\n    gate_scores = x @ Wg  # Shape: (batch_size, n_experts)\n\n    # Step 2: Select top-k experts per token\n    top_k_indices = np.argsort(gate_scores, axis=1)[:, -top_k:]  # Shape: (batch_size, top_k)\n    top_k_scores = np.take_along_axis(gate_scores, top_k_indices, axis=1)  # Shape: (batch_size, top_k)\n\n    # Step 3: Apply softmax to normalize gate scores\n    top_k_softmax = np.exp(top_k_scores - np.max(top_k_scores, axis=1, keepdims=True))\n    top_k_softmax /= np.sum(top_k_softmax, axis=1, keepdims=True)  # Shape: (batch_size, top_k)\n\n    # Step 4: Apply expert transformations\n    expert_outputs = np.zeros((batch_size, output_dim))\n    for i in range(batch_size):\n        for j in range(top_k):\n            expert_idx = top_k_indices[i, j]\n            expert_output = x[i] @ We[expert_idx]  # Shape: (output_dim,)\n            expert_outputs[i] += top_k_softmax[i, j] * expert_output\n\n    # Step 5: Round and convert to list\n    expert_outputs = np.round(expert_outputs, 4)\n    return expert_outputs.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input\n    x = np.array([[1.0, 2.0], [3.0, 4.0]])  # (batch_size=2, input_dim=2)\n    We = np.array([[[0.1, 0.2], [0.3, 0.4]],  # Expert 0\n                   [[0.5, 0.6], [0.7, 0.8]],  # Expert 1\n                   [[0.9, 1.0], [1.1, 1.2]]]) # Expert 2\n    Wg = np.array([[0.1, 0.2, 0.3],  # Gating weights\n                   [0.4, 0.5, 0.6]])  # (input_dim=2, n_experts=3)\n    n_experts = 3\n    top_k = 2\n\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(\"MoE Output:\", output)\n```\n\n### Explanation:\n- **Gating**: The input is passed through a gate (linear transformation with softmax) to compute attention scores for each expert.\n- **Top-k Routing**: Only the top-k experts with the highest gate scores are selected for each input token.\n- **Aggregation**: The outputs of the selected experts are weighted by their normalized gate scores and summed to produce the final output.\n\nThis script is fully self-contained and uses only NumPy."}
{"task_id": 126, "completion_id": 0, "completion": "Here's a self-contained Python script that implements **Group Normalization** for a 4D input tensor with shape `(B, C, H, W)`. The function normalizes over smaller groups of channels, then applies learned scale (`gamma`) and shift (`beta`).\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameter of shape (C,)\n    - beta: Learnable shift parameter of shape (C,)\n    - num_groups: Number of groups to divide the channels into\n    - epsilon: Small value for numerical stability\n\n    Returns:\n    - Normalized output as a Python list (rounded to 4 decimal places)\n    \"\"\"\n    B, C, H, W = X.shape\n    assert C % num_groups == 0, \"Number of channels must be divisible by num_groups\"\n\n    # Reshape X to group channels\n    X_grouped = X.reshape(B, num_groups, C // num_groups, H, W)\n\n    # Compute mean and variance for each group\n    mean = np.mean(X_grouped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_grouped, axis=(2, 3, 4), keepdims=True)\n\n    # Normalize\n    X_normalized = (X_grouped - mean) / np.sqrt(var + epsilon)\n\n    # Reshape back to original shape\n    X_normalized = X_normalized.reshape(B, C, H, W)\n\n    # Apply gamma and beta\n    X_output = gamma * X_normalized + beta\n\n    # Round to 4 decimal places and convert to list\n    return np.round(X_output, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    X = np.random.randn(2, 4, 3, 3)  # (B=2, C=4, H=3, W=3)\n    gamma = np.random.randn(4)       # (C=4,)\n    beta = np.random.randn(4)        # (C=4,)\n    num_groups = 2\n\n    # Apply group normalization\n    output = group_normalization(X, gamma, beta, num_groups)\n    print(\"Group Normalization Output:\")\n    print(output)\n```\n\n### Explanation:\n- **Group Normalization** divides the `C` channels into `num_groups` groups.\n- For each group, it computes the mean and variance across the spatial dimensions `(H, W)`.\n- Then it normalizes the data using the computed statistics.\n- Finally, it applies the learnable parameters `gamma` and `beta` for scaling and shifting.\n- The result is rounded to 4 decimal places and returned as a Python list using `tolist()`.\n\nThis script is self-contained and can be run directly."}
{"task_id": 127, "completion_id": 0, "completion": "Here's a self-contained Python script that uses **gradient descent** to find the minimum of the function $ f(x) = x^4 - 3x^3 + 2 $, starting from a given `start_x`. The algorithm iteratively updates the value of `x` in the direction of the negative gradient (i.e., the derivative of the function) until it converges to the minimum.\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    def f(x):\n        return x**4 - 3*x**3 + 2\n\n    def df(x):\n        return 4*x**3 - 9*x**2\n\n    x = start_x\n    for _ in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n\n    return round(x, 4)\n\n# Example usage:\nif __name__ == \"__main__\":\n    start_x = np.random.rand() * 10  # Random starting point\n    treasure_x = find_treasure(start_x)\n    print(f\"The treasure is hidden at x = {treasure_x}\")\n```\n\n### Explanation:\n- **`f(x)`**: The function representing the height of the land.\n- **`df(x)`**: The derivative of `f(x)`, used to compute the gradient.\n- **Gradient Descent**: We update `x` using the formula:  \n  $$\n  x_{\\text{new}} = x - \\text{learning_rate} \\cdot f'(x)\n  $$\n- **Stopping Condition**: The loop stops when the change in `x` is smaller than `tolerance` or after `max_iters` iterations.\n- **Rounding**: The final result is rounded to 4 decimal places as required.\n\nYou can run this script in any Python environment. The random starting point ensures the algorithm can find the global minimum even if it starts near a local minimum."}
{"task_id": 128, "completion_id": 0, "completion": "Here's a self-contained Python script that implements the **Dynamic Tanh (DyT)** function as described. The function replaces layer normalization in Transformer architectures and is designed to preserve squashing behavior while enabling stable training.\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function: Normalization-free transformation inspired by Tanh.\n    \n    Parameters:\n    - x: Input array (np.ndarray)\n    - alpha: Scaling factor for the input\n    - gamma: Scaling factor for the output\n    - beta: Shift factor for the output\n    \n    Returns:\n    - Python list of transformed values, rounded to 4 decimal places.\n    \"\"\"\n    # Apply Dynamic Tanh transformation\n    x_scaled = x * alpha\n    tanh_output = np.tanh(x_scaled)\n    output = gamma * tanh_output + beta\n    \n    # Round to 4 decimal places and convert to list\n    output_rounded = np.round(output, 4)\n    return output_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    x = np.array([1.0, 2.0, 3.0, -1.0, -2.0])\n    alpha = 0.5\n    gamma = 1.2\n    beta = 0.1\n\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    print(\"Dynamic Tanh Output:\", result)\n```\n\n### Explanation:\n- **`alpha`** scales the input before applying the `tanh` function.\n- **`gamma`** and **`beta`** are learnable parameters that scale and shift the output of the `tanh` function, similar to affine transformations in normalization layers.\n- The output is rounded to **4 decimal places** and returned as a **Python list** using `tolist()`.\n\nThis implementation is **normalization-free**, meaning it avoids using layer normalization while still maintaining the squashing behavior of the `tanh` function."}
