{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float: The resulting vector if the operation is valid, or -1 if the dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int|float): The input matrix to be transposed.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given matrix a into the specified shape new_shape.\n    If the reshape operation is not possible, return an empty list.\n    \n    :param a: List of lists representing the matrix to reshape.\n    :param new_shape: A tuple representing the desired new shape (rows, columns).\n    :return: The reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    a_np = np.array(a)\n    if a_np.size == new_shape[0] * new_shape[1]:\n        reshaped_np = a_np.reshape(new_shape)\n        return reshaped_np.tolist()\n    else:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists of floats representing the matrix.\n    :param mode: A string indicating the mode of calculation, either 'row' or 'column'.\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        return [np.mean(row) for row in matrix]\n    elif mode == 'column':\n        transposed_matrix = list(zip(*matrix))\n        return [np.mean(column) for column in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a given matrix by a scalar value.\n\n    :param matrix: A list of lists, where each inner list represents a row of the matrix.\n    :param scalar: A number by which each element of the matrix will be multiplied.\n    :return: A new matrix with each element multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: A list of eigenvalues sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    (eigenvalues, _) = np.linalg.eig(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return [float(val) for val in sorted_eigenvalues]\nmatrix = [[4, 2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        np.linalg.inv(T_np)\n        np.linalg.inv(S_np)\n        transformed_matrix = np.linalg.inv(T_np) @ A_np @ S_np\n        transformed_matrix = np.round(transformed_matrix, 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 4]]"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of two lists, each containing two elements.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it exists, otherwise None.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n    \n    Parameters:\n    a (list of list of int/float): The first matrix.\n    b (list of list of int/float): The second matrix.\n    \n    Returns:\n    list of list of int/float: The resulting matrix if multiplication is possible.\n    int: -1 if the matrices cannot be multiplied.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors: A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    A list of lists representing the covariance matrix.\n    \"\"\"\n    data = np.array(vectors)\n    cov_matrix = np.cov(data, bias=True)\n    return cov_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Constant terms.\n    n (int): Number of iterations.\n    \n    Returns:\n    list: The approximate solution vector x as a list.\n    \"\"\"\n    N = len(b)\n    x = np.zeros_like(b)\n    for it_count in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[4, -1, 0, 3], [1, 15.5, 3, 8], [0, -1.3, -4, 1.1], [14, 5, -2, 30]])\nb = np.array([1, 1, 1, 1])\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2x2 matrix using the Jacobian method.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 matrix.\n    \n    Returns:\n    tuple: A tuple containing the singular values rounded to the nearest 4th decimal.\n    \"\"\"\n    if A.shape != (2, 2):\n        raise ValueError('The input matrix must be 2x2.')\n    ATA = np.dot(A.T, A)\n    eigenvalues = np.linalg.eigvals(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = np.sort(singular_values)[::-1]\n    singular_values = tuple((round(sv, 4) for sv in singular_values))\n    return singular_values\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        Args:\n        minor (list of list of int|float): A 3x3 matrix represented as a list of lists.\n        \n        Returns:\n        float: The determinant of the 3x3 matrix.\n        \"\"\"\n        return minor[0][0] * (minor[1][1] * minor[2][2] - minor[1][2] * minor[2][1]) - minor[0][1] * (minor[1][0] * minor[2][2] - minor[1][2] * minor[2][0]) + minor[0][2] * (minor[1][0] * minor[2][1] - minor[1][1] * minor[2][0])\n\n    def get_minor(m: list[list[int | float]], i: int, j: int) -> list[list[int | float]]:\n        \"\"\"\n        Returns the minor matrix by removing the ith row and jth column from the matrix m.\n        \n        Args:\n        m (list of list of int|float): The original matrix.\n        i (int): The row index to remove.\n        j (int): The column index to remove.\n        \n        Returns:\n        list of list of int|float: The minor matrix.\n        \"\"\"\n        return [row[:j] + row[j + 1:] for row in m[:i] + m[i + 1:]]\n    det = 0\n    for j in range(4):\n        det += (-1) ** j * matrix[0][j] * determinant_3x3(get_minor(matrix, 0, j))\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    X (list of list of float): The feature matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    theta_rounded = np.round(theta, 4).tolist()\n    return theta_rounded\nX = [[1, 2], [2, 3], [4, 5]]\ny = [2, 3, 5]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix including a column of ones for the intercept.\n    y (np.ndarray): The target vector.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to perform.\n    \n    Returns:\n    list: The coefficients of the linear regression model.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / m\n        theta -= alpha * gradient\n    theta = np.round(theta, 4)\n    return theta.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array and returns two 2D lists: one scaled by standardization and one by min-max normalization.\n    Each value in the output lists is rounded to the nearest 4th decimal.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_max_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_data = np.round(min_max_data, 4).tolist()\n    return (standardized_data, min_max_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a list of points.\n    \n    Parameters:\n    points (list of tuples): A list of points, where each point is a tuple of coordinates.\n    k (int): The number of clusters to form.\n    initial_centroids (list of tuples): A list of initial centroid points, each a tuple of coordinates.\n    max_iterations (int): The maximum number of iterations to perform.\n    \n    Returns:\n    list of tuples: A list of the final centroids of the clusters, each rounded to the nearest fourth decimal.\n    \"\"\"\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_indices = np.argmin(distances, axis=0)\n        for i in range(k):\n            if np.any(closest_centroid_indices == i):\n                centroids_array[i] = points_array[closest_centroid_indices == i].mean(axis=0)\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Perform K-Fold Cross-Validation on the given dataset.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix of the dataset.\n    y (np.ndarray): The target vector of the dataset.\n    k (int): Number of folds for cross-validation.\n    shuffle (bool): Whether to shuffle the data before splitting into folds.\n    random_seed (int): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples, each containing train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    extra = X.shape[0] % k\n    start = 0\n    splits = []\n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        test_idx = indices[start:end]\n        train_idx = np.concatenate([indices[:start], indices[end:]])\n        splits.append((train_idx, test_idx))\n        start = end\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on a given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists, where each inner list represents a principal component (eigenvector).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    return [list(map(lambda x: round(x, 4), vec)) for vec in principal_components.T]"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree from the given examples and attributes.\n    \n    :param examples: A list of examples, each example is a dictionary of attribute-value pairs.\n    :param attributes: A list of attribute names.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples, target_attr):\n        \"\"\"Calculate the entropy of the target attribute in the examples.\"\"\"\n        counts = Counter([example[target_attr] for example in examples])\n        total = len(examples)\n        return -sum((count / total * math.log2(count / total) for count in counts.values()))\n\n    def information_gain(examples, attribute, target_attr):\n        \"\"\"Calculate the information gain of splitting on the given attribute.\"\"\"\n        base_entropy = entropy(examples, target_attr)\n        attribute_values = set([example[attribute] for example in examples])\n        weighted_entropy = 0\n        for value in attribute_values:\n            subset = [example for example in examples if example[attribute] == value]\n            weighted_entropy += len(subset) / len(examples) * entropy(subset, target_attr)\n        return base_entropy - weighted_entropy\n\n    def split_on_attribute(examples, attribute):\n        \"\"\"Split the examples based on the attribute values.\"\"\"\n        subsets = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in subsets:\n                subsets[value] = []\n            subsets[value].append(example)\n        return subsets\n\n    def most_common_value(examples, target_attr):\n        \"\"\"Find the most common value of the target attribute in the examples.\"\"\"\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    if not examples or not attributes:\n        return most_common_value(examples, target_attr)\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    best_attribute = max(attributes, key=lambda attribute: information_gain(examples, attribute, target_attr))\n    tree = {best_attribute: {}}\n    subsets = split_on_attribute(examples, best_attribute)\n    for (value, subset) in subsets.items():\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'windy': 'strong', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    n_samples = data.shape[0]\n    kernel_matrix = np.zeros((n_samples, n_samples))\n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                kernel_matrix[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\n    return kernel_matrix\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            pred = np.sum(alphas * labels * K[:, i]) + b\n            if labels[i] * pred < 1:\n                alphas = (1 - lambda_t * lambda_val) * alphas + lambda_t * labels[i] * K[:, i]\n            else:\n                alphas = (1 - lambda_t * lambda_val) * alphas\n        b = np.mean(labels - np.dot(K, alphas * labels))\n    alphas = np.round(alphas, 4)\n    return (alphas.tolist(), np.round(b, 4))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of the given list of scores.\n    \n    Args:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: A list of softmax values for each score, rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_scores"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with sigmoid activation for binary classification.\n    \n    Args:\n    features: A list of feature vectors for each example.\n    labels: True binary labels for each example.\n    weights: Weights for each feature.\n    bias: Bias term for the neuron.\n    \n    Returns:\n    A tuple containing:\n    - A list of predicted probabilities after sigmoid activation.\n    - The mean squared error between the predicted probabilities and true labels.\n    Both results are rounded to four decimal places.\n    \"\"\"\n    features_np = np.array(features)\n    labels_np = np.array(labels)\n    weights_np = np.array(weights)\n    z = np.dot(features_np, weights_np) + bias\n    predictions = np.array([sigmoid(x) for x in z])\n    mse = np.mean((predictions - labels_np) ** 2)\n    predictions_rounded = np.round(predictions, 4).tolist()\n    mse_rounded = np.round(mse, 4)\n    return (predictions_rounded, mse_rounded)\nfeatures = [[0, 0], [0, 1], [1, 0], [1, 1]]\nlabels = [0, 0, 0, 1]\nweights = [0.5, 0.5]\nbias = 0.0"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        mse = mse_loss(labels, predictions)\n        mse_values.append(round(mse, 4))\n        error = labels - predictions\n        adjustments = error * sigmoid_derivative(predictions)\n        weights += learning_rate * np.dot(features.T, adjustments)\n        bias += learning_rate * np.sum(adjustments)\n    return (weights.tolist(), round(bias, 4), mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([[0], [1], [1], [0]])\ninitial_weights = np.array([0.5, 0.5])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in basis B.\n    C (list of list of int): Basis vectors in basis C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from B to C.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    C_inv = np.linalg.inv(C_np)\n    P = np.dot(C_inv, B_np)\n    P_rounded = np.round(P, 4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the SVD of a 2x2 matrix A using eigenvalues and eigenvectors.\n    Returns U, S, V where A = U * S * V.T.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    S = np.sqrt(eigenvalues)\n    S = np.diag(np.round(S, decimals=4))\n    V = np.round(eigenvectors, decimals=4)\n    U = np.zeros_like(A)\n    for i in range(2):\n        if S[i, i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / S[i, i]\n        else:\n            U[:, i] = V[:, i]\n    U = np.round(U, decimals=4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in two numpy arrays, X and y, maintaining the corresponding order.\n    \n    Parameters:\n    - X: numpy array, first array to be shuffled.\n    - y: numpy array, second array to be shuffled.\n    - seed: int, optional random seed for reproducibility.\n    \n    Returns:\n    - Shuffled X and y arrays as python lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterator that yields batches of X and optionally y.\n    \n    Parameters:\n    X (np.ndarray): Input data.\n    y (np.ndarray, optional): Target data. Defaults to None.\n    batch_size (int): The size of each batch. Defaults to 64.\n    \n    Yields:\n    tuple or list: If y is provided, yields a tuple (batch_X, batch_y),\n                   otherwise yields a list batch_X.\n    \"\"\"\n    assert type(X) == np.ndarray, 'X must be a numpy array'\n    assert y is None or type(y) == np.ndarray, 'y must be a numpy array or None'\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_X = X[i:i + batch_size].tolist()\n        if y is not None:\n            batch_y = y[i:i + batch_size].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset based on whether the value of the specified feature is\n    greater than or equal to the given threshold.\n    \n    :param X: numpy.ndarray, the dataset to be divided.\n    :param feature_i: int, index of the feature to be used for division.\n    :param threshold: float, the threshold value for division.\n    :return: tuple, two lists representing the divided datasets.\n    \"\"\"\n    if not isinstance(X, np.ndarray):\n        raise ValueError('Input dataset must be a numpy array.')\n    if feature_i >= X.shape[1] or feature_i < 0:\n        raise ValueError('Feature index is out of bounds.')\n    split_func = lambda sample: sample[feature_i] >= threshold\n    X_1 = np.array([sample for sample in X if split_func(sample)]).tolist()\n    X_2 = np.array([sample for sample in X if not split_func(sample)]).tolist()\n    return (X_1, X_2)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    Parameters:\n    X : np.ndarray\n        The input samples with shape (n_samples, n_features).\n    degree : int\n        The degree of the polynomial features.\n        \n    Returns:\n    np.ndarray\n        The matrix of features, where columns are powers of the original features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    indices = np.arange(n_features)\n    combos = [combinations_with_replacement(indices, i) for i in range(0, degree + 1)]\n    flat_combos = [item for sublist in combos for item in sublist]\n    n_output_features = len(flat_combos)\n    poly_features = np.empty((n_samples, n_output_features))\n    for (i, combo) in enumerate(flat_combos):\n        poly_features[:, i] = np.prod(X[:, combo], axis=1)\n    return poly_features.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the dataset features.\n    y (np.ndarray): 1D numpy array representing the dataset labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be determined from the input array.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): 1D numpy array\n    \n    Returns:\n    list: 2D list representing the diagonal matrix\n    \"\"\"\n    x = np.array(x)\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): The true labels as a 1D numpy array.\n    y_pred (numpy.ndarray): The predicted labels as a 1D numpy array.\n    \n    Returns:\n    float: The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('y_true and y_pred must be of type numpy.ndarray')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 4])"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array\n    - Y: Optional 2D numpy array. If not provided, the correlation matrix of X with itself is calculated.\n    \n    Returns:\n    - A 2D numpy array representing the correlation matrix, rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    std_X = np.std(X, axis=0, keepdims=True).T\n    std_Y = np.std(Y, axis=0, keepdims=True)\n    std_matrix = np.dot(std_X, std_Y)\n    std_matrix[std_matrix == 0] = 1\n    correlation_matrix = cov_matrix / std_matrix\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nX = np.array([[1, 2], [3, 4], [5, 6]])\nY = np.array([[7, 8], [9, 10], [11, 12]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = None\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.where(X[:, feature] > threshold, 1, -1)\n                error = np.sum(weights[y != pred])\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'pred': pred}\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n        weights *= np.exp(-alpha * y * best_clf['pred'])\n        weights /= np.sum(weights)\n        best_clf['alpha'] = alpha\n        classifiers.append(best_clf)\n    return classifiers\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\ny = np.array([1, 1, -1, -1, -1])\nn_clf = 2\nclassifiers = adaboost_fit(X, y, n_clf)"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax function is defined as:\n    log_softmax(x_i) = x_i - log(sum(exp(x_j) for all j))\n    \n    Parameters:\n    scores (list): A list of numbers representing scores.\n    \n    Returns:\n    list: A list of numbers representing the log-softmax of the input scores.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    scores -= max_score\n    exp_scores = np.exp(scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_sum_exp_scores = np.log(sum_exp_scores)\n    log_softmax_scores = scores - log_sum_exp_scores\n    log_softmax_scores = np.round(log_softmax_scores, 4).tolist()\n    return log_softmax_scores"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = self.layer_input.T.dot(accum_grad)\n        self.optimizer_W.update(self.W, W_grad)\n        self.optimizer_w0.update(self.w0, w0_grad)\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution on the input_matrix using the given kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix to apply the convolution on.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The amount of padding to apply to the input matrix.\n    stride (int): The stride length for the convolution.\n    \n    Returns:\n    list: The result of the convolution as a list.\n    \"\"\"\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (input_height, input_width) = padded_input.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            row = i * stride\n            col = j * stride\n            output_matrix[i, j] = np.sum(padded_input[row:row + kernel_height, col:col + kernel_width] * kernel)\n    output_matrix = np.round(output_matrix, decimals=4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given the feature matrix, coefficients,\n    true labels, and regularization parameter.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The calculated Ridge loss.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    ridge_loss = mse + reg_term\n    return round(ridge_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value to the function.\n    - alpha (float): The slope for negative inputs. Default is 0.01.\n    \n    Returns:\n    - float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    precision_value = true_positives / predicted_positives\n    return precision_value\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using Mean Squared Error (MSE) as the loss function.\n    \n    :param X: numpy array of shape (n_samples, n_features), input data\n    :param y: numpy array of shape (n_samples,), target values\n    :param weights: numpy array of shape (n_features,), initial weights\n    :param learning_rate: float, the learning rate for gradient descent\n    :param n_iterations: int, number of iterations for gradient descent\n    :param batch_size: int, size of the batch for mini-batch gradient descent\n    :param method: str, method to use for gradient descent ('batch', 'stochastic', 'mini-batch')\n    :return: list, final weights after performing gradient descent\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                xi = X[i]\n                yi = y[i]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi * error\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                predictions = np.dot(X_batch, weights)\n                errors = predictions - y_batch\n                gradient = 2 * np.dot(X_batch.T, errors) / batch_size\n                weights -= learning_rate * gradient\n        else:\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradient = 2 * np.dot(X.T, errors) / n_samples\n            weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list of lists: The matrix in RREF form.\n    \"\"\"\n    mat = np.array(matrix, dtype=float)\n    (rows, cols) = mat.shape\n    row = 0\n    for col in range(cols):\n        if row >= rows:\n            break\n        pivot_row = np.argmax(np.abs(mat[row:, col])) + row\n        if mat[pivot_row, col] == 0:\n            continue\n        mat[[row, pivot_row]] = mat[[pivot_row, row]]\n        mat[row] = mat[row] / mat[row, col]\n        for other_row in range(rows):\n            if other_row != row:\n                mat[other_row] = mat[other_row] - mat[other_row, col] * mat[row]\n        row += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm to update the parameters of a given function.\n    \n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (np.array): Initial parameter values.\n    learning_rate (float): The step size.\n    beta1 (float): Exponential decay rate for the first moment estimates.\n    beta2 (float): Exponential decay rate for the second moment estimates.\n    epsilon (float): A small constant for numerical stability.\n    num_iterations (int): Number of iterations to run the optimizer.\n    \n    Returns:\n    list: Optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Perform Lasso Regression using Gradient Descent.\n    \n    Parameters:\n    X (np.array): Input features.\n    y (np.array): Target values.\n    alpha (float): Regularization parameter.\n    learning_rate (float): Learning rate for gradient descent.\n    max_iter (int): Maximum number of iterations.\n    tol (float): Tolerance for stopping criteria.\n    \n    Returns:\n    tuple: A tuple containing the learned weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -2 / n_samples * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)\n        db = -2 / n_samples * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        cost = 1 / (2 * n_samples) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    weights = np.round(weights, 4).tolist()\n    bias = np.round(bias, 4)\n    return (weights, bias)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of edits (insert, delete, substitute, transpose) required\n    to transform one string into another.\n    \n    :param source: The source string.\n    :param target: The target string.\n    :return: The OSA distance between the source and target strings.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    distance_matrix = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        distance_matrix[i][0] = i\n    for j in range(len_target + 1):\n        distance_matrix[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            distance_matrix[i][j] = min(distance_matrix[i - 1][j] + 1, distance_matrix[i][j - 1] + 1, distance_matrix[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                distance_matrix[i][j] = min(distance_matrix[i][j], distance_matrix[i - 2][j - 2] + cost)\n    return distance_matrix[len_source][len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    y_true (list): The true binary labels.\n    y_pred (list): The predicted binary labels.\n    \n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)\ny_true = [0, 1, 0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 1, 1, 0, 1, 0, 0]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): The input sequence of shape (seq_length, d_model).\n    W_q (numpy.ndarray): The weight matrix for query, of shape (d_model, d_k).\n    W_k (numpy.ndarray): The weight matrix for key, of shape (d_model, d_k).\n    W_v (numpy.ndarray): The weight matrix for value, of shape (d_model, d_v).\n    \n    Returns:\n    numpy.ndarray: The self-attention output of shape (seq_length, d_v).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(W_q.shape[1])\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    output = np.dot(attention_weights, V)\n    return np.round(output, 4).tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to process a sequence of input vectors and produce the final hidden state.\n    \n    Parameters:\n    input_sequence (list of list of float): Sequence of input vectors.\n    initial_hidden_state (list of float): Initial hidden state.\n    Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    b (list of float): Bias vector.\n    \n    Returns:\n    list of float: Final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    hidden_state = initial_hidden_state\n    for x in input_sequence:\n        hidden_state = np.tanh(np.dot(x, Wx) + np.dot(hidden_state, Wh) + b)\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of 2D points by tx and ty in the x and y directions, respectively.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of points.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_with_ones = np.hstack((points_array, ones))\n    translated_points = np.dot(translation_matrix, points_with_ones.T).T\n    translated_points_list = translated_points[:, :-1].tolist()\n    return translated_points_list\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of distribution P\n    - sigma_p: Standard deviation of distribution P\n    - mu_q: Mean of distribution Q\n    - sigma_q: Standard deviation of distribution Q\n    \n    Returns:\n    - kl_div: KL divergence between P and Q\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a linear system Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (np.array, optional): Initial guess for the solution vector. Defaults to None.\n    \n    Returns:\n    list: The approximated solution vector x after n iterations.\n    \"\"\"\n    if x_ini is None:\n        x = np.zeros_like(b)\n    else:\n        x = x_ini.copy()\n    for _ in range(n):\n        for i in range(A.shape[0]):\n            s = 0\n            for j in range(A.shape[1]):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 100\nx_ini = np.array([0.0, 0.0, 0.0, 0.0])"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of the system.\n    b (np.array): Constant terms of the system.\n    \n    Returns:\n    list: Solution vector x as a list.\n    \"\"\"\n    n = len(A)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        pivot_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        pivot = Ab[i, i]\n        for row in range(i + 1, n):\n            factor = Ab[row, i] / pivot\n            Ab[row, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n        for t in range(len(x)):\n            combined = np.vstack((hidden_state, x[t]))\n            forget_gate = self.sigmoid(self.Wf @ combined + self.bf)\n            input_gate = self.sigmoid(self.Wi @ combined + self.bi)\n            candidate_cell_state = self.tanh(self.Wc @ combined + self.bc)\n            output_gate = self.sigmoid(self.Wo @ combined + self.bo)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = hidden_state.round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_size = 3\nhidden_size = 2\nx = [np.array([1, 2, 3]).reshape(-1, 1) for _ in range(5)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty.')\n\n    def term_frequency(doc, term):\n        return doc.count(term) / len(doc)\n\n    def inverse_document_frequency(corpus, term):\n        df = sum((1 for doc in corpus if term in doc)) + 1\n        return math.log(len(corpus) / df) + 1\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            tf = term_frequency(doc, term)\n            idf = inverse_document_frequency(corpus, term)\n            tf_idf = tf * idf\n            doc_scores.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_scores)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'lazy', 'dog'], ['the', 'quick', 'dog'], ['the', 'lazy', 'fox']]\nquery = ['the', 'quick', 'lazy', 'fox', 'dog']\ntf_idf_scores = compute_tf_idf(corpus, query)"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision + recall == 0:\n        f_beta = 0\n    else:\n        f_beta = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hidden_states = np.zeros((T + 1, self.hidden_size, 1))\n        outputs = np.zeros((T, self.W_hy.shape[0], 1))\n        last_inputs = np.zeros((T, self.hidden_size, 1))\n        for t in range(T):\n            x = input_sequence[t].reshape(-1, 1)\n            hidden_states[t + 1] = np.tanh(self.W_xh @ x + self.W_hh @ hidden_states[t] + self.b_h)\n            outputs[t] = self.W_hy @ hidden_states[t + 1] + self.b_y\n            last_inputs[t] = x\n        return (outputs, last_inputs, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        for t in reversed(range(T)):\n            x = last_inputs[t]\n            h = hidden_states[t + 1]\n            h_prev = hidden_states[t]\n            dy = outputs[t] - expected_output[t].reshape(-1, 1)\n            dW_hy += dy @ h.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h ** 2) * dh\n            dW_xh += dh_raw @ x.T\n            dW_hh += dh_raw @ h_prev.T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, decimals=8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\nx = conjugate_gradient(A, b, n)"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    class_counts = np.bincount(y)\n    probabilities = class_counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, col_indices, row_pointers)\ndense_matrix = [[1, 0, 0, 2], [0, 3, 0, 0], [4, 0, 5, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    vector_projection = scalar_projection * L\n    projection_rounded = np.round(vector_projection, 3)\n    return projection_rounded.tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[0, 2, 0, 0], [3, 0, 0, 1], [0, 0, 4, 0], [5, 0, 0, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n    Returns the basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix and correspond to the independent columns.\n    \"\"\"\n    A = np.array(A)\n    (A_rref, pivot_columns) = row_echelon_form(A)\n    basis_vectors = A[:, pivot_columns]\n    return np.round(basis_vectors, 8).tolist()\ndef row_echelon_form(A):\n    \"\"\"\n    Computes the row echelon form of matrix A and returns the pivot columns.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (_, num_cols) = A.shape\n    lead = 0\n    row_count = len(A)\n    for r in range(row_count):\n        if lead >= num_cols:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == row_count:\n                i = r\n                lead += 1\n                if num_cols == lead:\n                    return (A, np.array(range(lead)))\n        A[[i, r]] = A[[r, i]]\n        A[r] = A[r] / A[r, lead]\n        for i in range(row_count):\n            if i != r:\n                A[i] = A[i] - A[i, lead] * A[r]\n        lead += 1\n    pivot_columns = np.where(np.max(np.abs(A), axis=0) > 1e-08)[0]\n    return (A, pivot_columns)\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression model.\n    \n    Parameters:\n    y_true (np.array): Array of true values.\n    y_pred (np.array): Array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of actual values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the input arrays have different shapes or are empty.\n    TypeError: If the input is not a numpy array.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    error = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    return round(error, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Args:\n    y_true (np.ndarray): True binary labels.\n    y_pred (np.ndarray): Predicted binary labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    elements_true = np.sum(y_true)\n    elements_pred = np.sum(y_pred)\n    if elements_true == 0 and elements_pred == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / (elements_true + elements_pred)\n    return round(dice, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_hypervector(dim, seed):\n    \"\"\"Create a random hypervector of a given dimension with a specific seed.\"\"\"\n    np.random.seed(seed)\n    hv = np.random.choice([-1, 1], size=dim)\n    return hv\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Bind two hypervectors by element-wise multiplication followed by normalization.\"\"\"\n    return hv1 * hv2\ndef bundle_hypervectors(hvs):\n    \"\"\"Bundle a list of hypervectors by summing them up and normalizing.\"\"\"\n    return np.sign(np.sum(hvs, axis=0))\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row using HDC.\"\"\"\n    feature_hvs = []\n    feature_name_seed = 42\n    feature_name_hv = create_hypervector(dim, feature_name_seed)\n    for (feature, value) in row.items():\n        value_hv = create_hypervector(dim, random_seeds[feature])\n        bound_hv = bind_hypervectors(feature_name_hv, value_hv)\n        feature_hvs.append(bound_hv)\n    composite_hv = bundle_hypervectors(feature_hvs)\n    return composite_hv.tolist()\nrow = {'feature1': 10, 'feature2': 20, 'feature3': 30}\ndim = 1000\nrandom_seeds = {'feature1': 1, 'feature2': 2, 'feature3': 3}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    Parameters:\n    data (List[List[int]]): A list of lists, where each inner list represents a pair [y_true, y_pred] for one observation.\n    \n    Returns:\n    List[List[int]]: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    counts = Counter((tuple(pair) for pair in data))\n    true_positive = counts[1, 1]\n    true_negative = counts[0, 0]\n    false_positive = counts[0, 1]\n    false_negative = counts[1, 0]\n    matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors v1 and v2.\n    \n    Parameters:\n    v1 (np.array): First input vector.\n    v2 (np.array): Second input vector.\n    \n    Returns:\n    float: Cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        raise ValueError('Vectors cannot be empty or have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    return round(cos_sim, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = np.zeros((2, 2), dtype=int)\n    for (a, p) in zip(actual, predicted):\n        confusion_matrix[a, p] += 1\n    (tp, fp, fn, tn) = (confusion_matrix[1, 1], confusion_matrix[0, 1], confusion_matrix[1, 0], confusion_matrix[0, 0])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn > 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): A list or numpy array of numerical values.\n    \n    Returns:\n    dict: A dictionary containing the mean, median, mode, variance, standard deviation,\n          25th percentile, 50th percentile, 75th percentile, and interquartile range.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data, ddof=1).round(4)\n    standard_deviation = np.std(data, ddof=1).round(4)\n    percentile_25 = np.percentile(data, 25).round(4)\n    percentile_50 = np.percentile(data, 50).round(4)\n    percentile_75 = np.percentile(data, 75).round(4)\n    interquartile_range = (percentile_75 - percentile_25).round(4)\n    stats_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': interquartile_range}\n    return stats_dict\ndata = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input must be a 2D numpy array.')\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    poly = np.polynomial.Polynomial([0, 1])\n    transformed_data = [poly(data_point) ** degree for data_point in data_array]\n    result = [list(map(lambda x: round(x, 8), np.polynomial.polyutils.mapdomain([point], [-1, 1], [0, 1]))) for point in transformed_data]\n    return result"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encodings for a given sequence length and model dimensionality.\n    \n    Args:\n    position (int): The length of the sequence.\n    d_model (int): The dimensionality of the model.\n    \n    Returns:\n    list: A list of positional encodings, or -1 if input constraints are not met.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    if isinstance(parameter, np.ndarray):\n        return (parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())\n    else:\n        return (round(parameter, 5), round(m, 5), round(v, 5))\nparameter = np.array([1.0, 2.0])\ngrad = np.array([0.1, 0.2])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    token_embeddings = params['wte'][input_ids]\n    position_ids = np.arange(len(input_ids))\n    positional_embeddings = params['wpe'][position_ids]\n    x = token_embeddings + positional_embeddings\n    attention_weights = np.random.rand(hparams['n_head'], x.shape[0], x.shape[0])\n    attention_output = np.matmul(attention_weights, x)\n    feed_forward_output = np.matmul(attention_output, np.random.rand(x.shape[1], x.shape[1]))\n    mean = np.mean(feed_forward_output, axis=-1, keepdims=True)\n    var = np.var(feed_forward_output, axis=-1, keepdims=True)\n    x = (feed_forward_output - mean) / np.sqrt(var + 1e-05)\n    x = x * params['ln_f']['g'] + params['ln_f']['b']\n    generated_tokens = np.random.randint(0, len(encoder.encoder_dict), n_tokens_to_generate)\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    weights = np.random.rand(n, dimension)\n    values = np.array(crystal_values).reshape(-1, 1) * np.ones((n, dimension))\n    attention_scores = np.dot(values, weights.T)\n    attention_scores = softmax(attention_scores)\n    weighted_pattern = np.dot(attention_scores, values)\n    return [round(pattern, 4) for pattern in weighted_pattern.diagonal()]\nn = 4\ndimension = 2"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus based on the query.\n    \n    :param corpus: List of documents, where each document is a list of words.\n    :param query: List of words representing the query.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    avgdl = sum((len(doc) for doc in corpus)) / len(corpus)\n    idf_values = {}\n    for term in set(query):\n        df = sum((1 for doc in corpus if term in doc))\n        idf_values[term] = np.log((len(corpus) - df + 0.5) / (df + 0.5) + 1)\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = doc.count(term)\n                idf = idf_values[term]\n                doc_len = len(doc)\n                score += idf * (tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_len / avgdl)))\n        scores.append(score)\n    return [round(score, 3) for score in scores]\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'lazy', 'dog'], ['the', 'quick', 'dog'], ['the', 'lazy', 'fox', 'jumps']]\nquery = ['the', 'quick', 'fox']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = sum([1 for (i, j) in zip(y_true, y_pred) if i == j == 1])\n    fp = sum([1 for (i, j) in zip(y_true, y_pred) if i == 0 and j == 1])\n    fn = sum([1 for (i, j) in zip(y_true, y_pred) if i == 1 and j == 0])\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport scipy.stats as stats\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    X = np.array(range(1, len(consumption_data) + 1)).reshape((-1, 1))\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    final_consumption = math.ceil(consumption_day_15 * 1.05)\n    return final_consumption\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices from the input X using the weight matrices W_q, W_k, and W_v.\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention for a single head.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    head_dim = Q.shape[-1] // n_heads\n    Q_heads = np.split(Q, n_heads, axis=-1)\n    K_heads = np.split(K, n_heads, axis=-1)\n    V_heads = np.split(V, n_heads, axis=-1)\n    outputs = []\n    for (q, k, v) in zip(Q_heads, K_heads, V_heads):\n        output = self_attention(q, k, v, n_heads)\n        outputs.append(output)\n    concatenated_output = np.concatenate(outputs, axis=-1)\n    rounded_output = np.round(concatenated_output, 4)\n    return rounded_output.tolist()\nX = np.random.rand(3, 10)\nW_q = np.random.rand(10, 20)\nW_k = np.random.rand(10, 20)\nW_v = np.random.rand(10, 20)\nn_heads = 4"}
{"task_id": 95, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x)\n    y_array = np.array(y)\n    contingency_table = np.array([[np.sum((x_array == 0) & (y_array == 0)), np.sum((x_array == 0) & (y_array == 1))], [np.sum((x_array == 1) & (y_array == 0)), np.sum((x_array == 1) & (y_array == 1))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    phi = np.sqrt(chi2 / np.sum(contingency_table))\n    return round(phi, 4)\nx = [0, 0, 1, 1, 0, 1, 0, 1, 1, 0]\ny = [1, 0, 1, 0, 1, 0, 1, 0, 0, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    clipped_objective = np.minimum(rhos * A, clipped_rhos * A).mean()\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_objective_value = clipped_objective - beta * kl_divergence\n    return round(grpo_objective_value, 6)\nrhos = [1.1, 0.9, 1.0, 1.2, 0.8]\nA = [0.5, -0.3, 0.2, -0.1, 0.4]\npi_theta_old = [0.7, 0.3, 0.6, 0.4, 0.8]\npi_theta_ref = [0.6, 0.4, 0.5, 0.5, 0.7]\nepsilon = 0.2\nbeta = 0.01"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    B = np.zeros((n_features, n_classes))\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        y_one_hot = np.eye(n_classes)[y]\n        gradient = np.dot(X.T, probs - y_one_hot) / n_samples\n        B -= learning_rate * gradient\n        log_probs = -np.log(probs[range(n_samples), y])\n        loss = np.sum(log_probs) / n_samples\n        losses.append(round(float(loss), 4))\n    B = np.round(B, 4).tolist()\n    losses = np.round(losses, 4).tolist()\n    return (B, losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function to map predictions to probabilities.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.insert(X, 0, 1, axis=1)\n    weights = np.zeros(X.shape[1])\n    loss_values = []\n    for i in range(iterations):\n        predictions = sigmoid(np.dot(X, weights))\n        error = predictions - y\n        gradient = np.dot(X.T, error) / len(y)\n        weights -= learning_rate * gradient\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), loss_values)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    attention_scores += mask\n    attention_weights = np.exp(attention_scores)\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nfrom math import log\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is calculated using the Shannon entropy formula.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    entropy = -sum((count / total_apples * log(count / total_apples, 2) for count in color_counts.values()))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, seq_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) - scaling parameter\n    - beta: np.ndarray of shape (feature_dim,) - shifting parameter\n    - epsilon: float - small constant to avoid division by zero\n    \n    Returns:\n    - np.ndarray: Layer normalized tensor rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = X_norm * gamma + beta\n    return np.round(X_scaled, decimals=5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score for evaluating the quality of a candidate translation\n    against a reference translation.\n    \n    :param reference: A string representing the reference translation.\n    :param candidate: A string representing the candidate translation.\n    :param alpha: Parameter to control the weight of fragmentation penalty.\n    :param beta: Parameter to control the weight of the F-mean score.\n    :param gamma: Parameter to control the weight of the fragmentation penalty.\n    :return: The METEOR score as a float rounded to 3 decimal places.\n    \"\"\"\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n    matches = sum((Counter(reference_tokens) & Counter(candidate_tokens)).values())\n    precision = matches / len(candidate_tokens) if candidate_tokens else 0\n    recall = matches / len(reference_tokens) if reference_tokens else 0\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    ref_length = len(reference_tokens)\n    cand_length = len(candidate_tokens)\n    if ref_length == 0 or cand_length == 0:\n        fragmentation = 0\n    else:\n        fragmentation = max(0, 1 - gamma * (cand_length - matches) / ref_length)\n    meteor = (1 - alpha) * f_mean + alpha * fragmentation\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    :param joint_counts: int, the number of times both events x and y occur together.\n    :param total_counts_x: int, the total number of occurrences of event x.\n    :param total_counts_y: int, the total number of occurrences of event y.\n    :param total_samples: int, the total number of samples.\n    :return: float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0 or actual_joint_prob == 0:\n        return 0.0\n    pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    Parameters:\n    x (list[int]): A list of integer values to be normalized.\n    \n    Returns:\n    list[float]: A list of normalized float values, each rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0 for _ in x]\n    normalized_list = [(i - min_val) / (max_val - min_val) for i in x]\n    return [round(val, 4) for val in normalized_list]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Args:\n    x (np.ndarray): Input array of shape (batch_size, input_dim).\n    w1 (np.ndarray): Weight matrix for the first layer of shape (input_dim, hidden_dim).\n    w2 (np.ndarray): Weight matrix for the second layer of shape (hidden_dim, input_dim).\n    \n    Returns:\n    np.ndarray: Output array after passing through the residual block, rounded to 4 decimal places.\n    \"\"\"\n\n    def relu(z):\n        return np.maximum(0, z)\n    z1 = relu(np.dot(x, w1))\n    z2 = relu(np.dot(z1, w2))\n    output = relu(z2 + x)\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    - x: A 3D NumPy array of shape (height, width, channels)\n    \n    Returns:\n    - A 1D NumPy array of shape (channels,) where each element is the average\n      of all values in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies batch normalization to the input tensor X in BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch, channels, height, width).\n    gamma (np.ndarray): Scale parameter of shape (channels,).\n    beta (np.ndarray): Shift parameter of shape (channels,).\n    epsilon (float): Small value to prevent division by zero. Default is 1e-5.\n    \n    Returns:\n    np.ndarray: Normalized tensor rounded to 4 decimal places.\n    \"\"\"\n    (batch, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = X_norm * gamma.reshape(1, channels, 1, 1) + beta.reshape(1, channels, 1, 1)\n    return np.round(X_scaled, decimals=4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = c * n * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by the given list of 2D vectors using the Gram-Schmidt process.\n    \n    :param vectors: List of 2D vectors (each vector is a list of two floats).\n    :param tol: Tolerance value to determine linear independence.\n    :return: List of orthonormal vectors that span the same subspace.\n    \"\"\"\n    V = np.array(vectors)\n    basis = []\n    for v in V:\n        u = v\n        for w in basis:\n            u -= np.dot(v, w) * w\n        norm = np.linalg.norm(u)\n        if norm > tol:\n            u /= norm\n            basis.append(u.tolist())\n    basis = [np.round(np.array(b), 4).tolist() for b in basis]\n    return basis\nvectors = [[1, 2], [2, 1], [3, 3]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): The first 3D vector.\n    b (list/tuple): The second 3D vector.\n    \n    Returns:\n    list: The cross product vector rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cross_prod = np.cross(a_np, b_np)\n    return np.round(cross_prod, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.ndarray): A square coefficient matrix.\n    b (np.ndarray): A constant vector.\n    \n    Returns:\n    list: The solution vector x rounded to the nearest 4th decimal or -1 if no unique solution exists.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    if A.shape[0] != A.shape[1]:\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(A.shape[0]):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = [[3, 2], [1, -1]]\nb = [7, 1]"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two probability distributions p and q.\n    If the inputs have different lengths or are empty, returns 0.0.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = sum((np.sqrt(pi * qi) for (pi, qi) in zip(p, q)))\n    bd = -np.log(bc)\n    return round(bd, 4)\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n    \n    Returns:\n    list[int|float] | int: A new vector representing the element-wise sum if the operation is valid,\n                            otherwise -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n    theta (np.ndarray): A 2D NumPy array of shape (num_states, num_actions) representing the policy parameters.\n    episodes (list[list[tuple[int, int, float]]]): A list of episodes, where each episode is a list of (state, action, reward) tuples.\n    \n    Returns:\n    list: The average gradient of the log-policy multiplied by the return at each time step, rounded to the nearest 4th decimal.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = 0\n        for t in reversed(range(len(episode))):\n            (state, action, reward) = episode[t]\n            returns += reward\n            policy = softmax(theta[state, :])\n            log_policy = np.log(policy[action])\n            gradient[state, action] += returns * log_policy\n    average_gradient = gradient / len(episodes)\n    return np.round(average_gradient, 4).tolist()\ntheta = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\nepisodes = [[(0, 1, 1.0), (1, 2, 1.0), (2, 0, 1.0)], [(0, 2, 1.0), (1, 0, 1.0), (2, 1, 1.0)], [(0, 0, 1.0), (1, 1, 1.0), (2, 2, 1.0)]]"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active experts.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n    \n    Returns:\n    float: The percentage of computational cost savings, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out\n    savings = flops_dense - flops_moe\n    savings_percentage = savings / flops_dense * 100\n    return round(savings_percentage, 1)\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating mechanism for Mixture-of-Experts models.\n    \n    Parameters:\n    X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n    W_g (np.ndarray): Weight matrix for gating of shape (input_dim, num_experts).\n    W_noise (np.ndarray): Weight matrix for noise of shape (input_dim, num_experts).\n    N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    k (int): Sparsity constraint, number of experts to select.\n    \n    Returns:\n    list: Gating probabilities matrix rounded to 4 decimal places.\n    \"\"\"\n    logits = X @ W_g + N * (X @ W_noise)\n    exp_logits = np.exp(logits)\n    sum_exp_logits = np.sum(exp_logits, axis=1, keepdims=True)\n    probs = exp_logits / sum_exp_logits\n    topk_indices = np.argpartition(probs, -k, axis=1)[:, -k:]\n    topk_probs = np.zeros_like(probs)\n    for (i, indices) in enumerate(topk_indices):\n        topk_probs[i, indices] = probs[i, indices]\n    sum_topk_probs = np.sum(topk_probs, axis=1, keepdims=True)\n    sum_topk_probs[sum_topk_probs == 0] = 1\n    topk_probs /= sum_topk_probs\n    return np.round(topk_probs, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n    \n    Returns:\n    np.ndarray: Output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[-1]\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores - gating_scores.max(axis=-1, keepdims=True))\n    gating_probs /= gating_probs.sum(axis=-1, keepdims=True)\n    (top_k_probs, top_k_indices) = (np.zeros((batch_size, top_k)), np.zeros((batch_size, top_k), dtype=int))\n    for i in range(batch_size):\n        top_k_indices[i] = np.argsort(gating_probs[i])[-top_k:]\n        top_k_probs[i] = gating_probs[i][top_k_indices[i]]\n    top_k_probs /= top_k_probs.sum(axis=-1, keepdims=True)\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for (j, idx) in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = np.dot(x[i], We[idx])\n    final_output = np.sum(expert_outputs * top_k_probs[:, :, np.newaxis], axis=1)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'Number of channels must be divisible by the number of groups'\n    X = X.reshape(B, G, C // G, H, W)\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_out = gamma * X_norm + beta\n    return np.round(X_out, 4).tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Finds the value of x where the function f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    \n    Parameters:\n    - start_x: Initial guess for the x value.\n    - learning_rate: Step size for the gradient descent.\n    - tolerance: The threshold for determining when to stop the iteration.\n    - max_iters: Maximum number of iterations to perform.\n    \n    Returns:\n    - The x value where f(x) reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        derivative = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * derivative\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)\nstart_x = np.random.uniform(-10, 10)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input array x.\n    The DyT function is defined as: DyT(x) = gamma * tanh(alpha * x + beta)\n    where alpha, gamma, and beta are learnable parameters.\n    \n    Parameters:\n    x (np.ndarray): Input array.\n    alpha (float): Scaling factor for the input.\n    gamma (float): Scaling factor for the output.\n    beta (float): Bias term for the input.\n    \n    Returns:\n    list[float]: The transformed array, rounded to the nearest 4th decimal.\n    \"\"\"\n    transformed = gamma * np.tanh(alpha * x + beta)\n    return np.round(transformed, 4).tolist()"}
