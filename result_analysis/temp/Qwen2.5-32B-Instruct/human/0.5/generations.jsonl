{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix 'a' and a vector 'b'. Returns the resulting vector as a list\n    if the operation is valid, or -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    # Check if the number of columns in the matrix 'a' matches the length of the vector 'b'\n    if not a or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result vector\n    result = []\n    \n    # Compute the dot product for each row in 'a' with 'b'\n    for row in a:\n        row_dot_product = sum(x * b[i] for i, x in enumerate(row))\n        result.append(row_dot_product)\n    \n    return result\n\n# Function to test the matrix_dot_vector function with different inputs\ndef test_matrix_dot_vector():\n    a1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    b1 = [1, 2, 3]\n    a2 = [[1, 2], [3, 4]]\n    b2 = [1, 2, 3]\n    print(matrix_dot_vector(a1, b1))  # Expected: [14, 32, 50]\n    print(matrix_dot_vector(a2, b2))  # Expected: -1\n\n# Run the test function\ntest_matrix_dot_vector()\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Computes the transpose of the given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix.\n    \n    Returns:\n    list of list of int/float: The transpose of the input matrix.\n    \"\"\"\n    # Using list comprehension to transpose the matrix\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    \n    transposed_matrix = transpose_matrix(matrix)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes the given matrix a into the specified new_shape if possible.\n    If the reshape operation cannot be performed, returns an empty list.\n    \n    :param a: Original matrix as a list of lists.\n    :param new_shape: A tuple indicating the new shape (rows, columns).\n    :return: The reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    a_np = np.array(a)\n    # Check if the reshape operation is possible\n    if a_np.size == new_shape[0] * new_shape[1]:\n        # Perform the reshape operation\n        reshaped_np = a_np.reshape(new_shape)\n        # Convert the numpy array back to a list of lists and return\n        return reshaped_np.tolist()\n    else:\n        # Return an empty list if reshape is not possible\n        return []\n\n# Example check function to verify the correctness of the solution\ndef check_reshape():\n    example1 = [[1, 2], [3, 4]]\n    new_shape1 = (1, 4)\n    assert reshape_matrix(example1, new_shape1) == [[1, 2, 3, 4]], \"Example 1 failed\"\n    \n    example2 = [[1, 2], [3, 4]]\n    new_shape2 = (2, 2)\n    assert reshape_matrix(example2, new_shape2) == [[1, 2], [3, 4]], \"Example 2 failed\"\n    \n    example3 = [[1, 2], [3, 4]]\n    new_shape3 = (3, 3)\n    assert reshape_matrix(example3, new_shape3) == [], \"Example 3 failed\"\n    \n    print(\"All checks passed.\")\n\n# Run the check function\ncheck_reshape()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    Parameters:\n    - matrix: A list of lists of floats representing the matrix.\n    - mode: A string, either 'row' or 'column', specifying the mean calculation mode.\n    \n    Returns:\n    - A list of floats representing the means calculated according to the specified mode.\n    \"\"\"\n    # Convert the matrix to a numpy array for easier manipulation\n    np_matrix = np.array(matrix)\n    \n    if mode == 'row':\n        # Calculate the mean of each row\n        means = np_matrix.mean(axis=1)\n    elif mode == 'column':\n        # Calculate the mean of each column\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    # Convert the numpy array of means back to a list and return\n    return means.tolist()\n\n# Example usage:\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0]\n]\n\n# Calculate mean by row\nrow_means = calculate_matrix_mean(matrix, 'row')\nprint(\"Row means:\", row_means)\n\n# Calculate mean by column\ncolumn_means = calculate_matrix_mean(matrix, 'column')\nprint(\"Column means:\", column_means)\n```\n\nThis solution uses the `numpy` library for matrix manipulation and calculation of means, which is efficient and concise. The function `calculate_matrix_mean` is defined to take a matrix (as a list of lists of floats) and a mode ('row' or 'column') as inputs and returns a list of means according to the specified mode."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the scalar value and returns the resulting matrix.\n    \n    :param matrix: A list of lists representing the matrix, where each inner list is a row.\n    :param scalar: An integer or float representing the scalar value to multiply the matrix by.\n    :return: A new matrix (list of lists) with each element multiplied by the scalar.\n    \"\"\"\n    # Multiply each element in the matrix by the scalar\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a matrix and a scalar\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar = 2\n\n    # Perform scalar multiplication\n    result = scalar_multiply(matrix, scalar)\n\n    # Print the result\n    for row in result:\n        print(row)\n```\n\nThis script defines a function `scalar_multiply` that takes a matrix and a scalar as input, multiplies each element of the matrix by the scalar, and returns the new matrix. The example usage at the bottom demonstrates how to call this function and print the resulting matrix."}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: A list of eigenvalues sorted from highest to lowest.\n    \"\"\"\n    # Convert the matrix to a numpy array for easy manipulation\n    np_matrix = np.array(matrix)\n    \n    # Calculate the eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    \n    # Sort the eigenvalues from highest to lowest\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    \n    # Return the sorted eigenvalues\n    return sorted_eigenvalues\n\n# Example usage\nif __name__ == \"__main__\":\n    example_matrix = [[4, 2], [3, 1]]\n    print(calculate_eigenvalues(example_matrix))\n```\n\nThis code snippet defines a function `calculate_eigenvalues` that takes a 2x2 matrix as input and returns its eigenvalues sorted from highest to lowest. The function uses NumPy's `linalg.eigvals` to compute the eigenvalues and then sorts them accordingly. An example usage of the function is also provided."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        # Convert lists to numpy arrays for easier manipulation\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        \n        # Check if T and S are invertible\n        if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n            return -1\n        \n        # Calculate the inverse of T\n        T_inv = np.linalg.inv(T_np)\n        \n        # Perform the transformation T^{-1}AS\n        transformed_matrix = np.round(T_inv @ A_np @ S_np, 4)\n        \n        # Convert the result back to a python list and return\n        return transformed_matrix.tolist()\n    except Exception as e:\n        # Catch any other exceptions and return -1\n        return -1\n\n# Example usage\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 4]]\nprint(transform_matrix(A, T, S))\n```\n\nThis Python script defines a function `transform_matrix` that performs the specified matrix transformation operation $T^{-1}AS$ and handles cases where the matrices $T$ or $S$ are not invertible. It uses numpy for matrix operations and ensures the output is formatted as required."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix if it exists.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it is invertible, otherwise None.\n    \"\"\"\n    # Check if the input is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        return None\n    \n    # Calculate the determinant\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    \n    # Check if the determinant is zero\n    if det == 0:\n        return None\n    \n    # Calculate the inverse\n    inv_matrix = [[matrix[1][1] / det, -matrix[0][1] / det],\n                  [-matrix[1][0] / det, matrix[0][0] / det]]\n    \n    return inv_matrix\n\n# Function to check the correctness of the inverse_2x2 function\ndef check_inverse(matrix):\n    inv = inverse_2x2(matrix)\n    if inv is None:\n        print(f\"The matrix {matrix} is not invertible.\")\n    else:\n        print(f\"The inverse of the matrix {matrix} is {inv}\")\n        # Verify the inverse by multiplying the original matrix with its inverse\n        prod = np.dot(matrix, inv)\n        print(f\"The product of the matrix and its inverse is {prod}\")\n\n# Test cases\ncheck_inverse([[1, 2], [3, 4]])\ncheck_inverse([[1, 0], [0, 1]])\ncheck_inverse([[1, 1], [1, 1]])\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n    Returns the resulting matrix if multiplication is possible, otherwise returns -1.\n    \"\"\"\n    # Check if multiplication is possible\n    if len(a[0]) != len(b):\n        return -1\n\n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n\n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n\n    return result\n\n# Check function to test the matrixmul function with provided data points\ndef check_matrixmul():\n    a = [[1, 2], [3, 4]]\n    b = [[2, 0], [1, 2]]\n    assert matrixmul(a, b) == [[4, 4], [10, 8]], \"Test case 1 failed\"\n\n    a = [[1, 2, 3], [4, 5, 6]]\n    b = [[7, 8], [9, 10], [11, 12]]\n    assert matrixmul(a, b) == [[58, 64], [139, 154]], \"Test case 2 failed\"\n\n    a = [[1, 2]]\n    b = [[3, 4], [5, 6]]\n    assert matrixmul(a, b) == -1, \"Test case 3 failed\"\n\n    print(\"All test cases passed!\")\n\n# Uncomment the line below to run the check function\n# check_matrixmul()\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors: A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    A list of lists representing the covariance matrix.\n    \"\"\"\n    # Convert the list of lists into a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Calculate the covariance matrix using numpy's cov function\n    covariance_matrix = np.cov(data, bias=True)\n    \n    # Convert the numpy array back to a list of lists\n    return covariance_matrix.tolist()\n\n# Test cases to verify the correctness of the function\ndef check_covariance_matrix():\n    # Test case 1: Two features with three observations each\n    vectors1 = [[1, 2, 3], [4, 5, 6]]\n    expected1 = [[1.0, 1.0], [1.0, 1.0]]\n    assert np.allclose(calculate_covariance_matrix(vectors1), expected1), \"Test case 1 failed\"\n    \n    # Test case 2: Three features with four observations each\n    vectors2 = [[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]]\n    expected2 = [[1.6666666666666665, 1.6666666666666665, 1.6666666666666665],\n                 [1.6666666666666665, 1.6666666666666665, 1.6666666666666665],\n                 [1.6666666666666665, 1.6666666666666665, 1.6666666666666665]]\n    assert np.allclose(calculate_covariance_matrix(vectors2), expected2), \"Test case 2 failed\"\n    \n    print(\"All test cases passed\")\n\n# Run the test cases\ncheck_covariance_matrix()\n```\nThis Python script defines a function to calculate the covariance matrix for a given set of vectors. It uses NumPy for the calculation and includes test cases to verify the correctness of the implementation."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): A square matrix representing the coefficients of the system of equations.\n    b (np.ndarray): A column vector representing the constants of the system of equations.\n    n (int): The number of iterations to perform.\n    \n    Returns:\n    list: The approximate solution vector x as a list, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be a square matrix\"\n    \n    # Ensure dimensions match\n    assert A.shape[0] == b.shape[0], \"Matrix A and vector b dimensions do not match\"\n    \n    # Initialize the solution vector with zeros\n    x = np.zeros_like(b, dtype=np.float64)\n    \n    # Iterate n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        # Round to 4 decimal places\n        x = np.round(x, decimals=4)\n    \n    # Convert to list and return\n    return x.tolist()\n\n# Example usage:\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=np.float64)\nb = np.array([4, 7, 3], dtype=np.float64)\nn = 25  # Number of iterations\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\nThis script defines a function `solve_jacobi` that takes a square matrix `A`, a column vector `b`, and an integer `n` representing the number of iterations. It uses the Jacobi method to approximate the solution to the system of linear equations `Ax = b`. The solution is rounded to four decimal places and returned as a list. The script includes an example usage demonstrating how to call the function and print the solution."}
{"task_id": 12, "completion_id": 0, "completion": "To solve the task of approximating the Singular Value Decomposition (SVD) for a 2x2 matrix using the Jacobian method, we can follow these steps:\n\n1. Define the function `svd_2x2_singular_values` that takes a 2x2 matrix `A` as input.\n2. Implement the Jacobian method to approximate the SVD.\n3. Return the singular values rounded to the nearest 4th decimal.\n\nThe Jacobian method here is not a standard term for approximating SVD, but it can be interpreted as using iterative methods to approximate the SVD. For a 2x2 matrix, however, we can directly compute the singular values using the eigenvalues of \\(A^T A\\) or \\(A A^T\\).\n\nHere is the Python code that accomplishes this:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix A using the eigenvalues\n    of A^T A. The singular values are returned as a tuple rounded to the nearest 4th decimal.\n    \n    :param A: A 2x2 numpy array\n    :return: A tuple containing the singular values of A\n    \"\"\"\n    # Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Compute the eigenvalues of A^T A\n    eigenvalues = np.linalg.eigvals(ATA)\n    \n    # The singular values are the square roots of the eigenvalues of A^T A\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Sort the singular values in descending order\n    singular_values = np.sort(singular_values)[::-1]\n    \n    # Round the singular values to the nearest 4th decimal\n    singular_values_rounded = np.round(singular_values, 4)\n    \n    return tuple(singular_values_rounded)\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(singular_values)\n```\n\nThis script defines the function `svd_2x2_singular_values` which computes the singular values of a 2x2 matrix `A` using the eigenvalues of \\(A^T A\\). The singular values are then rounded to the nearest 4th decimal and returned as a tuple. The example usage demonstrates how to call this function with a sample 2x2 matrix."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n    def determinant_3x3(sub_matrix: list[list[int|float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        :param sub_matrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the 3x3 matrix.\n        \"\"\"\n        return (sub_matrix[0][0] * (sub_matrix[1][1] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][1]) -\n                sub_matrix[0][1] * (sub_matrix[1][0] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][0]) +\n                sub_matrix[0][2] * (sub_matrix[1][0] * sub_matrix[2][1] - sub_matrix[1][1] * sub_matrix[2][0]))\n\n    def minor(matrix: list[list[int|float]], i: int, j: int) -> list[list[int|float]]:\n        \"\"\"\n        Returns the minor of the matrix by removing the ith row and jth column.\n        \n        :param matrix: The matrix from which to calculate the minor.\n        :param i: The row to remove.\n        :param j: The column to remove.\n        :return: The minor matrix.\n        \"\"\"\n        return [row[:j] + row[j+1:] for row in (matrix[:i] + matrix[i+1:])]\n\n    det = 0\n    for col in range(4):\n        sign = (-1) ** (0 + col)  # First row is 0th row, col is the current column.\n        sub_det = determinant_3x3(minor(matrix, 0, col))\n        det += sign * matrix[0][col] * sub_det\n    return det\n\n# Testing the function\nif __name__ == \"__main__\":\n    test_matrix = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n        [9, 10, 11, 12],\n        [13, 14, 15, 16]\n    ]\n    print(f\"Determinant of the test matrix is: {determinant_4x4(test_matrix)}\")\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    - X: A list of lists of floats representing the features matrix.\n    - y: A list of floats representing the target vector.\n    \n    Returns:\n    - A list of floats representing the coefficients of the linear regression model.\n    \"\"\"\n    # Convert X and y to numpy arrays for easier manipulation\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    \n    # Apply the normal equation to find the optimal theta values\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    \n    # Round the results to 4 decimal places\n    theta_rounded = np.round(theta, decimals=4)\n    \n    # Convert back to a list of floats and return\n    return theta_rounded.tolist()\n\n# Example usage\nX_example = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny_example = [2, 3, 4, 5]\nprint(linear_regression_normal_equation(X_example, y_example))\n```\n\nThis script defines a function that performs linear regression using the normal equation, including the necessary steps of adding an intercept term and rounding the results to four decimal places. It includes an example usage that demonstrates how to call the function with a sample input."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input features with a column of ones for the intercept.\n    y (np.ndarray): The target values.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to perform.\n    \n    Returns:\n    list: The coefficients of the linear regression model rounded to 4 decimal places.\n    \"\"\"\n    # Initialize coefficients\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    \n    for _ in range(iterations):\n        # Predictions\n        predictions = np.dot(X, theta)\n        # Errors\n        errors = predictions - y\n        # Gradient\n        gradient = np.dot(X.T, errors) / m\n        # Update theta\n        theta -= alpha * gradient\n    \n    # Round coefficients to 4 decimal places and return as list\n    return np.round(theta, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some example data\n    np.random.seed(42)\n    X = np.random.rand(100, 1)\n    y = 2 + 3 * X + np.random.rand(100, 1) / 10\n    X = np.hstack([np.ones((X.shape[0], 1)), X])  # Add intercept term\n    \n    # Set parameters\n    alpha = 0.01\n    iterations = 1000\n    \n    # Run gradient descent\n    coefficients = linear_regression_gradient_descent(X, y.flatten(), alpha, iterations)\n    print(coefficients)\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array and performs two types of feature scaling:\n    Standardization and Min-Max Normalization. Returns two 2D lists with\n    scaled values rounded to the nearest 4th decimal.\n    \"\"\"\n    # Standardization: (x - mean) / std\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Min-Max Normalization: (x - min) / (max - min)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    normalized_data = (data - min_vals) / (max_vals - min_vals)\n    \n    # Rounding and converting to list\n    standardized_data = np.round(standardized_data, decimals=4).tolist()\n    normalized_data = np.round(normalized_data, decimals=4).tolist()\n    \n    return standardized_data, normalized_data\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating some random data for demonstration\n    np.random.seed(0)  # for reproducibility\n    data = np.random.rand(5, 3) * 100  # 5 samples, 3 features, scaled 0-100\n    \n    standardized, normalized = feature_scaling(data)\n    print(\"Standardized Data:\\n\", standardized)\n    print(\"Normalized Data:\\n\", normalized)\n```\n\nThis script defines a function `feature_scaling` that performs both standardization and min-max normalization on a given 2D NumPy array. After performing the scaling, the function rounds the results to the nearest 4th decimal and converts them to Python lists before returning. The example usage section demonstrates how to call this function with a sample dataset."}
{"task_id": 17, "completion_id": 0, "completion": "Here's a Python script that implements the k-Means clustering algorithm according to the task description. The script includes the function `k_means_clustering` and uses NumPy for efficient computation.\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert lists of tuples to NumPy arrays for easier manipulation\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis])**2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        \n        # Compute new centroids based on the mean of the points in each cluster\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        \n        # Check for convergence (if centroids do not change)\n        if np.all(centroids_array == new_centroids):\n            break\n        \n        centroids_array = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal and convert back to list of tuples\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    \n    return final_centroids\n\n# Example usage\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nresult = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(result)\n```\n\nThis script defines the `k_means_clustering` function which takes the specified inputs and returns the final centroids after performing the k-Means clustering algorithm. The centroids are rounded to the nearest fourth decimal as required. The example usage at the bottom demonstrates how to call the function with a set of points, the number of clusters, initial centroids, and the maximum number of iterations."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation and returns the train-test splits.\n    \n    :param X: np.ndarray, feature matrix of the dataset.\n    :param y: np.ndarray, labels of the dataset.\n    :param k: int, number of folds for cross-validation.\n    :param shuffle: bool, whether to shuffle the dataset before splitting.\n    :param random_seed: int, seed for random number generator.\n    :return: list of tuples, each tuple contains train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    fold_size = n_samples // k\n    remainder = n_samples % k\n    \n    start_idx = 0\n    splits = []\n    for i in range(k):\n        end_idx = start_idx + fold_size + (1 if i < remainder else 0)\n        test_idx = indices[start_idx:end_idx]\n        train_idx = np.concatenate([indices[:start_idx], indices[end_idx:]])\n        splits.append((train_idx, test_idx))\n        start_idx = end_idx\n    \n    return splits\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.random.rand(100, 5)  # 100 samples, 5 features\n    y = np.random.randint(0, 2, 100)  # Binary labels\n\n    k = 5\n    splits = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=42)\n    \n    for i, (train_idx, test_idx) in enumerate(splits):\n        print(f\"Fold {i+1}:\")\n        print(f\"  Train indices: {train_idx}\")\n        print(f\"  Test indices: {test_idx}\")\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Performs Principal Component Analysis (PCA) on a given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    A list of lists, where each inner list represents a principal component (eigenvector) and the components are sorted by their corresponding eigenvalues in descending order.\n    \"\"\"\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Sort the eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Convert to list of lists and round to 4 decimal places\n    principal_components_list = [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]\n    \n    return principal_components_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example dataset\n    data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n    k = 2\n    # Perform PCA\n    principal_components = pca(data, k)\n    print(principal_components)\n```"}
{"task_id": 20, "completion_id": 0, "completion": "Here is a self-contained Python script that implements the decision tree learning algorithm for classification based on the task description:\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    # If all examples have the same target attribute, return the target value\n    if all(examples[0][target_attr] == example[target_attr] for example in examples):\n        return examples[0][target_attr]\n    \n    # If no attributes left, return the most common target attribute value\n    if not attributes:\n        return Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n    \n    # Choose the attribute that provides the highest information gain\n    best_attr = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    \n    # Create a new decision tree/node with the best attribute and an empty dictionary to hold branches\n    tree = {best_attr: {}}\n    \n    # Create a subtree for each value of the best attribute\n    for value in set(example[best_attr] for example in examples):\n        # Create a subset of examples where the best attribute has the current value\n        subset = [example for example in examples if example[best_attr] == value]\n        \n        # If the subset is empty, add the most common target attribute value\n        if not subset:\n            tree[best_attr][value] = Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n        else:\n            # Recursively build the subtree for the subset\n            subtree = learn_decision_tree(subset, [attr for attr in attributes if attr != best_attr], target_attr)\n            tree[best_attr][value] = subtree\n    \n    return tree\n\ndef entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"Calculate the entropy of the examples based on the target attribute.\"\"\"\n    counts = Counter(example[target_attr] for example in examples)\n    total = len(examples)\n    return -sum((count / total) * math.log2(count / total) for count in counts.values())\n\ndef information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"Calculate the information gain of splitting the examples by the attribute.\"\"\"\n    total_entropy = entropy(examples, target_attr)\n    values = set(example[attribute] for example in examples)\n    weighted_entropy = sum(\n        (sum(1 for example in examples if example[attribute] == value) / len(examples)) *\n        entropy([example for example in examples if example[attribute] == value], target_attr)\n        for value in values\n    )\n    return total_entropy - weighted_entropy\n\n# Example usage:\nexamples = [\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}\n]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```\n\nThis script defines a `learn_decision_tree` function that constructs a decision tree based on the provided examples and attributes. It uses recursive binary splitting based on entropy and information gain to determine the best attribute to split the data on at each step. The `entropy` and `information_gain` helper functions are used to calculate the entropy of a set of examples and the information gain of splitting on a particular attribute, respectively. The example usage at the bottom demonstrates how to use the `learn_decision_tree` function with a sample dataset."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implementation of a deterministic version of the Pegasos algorithm for training a kernel SVM.\n    \n    Parameters:\n    data (np.ndarray): The input data matrix.\n    labels (np.ndarray): The labels for the input data.\n    kernel (str): The kernel to use, either 'linear' or 'RBF'.\n    lambda_val (float): The regularization parameter.\n    iterations (int): The number of iterations to run the algorithm.\n    sigma (float): The bandwidth parameter for the RBF kernel.\n    \n    Returns:\n    (list, float): A list of alpha coefficients and the bias term.\n    \"\"\"\n    \n    # Initialize alpha coefficients\n    alphas = np.zeros(data.shape[0])\n    \n    # Compute the kernel matrix\n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'RBF':\n        pairwise_dists = np.sum(data**2, axis=1).reshape(-1, 1) + np.sum(data**2, axis=1) - 2 * np.dot(data, data.T)\n        kernel_matrix = np.exp(-pairwise_dists / (2 * sigma**2))\n    else:\n        raise ValueError(\"Kernel must be either 'linear' or 'RBF'.\")\n    \n    for t in range(1, iterations + 1):\n        # Learning rate\n        eta_t = 1.0 / (lambda_val * t)\n        \n        # Compute predictions\n        predictions = np.dot(kernel_matrix * np.outer(labels, labels), alphas)\n        \n        # Compute the gradient\n        gradient = np.where(labels * predictions < 1, -labels, 0)\n        \n        # Update alpha coefficients\n        alphas -= eta_t * (np.dot(kernel_matrix * np.outer(gradient, labels), alphas) + lambda_val * alphas)\n        \n        # Project onto the simplex\n        alphas = np.where(alphas > 0, alphas, 0)\n    \n    # Compute the bias term\n    support_indices = np.where(alphas > 0)[0]\n    bias = np.mean(labels[support_indices] - np.dot(kernel_matrix[support_indices, :][:, support_indices], alphas[support_indices]))\n    \n    return alphas.tolist(), round(float(bias), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 3], [4, 5]])\n    y = np.array([1, 1, -1, -1])\n    \n    # Train the model\n    alphas, bias = pegasos_kernel_svm(X, y, kernel='linear', lambda_val=0.01, iterations=100)\n    print(\"Alphas:\", alphas)\n    print(\"Bias:\", bias)\n```"}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid of the input z.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example usage\nprint(sigmoid(0))  # Expected output: 0.5\nprint(sigmoid(5))  # Expected output: 0.9933\nprint(sigmoid(-3)) # Expected output: 0.0475\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of the given list of scores.\n    \n    Args:\n    scores (list of float): The list of scores to compute the softmax for.\n    \n    Returns:\n    list of float: The softmax values of the given scores, each rounded to 4 decimal places.\n    \"\"\"\n    # Calculate the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Sum up all the exponentiated scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax for each score\n    softmax_scores = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    \n    return softmax_scores\n\n# Function to test the correctness of the softmax function\ndef check_function():\n    test_scores = [1.0, 2.0, 3.0]\n    expected_output = [0.0900, 0.2447, 0.6653]\n    assert softmax(test_scores) == expected_output, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Running the check function to verify the solution\ncheck_function()\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: List of feature vectors for each example.\n    :param labels: List of true binary labels for each example.\n    :param weights: List of weights for each feature.\n    :param bias: Bias term for the neuron.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    \n    predictions = []\n    for feature_vector in features:\n        # Calculate the weighted sum of features plus bias\n        weighted_sum = sum(w * f for w, f in zip(weights, feature_vector)) + bias\n        # Apply the sigmoid activation function\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    \n    # Calculate the mean squared error\n    mse = sum((p - l) ** 2 for p, l in zip(predictions, labels)) / len(labels)\n    \n    # Round the results to the nearest 4th decimal\n    predictions_rounded = [round(p, 4) for p in predictions]\n    mse_rounded = round(mse, 4)\n    \n    return predictions_rounded, mse_rounded\n\n# Example usage:\nfeatures = [[0.5, 0.3], [0.1, 0.7], [0.2, 0.4]]\nlabels = [1, 0, 1]\nweights = [0.8, 0.2]\nbias = 0.1\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predictions:\", predictions)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines a `single_neuron_model` function that simulates a single neuron with a sigmoid activation function for binary classification. The function takes lists of feature vectors, true labels, neuron weights, and a bias as input. It returns the predicted probabilities and the mean squared error, both rounded to four decimal places. The example usage demonstrates how to call the function with sample data and prints the predictions and mean squared error."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ndef mse_loss(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    \n    for _ in range(epochs):\n        # Forward pass\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        \n        # Compute loss and append to mse_values\n        mse = mse_loss(labels, predictions)\n        mse_values.append(round(mse, 4))\n        \n        # Backward pass\n        error = predictions - labels\n        derivative = sigmoid_derivative(predictions)\n        gradient = error * derivative\n        \n        # Update weights and bias\n        weights_gradient = np.dot(features.T, gradient)\n        bias_gradient = np.sum(gradient)\n        \n        weights -= learning_rate * weights_gradient\n        bias -= learning_rate * bias_gradient\n    \n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias)\nprint(\"MSE values:\", mse)\n```\nThis script defines a function `train_neuron` that simulates a single neuron with sigmoid activation and implements backpropagation to update the neuron's weights and bias based on the mean squared error (MSE) loss. The function takes in the specified parameters and returns the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places. The example usage demonstrates how to use the function with a simple XOR dataset."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n        \n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    B (list of lists of int): Basis vectors in basis B.\n    C (list of lists of int): Basis vectors in basis C.\n    \n    Returns:\n    list of lists of float: The transformation matrix P from basis B to C.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    B_np = np.array(B)\n    C_np = np.array(C)\n    \n    # Invert basis C to find the transformation matrix\n    C_inv = np.linalg.inv(C_np)\n    \n    # Compute the transformation matrix P\n    P = C_inv @ B_np\n    \n    # Round to the nearest 4th decimal and convert to list\n    P_rounded = np.round(P, decimals=4).tolist()\n    \n    return P_rounded\n\n# Example usage\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nprint(transform_basis(B, C))\n```\n\nThis script defines a function `transform_basis` that takes two bases, `B` and `C`, as input and calculates the transformation matrix `P` from `B` to `C`. It uses numpy for matrix operations, including inversion and multiplication, and ensures the output is rounded to the nearest 4th decimal and returned as a list of lists."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the SVD of a 2x2 matrix A using eigendecomposition of A^T A.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 numpy array.\n    \n    Returns:\n    tuple: A tuple containing U, S, and V matrices rounded to 4 decimal places.\n    \"\"\"\n    # Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Eigendecomposition of A^T A\n    eigenvalues, eigenvectors = np.linalg.eig(ATA)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Construct the singular values matrix S\n    S = np.sqrt(eigenvalues)\n    S = np.diag(S)\n    \n    # Construct the matrix V from the eigenvectors of A^T A\n    V = eigenvectors\n    \n    # Compute U by normalizing A v_i for each non-zero singular value\n    U = np.zeros_like(A)\n    for i in range(2):\n        if S[i, i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / S[i, i]\n    \n    # Ensure that the determinant of U is positive\n    if np.linalg.det(U) < 0:\n        U[:, 1] = -U[:, 1]\n    \n    # Round the results to 4 decimal places and convert to lists\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    \n    return U, S, V\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\nThis script defines a function `svd_2x2` that computes the Singular Value Decomposition of a 2x2 matrix using the eigendecomposition approach, and returns the matrices U, S, and V rounded to the nearest 4th decimal place in list form."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffle the samples in two numpy arrays, X and y, maintaining the corresponding order.\n    \n    :param X: numpy array, first array to be shuffled.\n    :param y: numpy array, second array to be shuffled.\n    :param seed: Optional, the seed for random number generator for reproducibility.\n    :return: Two shuffled numpy arrays. The arrays are shuffled in the same way to maintain the correspondence between them.\n    \"\"\"\n    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):\n        raise ValueError(\"X and y must be numpy arrays\")\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n\n    np.random.seed(seed)\n    permutation_indices = np.random.permutation(X.shape[0])\n    shuffled_X = X[permutation_indices]\n    shuffled_y = y[permutation_indices]\n    \n    return shuffled_X.tolist(), shuffled_y.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])\nshuffled_X, shuffled_y = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X:\", shuffled_X)\nprint(\"Shuffled y:\", shuffled_y)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Creates a batch iterator that yields batches of X and optionally y.\n    \n    Parameters:\n    X (np.ndarray): The input data.\n    y (np.ndarray, optional): The target data. Defaults to None.\n    batch_size (int): The size of each batch. Defaults to 64.\n    \n    Yields:\n    tuple or list: A tuple of (batch_X, batch_y) if y is not None, otherwise a list batch_X.\n    \"\"\"\n    # Ensure X is a numpy array\n    X = np.array(X)\n    n_samples = X.shape[0]\n    \n    # Ensure y is a numpy array if it is not None\n    if y is not None:\n        y = np.array(y)\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n    \n    # Generate the batch indices\n    for i in range(0, n_samples, batch_size):\n        batch_X = X[i:i + batch_size].tolist()\n        if y is not None:\n            batch_y = y[i:i + batch_size].tolist()\n            yield batch_X, batch_y\n        else:\n            yield batch_X\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Generate some dummy data\n    X = np.random.randn(100, 10)\n    y = np.random.randn(100, 1)\n    \n    # Iterate over batches\n    for batch_X, batch_y in batch_iterator(X, y, batch_size=10):\n        print(np.array(batch_X).shape, np.array(batch_y).shape)\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n    \n    :param X: numpy.ndarray, the dataset to be divided.\n    :param feature_i: int, the index of the feature to use for dividing the dataset.\n    :param threshold: float, the threshold value to compare the feature values against.\n    :return: tuple of two lists, the first list contains samples where the feature value is >= threshold,\n             and the second list contains samples where the feature value is < threshold.\n    \"\"\"\n    # Ensure X is a numpy array for indexing\n    X = np.array(X)\n    \n    # Indexing the feature column\n    feature_values = X[:, feature_i]\n    \n    # Splitting the dataset\n    above_threshold = X[feature_values >= threshold].tolist()\n    below_threshold = X[feature_values < threshold].tolist()\n    \n    return above_threshold, below_threshold\n\n# Example usage\nif __name__ == \"__main__\":\n    # Creating a dummy dataset\n    np.random.seed(0)\n    X = np.random.rand(10, 3)  # 10 samples, 3 features each\n    feature_i = 1  # We want to split on the second feature\n    threshold = 0.5  # The threshold value\n    \n    # Dividing the dataset\n    above, below = divide_on_feature(X, feature_i, threshold)\n    \n    print(\"Above threshold:\", above)\n    print(\"Below threshold:\", below)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    Parameters:\n    X : np.ndarray\n        The input samples with shape (n_samples, n_features).\n    degree : int\n        The degree of the polynomial features.\n    \n    Returns:\n    np.ndarray\n        The matrix of features, where columns are powers of the original features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    \n    n_samples, n_features = X.shape\n    indices = np.arange(n_features)\n    combinations = [combinations_with_replacement(indices, i) for i in range(0, degree + 1)]\n    flat_combinations = [item for sublist in combinations for item in sublist]\n    \n    n_output_features = len(flat_combinations)\n    X_new = np.empty((n_samples, n_output_features))\n    \n    for i, index_combination in enumerate(flat_combinations):\n        X_new[:, i] = np.prod(X[:, index_combination], axis=1)\n    \n    return X_new.tolist()\n\n# Example usage\nX = np.array([[2, 3], [4, 5]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis script defines a function `polynomial_features` that will generate all polynomial combinations of input features up to the specified degree. It uses `combinations_with_replacement` from the itertools module to generate the combinations of feature indices and then computes the product of the features for each combination to construct the output matrix. The output matrix is then converted to a list using `tolist()` before being returned."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)  # Ensure reproducibility\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n    y = np.array([0, 1, 0, 1])\n    n_subsets = 3\n    subsets = get_random_subsets(X, y, n_subsets, replacements=False)\n    for i, (X_subset, y_subset) in enumerate(subsets):\n        print(f\"Subset {i+1}:\")\n        print(\"X:\\n\", np.array(X_subset))\n        print(\"y:\\n\", np.array(y_subset))\n        print()\n```\nThis script defines a function `get_random_subsets` that generates a specified number of random subsets from a dataset, with or without replacements, and returns them as a list of tuples. Each tuple contains a subset of the features and labels of the original dataset. The example usage demonstrates how to call this function and print the resulting subsets."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (np.array): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. If not provided, it will be determined from the input array.\n    \n    Returns:\n    list: A python list of one-hot encoded values.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Check function to verify the correctness of the to_categorical function\ndef check_to_categorical():\n    test_cases = [\n        (np.array([0, 1, 2, 3]), None, [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]),\n        (np.array([1, 0, 3, 2]), 4, [[0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0]]),\n        (np.array([0, 0, 0, 0]), 3, [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]])\n    ]\n    \n    for x, n_col, expected in test_cases:\n        result = to_categorical(x, n_col)\n        assert result == expected, f\"Failed for input: x={x}, n_col={n_col}. Expected {expected}, got {result}\"\n    print(\"All test cases passed.\")\n\n# Run the check function to verify the solution\ncheck_to_categorical()\n```\n\nThis script defines a function `to_categorical` that performs one-hot encoding on a 1D numpy array of integer values. It also includes a check function `check_to_categorical` that tests the `to_categorical` function against predefined test cases to ensure its correctness."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Args:\n    x (np.array): A 1D numpy array.\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Ensure that the input is a numpy array\n    x = np.array(x)\n    \n    # Create a diagonal matrix using np.diag\n    diagonal_matrix = np.diag(x)\n    \n    # Convert the numpy array to a list for output\n    return diagonal_matrix.tolist()\n\n# Example usage\nx = np.array([1, 2, 3])\nprint(make_diagonal(x))\n```\n\nThis script defines a function `make_diagonal` which takes a 1D numpy array as input and returns a 2D list representing the diagonal matrix formed from the input array. The numpy `diag` function is used to create the diagonal matrix, and the `tolist()` method is used to convert the resulting numpy array into a list, as per the output constraints."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError(\"y_true and y_pred must be numpy arrays\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n    \n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 2, 2, 2])\n    y_pred = np.array([0, 0, 2, 2, 1])\n    print(accuracy_score(y_true, y_pred))\n```\n\nThis code snippet defines a function `accuracy_score` that calculates the accuracy of predictions compared to true labels, using numpy arrays for input. It includes basic validation checks and rounds the result to 4 decimal places as required."}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array.\n    Y (numpy.ndarray, optional): A 2D numpy array. If not provided, Y is set to X.\n    \n    Returns:\n    list: A 2D list representing the correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n\n    # Calculate the means of X and Y\n    mean_X = np.mean(X, axis=0)\n    mean_Y = np.mean(Y, axis=0)\n\n    # Subtract the mean from X and Y\n    X_zero_mean = X - mean_X\n    Y_zero_mean = Y - mean_Y\n\n    # Calculate the covariance matrix\n    cov_matrix = np.dot(X_zero_mean.T, Y_zero_mean)\n\n    # Calculate the standard deviation of X and Y\n    std_X = np.std(X, axis=0, ddof=0)\n    std_Y = np.std(Y, axis=0, ddof=0)\n\n    # Compute the outer product of the standard deviations\n    std_outer_product = np.outer(std_X, std_Y)\n\n    # Avoid division by zero by adding a small constant to the denominator\n    correlation_matrix = cov_matrix / (std_outer_product + 1e-10)\n\n    # Round to the nearest 4th decimal and convert to a list\n    return np.round(correlation_matrix, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n    \n    # Test the function without Y\n    print(\"Correlation matrix of X with itself:\")\n    print(calculate_correlation_matrix(X))\n    \n    # Test the function with Y\n    print(\"Correlation matrix of X with Y:\")\n    print(calculate_correlation_matrix(X, Y))\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    # Initialize weights\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = None\n        \n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.ones(y.shape)\n                pred[X[:, feature] < threshold] = -1\n                \n                error = np.sum(weights[y != pred])\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'prediction': pred.copy()}\n        \n        # Calculate alpha\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n        best_clf['alpha'] = alpha\n        \n        # Update weights\n        weights *= np.exp(-alpha * y * best_clf['prediction'])\n        weights /= np.sum(weights)\n        \n        classifiers.append(best_clf)\n    \n    return classifiers\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([1, 1, -1, -1])\n    n_clf = 2\n    classifiers = adaboost_fit(X, y, n_clf)\n    for clf in classifiers:\n        print(f\"Feature: {clf['feature']}, Threshold: {clf['threshold']}, Alpha: {clf['alpha']:.4f}\")\n```\n\nThis script defines the `adaboost_fit` function that implements the AdaBoost algorithm to train a classifier on a given dataset. The function iteratively selects the best weak classifier by evaluating thresholds for each feature, calculates the error, and updates the sample weights. The classifiers are stored in a list with their parameters, including the feature index, threshold, and alpha value. The provided example usage demonstrates how to call the function and prints out the parameters of the trained classifiers."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax is computed as:\n    log_softmax(i) = log(exp(scores[i]) / sum_j(exp(scores[j])))\n    \n    This function ensures numerical stability by subtracting the max score\n    from all scores before applying the log-softmax formula.\n    \n    Parameters:\n    - scores (list): A list of scores.\n    \n    Returns:\n    - list: A list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    scores -= max_score  # Subtract max score for numerical stability\n    exp_scores = np.exp(scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_softmax_scores = scores - np.log(sum_exp_scores)\n    return [round(score, 4) for score in log_softmax_scores.tolist()]\n\n# Check function to verify the correctness of the generated function\ndef check_log_softmax():\n    test_scores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n    expected_output = [-3.4519, -2.4519, -1.4519, -0.4519, -3.4519, -2.4519, -1.4519]\n    assert log_softmax(test_scores) == expected_output, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Example usage\nscores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\nprint(log_softmax(scores))\n\n# Run check function\ncheck_log_softmax()\n```\n\nThis code snippet defines a function `log_softmax` that computes the log-softmax of a given list of scores, ensuring numerical stability by subtracting the maximum score from all scores before applying the log-softmax formula. The output is rounded to the nearest 4th decimal and returned as a list. A check function `check_log_softmax` is also provided to verify the correctness of the implementation."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        # Initialize weights and biases\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        # Initialize optimizers\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        # Total number of parameters\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        # Save input for backpropagation\n        self.layer_input = X\n        # Compute output\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Compute gradients\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = self.layer_input.T.dot(accum_grad)\n        # Update weights and biases\n        self.W = self.optimizer_W.update(self.W, W_grad)\n        self.w0 = self.optimizer_w0.update(self.w0, w0_grad)\n        # Return accumulated gradient for the previous layer\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        # Output shape is determined by the number of units\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    class Optimizer:\n        def update(self, param, grad):\n            # Simple SGD optimizer with learning rate 0.01\n            return param - 0.01 * grad\n\n    dense_layer = Dense(n_units=5, input_shape=(3,))\n    dense_layer.initialize(Optimizer())\n    input_data = np.array([[1, 2, 3], [4, 5, 6]])\n    output = dense_layer.forward_pass(input_data, training=True)\n    print(\"Output:\", np.round(output, 4).tolist())\n    grad = np.array([[0.1, 0.2, 0.3, 0.4, 0.5], [0.6, 0.7, 0.8, 0.9, 1.0]])\n    prev_grad = dense_layer.backward_pass(grad)\n    print(\"Gradient to previous layer:\", np.round(prev_grad, 4).tolist())\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution operation on the input matrix using the given kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix on which the convolution will be applied.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The padding to be applied to the input matrix.\n    stride (int): The stride for the convolution operation.\n    \n    Returns:\n    list: The result of the convolution operation as a list.\n    \"\"\"\n    # Padding the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Dimensions\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Output dimensions\n    output_height = (input_height + 2*padding - kernel_height) // stride + 1\n    output_width = (input_width + 2*padding - kernel_width) // stride + 1\n    \n    # Initialize output\n    output = np.zeros((output_height, output_width))\n    \n    # Apply convolution\n    for y in range(0, output_height):\n        for x in range(0, output_width):\n            output[y, x] = np.sum(padded_input[y*stride:y*stride+kernel_height, x*stride:x*stride+kernel_width] * kernel)\n    \n    # Rounding to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Return as list\n    return output.tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```\n\nThis script defines a function `simple_conv2d` that performs a 2D convolution operation on a given input matrix with a specified kernel, padding, and stride. The function returns the result as a list after rounding to the nearest 4th decimal. The example usage demonstrates how to use the function with a sample input matrix, kernel, padding, and stride."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A float number input to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)\n\n# Test the function with some example values\ntest_values = [-3.0, -1.5, 0, 1.2, 4.5]\noutput_values = [relu(z) for z in test_values]\n\n# Print the output values to verify correctness\nprint(\"Input values:\", test_values)\nprint(\"Output values after applying ReLU:\", output_values)\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss function given the feature matrix X, \n    coefficients w, true labels y_true, and regularization parameter alpha.\n    The Ridge loss is the sum of the Mean Squared Error (MSE) and the \n    regularization term (alpha * sum of squares of coefficients).\n    \n    Parameters:\n    X (np.ndarray): 2D array representing the feature matrix.\n    w (np.ndarray): 1D array representing the coefficients.\n    y_true (np.ndarray): 1D array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss value.\n    \"\"\"\n    # Predicted values\n    y_pred = X @ w\n    # Mean Squared Error\n    mse = np.mean((y_true - y_pred) ** 2)\n    # Regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    # Ridge loss\n    loss = mse + reg_term\n    return round(loss, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, -0.5])\n    y_true = np.array([1, 2, 3])\n    alpha = 0.1\n    # Calculate Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```\nThis script defines the `ridge_loss` function which calculates the loss for Ridge Regression, including both the Mean Squared Error and the regularization term, and prints the result for a sample dataset."}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the Leaky ReLU function.\n    alpha (float): The slope for negative inputs. Defaults to 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Example test cases\nprint(leaky_relu(-10))  # Should print -0.1\nprint(leaky_relu(10))   # Should print 10\nprint(leaky_relu(-5, 0.1))  # Should print -0.5\nprint(leaky_relu(5, 0.1))   # Should print 5\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product of the two vectors.\n    \n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The computed linear kernel value.\n    \n    Raises:\n    ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError(\"Input vectors must be of the same length.\")\n    \n    return np.dot(x1, x2)\n\n# Check function to verify the correctness of the generated function\ndef check_function():\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n    expected_result = 32  # Expected linear kernel result for x1 and x2\n    assert kernel_function(x1, x2) == expected_result, \"The result does not match the expected output.\"\n    print(\"Function is working correctly with the provided data points.\")\n\n# Running the check function\ncheck_function()\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n    \n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    \n    if predicted_positives == 0:\n        return 1.0  # No positive predictions, precision is undefined. By convention, return 1.0\n    \n    precision_score = true_positives / predicted_positives\n    return precision_score\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(\"Precision:\", precision(y_true, y_pred))\n```\n\nThis solution includes a function `precision` that calculates the precision metric based on the provided true and predicted binary labels. It handles the case where there are no positive predictions by returning a precision of 1.0, as per convention."}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent for linear regression using MSE loss.\n    \n    :param X: numpy array of shape (n_samples, n_features) containing the input data\n    :param y: numpy array of shape (n_samples,) containing the target values\n    :param weights: numpy array of shape (n_features,) containing the initial weights\n    :param learning_rate: float, the learning rate for the gradient descent\n    :param n_iterations: int, the number of iterations to perform\n    :param batch_size: int, the size of the batch for mini-batch gradient descent\n    :param method: str, the type of gradient descent to perform ('batch', 'stochastic', 'mini-batch')\n    :return: numpy array of shape (n_features,) containing the final weights\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.array(weights, dtype=np.float64)\n\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                x_i = X[i:i+1]\n                y_i = y[i:i+1]\n                prediction = np.dot(x_i, weights)\n                error = prediction - y_i\n                gradient = 2 * np.dot(x_i.T, error) / len(x_i)\n                weights -= learning_rate * gradient\n                \n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                x_i = X[i:i+batch_size]\n                y_i = y[i:i+batch_size]\n                prediction = np.dot(x_i, weights)\n                error = prediction - y_i\n                gradient = 2 * np.dot(x_i.T, error) / len(x_i)\n                weights -= learning_rate * gradient\n                \n        elif method == 'batch':\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * np.dot(X.T, error) / n_samples\n            weights -= learning_rate * gradient\n    \n    return np.round(weights, decimals=4).tolist()\n\n# Example usage\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\nmethod = 'mini-batch'\n\nfinal_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method)\nprint(final_weights)\n```\n\nThis script defines a function `gradient_descent` that can perform three types of gradient descent (Stochastic, Batch, and Mini-Batch) for linear regression using Mean Squared Error as the loss function. The function is self-contained and includes an example usage at the end. The final weights are returned as a python list with each element rounded to the nearest 4th decimal."}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts the given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): The input matrix to be converted to RREF.\n    \n    Returns:\n    list of lists: The RREF of the input matrix.\n    \"\"\"\n    # Convert the input matrix to a numpy array for easier manipulation\n    mat = np.array(matrix, dtype=float)\n    rows, cols = mat.shape\n    lead = 0\n    \n    # Loop through rows\n    for r in range(rows):\n        if cols <= lead:\n            return mat.tolist()\n        \n        # Find the row with the largest absolute value in the current column\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        \n        # Swap rows to bring the largest pivot to the top\n        mat[[i, r]] = mat[[r, i]]\n        \n        # Normalize the pivot row\n        pivot = mat[r, lead]\n        mat[r] = mat[r] / pivot\n        \n        # Eliminate the pivot column in other rows\n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        \n        # Move to the next row and column\n        lead += 1\n    \n    return mat.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    input_matrix = [\n        [1, 2, -1, -4],\n        [-2, -3, 4, 4],\n        [3, 5, -5, -7]\n    ]\n    output_matrix = rref(input_matrix)\n    print(output_matrix)\n```\n\nThis script includes the `rref` function that takes a matrix as its input and returns the matrix in its Reduced Row Echelon Form (RREF). The function uses numpy for its matrix operations and handles cases where rows may be all zeros or where diagonal entries can be zero. The example usage at the bottom demonstrates how to call the function with a sample input matrix and print the resulting RREF matrix."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Implements the Adam optimization algorithm to minimize the function f.\n    \n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): Function that computes the gradient of f.\n    x0 (np.array): Initial parameter values.\n    learning_rate (float): The step size.\n    beta1 (float): Exponential decay rate for the first moment estimates.\n    beta2 (float): Exponential decay rate for the second moment estimates.\n    epsilon (float): A small constant for numerical stability.\n    num_iterations (int): Number of iterations to run the optimizer.\n    \n    Returns:\n    list: Optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    x = x0\n    \n    for i in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return np.round(x, 4).tolist()\n\n# Example usage\ndef quadratic_function(x):\n    return x[0]**2 + 2*x[1]**2\n\ndef gradient(x):\n    return np.array([2*x[0], 4*x[1]])\n\ninitial_point = np.array([3.0, 4.0])\noptimized_params = adam_optimizer(quadratic_function, gradient, initial_point)\nprint(optimized_params)\n```\nThis script provides an implementation of the Adam optimization algorithm for minimizing a given function. It uses the specified parameters and returns the optimized parameters as a list, rounded to the nearest 4th decimal."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using gradient descent with L1 regularization.\n    \n    Parameters:\n    - X: np.array, feature matrix.\n    - y: np.array, target vector.\n    - alpha: float, regularization strength.\n    - learning_rate: float, learning rate for gradient descent.\n    - max_iter: int, maximum number of iterations.\n    - tol: float, tolerance for stopping criteria.\n    \n    Returns:\n    - weights: list, learned weights.\n    - bias: float, learned bias.\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = np.inf\n    \n    for _ in range(max_iter):\n        # Predicted values\n        y_pred = np.dot(X, weights) + bias\n        # Compute the gradients\n        dw = - (2 * np.dot(X.T, y - y_pred) + alpha * np.sign(weights)) / n_samples\n        db = - 2 * np.sum(y - y_pred) / n_samples\n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        # Compute cost\n        cost = (1 / (2 * n_samples)) * np.sum((y - y_pred)**2) + (alpha / (2 * n_samples)) * np.sum(np.abs(weights))\n        # Check for convergence\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    # Return results rounded to 4 decimal places\n    return weights.round(4).tolist(), round(bias, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating some synthetic data\n    np.random.seed(42)\n    X = np.random.rand(100, 1)\n    y = 2 + 3 * X + np.random.rand(100, 1).flatten()\n    \n    # Call the function with the synthetic data\n    weights, bias = l1_regularization_gradient_descent(X, y)\n    print(f\"Weights: {weights}, Bias: {bias}\")\n```\n\nThis script defines a function `l1_regularization_gradient_descent` that implements Lasso Regression using gradient descent with L1 regularization. It takes as input a feature matrix `X`, a target vector `y`, a regularization parameter `alpha`, a learning rate, a maximum number of iterations, and a tolerance for stopping criteria. The function returns the learned weights and bias, both rounded to the nearest 4th decimal. The example usage at the end demonstrates how to generate synthetic data and call the function."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    source (str): The source string.\n    target (str): The target string.\n    \n    Returns:\n    int: The OSA distance between the source and target strings.\n    \"\"\"\n    len_source = len(source)\n    len_target = len(target)\n    \n    # Initialize the matrix with zeros\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Fill the first row and column\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    \n    # Fill the rest of the matrix\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i-1] == target[j-1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(\n                dp[i-1][j] + 1,                # Deletion\n                dp[i][j-1] + 1,                # Insertion\n                dp[i-1][j-1] + cost            # Substitution\n            )\n            # Transposition\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                dp[i][j] = min(dp[i][j], dp[i-2][j-2] + cost)\n    \n    return dp[len_source][len_target]\n\n# Example usage\nsource = \"caper\"\ntarget = \"acer\"\nprint(OSA(source, target))  # Output: 2\n```\n\nThis code snippet defines a function `OSA` that calculates the Optimal String Alignment distance between two strings, incorporating the additional operation of transposition. The dynamic programming approach is used to fill a matrix where each cell represents the minimum number of operations needed to transform a substring of the source into a substring of the target. The final result is found in the bottom-right cell of the matrix."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List[int], true binary labels.\n    - y_pred: List[int], predicted binary labels.\n    \n    Returns:\n    float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate TP and FN\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Recall\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(recall(y_true, y_pred))  # Output should be the recall value\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input sequence of shape (sequence_length, input_dim).\n    W_q (numpy.ndarray): Query weight matrix of shape (input_dim, hidden_dim).\n    W_k (numpy.ndarray): Key weight matrix of shape (input_dim, hidden_dim).\n    W_v (numpy.ndarray): Value weight matrix of shape (input_dim, hidden_dim).\n    \n    Returns:\n    numpy.ndarray: Output of the self-attention mechanism.\n    \"\"\"\n    # Compute the query, key, and value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the attention scores\n    attention_scores = np.dot(Q, K.T)\n    scaled_attention_scores = attention_scores / np.sqrt(K.shape[1])\n    \n    # Apply the softmax function to obtain the attention weights\n    attention_weights = np.exp(scaled_attention_scores) / np.sum(np.exp(scaled_attention_scores), axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the value matrix\n    output = np.dot(attention_weights, V)\n    \n    # Round to the nearest 4th decimal and convert to list\n    output = np.round(output, 4).tolist()\n    \n    return output\n\n# Example usage\nX = np.array([[1, 0], [0, 1], [1, 1]])\nW_q = np.array([[1, 0], [0, 1]])\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 0], [0, 1]])\n\noutput = self_attention(X, W_q, W_k, W_v)\nprint(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell that processes a sequence of input vectors and produces the final hidden state.\n    \n    Parameters:\n    input_sequence (list of list of float): Sequence of input vectors.\n    initial_hidden_state (list of float): Initial hidden state.\n    Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    b (list of float): Bias vector.\n    \n    Returns:\n    list of float: Final hidden state after processing the entire sequence.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize hidden state\n    hidden_state = initial_hidden_state\n    \n    # Process each input in the sequence\n    for x in input_sequence:\n        # Update hidden state using tanh activation\n        hidden_state = np.tanh(Wx @ x + Wh @ hidden_state + b)\n    \n    # Round the final hidden state to 4 decimal places and convert to list\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    \n    return final_hidden_state\n\n# Example usage\ninput_sequence = [[1, 2], [3, 4], [5, 6]]\ninitial_hidden_state = [0, 0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]\n\nfinal_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_state)\n```\n\nThis code defines a simple RNN cell which, given a sequence of inputs, an initial hidden state, and the necessary weight matrices and bias vector, computes the final hidden state after processing the entire sequence. The function uses the tanh activation function for the hidden state updates and ensures that the result is rounded to four decimal places as required."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a set of points.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of [x, y] coordinates.\n    \"\"\"\n    # Convert the list of points to a numpy array for vectorized operations\n    points_array = np.array(points)\n    # Create a translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    # Add a column of ones to the points array for matrix multiplication\n    ones = np.ones((points_array.shape[0], 1))\n    points_homogeneous = np.hstack((points_array, ones))\n    # Perform the translation\n    translated_points_homogeneous = np.dot(translation_matrix, points_homogeneous.T).T\n    # Convert back to the original 2D points format\n    translated_points = translated_points_homogeneous[:, :2].tolist()\n    return translated_points\n\n# Check function to verify the correctness of the code\ndef check_translation():\n    points = [[1, 2], [3, 4], [5, 6]]\n    tx, ty = 10, 20\n    expected_output = [[11, 22], [13, 24], [15, 26]]\n    output = translate_object(points, tx, ty)\n    assert output == expected_output, f\"Expected {expected_output}, but got {output}\"\n    print(\"Test passed!\")\n\n# Check the function with provided data points\ncheck_translation()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the KL divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p (float): Mean of distribution P.\n    - sigma_p (float): Standard deviation of distribution P.\n    - mu_q (float): Mean of distribution Q.\n    - sigma_q (float): Standard deviation of distribution Q.\n    \n    Returns:\n    - float: KL divergence between P and Q.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive\")\n    \n    # KL divergence formula for normal distributions\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n    return kl_div\n\n# Example usage\nif __name__ == \"__main__\":\n    mu_p, sigma_p = 0, 1  # Mean and standard deviation for distribution P\n    mu_q, sigma_q = 2, 2  # Mean and standard deviation for distribution Q\n    print(f\"KL Divergence: {kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method for solving a system of linear equations.\n    \n    :param A: Coefficient matrix (square matrix)\n    :param b: Right-hand side vector\n    :param n: Number of iterations\n    :param x_ini: Initial guess for the solution vector (optional)\n    :return: Approximated solution vector (x) as a list\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n    \n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    for _ in range(n):\n        for i in range(size):\n            s = sum(A[i][j] * x[j] for j in range(size) if j != i)\n            x[i] = (b[i] - s) / A[i][i]\n    \n    # Round to 4 decimal places and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage:\nA = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\nb = [4, 7, 3]\niterations = 100\n\nsolution = gauss_seidel(A, b, iterations)\nprint(solution)\n```\nThis script defines the `gauss_seidel` function which implements the Gauss-Seidel method for solving a system of linear equations given by `Ax = b`. The function accepts an optional initial guess for the solution vector and returns the approximated solution vector after a specified number of iterations, with each value rounded to the nearest 4th decimal place."}
{"task_id": 58, "completion_id": 0, "completion": "Below is a self-contained Python script that implements the Gaussian Elimination method with partial pivoting to solve a system of linear equations, as per the task description. The implementation includes backward substitution to find the solution vector.\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the system of linear equations Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Constant terms vector.\n    \n    Returns:\n    list: Solution vector x rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    # Augment A with b\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    \n    # Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        pivot_row = i + np.argmax(np.abs(Ab[i:, i]))\n        # Swap rows\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        # Eliminate\n        for j in range(i+1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n-1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    \n    # Rounding the solution to 4 decimal places and converting to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nsolution = gaussian_elimination(A, b)\nprint(\"Solution vector:\", solution)\n```\n\nThis script defines the `gaussian_elimination` function, which performs Gaussian Elimination with partial pivoting and backward substitution to solve the linear system. It then provides an example of how to use the function with a sample system of equations. The solution vector is printed out, rounded to four decimal places."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n        \n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    def tanh(self, x):\n        return np.tanh(x)\n    \n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n        \n        for t in range(len(x)):\n            # Concatenate input and hidden state\n            concat = np.vstack((hidden_state, x[t]))\n            \n            # Compute gates\n            forget_gate = self.sigmoid(self.Wf @ concat + self.bf)\n            input_gate = self.sigmoid(self.Wi @ concat + self.bi)\n            candidate_cell = self.tanh(self.Wc @ concat + self.bc)\n            output_gate = self.sigmoid(self.Wo @ concat + self.bo)\n            \n            # Update cell state\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell\n            \n            # Update hidden state\n            hidden_state = output_gate * self.tanh(cell_state)\n            \n            # Collect hidden states\n            hidden_states.append(hidden_state)\n        \n        # Collect final hidden state and cell state\n        final_hidden_state = hidden_state\n        final_cell_state = cell_state\n        \n        # Convert to lists and round to 4 decimal places\n        hidden_states_rounded = [np.round(state, 4).tolist() for state in hidden_states]\n        final_hidden_state_rounded = np.round(final_hidden_state, 4).tolist()\n        final_cell_state_rounded = np.round(final_cell_state, 4).tolist()\n        \n        return hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded\n\n# Example usage\ninput_size = 3\nhidden_size = 2\nlstm = LSTM(input_size, hidden_size)\nx = [np.random.randn(input_size, 1) for _ in range(5)]  # Example input sequence\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))\n\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\nprint(\"Hidden States:\", hidden_states)\nprint(\"Final Hidden State:\", final_hidden_state)\nprint(\"Final Cell State:\", final_cell_state)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus is empty.\")\n    \n    # Calculate term frequencies for each document\n    def term_frequency(doc, term):\n        return doc.count(term) / len(doc)\n    \n    # Calculate inverse document frequency for each term\n    def inverse_document_frequency(corpus, term):\n        df = sum(1 for doc in corpus if term in doc) + 1  # Add 1 for smoothing\n        return math.log(len(corpus) / df)\n    \n    # Compute TF-IDF scores for each query term in each document\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_scores = []\n        for term in query:\n            tf = term_frequency(doc, term)\n            idf = inverse_document_frequency(corpus, term)\n            doc_scores.append(round(tf * idf, 4))\n        tf_idf_scores.append(doc_scores)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage\ncorpus = [[\"hello\", \"world\"], [\"good\", \"morning\", \"world\"], [\"hello\", \"good\", \"bye\"]]\nquery = [\"hello\", \"good\", \"world\"]\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script provides a robust implementation of the TF-IDF computation for a given query and corpus. It handles edge cases and ensures that the output is rounded to the nearest fourth decimal place, as required."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_beta_score = 0\n    else:\n        f_beta_score = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta_score, 3)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0\nprint(f_score(y_true, y_pred, beta))  # Output: 0.75\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Processes a sequence of inputs and returns the output, the last inputs and the hidden states.\n        \"\"\"\n        T = len(input_sequence)\n        hiddens = [np.zeros((self.hidden_size, 1))]  # h_0 is a zero vector\n        outputs = []\n        last_inputs = []\n\n        for t in range(T):\n            x = input_sequence[t].reshape(-1, 1)\n            last_inputs.append(x)\n            h = np.tanh(self.W_xh @ x + self.W_hh @ hiddens[-1] + self.b_h)\n            hiddens.append(h)\n            y = self.W_hy @ h + self.b_y\n            outputs.append(y)\n\n        return outputs, last_inputs, hiddens\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, last_inputs, hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hiddens[0])\n\n        # Compute loss gradient for each time step and accumulate\n        for t in reversed(range(T)):\n            y = outputs[t]\n            x = last_inputs[t]\n            h = hiddens[t + 1]\n\n            # Output layer gradient\n            dy = y - expected_output[t].reshape(-1, 1)\n            dW_hy += dy @ h.T\n            db_y += dy\n\n            # Hidden layer gradient\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h ** 2) * dh\n            dW_xh += dh_raw @ x.T\n            dW_hh += dh_raw @ hiddens[t].T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n\n        # Update weights with gradient descent\n        for param, dparam in zip([self.W_xh, self.W_hh, self.W_hy, self.b_h, self.b_y],\n                                 [dW_xh, dW_hh, dW_hy, db_h, db_y]):\n            param -= learning_rate * dparam\n\n        # Compute loss for debugging\n        loss = np.sum(0.5 * (expected_output - np.hstack(outputs)) ** 2) / T\n        return loss\n\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 2\n    hidden_size = 2\n    output_size = 2\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n    input_sequence = [np.array([1, 0]), np.array([0, 1]), np.array([1, 1])]\n    expected_output = [np.array([0, 1]), np.array([1, 0]), np.array([1, 1])]\n    outputs, last_inputs, hiddens = rnn.rnn_forward(input_sequence)\n    loss = rnn.rnn_backward(input_sequence, expected_output, outputs, last_inputs, hiddens, 0.1)\n    print(f\"Loss: {loss}\")\n```\nThis script defines a simple RNN and implements both the forward pass and backpropagation through time (BPTT) for training the network. The example usage at the bottom demonstrates how to initialize the RNN, perform a forward pass, and then adjust the weights using backpropagation based on the expected output."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    \n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + (rsnew / rsold) * p\n        rsold = rsnew\n    \n    return x.round(8).tolist()\n\n# Example usage:\n# Define a symmetric positive-definite matrix A and a vector b\nA = np.array([[4, 1], [1, 3]], dtype=float)\nb = np.array([1, 2], dtype=float)\nn = 100  # Maximum number of iterations\n\n# Call the conjugate_gradient function\nsolution = conjugate_gradient(A, b, n)\nprint(solution)\n```\n\nThis script contains a function `conjugate_gradient` that implements the Conjugate Gradient method for solving the linear system Ax = b, as specified in the task. It also includes an example usage section that demonstrates how to call this function with a sample matrix `A` and vector `b`."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = np.bincount(y)\n    probabilities = class_counts / float(len(y))\n    gini = 1 - np.sum(probabilities**2)\n    return round(gini, 3)\n\n# Example usage:\n# y = [1, 1, 2, 2, 3, 3, 3, 3]\n# print(gini_impurity(y))  # Output should be 0.643\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    import numpy as np\n\n    # Initialize the lists\n    values = []\n    column_indices = []\n    row_pointers = [0]  # The first row pointer is always 0\n\n    # Loop through each row in the dense matrix\n    for row in dense_matrix:\n        # Count the non-zero elements and append their value and column index\n        for col_idx, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n        # Calculate the row pointer for the next row\n        row_pointers.append(len(values))\n\n    # Convert the lists to numpy arrays for more efficient storage and computation\n    values_array = np.array(values)\n    column_indices_array = np.array(column_indices)\n    row_pointers_array = np.array(row_pointers)\n\n    return values_array, column_indices_array, row_pointers_array\n\n# Example usage:\ndense_matrix = [\n    [1, 0, 0, 2],\n    [0, 3, 0, 0],\n    [4, 0, 5, 0]\n]\n\nvalues, col_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", col_indices)\nprint(\"Row Pointers:\", row_pointers)\n```\n\nThis code defines a function `compressed_row_sparse_matrix` that converts a dense matrix into its CSR format. It uses lists to collect the non-zero values, their column indices, and the row pointers. The row pointers are calculated by keeping track of the cumulative count of non-zero elements as each row is processed. The final output is a tuple of numpy arrays for the values, column indices, and row pointers, providing a compact representation of the sparse matrix."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    v = np.array(v)\n    L = np.array(L)\n    \n    # Calculate the projection using the formula: proj_L(v) = (v\u00b7L / ||L||^2) * L\n    projection = ((np.dot(v, L) / np.dot(L, L)) * L).round(3)\n    \n    # Convert the numpy array back to list before returning\n    return projection.tolist()\n\n# Example usage:\n# vector v to be projected\nv = [1, 2, 3]\n# line vector L\nL = [4, 5, 6]\nprint(orthogonal_projection(v, L))  # Output should be the projection of v onto L\n```\n\nThis solution uses the mathematical formula for orthogonal projection and the `numpy` library to perform the necessary vector operations. The result is rounded to three decimal places as requested."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    # Initialize the column pointer with the first element as 0\n    col_ptr = [0]\n    # Initialize the values and row_indices lists\n    values = []\n    row_indices = []\n\n    # Iterate over the matrix in column-major order\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        # Append the current length of values list to col_ptr\n        col_ptr.append(len(values))\n\n    return (values, row_indices, col_ptr)\n\n# Example usage\ndense_matrix = [\n    [0, 1, 0],\n    [2, 0, 3],\n    [0, 4, 0]\n]\n\ncsc_representation = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", csc_representation[0])\nprint(\"Row Indices:\", csc_representation[1])\nprint(\"Column Pointer:\", csc_representation[2])\n```\n\nThis script defines the `compressed_col_sparse_matrix` function that takes a dense matrix and converts it into its Compressed Column Sparse (CSC) representation. The CSC format is represented by a tuple containing three lists: `values`, `row_indices`, and `column_pointer`. The provided example demonstrates how to use this function with a sample dense matrix and prints out the resulting CSC representation."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n    Returns the basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix and correspond to the independent columns.\n    \"\"\"\n    # Convert the input to a numpy matrix\n    A = np.array(A)\n    # Get the row echelon form of A\n    A_rref, pivot_columns = to_row_echelon_form(A)\n    # Extract the corresponding columns from the original matrix A\n    basis_vectors = A[:, pivot_columns]\n    # Round to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n\ndef to_row_echelon_form(A):\n    \"\"\"\n    Converts a matrix to its row echelon form and returns the pivot columns.\n    \"\"\"\n    A = np.array(A, dtype=float) # Ensure the matrix is in float for division\n    m, n = A.shape\n    pivot_columns = []\n    i, j = 0, 0\n    while i < m and j < n:\n        # Find the maximum element in the current column\n        max_row = i + np.argmax(np.abs(A[i:, j]))\n        if A[max_row, j] == 0:\n            j += 1\n            continue\n        # Swap rows\n        A[[i, max_row]] = A[[max_row, i]]\n        pivot_columns.append(j)\n        # Make all elements below the pivot zero\n        for k in range(i + 1, m):\n            factor = A[k, j] / A[i, j]\n            A[k, j:] -= factor * A[i, j:]\n        i += 1\n        j += 1\n    return A, pivot_columns\n\n# Example usage\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix_image(A))\n```\n\nThe provided Python script defines a function `matrix_image(A)` that computes the column space (image) of a given matrix `A`. The function first converts the input matrix to its row echelon form and identifies the pivot columns, which correspond to the independent columns of the original matrix. These columns are then extracted and returned as the basis vectors that span the column space of `A`. The result is rounded to 8 decimal places and converted to a list before being returned."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) for a regression model.\n    \n    Parameters:\n    y_true : np.ndarray\n        Array of true values.\n    y_pred : np.ndarray\n        Array of predicted values.\n        \n    Returns:\n    float\n        The R-squared value rounded to three decimal places.\n    \"\"\"\n    # Calculate the total sum of squares\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    # Calculate the residual sum of squares\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    # Calculate the R-squared value\n    r2 = 1 - (ss_res / ss_tot)\n    return np.round(r2, 3)\n\n# Example usage\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(r_squared(y_true, y_pred))\n```\n\nThe provided script defines a function `r_squared` that calculates the R-squared value for regression analysis given the true and predicted values. The function uses numpy for efficient computation and rounds the result to three decimal places as required."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D matrix.\n    Returns the average brightness rounded to two decimal places, or -1 for invalid inputs.\n    \"\"\"\n    # Check if the image is empty\n    if not img or not img[0]:\n        return -1\n    \n    total_brightness = 0\n    pixel_count = 0\n    \n    # Iterate over the image matrix to calculate total brightness\n    for row in img:\n        if len(row) != len(img[0]):  # Check for inconsistent row lengths\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:  # Check for invalid pixel values\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    \n    # Calculate the average brightness\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    \n    return round(average_brightness, 2)\n\n# Example usage\nimage = [\n    [100, 150, 200],\n    [120, 130, 140],\n    [200, 210, 220]\n]\nprint(calculate_brightness(image))  # Should print the average brightness of the image\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of actual values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have different shapes or are empty.\n    TypeError: If input types are not numpy arrays or lists.\n    \"\"\"\n    # Check if inputs are numpy arrays or can be converted\n    if not isinstance(y_true, (np.ndarray, list)) or not isinstance(y_pred, (np.ndarray, list)):\n        raise TypeError(\"Inputs must be numpy arrays or lists.\")\n    \n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check for empty arrays\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check if the shapes of the input arrays are the same\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate RMSE\n    squared_errors = (y_true - y_pred) ** 2\n    mse = np.mean(squared_errors)\n    rmse_val = np.sqrt(mse)\n    \n    return round(rmse_val, 3)\n\n# Test the function with some example data\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(rmse(y_true, y_pred))  # Expected output: 0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    - y_true: A binary numpy array of true labels.\n    - y_pred: A binary numpy array of predicted labels.\n    \n    Returns:\n    - The Jaccard Index as a float rounded to three decimal places.\n    \"\"\"\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where union is zero to avoid division by zero\n    if union == 0:\n        return 0.0\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    # Return rounded Jaccard Index\n    return round(jaccard, 3)\n\n# Example usage\ny_true = np.array([1, 1, 0, 0, 1, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])\n\nprint(jaccard_index(y_true, y_pred))\n```\n\nIn this solution, the `jaccard_index` function is defined to calculate the Jaccard Index for binary classification tasks. It uses `numpy` for efficient computation of intersection and union, and handles edge cases such as no overlap or all zeros in the input arrays. The result is rounded to three decimal places to ensure consistency with the output constraints."}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.ndarray): True binary labels.\n    y_pred (np.ndarray): Predicted binary labels.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of true and predicted labels\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the number of elements in each set\n    elements_true = np.sum(y_true)\n    elements_pred = np.sum(y_pred)\n    \n    # Calculate the Dice Score\n    if elements_true == 0 and elements_pred == 0:\n        # Handle edge case where there are no true or predicted positives\n        dice = 1.0\n    else:\n        dice = (2.0 * intersection) / (elements_true + elements_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Check function with provided data points\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(dice_score(y_true, y_pred))  # Expected output: 0.8\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_hv(seed, dim):\n    \"\"\"Generate a hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.where(np.random.rand(dim) > 0.5, 1, -1)\n\ndef bind_hvs(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication and normalization.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors by averaging them.\"\"\"\n    return np.mean(hvs, axis=0)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row.\"\"\"\n    feature_hvs = []\n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_hv = create_hv(hash(feature), dim)\n        # Generate hypervector for the value using the feature's seed\n        value_hv = create_hv(random_seeds[feature], dim)\n        # Bind the feature and value hypervectors\n        bound_hv = bind_hvs(feature_hv, value_hv)\n        # Add to the list of feature hypervectors\n        feature_hvs.append(bound_hv)\n    # Bundle all feature hypervectors to create the row hypervector\n    row_hv = bundle_hvs(feature_hvs)\n    return row_hv.tolist()\n\n# Example usage:\ndim = 10\nrow = {'age': 30, 'income': 50000, 'education': 'bachelor'}\nrandom_seeds = {'age': 10, 'income': 20, 'education': 30}\nprint(create_row_hv(row, dim, random_seeds))\n```\n\nThis script includes the necessary functions to generate hypervectors for individual features and their values, bind them together, and then bundle them to create a composite hypervector for the entire row. The `hash()` function is used to create a consistent seed for the feature names, ensuring that the same feature always gets the same hypervector representation. The `random_seeds` dictionary provides the seeds for generating hypervectors for the values of each feature, ensuring reproducibility."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List\n\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    Parameters:\n    - data: A list of lists, where each inner list is [y_true, y_pred].\n    \n    Returns:\n    - A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    # Initialize the confusion matrix with zeros\n    cm = [[0, 0],\n          [0, 0]]\n    \n    # Count the occurrences of each pair\n    counts = Counter(tuple(pair) for pair in data)\n    \n    # Fill the confusion matrix based on counts\n    cm[0][0] = counts[(0, 0)]  # True Negatives\n    cm[0][1] = counts[(0, 1)]  # False Positives\n    cm[1][0] = counts[(1, 0)]  # False Negatives\n    cm[1][1] = counts[(1, 1)]  # True Positives\n    \n    return cm\n\n# Check function to test the confusion_matrix function with provided data points\ndef check_confusion_matrix():\n    test_data = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1]]\n    expected_output = [[2, 1], [1, 2]]\n    assert confusion_matrix(test_data) == expected_output, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Run the check function\ncheck_confusion_matrix()\n```\n\nThis script provides a function `confusion_matrix` that calculates the confusion matrix for binary classification based on the provided data. It also includes a check function to verify the correctness of the implementation with a simple test case."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (numpy.ndarray): First input vector.\n    v2 (numpy.ndarray): Second input vector.\n    \n    Returns:\n    float: Cosine similarity rounded to three decimal places.\n    \"\"\"\n    # Ensure the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both input vectors must have the same shape.\")\n    \n    # Ensure vectors are not empty and have non-zero magnitude\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Input vectors cannot be empty or have zero magnitude.\")\n    \n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n    # Calculate the norm (magnitude) of the vectors\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    # Calculate cosine similarity\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    \n    # Return the cosine similarity, rounded to three decimal places\n    return round(cos_sim, 3)\n\n# Example usage\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))\n```\n\nThis script defines a `cosine_similarity` function that computes the cosine similarity between two vectors, ensuring that the vectors meet the specified constraints. The example usage demonstrates how to call the function with two vectors, `v1` and `v2`."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nimport numpy as np\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification problem.\n    \n    Parameters:\n    actual (list of int): The actual class labels.\n    predicted (list of int): The predicted class labels.\n    \n    Returns:\n    tuple: A tuple containing confusion matrix, accuracy, F1 score, specificity,\n           and negative predictive value.\n    \"\"\"\n    # Validate input\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of 'actual' and 'predicted' lists must be the same.\")\n    \n    # Calculate confusion matrix\n    confusion_matrix = np.zeros((2, 2), dtype=int)\n    for a, p in zip(actual, predicted):\n        confusion_matrix[a, p] += 1\n    \n    tp = confusion_matrix[1, 1]  # True positives\n    tn = confusion_matrix[0, 0]  # True negatives\n    fp = confusion_matrix[0, 1]  # False positives\n    fn = confusion_matrix[1, 0]  # False negatives\n    \n    # Calculate metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if (tn + fp) > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if (tn + fn) > 0 else 0\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n\n# Example usage\nactual = [1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\nmetrics = performance_metrics(actual, predicted)\nfor metric in metrics:\n    print(metric)\n```"}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): The dataset for which to calculate statistics.\n    \n    Returns:\n    dict: A dictionary containing the calculated statistics.\n    \"\"\"\n    # Convert data to numpy array if it's not\n    data = np.array(data)\n    \n    # Calculate statistics\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data).mode[0]  # Mode can return multiple values, take the first\n    variance = np.var(data, ddof=1).round(4)  # Sample variance\n    standard_deviation = np.std(data, ddof=1).round(4)  # Sample standard deviation\n    percentile_25 = np.percentile(data, 25).round(4)\n    percentile_50 = np.percentile(data, 50).round(4)\n    percentile_75 = np.percentile(data, 75).round(4)\n    iqr = (percentile_75 - percentile_25).round(4)\n    \n    # Create and return the dictionary\n    statistics = {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': standard_deviation,\n        '25th_percentile': percentile_25,\n        '50th_percentile': percentile_50,\n        '75th_percentile': percentile_75,\n        'interquartile_range': iqr\n    }\n    return statistics\n\n# Example usage\ndata = np.random.normal(loc=50, scale=10, size=100)\nresult = descriptive_statistics(data)\nprint(result)\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# print(binomial_probability(10, 3, 0.5))  # Should output the probability of getting exactly 3 heads in 10 coin flips\n```\n\nThis Python script defines a function `binomial_probability` that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials, each with a probability `p` of success, using the Binomial distribution formula. The result is returned rounded to five decimal places to meet the output constraints specified in the task."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\n\n# Example usage:\n# Calculate the PDF of a normal distribution with mean 0 and standard deviation 1 at x = 0\nprint(normal_pdf(0, 0, 1))  # Output should be approximately 0.39894\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    try:\n        # Calculate the factorial of k\n        factorial_k = math.factorial(k)\n        # Calculate e^(-lam)\n        exp_term = math.exp(-lam)\n        # Calculate lam^k\n        lam_pow_k = lam ** k\n        # Poisson probability formula: (lam^k * e^(-lam)) / k!\n        probability = (lam_pow_k * exp_term) / factorial_k\n        # Round the result to 5 decimal places\n        return round(probability, 5)\n    except Exception as e:\n        # In case of an error, print the error and return None\n        print(f\"An error occurred: {e}\")\n        return None\n\n# Check function with provided data points\nprint(poisson_probability(3, 2))  # Expected output: 0.18045\nprint(poisson_probability(0, 1))  # Expected output: 0.36788\nprint(poisson_probability(5, 5))  # Expected output: 0.17547\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    # Ensure the image is a numpy array\n    img = np.array(img)\n    # Find the maximum and minimum pixel values in the image\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    # Calculate the contrast as the difference between the maximum and minimum pixel values\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\n\n# Example usage:\n# Create a sample 2D array representing a grayscale image\nsample_img = np.array([[10, 20, 30],\n                       [40, 255, 60],\n                       [70, 80, 90]])\n\n# Calculate and print the contrast of the sample image\nprint(\"Contrast of the image:\", calculate_contrast(sample_img))\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\n# vec1 = np.array([1, 2, 3])\n# vec2 = np.array([4, 5, 6])\n# print(calculate_dot_product(vec1, vec2))  # Expected output: 32\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the list to a numpy array for vectorized operations\n    data_array = np.array(data).reshape(-1, 1)\n    \n    # Generate the polynomial features\n    poly_features = []\n    for deg in range(1, degree + 1):\n        poly_features.append(data_array ** deg)\n    \n    # Concatenate all the features and round to 8 decimal places\n    transformed_data = np.concatenate(poly_features, axis=1)\n    transformed_data_rounded = np.round(transformed_data, 8)\n    \n    # Convert back to a list of lists for the output\n    return transformed_data_rounded.tolist()\n\n# Check function with provided data points\ndata = [1.0, 2.0, 3.0]\ndegree = 3\ntransformed_data = phi_transform(data, degree)\nprint(transformed_data)\n```\n\nThe provided code snippet will generate polynomial features for the input data up to the specified degree. If the degree is negative, it will return an empty list as required. It ensures that the output is rounded to 8 decimal places and is in the form of a list of lists, as specified."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a given position and model dimensionality.\n    \n    Args:\n    position (int): The length of the sequence.\n    d_model (int): The dimensionality of the model.\n    \n    Returns:\n    list: The positional encoding as a list, or -1 if input parameters are invalid.\n    \n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle_rate = pos / np.power(10000, (2 * i) / d_model)\n            pe[pos, i] = np.sin(angle_rate)\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(angle_rate)\n    return pe.tolist()\n\n# Example check function to verify the correctness of the generated function\ndef check_pos_encoding():\n    print(\"Checking a few values for correctness:\")\n    result = pos_encoding(3, 4)\n    expected = [[0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, -1.0], [0.0, -1.0, 0.0, 1.0]]\n    assert len(result) == len(expected), \"The result length does not match the expected length.\"\n    for res, exp in zip(result, expected):\n        assert len(res) == len(exp), \"The result sub-array length does not match the expected sub-array length.\"\n        for r, e in zip(res, exp):\n            assert np.isclose(r, e, atol=1e-4), f\"Value {r} does not closely match expected {e}.\"\n    print(\"All checks passed!\")\n\n# Run the check function\ncheck_pos_encoding()\n```\n\nThis code snippet defines a function `pos_encoding` that calculates the positional encodings for a given sequence length and model dimensionality, adhering to the constraints and requirements specified in the task. It also includes a simple check function to verify the correctness of the positional encoding calculation."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    # Check for underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # Good fit if neither overfitting nor underfitting\n    else:\n        return 0\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    print(model_fit_quality(0.9, 0.6))  # Expected output: 1 (Overfitting)\n    print(model_fit_quality(0.65, 0.6))  # Expected output: -1 (Underfitting)\n    print(model_fit_quality(0.8, 0.75))  # Expected output: 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first and second moment estimates\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Apply bias correction\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Ensure scalar and array inputs are handled\n    if isinstance(parameter, np.ndarray):\n        updated_parameter = parameter.tolist()\n        updated_m = m.tolist()\n        updated_v = v.tolist()\n    else:\n        updated_parameter = parameter\n        updated_m = m\n        updated_v = v\n    \n    # Round to 5 decimal places\n    updated_parameter = np.round(updated_parameter, 5)\n    updated_m = np.round(updated_m, 5)\n    updated_v = np.round(updated_v, 5)\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(updated_parameter, updated_m, updated_v)\n```\n\nThis script implements the Adam optimizer update step function with the specified parameters and constraints. It handles both scalar and array inputs and applies bias correction to the moving averages. The results are rounded to 5 decimal places and converted to a list if necessary."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    \n    # Ensure we don't exceed context length\n    if len(token_ids) + n_tokens_to_generate > hparams[\"n_ctx\"]:\n        raise ValueError(\"Prompt and generation length exceed model's context size.\")\n    \n    # Token embeddings\n    token_embeddings = params[\"wte\"][token_ids]\n    \n    # Positional embeddings\n    pos_embeddings = params[\"wpe\"][:len(token_ids)]\n    \n    # Sum token and positional embeddings\n    x = token_embeddings + pos_embeddings\n    \n    # Multi-head attention and feed-forward network (simplified)\n    # Normally, this would involve complex operations, but for simplicity, we'll skip them.\n    # Assume that after these operations, the output is still x\n    \n    # Layer normalization\n    def layer_norm(x, g, b, eps=1e-12):\n        u = x.mean(-1, keepdims=True)\n        s = (x - u).mean(-1, keepdims=True) ** 0.5\n        x = (x - u) / (s + eps)\n        return x * g + b\n    \n    x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n    \n    # Assuming the last layer output is directly used to predict the next token\n    # In a real model, this would involve a softmax operation to get probabilities\n    # and sampling from the distribution, but for simplicity, we'll just take argmax\n    next_token_id = np.argmax(x[-1])\n    \n    # Extend token_ids with the generated token\n    token_ids.append(next_token_id)\n    \n    # Repeat the process for the desired number of tokens\n    for _ in range(n_tokens_to_generate - 1):\n        token_embeddings = params[\"wte\"][next_token_id]\n        x = token_embeddings + pos_embeddings[-1]\n        x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        next_token_id = np.argmax(x)\n        token_ids.append(next_token_id)\n    \n    # Decode the generated token IDs back to text\n    generated_text = encoder.decode(token_ids)\n    return generated_text\n\n# Example usage\nprint(gen_text(\"hello world\", 5))\n```\n\nThis script provides a simplified version of the GPT-2 text generation process. It includes token and positional embeddings, a placeholder for multi-head attention and feed-forward network operations, layer normalization, and a mechanism for generating text based on the provided prompt. Note that this is a highly simplified version and does not represent the full complexity of the GPT-2 model."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function first\n        e_values = np.exp(values - np.max(values))\n        return e_values / e_values.sum()\n    \n    # Initialize the weights for the self-attention mechanism\n    weights = np.random.rand(n, dimension)\n    \n    # Transform the crystal values into the higher dimensional space\n    transformed_crystals = np.array(crystal_values)[:, None] * weights\n    \n    # Calculate attention scores\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Calculate the dot product between the crystals in the higher dimensional space\n                dot_product = np.dot(transformed_crystals[i], transformed_crystals[j])\n                attention_scores[i, j] = dot_product\n    \n    # Apply softmax to the attention scores to get the attention weights\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=attention_scores)\n    \n    # Calculate the final weighted pattern\n    weighted_patterns = np.dot(attention_weights, transformed_crystals)\n    \n    # Return the final patterns, rounded to the 4th decimal place\n    return [round(pattern, 4) for pattern in np.mean(weighted_patterns, axis=1)]\n\n# Example usage:\nn = 5\ncrystal_values = [1, 2, 3, 4, 5]\ndimension = 3\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script defines a function `pattern_weaver` that takes the number of crystals `n`, a list of their values `crystal_values`, and the dimension in which to transform the crystals `dimension` as input. It then performs a simplified self-attention mechanism, where each crystal's value is first transformed into a higher dimensional space, attention scores are calculated using the dot product between each pair of crystals, and softmax is applied to these scores to derive attention weights. Finally, the weighted patterns are computed and returned, with each value rounded to the 4th decimal place."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus based on the query.\n    \n    :param corpus: List of documents, where each document is represented as a list of words.\n    :param query: A list of words representing the query.\n    :param k1: Free parameter, usually set to 1.5 or 2.0.\n    :param b: Free parameter, usually set to 0.75.\n    :return: A list of BM25 scores for each document in the corpus.\n    \"\"\"\n    # Calculate the average document length\n    avgdl = sum(len(doc) for doc in corpus) / len(corpus)\n    \n    # Calculate IDF for each term in the query\n    idf = {term: np.log((len(corpus) - sum(term in doc for doc in corpus) + 0.5) / (sum(term in doc for doc in corpus) + 0.5)) for term in query}\n    \n    scores = []\n    for doc in corpus:\n        score = 0\n        doc_len = len(doc)\n        for term in query:\n            tf = doc.count(term)\n            numerator = idf[term] * tf * (k1 + 1)\n            denominator = tf + k1 * (1 - b + b * doc_len / avgdl)\n            score += numerator / denominator\n        scores.append(score)\n    \n    # Round scores to three decimal places\n    return [round(score, 3) for score in scores]\n\n# Example usage\ncorpus = [\n    ['the', 'cat', 'in', 'the', 'hat'],\n    ['the', 'dog', 'ate', 'my', 'homework'],\n    ['the', 'cat', 'ate', 'the', 'fish']\n]\nquery = ['the', 'cat']\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines a function `calculate_bm25_scores` that calculates BM25 scores for a given corpus and query. It uses the provided parameters `k1` and `b` to adjust the scoring according to the BM25 formula. The example usage demonstrates how to call the function with a simple corpus and query, and it prints out the BM25 scores for each document in the corpus."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize true positive, false positive, and false negative counters\n    tp = fp = fn = 0\n    \n    # Convert lists to sets for easier operations\n    set_true = set(y_true)\n    set_pred = set(y_pred)\n    \n    # Calculate true positives, false positives, and false negatives\n    for label in set_true.union(set_pred):\n        if label in set_true and label in set_pred:\n            tp += min(y_true.count(label), y_pred.count(label))\n        elif label in set_pred:\n            fp += y_pred.count(label)\n        elif label in set_true:\n            fn += y_true.count(label)\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1 score\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Return F1 score rounded to three decimal places\n    return round(f1, 3)\n\n# Example usage\ny_true = [0, 1, 2, 1, 0]\ny_pred = [0, 2, 1, 0, 0]\nprint(calculate_f1_score(y_true, y_pred))  # Output will depend on the input data\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Subtract daily fluctuation, fit a linear regression model to the detrended data,\n    predict day 15's base consumption, add back the fluctuation for day 15,\n    and finally include a 5% safety margin.\n    \"\"\"\n    # Step 1: Subtract the daily fluctuation from each data point\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    fluctuation = 10 * np.sin(2 * PI * days / 10)\n    detrended_data = np.array(consumption_data).reshape(-1, 1) - fluctuation\n\n    # Step 2: Perform linear regression on the detrended data\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n\n    # Step 3: Predict day 15's base consumption\n    day_15 = np.array([[15]])\n    base_day_15 = model.predict(day_15)\n\n    # Step 4: Add the day 15 fluctuation back\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = base_day_15 + fluctuation_day_15\n\n    # Step 5: Round, then add a 5% safety margin (rounded up)\n    final_prediction = math.ceil(predicted_consumption * 1.05)\n\n    return final_prediction[0]\n\n# Example usage:\nconsumption_data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script will provide the solution to the problem as described in the task. It uses the `numpy` library for numerical operations and `sklearn`'s `LinearRegression` for performing the linear regression. The example usage provides a simple linear consumption data for demonstration purposes."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    return round(mean_absolute_error, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices for the self-attention mechanism.\n    :param X: Input data matrix.\n    :param W_q: Weight matrix for the Query.\n    :param W_k: Weight matrix for the Key.\n    :param W_v: Weight matrix for the Value.\n    :return: Tuple containing the Query, Key, and Value matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention for a single head.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: The attention output.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), 1, scores)\n    output = np.dot(attention_weights, V)\n    return output\n\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention.\n    :param X: Input data matrix.\n    :param W_q: Weight matrix for the Query.\n    :param W_k: Weight matrix for the Key.\n    :param W_v: Weight matrix for the Value.\n    :param n_heads: Number of attention heads.\n    :return: The attention output as a list.\n    \"\"\"\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    head_size = Q.shape[-1] // n_heads\n    outputs = []\n    for i in range(n_heads):\n        Q_head = Q[:, i * head_size:(i + 1) * head_size]\n        K_head = K[:, i * head_size:(i + 1) * head_size]\n        V_head = V[:, i * head_size:(i + 1) * head_size]\n        output = self_attention(Q_head, K_head, V_head)\n        outputs.append(output)\n    concatenated = np.hstack(outputs)\n    rounded = np.round(concatenated, 4)\n    return rounded.tolist()\n\n\n# Example usage\nX = np.array([[1, 2, 3], [4, 5, 6]])\nW_q = np.array([[1, 0], [0, 1], [1, 1]])\nW_k = np.array([[1, 0], [0, 1], [1, 1]])\nW_v = np.array([[1, 0], [0, 1], [1, 1]])\nn_heads = 2\nresult = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(result)\n```\n\nThis script implements the multi-head attention mechanism, a core component of the Transformer model, by first computing the Query, Key, and Value matrices, then calculating the self-attention for each head, and finally combining the outputs of all heads to produce the final result. The output is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    \n    # Calculate the contingency table\n    contingency_table = np.array([\n        [np.sum((x_arr == 0) & (y_arr == 0)), np.sum((x_arr == 0) & (y_arr == 1))],\n        [np.sum((x_arr == 1) & (y_arr == 0)), np.sum((x_arr == 1) & (y_arr == 1))]\n    ])\n    \n    # Perform chi-squared test and get the chi2 statistic\n    chi2, _, _, _ = chi2_contingency(contingency_table)\n    \n    # Calculate the Phi coefficient\n    phi = np.sqrt(chi2 / np.sum(contingency_table))\n    \n    return round(phi, 4)\n\n# Example usage\nx = [0, 0, 1, 1, 0, 1, 0, 1]\ny = [0, 1, 0, 0, 1, 1, 1, 0]\nprint(phi_corr(x, y))  # Output should be the Phi coefficient between x and y\n```"}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Test the function with some input values\ntest_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\nfor value in test_values:\n    print(f\"hard_sigmoid({value}) = {hard_sigmoid(value)}\")\n```\n\nThis script defines the `hard_sigmoid` function and tests it with a variety of input values to demonstrate its behavior across the range of possible inputs."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n        \n# Example usage and check function\ndef check_elu():\n    test_cases = [\n        (-1.0, 1.0, -0.6321),\n        (0.0, 1.0, 0.0000),\n        (1.0, 1.0, 1.0000),\n        (-2.0, 0.5, -0.3935)\n    ]\n    for x, alpha, expected in test_cases:\n        result = round(elu(x, alpha), 4)\n        assert result == expected, f\"elu({x}, {alpha}) = {result}, expected {expected}\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the implementation\ncheck_elu()\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test the prelu function with a few data points\n    test_values = [-1.0, 0.0, 1.0, -2.0, 2.0]\n    alpha = 0.25\n    for val in test_values:\n        print(f\"PReLU({val}) = {prelu(val, alpha)}\")\n```\n\nThis script defines the `prelu` function as requested, and includes a simple test case to demonstrate its usage with a few input values. The function uses the `max` function to ensure that positive values remain unchanged and applies the `alpha` parameter to negative values using the `min` function."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handling edge cases to prevent numerical overflow or underflow\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    print(softplus(1.0))  # Expected output: 1.3133\n    print(softplus(-1.0)) # Expected output: 0.3133\n    print(softplus(100.0)) # Edge case, expected output: 100.0\n    print(softplus(-100.0)) # Edge case, expected output: 0.0\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_value = 2.0\n    print(softsign(test_value))  # Expected output: 0.6667\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the clipped objective\n    clipped_objective = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # Compute the KL divergence between old and reference policy\n    kl_divergence = np.mean(np.log(pi_theta_old / pi_theta_ref))\n    \n    # Compute the GRPO objective value\n    grpo_objective_value = clipped_objective - beta * kl_divergence\n    \n    # Return the objective value rounded to the nearest 6th decimal\n    return round(grpo_objective_value, 6)\n\n# Example usage\nrhos = [1.2, 0.9, 1.5, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.3, 0.4, 0.6, 0.5]\npi_theta_ref = [0.2, 0.3, 0.5, 0.4]\n\nprint(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\nThis solution defines the function `grpo_objective` that computes the GRPO objective value given the necessary parameters. It applies the clipping mechanism to the likelihood ratios and calculates the KL divergence between the old policy and the reference policy to penalize large changes in policy. The final objective value is returned rounded to the nearest 6th decimal as per the requirements."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)\n\n# Example usage:\nprint(swish(0.5))  # Example input\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Test the function with some values\ntest_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\nresults = [round(selu(x), 4) for x in test_values]\nprint(results)\n```\n\nThis script implements the SELU activation function as described and applies it to a list of test values, rounding the output to the nearest 4th decimal."}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the sigmoid function for the input z.\n\n    Args:\n        z: Input array\n\n    Returns:\n        Sigmoid of the input array\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00c3\u0097 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    z = np.dot(X, weights) + bias\n    # Apply the sigmoid function to get probabilities\n    probabilities = sigmoid(z)\n    # Convert probabilities to binary predictions\n    predictions = (probabilities >= 0.5).astype(int)\n    # Return predictions as a list\n    return predictions.tolist()\n\n# Example usage:\n# Define a sample input matrix and weights\nX_sample = np.array([[1, 2], [2, 3], [3, 4]])\nweights_sample = np.array([0.5, -0.2])\nbias_sample = 0.1\n\n# Perform prediction\npredictions = predict_logistic(X_sample, weights_sample, bias_sample)\nprint(predictions)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n    return e_z / e_z.sum(axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Compute the cross entropy loss.\"\"\"\n    n_samples = y_true.shape[0]\n    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / n_samples\n    return loss\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = y.shape[1]\n    W = np.zeros((n_features, n_classes))\n    losses = []\n\n    for i in range(iterations):\n        # Compute class probabilities\n        scores = X @ W\n        predictions = softmax(scores)\n\n        # Compute gradient\n        error = predictions - y\n        gradient = X.T @ error / n_samples\n\n        # Update weights\n        W -= learning_rate * gradient\n\n        # Compute and store the loss\n        loss = cross_entropy_loss(y, predictions)\n        losses.append(round(loss, 4))\n\n    B = W.round(4).tolist()\n    return B, losses\n\n# Example usage:\n# X = np.random.rand(100, 5)\n# y = np.eye(3)[np.random.choice(3, 100)]\n# learning_rate = 0.01\n# iterations = 1000\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(B, losses)\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add a column of ones to X to account for the intercept term\n    X = np.insert(X, 0, 1, axis=1)\n    # Initialize weights with zeros\n    weights = np.zeros(X.shape[1])\n    # List to store loss values\n    loss_values = []\n    \n    for i in range(iterations):\n        # Predicted values\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        \n        # Compute gradient\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        # Update weights\n        weights -= learning_rate * gradient\n        \n        # Compute loss\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        # Append loss to the list\n        loss_values.append(round(loss, 4))\n        \n    # Return weights and loss values\n    return weights.tolist(), loss_values\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 1])\n# learning_rate = 0.1\n# iterations = 100\n# weights, losses = train_logreg(X, y, learning_rate, iterations)\n# print(\"Coefficients:\", weights)\n# print(\"Losses:\", losses)\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    scores += mask  # Apply mask, where mask contains very small values (e.g., -np.inf) for positions to be masked\n    p_attn = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), axis=-1, arr=scores)\n    output = np.matmul(p_attn, V)\n    return output.tolist()\n\n# Example usage\nX = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nW_q = np.array([[1, 0], [0, 1], [1, 1]])\nW_k = np.array([[1, 1], [1, 0], [0, 1]])\nW_v = np.array([[1, 0], [0, 1], [1, 1]])\nmask = np.array([[-np.inf, 0, 0], [-np.inf, -np.inf, 0], [-np.inf, -np.inf, 0]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```\n\nThis script first defines the `compute_qkv` function, which computes the query, key, and value matrices from the input matrix `X` and the corresponding weight matrices `W_q`, `W_k`, and `W_v`. Then, it defines the `masked_attention` function, which computes the masked self-attention. This function applies a mask to the scores before applying the softmax function to ensure that certain positions are ignored during the attention computation. The final output is returned as a Python list using `tolist()` method."}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom math import log\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is calculated using Shannon entropy, which is a measure of uncertainty\n    associated with a random variable. In this case, the random variable is the color of\n    an apple in the basket. The entropy is 0 when all apples are of the same color and\n    increases as the variety of colors increases.\n    \"\"\"\n    if not apples:\n        return 0.0\n\n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the Shannon entropy\n    entropy = -sum((count / total_apples) * log(count / total_apples, 2) for count in color_counts.values())\n    \n    return round(entropy, 4)\n\n# Test cases to verify the correctness of the function\nprint(disorder([0,0,0,0]))  # Expected output: 0.0\nprint(disorder([1,1,0,0]))  # Expected output: 1.0\nprint(disorder([0,1,2,3]))  # Expected output: 2.0\nprint(disorder([0,0,1,1,2,2,3,3]))  # Expected output: 2.0\nprint(disorder([0,0,0,0,0,1,2,3]))  # Expected output: 1.3478\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    X: Input tensor of shape (batch_size, seq_length, feature_dim)\n    gamma: Scaling parameters of shape (feature_dim,)\n    beta: Shifting parameters of shape (feature_dim,)\n    epsilon: Small constant for numerical stability\n    \n    Returns:\n    A numpy array of the same shape as X, normalized across the feature dimension.\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input X\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_scaled = X_norm * gamma + beta\n    \n    # Return the result rounded to 5 decimal places and converted to a list\n    return np.round(X_scaled, decimals=5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input tensor X, scaling and shifting parameters\n    X = np.random.randn(2, 3, 4)  # Batch size: 2, Sequence length: 3, Feature dimensions: 4\n    gamma = np.random.randn(4)    # Scaling parameters\n    beta = np.random.randn(4)     # Shifting parameters\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    # Print the normalized tensor\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"\n    Compute the harmonic mean for METEOR score.\n    \"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\ndef fragmentation_cost(reference, candidate):\n    \"\"\"\n    Compute the fragmentation cost based on the number of segments.\n    \"\"\"\n    ref_segments = reference.split()\n    can_segments = candidate.split()\n    ref_set = set(ref_segments)\n    can_set = set(can_segments)\n    common = ref_set.intersection(can_set)\n    ref_order = [i for i, word in enumerate(ref_segments) if word in common]\n    can_order = [i for i, word in enumerate(can_segments) if word in common]\n    if not ref_order or not can_order:\n        return 1.0\n    cost = 0.0\n    for i in range(1, len(ref_order)):\n        cost += abs(ref_order[i] - can_order[i]) != abs(can_order[i] - can_order[i-1])\n    return cost / len(ref_order)\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score for two strings: a reference and a candidate translation.\n    \"\"\"\n    ref_words = reference.split()\n    can_words = candidate.split()\n    ref_count = Counter(ref_words)\n    can_count = Counter(can_words)\n    \n    matches = sum((ref_count & can_count).values())\n    if matches == 0:\n        return 0.0\n    \n    precision = matches / len(can_words)\n    recall = matches / len(ref_words)\n    f_mean = harmonic_mean(precision, recall, beta)\n    \n    fragment_cost = fragmentation_cost(reference, candidate)\n    penalty = gamma * fragment_cost\n    score = (1 - alpha) * precision + alpha * f_mean - penalty\n    \n    return round(score, 3)\n\n# Test the function with a simple example\nref = \"the cat is on the mat\"\ncan = \"the cat is on the mat\"\nprint(meteor_score(ref, can))  # Expected output: 1.0\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    PMI is calculated as:\n    PMI(x, y) = log(P(x, y) / (P(x)P(y)))\n              = log((count(x, y) / N) / ((count(x) / N) * (count(y) / N)))\n              = log(count(x, y) * N / (count(x) * count(y)))\n    \n    :param joint_counts: The number of times x and y occur together.\n    :param total_counts_x: The total number of occurrences of x.\n    :param total_counts_y: The total number of occurrences of y.\n    :param total_samples: The total number of samples.\n    :return: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0:\n        return float('-inf')\n    pmi = np.log2(joint_counts * total_samples / (total_counts_x * total_counts_y))\n    return round(pmi, 3)\n\n# Example usage and a simple check\njoint_counts = 10\ntotal_counts_x = 50\ntotal_counts_y = 40\ntotal_samples = 1000\npmi_result = compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples)\n\n# Output the result\nprint(\"PMI:\", pmi_result)\n\n# Expected output check (for the given values, the expected PMI is approximately 0.322)\nassert pmi_result == 0.322, \"The PMI calculation is incorrect.\"\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    Each value is scaled to the range [0, 1] based on the minimum and maximum values in the list.\n    The result is rounded to 4 decimal places.\n    \n    Args:\n    x: A list of integers.\n    \n    Returns:\n    A list of floats representing the normalized values.\n    \"\"\"\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0] * len(x)\n    \n    normalized = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    return [round(val, 4) for val in normalized]\n\n# Example check function to verify the correctness of the generated function.\ndef check_min_max_normalization():\n    test_data = [10, 20, 15, 30, 25]\n    expected_output = [0.0, 0.4, 0.2, 1.0, 0.8]\n    assert min_max(test_data) == expected_output, \"Test failed for input: {}\".format(test_data)\n    print(\"Test passed!\")\n\n# Running the check function\ncheck_min_max_normalization()\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Args:\n        x (np.ndarray): 1D input array.\n        w1 (np.ndarray): Weight matrix for the first layer.\n        w2 (np.ndarray): Weight matrix for the second layer.\n    \n    Returns:\n        np.ndarray: The output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure x is in the right shape for matrix multiplication\n    x = x.reshape(1, -1)\n    \n    # First layer: Matrix multiplication followed by ReLU activation\n    z1 = np.matmul(x, w1)\n    a1 = np.maximum(z1, 0)  # ReLU activation\n    \n    # Second layer: Matrix multiplication followed by ReLU activation\n    z2 = np.matmul(a1, w2)\n    a2 = np.maximum(z2, 0)  # ReLU activation\n    \n    # Shortcut connection: Add the input x to the second layer output\n    z_shortcut = a2 + x\n    \n    # Final ReLU activation\n    a_final = np.maximum(z_shortcut, 0)\n    \n    # Round to 4 decimal places and return as a list\n    return a_final.round(4).tolist()[0]\n\n# Example usage\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n\noutput = residual_block(x, w1, w2)\nprint(output)\n```\n\nThis script follows the given task description to implement a simple residual block function using NumPy, ensuring the output is rounded to 4 decimal places and returned as a list."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    - x: A NumPy array of shape (height, width, channels) representing feature maps.\n    \n    Returns:\n    - A 1D NumPy array of shape (channels,) where each element is the average of\n      all values in the corresponding feature map.\n    \"\"\"\n    # Calculate the average of each feature map across the height and width\n    return np.mean(x, axis=(0, 1))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample 3D array of shape (4, 4, 3) representing 3 feature maps\n    np.random.seed(0)\n    sample_input = np.random.rand(4, 4, 3)\n    print(\"Input shape:\", sample_input.shape)\n    \n    # Perform Global Average Pooling\n    pooled_output = global_avg_pool(sample_input)\n    print(\"Pooled output:\", pooled_output)\n    print(\"Output shape:\", pooled_output.shape)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Applies Batch Normalization to the input array X using the scale (gamma) and shift (beta) parameters.\n    The input X is expected to be in BCHW format (batch, channels, height, width).\n    \n    Parameters:\n    X: np.ndarray\n        The input array in BCHW format.\n    gamma: np.ndarray\n        Scale parameter for each channel.\n    beta: np.ndarray\n        Shift parameter for each channel.\n    epsilon: float\n        A small value to add to the variance for numerical stability.\n        \n    Returns:\n    np.ndarray\n        The normalized, scaled, and shifted input array.\n    \"\"\"\n    # Compute the mean and variance for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    X_scaled_shifted = gamma[:, None, None] * X_norm + beta[:, None, None]\n    \n    # Round off to 4 decimal places and convert to list\n    return np.round(X_scaled_shifted, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)  # For reproducible results\n    X = np.random.randn(2, 3, 4, 4)  # Batch of 2, 3 channels, 4x4 feature maps\n    gamma = np.array([1.0, 2.0, 3.0])  # Scale parameters for each channel\n    beta = np.array([0.5, 0.5, 0.5])  # Shift parameters for each channel\n    normalized_X = batch_normalization(X, gamma, beta)\n    print(normalized_X)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term at a given point.\n\n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    # Applying the power rule: d/dx(c * x^n) = c * n * x^(n-1)\n    derivative_value = c * n * (x ** (n - 1))\n    return round(derivative_value, 4)\n\n# Test the function with provided data points\ndef check_poly_term_derivative():\n    # Test cases to verify the correctness of the function\n    test_cases = [\n        (3.0, 2.0, 2.0, 12.0000),\n        (5.0, 3.0, 3.0, 135.0000),\n        (2.5, 1.0, 4.0, 10.0000),\n        (0.5, 0.0, 5.0, 0.0000),\n        (-2.0, -3.0, 2.0, 12.0000)\n    ]\n    for c, x, n, expected in test_cases:\n        result = poly_term_derivative(c, x, n)\n        assert result == expected, f\"Expected {expected}, got {result} for inputs ({c}, {x}, {n})\"\n    print(\"All test cases passed successfully.\")\n\n# Running the test function to verify correctness\ncheck_poly_term_derivative()\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Apply the Gram-Schmidt process to a list of 2D vectors to find an orthonormal basis.\n    \n    Parameters:\n    vectors (list of lists of float): The input vectors to process.\n    tol (float): Tolerance for considering vectors linearly independent.\n    \n    Returns:\n    list of lists of float: The orthonormal basis vectors.\n    \"\"\"\n    # Convert the list of vectors into a numpy array for easier manipulation\n    V = np.array(vectors).T\n    m, n = V.shape\n    Q = np.zeros((m, n))\n    for i in range(n):\n        v = V[:, i]\n        for j in range(i):\n            q = Q[:, j]\n            v = v - np.dot(q, V[:, i]) * q\n        if np.linalg.norm(v) < tol:\n            # If v is nearly zero, it's linearly dependent, so skip it\n            continue\n        Q[:, i] = v / np.linalg.norm(v)\n    # Remove zero columns (linearly dependent vectors)\n    Q = Q[:, ~np.all(Q == 0, axis=0)]\n    # Convert to list of lists with elements rounded to 4 decimal places\n    return [list(map(lambda x: round(x, 4), Q[:, i].tolist())) for i in range(Q.shape[1])]\n\n# Check function to test the orthonormal_basis function\ndef check_orthonormal_basis():\n    # Test data points\n    test_vectors = [[1, 2], [2, 4], [1, 0], [0, 1]]\n    expected_output = [[0.6325, 0.7746], [-0.7746, 0.6325]]\n    # Compute the orthonormal basis\n    output = orthonormal_basis(test_vectors)\n    # Check if the output matches the expected output\n    assert np.allclose(output, expected_output), \"The output does not match the expected output.\"\n    print(\"All tests passed!\")\n\n# Run the check function\ncheck_orthonormal_basis()\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (list or numpy array): A 3-dimensional vector.\n    b (list or numpy array): Another 3-dimensional vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for vectorized operations\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product using numpy's cross function\n    cross_prod = np.cross(a, b)\n    \n    # Round the result to 4 decimal places and convert to list\n    return np.round(cross_prod, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    vector1 = [1.0, 2.0, 3.0]\n    vector2 = [4.0, 5.0, 6.0]\n    print(cross_product(vector1, vector2))\n```\n\nThis script defines a `cross_product` function that computes the cross product of two 3D vectors, adhering to the provided guidelines and constraints."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n    \n    Returns:\n    list: The solution vector x as a list, rounded to the nearest 4th decimal.\n    If no unique solution exists, returns -1.\n    \"\"\"\n    # Check if A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        return -1\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, there's no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column of A\n    for i in range(A.shape[1]):\n        # Create a copy of A and replace the ith column with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the modified matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the ith component of the solution vector\n        x_i = det_A_i / det_A\n        x.append(round(x_i, 4))\n    \n    # Convert numpy array to list\n    return np.array(x).tolist()\n\n# Example usage\nA = np.array([[3, 1], [1, 2]])\nb = np.array([9, 8])\nprint(cramers_rule(A, b))  # Expected output: [2.0, 3.0]\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two probability distributions p and q.\n    If the distributions are of different lengths or empty, return 0.0.\n    Round the result to 4 decimal places.\n    \"\"\"\n    # Check if the inputs are valid\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(np.multiply(p, q)))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    # Return the Bhattacharyya distance rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]\nprint(bhattacharyya_distance(p, q))  # Example output: 0.0992\n```\n\nThis code snippet defines a function `bhattacharyya_distance` that calculates the Bhattacharyya distance between two probability distributions `p` and `q`. The result is rounded to four decimal places as required. If the inputs are not valid (different lengths or empty), it returns `0.0`."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    - a: list[int|float] - The first vector.\n    - b: list[int|float] - The second vector.\n    \n    Returns:\n    - A list[int|float] representing the element-wise sum of the vectors if they have the same length.\n    - -1 if the vectors have different lengths.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    \n    return [x + y for x, y in zip(a, b)]\n\n# Check function to verify the correctness of the solution\ndef check_vector_sum():\n    assert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9], \"Test case 1 failed\"\n    assert vector_sum([1.5, 2.5], [3.5, 4.5]) == [5.0, 7.0], \"Test case 2 failed\"\n    assert vector_sum([1, 2], [3, 4, 5]) == -1, \"Test case 3 failed\"\n    assert vector_sum([], []) == [], \"Test case 4 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the solution\ncheck_vector_sum()\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        returns = 0\n        returns_list = []\n        # Compute the returns for each step in the episode\n        for _, _, reward in reversed(episode):\n            returns = 0.99 * returns + reward  # Assuming a discount factor of 0.99\n            returns_list.append(returns)\n        returns_list = list(reversed(returns_list))\n        \n        for i, (state, action, _) in enumerate(episode):\n            policy = softmax(theta[state, :])\n            log_policy = np.log(policy[action])\n            gradient[state, action] += log_policy * returns_list[i]\n            for a in range(num_actions):\n                if a != action:\n                    gradient[state, a] += np.log(1 - policy[action]) * returns_list[i]\n    \n    average_gradient = gradient / len(episodes)\n    return np.round(average_gradient, 4).tolist()\n\n# Example usage\ntheta = np.array([[1, 2], [3, 4], [5, 6]], dtype=float)\nepisodes = [\n    [(0, 0, 1.0), (1, 1, 1.0)],\n    [(0, 1, 1.0), (1, 0, 1.0), (2, 1, 1.0)]\n]\ngradient = compute_policy_gradient(theta, episodes)\nprint(gradient)\n```\n\nThis script defines the `compute_policy_gradient` function that computes the average policy gradient over a set of episodes using the REINFORCE algorithm. The episodes are provided as lists of tuples, each containing a state, an action, and a reward. The function uses a softmax function to compute the policy and applies the policy gradient formula to calculate the gradient. The result is rounded to four decimal places and returned as a list."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active experts.\n    d_in (int): Input dimension.\n    d_out (int): Output dimension.\n    \n    Returns:\n    float: Percentage of computational cost savings rounded to the nearest 1th decimal.\n    \"\"\"\n    # Compute FLOPs for dense layer\n    flops_dense = 2 * d_in * d_out\n    \n    # Compute FLOPs for MoE layer\n    flops_expert = 2 * d_in * d_out\n    flops_gating = 2 * d_in * n_experts  # Assuming gating mechanism has similar complexity\n    flops_moe = (flops_expert * k_active / n_experts) + flops_gating\n    \n    # Calculate savings\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    \n    return round(savings, 1)\n\n# Example usage\nn_experts = 128\nk_active = 4\nd_in = 512\nd_out = 512\n\nprint(compute_efficiency(n_experts, k_active, d_in, d_out))\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost savings of using a Mixture-of-Experts (MoE) layer over a traditional dense layer. The function takes four parameters: the total number of experts (`n_experts`), the number of active experts (`k_active`), and the input and output dimensions (`d_in` and `d_out`). It computes the floating-point operations (FLOPs) for both the dense layer and the MoE layer, then calculates and returns the percentage of computational savings rounded to the nearest 1th decimal."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input matrix of shape (batch_size, input_dim).\n    W_g (np.ndarray): Weight matrix for gating of shape (input_dim, num_experts).\n    W_noise (np.ndarray): Weight matrix for noise of shape (input_dim, num_experts).\n    N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    k (int): Number of experts to select.\n    \n    Returns:\n    list: Final gating probabilities matrix as a list rounded to 4 decimal places.\n    \"\"\"\n    # Compute logits\n    logits = X @ W_g\n    noise = np.abs(X @ W_noise) * N\n    noisy_logits = logits + noise\n    \n    # Compute probabilities\n    probabilities = np.exp(noisy_logits) / np.sum(np.exp(noisy_logits), axis=-1, keepdims=True)\n    \n    # Select top-k experts and normalize their probabilities\n    topk_indices = np.argsort(probabilities, axis=-1)[:, -k:]\n    topk_probabilities = np.take_along_axis(probabilities, topk_indices, axis=-1)\n    sum_topk = np.sum(topk_probabilities, axis=-1, keepdims=True)\n    topk_probabilities_normalized = topk_probabilities / sum_topk\n    \n    # Create final probabilities matrix\n    final_probabilities = np.zeros_like(probabilities)\n    np.put_along_axis(final_probabilities, topk_indices, topk_probabilities_normalized, axis=-1)\n    \n    # Round and convert to list\n    final_probabilities_rounded = np.round(final_probabilities, 4).tolist()\n    \n    return final_probabilities_rounded\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example inputs\n    batch_size, input_dim, num_experts = 2, 3, 4\n    X = np.random.randn(batch_size, input_dim)\n    W_g = np.random.randn(input_dim, num_experts)\n    W_noise = np.random.randn(input_dim, num_experts)\n    N = np.random.randn(batch_size, num_experts)\n    k = 2\n    \n    # Call the function with the example inputs\n    result = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(result)\n```"}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implement a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to consider for each input token.\n    \n    Returns:\n    np.ndarray: Output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    # Compute gating scores\n    gating_scores = np.dot(x, Wg)\n    # Apply softmax to get gating probabilities\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    \n    # Compute top-k experts for each token\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    \n    # Normalize top-k probabilities\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=-1, keepdims=True)\n    \n    # Compute expert outputs\n    expert_outputs = np.dot(x[:, None, :], We[top_k_indices]).squeeze()\n    \n    # Weight expert outputs by normalized gating probabilities\n    weighted_outputs = expert_outputs * top_k_probs_normalized[..., None]\n    \n    # Aggregate weighted outputs\n    final_output = np.sum(weighted_outputs, axis=-2)\n    \n    # Round to the nearest 4th decimal and return as a list\n    return np.round(final_output, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    batch_size, input_dim, output_dim = 2, 3, 4\n    n_experts, top_k = 5, 2\n    \n    # Random initialization\n    x = np.random.randn(batch_size, input_dim)\n    We = np.random.randn(n_experts, input_dim, output_dim)\n    Wg = np.random.randn(input_dim, n_experts)\n    \n    # Run MoE layer\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (B, C, H, W).\n    gamma (np.ndarray): Scale parameter tensor.\n    beta (np.ndarray): Shift parameter tensor.\n    num_groups (int): Number of groups to divide the channels into.\n    epsilon (float): Small value to avoid division by zero.\n    \n    Returns:\n    np.ndarray: Normalized tensor.\n    \"\"\"\n    B, C, H, W = X.shape\n    G = num_groups\n    \n    # Check if number of channels is divisible by number of groups\n    assert C % G == 0, \"Number of channels must be divisible by number of groups.\"\n    \n    # Reshape into groups\n    X = X.reshape(B, G, -1)\n    \n    # Calculate mean and variance for each group\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original shape\n    X_norm = X_norm.reshape(B, C, H, W)\n    \n    # Apply scale and shift\n    X_norm = gamma * X_norm + beta\n    \n    # Round the results to the nearest 4th decimal\n    X_norm = np.round(X_norm, decimals=4)\n    \n    # Convert to list for output\n    return X_norm.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    B, C, H, W = 2, 6, 4, 4  # Example shape\n    num_groups = 2  # Example number of groups\n    X = np.random.rand(B, C, H, W)  # Random input tensor\n    gamma = np.random.rand(C, 1, 1)  # Random scale parameters\n    beta = np.random.rand(C, 1, 1)  # Random shift parameters\n    \n    normalized_X = group_normalization(X, gamma, beta, num_groups)\n    print(normalized_X)\n```\n\nThis script implements Group Normalization as described, ensuring the input tensor is reshaped into groups, normalized, and then reshaped back to its original form before applying the learned scale and shift parameters. The output is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the minimum of the function f(x) = x^4 - 3x^3 + 2 starting from start_x using gradient descent.\n    \n    Parameters:\n    start_x (float): The starting point for the search.\n    learning_rate (float): The step size at each iteration while moving toward a minimum of the function.\n    tolerance (float): The minimum change in the value of x to continue the iterations.\n    max_iters (int): The maximum number of iterations to perform.\n    \n    Returns:\n    float: The value of x where the function reaches its minimum, rounded to 4 decimal places.\n    \"\"\"\n    \n    # Define the function f(x)\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    # Define the derivative of the function f'(x)\n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    x = start_x\n    \n    for _ in range(max_iters):\n        # Calculate the gradient\n        gradient = df(x)\n        \n        # Update x using the learning rate and gradient\n        x -= learning_rate * gradient\n        \n        # Check if the change is less than the tolerance\n        if np.abs(gradient) < tolerance:\n            break\n    \n    return round(x, 4)\n\n# Example usage\nstart_point = 0.5\nresult = find_treasure(start_point)\nprint(f\"The minimum is at: {result}\")\n```\n\nThis solution uses gradient descent to find the minimum of the given function. The function `find_treasure` implements the gradient descent algorithm with a specified learning rate, tolerance, and maximum number of iterations. The derivative of the function is used to determine the direction of the steepest descent. The process continues until the change (gradient) is below a certain tolerance or the maximum number of iterations is reached. The result is the value of `x` where the function reaches its minimum, rounded to the nearest 4th decimal place."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function which is a normalization-free transformation.\n    It takes an array of input values and applies the DyT transformation with given parameters.\n    \n    Parameters:\n    x (np.ndarray): Input array of values.\n    alpha (float): Scaling parameter for the input.\n    gamma (float): Scaling parameter for the tanh output.\n    beta (float): Bias parameter for the tanh output.\n    \n    Returns:\n    list[float]: List of transformed values, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Apply the transformation\n    transformed = gamma * np.tanh(alpha * x) + beta\n    \n    # Round the results to the nearest 4th decimal and return as a list\n    return np.round(transformed, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    x = np.array([0.1, 0.5, 1.0, -0.5, -1.0])\n    alpha = 1.5\n    gamma = 1.2\n    beta = 0.3\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    print(result)\n```\n\nThis script defines the `dynamic_tanh` function as requested, which applies a transformation inspired by the Tanh function with parameters `alpha`, `gamma`, and `beta` to an input array `x`. The result is rounded to the nearest 4th decimal and returned as a list. An example usage is also provided to demonstrate how to use the function."}
