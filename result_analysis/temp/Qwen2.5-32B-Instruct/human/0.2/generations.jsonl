{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float: The resulting vector if the operation is valid, or -1 if dimensions are incompatible.\n    \"\"\"\n    # Check if the number of columns in the matrix equals the length of the vector\n    if not a or len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    vector = [1, 0, 1]\n    print(matrix_dot_vector(matrix, vector))  # Output: [4, 10, 16]\n\n    matrix_incompatible = [[1, 2], [3, 4]]\n    vector_incompatible = [1, 2, 3]\n    print(matrix_dot_vector(matrix_incompatible, vector_incompatible))  # Output: -1\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transposes the given matrix `a`.\n    \n    Parameters:\n    a (list of list of int/float): The matrix to be transposed.\n    \n    Returns:\n    list of list of int/float: The transposed matrix.\n    \"\"\"\n    # Using list comprehension to transpose the matrix\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    transposed_matrix = transpose_matrix(matrix)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nTransposed Matrix:\")\n    for row in transposed_matrix:\n        print(row)\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape the given matrix a into the specified new_shape.\n    If the reshape is not possible, return an empty list.\n    \n    :param a: List of lists representing the matrix to be reshaped.\n    :param new_shape: Tuple representing the new shape (rows, columns).\n    :return: Reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    # Convert the list of lists to a numpy array\n    a_np = np.array(a)\n    \n    # Check if the reshape is possible\n    if a_np.size == new_shape[0] * new_shape[1]:\n        # Reshape the numpy array\n        reshaped_np = a_np.reshape(new_shape)\n        # Convert the reshaped numpy array back to a list of lists\n        return reshaped_np.tolist()\n    else:\n        # Return an empty list if reshape is not possible\n        return []\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[1, 2], [3, 4]]\n    new_shape = (1, 4)\n    print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3, 4]]\n    \n    matrix = [[1, 2], [3, 4]]\n    new_shape = (3, 3)\n    print(reshape_matrix(matrix, new_shape))  # Output: []\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists representing the matrix.\n    :param mode: A string indicating whether to calculate the mean by 'row' or 'column'.\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        return [np.mean(row) for row in matrix]\n    elif mode == 'column':\n        # Transpose the matrix to calculate column means as row means\n        transposed_matrix = np.array(matrix).T.tolist()\n        return [np.mean(col) for col in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'.\")\n\n# Example usage\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\n# Calculate row means\nrow_means = calculate_matrix_mean(matrix, 'row')\nprint(\"Row means:\", row_means)\n\n# Calculate column means\ncolumn_means = calculate_matrix_mean(matrix, 'column')\nprint(\"Column means:\", column_means)\n```\n\nThis solution uses the `numpy` library for its concise and efficient mean calculation capabilities. The function `calculate_matrix_mean` is designed to handle both 'row' and 'column' modes, with an additional check to ensure the mode is correctly specified."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the scalar and returns the resulting matrix.\n    \n    :param matrix: A list of lists representing the matrix where each inner list is a row.\n    :param scalar: An integer or float representing the scalar value to multiply the matrix by.\n    :return: A new matrix (list of lists) with each element multiplied by the scalar.\n    \"\"\"\n    # Multiply each element of the matrix by the scalar\n    result = [[element * scalar for element in row] for row in matrix]\n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 2\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(f\"\\nMatrix after multiplying by {scalar}:\")\n    for row in scalar_multiply(matrix, scalar):\n        print(row)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of two lists, each containing two elements (floats or ints).\n    :return: A list of eigenvalues sorted from highest to lowest.\n    \"\"\"\n    # Convert the input list to a numpy array for easier manipulation\n    matrix_np = np.array(matrix)\n    \n    # Calculate the eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    \n    # Sort the eigenvalues in descending order and return them as a list\n    return sorted(eigenvalues, reverse=True)\n\n# Example usage\nif __name__ == \"__main__\":\n    example_matrix = [[4, 2], [3, 1]]\n    print(calculate_eigenvalues(example_matrix))\n```\n\nThis solution leverages the `numpy` library to calculate the eigenvalues of a 2x2 matrix and then sorts them from highest to lowest as requested."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        # Convert lists to numpy arrays\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        \n        # Check if T and S are invertible\n        if np.linalg.det(T_np) == 0 or np.linalg.det(S_np) == 0:\n            return -1\n        \n        # Compute the inverse of T\n        T_inv = np.linalg.inv(T_np)\n        \n        # Perform the transformation T^{-1}AS\n        transformed_matrix = np.dot(T_inv, np.dot(A_np, S_np))\n        \n        # Round to the nearest 4th decimal and convert to list\n        transformed_matrix_rounded = np.round(transformed_matrix, 4).tolist()\n        \n        return transformed_matrix_rounded\n    \n    except Exception as e:\n        # In case of any error, return -1\n        return -1\n\n# Example usage\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 4]]\n\nprint(transform_matrix(A, T, S))\n```\n\nThis script defines a function `transform_matrix` that performs the specified matrix transformation and includes error handling for non-invertible matrices and other potential issues."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the matrix if it is invertible, otherwise None.\n    \"\"\"\n    # Extract the elements of the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate the determinant\n    det = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if det == 0:\n        return None\n    \n    # Calculate the inverse matrix\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    \n    return inv_matrix\n\n# Check function to verify the correctness of the generated function\ndef check_inverse():\n    test_cases = [\n        ([[2, 3], [1, 4]], [[0.8, -0.6], [-0.2, 0.4]]),\n        ([[1, 2], [3, 4]], [[-2, 1], [1.5, -0.5]]),\n        ([[1, 0], [0, 1]], [[1, 0], [0, 1]]),\n        ([[0, 1], [1, 0]], [[0, 1], [1, 0]]),\n        ([[1, 2], [2, 4]], None)  # This matrix is not invertible\n    ]\n    \n    for i, (matrix, expected) in enumerate(test_cases):\n        result = inverse_2x2(matrix)\n        # Allow for some floating point error\n        if expected is None:\n            assert result is None, f\"Test case {i} failed: expected None, got {result}\"\n        else:\n            assert np.allclose(result, expected), f\"Test case {i} failed: expected {expected}, got {result}\"\n    print(\"All test cases passed.\")\n\n# Run the check function\ncheck_inverse()\n```\n\nThis solution defines a function `inverse_2x2` that calculates the inverse of a 2x2 matrix. It also includes a check function `check_inverse` that tests the `inverse_2x2` function with several test cases to ensure its correctness. The solution uses basic arithmetic operations to calculate the determinant and the inverse matrix, and it returns `None` if the matrix is not invertible (i.e., if the determinant is zero)."}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n    Returns the resulting matrix if multiplication is possible, otherwise returns -1.\n    \"\"\"\n    # Check if the number of columns in the first matrix equals the number of rows in the second matrix\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    a = [[1, 2], [3, 4]]\n    b = [[2, 0], [1, 2]]\n    print(matrixmul(a, b))  # Expected output: [[4, 4], [10, 8]]\n\n    c = [[1, 2, 3], [4, 5, 6]]\n    d = [[1, 2], [3, 4], [5, 6]]\n    print(matrixmul(c, d))  # Expected output: [[22, 28], [49, 64]]\n\n    e = [[1, 2], [3, 4]]\n    f = [[1, 2, 3], [4, 5, 6]]\n    print(matrixmul(e, f))  # Expected output: -1\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    # Convert the list of lists to a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Calculate the covariance matrix using numpy's cov function\n    cov_matrix = np.cov(data, bias=True)\n    \n    # Convert the numpy array back to a list of lists\n    return cov_matrix.tolist()\n\n# Test cases to verify the correctness of the function\ndef check_covariance_matrix():\n    vectors = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    expected_result = [\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0]\n    ]\n    result = calculate_covariance_matrix(vectors)\n    assert np.allclose(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n    \n    vectors = [\n        [2, 4, 6],\n        [1, 3, 5]\n    ]\n    expected_result = [\n        [2.0, 2.0],\n        [2.0, 2.0]\n    ]\n    result = calculate_covariance_matrix(vectors)\n    assert np.allclose(result, expected_result), f\"Expected {expected_result}, but got {result}\"\n    \n    print(\"All test cases passed.\")\n\n# Run the test cases\ncheck_covariance_matrix()\n```\n\nThis script defines a function `calculate_covariance_matrix` that calculates the covariance matrix for a given set of vectors. It uses NumPy for the calculation and includes test cases to verify the correctness of the implementation."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Constant vector.\n    n (int): Number of iterations.\n    \n    Returns:\n    list: Approximate solution vector x.\n    \"\"\"\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Ensure A and b have compatible dimensions\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"Matrix A and vector b have incompatible dimensions.\")\n    \n    # Initialize the solution vector\n    x = np.zeros_like(b)\n    \n    # Iterate n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new\n        \n        # Round to 4 decimal places\n        x = np.round(x, 4)\n    \n    # Return the solution as a list\n    return x.tolist()\n\n# Example usage:\nA = np.array([[4, -1, 0, 0],\n              [-1, 4, -1, 0],\n              [0, -1, 4, -1],\n              [0, 0, -1, 3]])\nb = np.array([12, 12, 12, 12])\nn = 25\n\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\nThis script defines a function `solve_jacobi` that implements the Jacobi method for solving systems of linear equations. It iterates a specified number of times, rounding the solution vector to four decimal places after each iteration, and returns the approximate solution as a list."}
{"task_id": 12, "completion_id": 0, "completion": "To solve the task of approximating the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method, we will follow these steps:\n\n1. Define the function `svd_2x2_singular_values` that takes a 2x2 matrix `A` as input.\n2. Use the Jacobian method to approximate the SVD. This involves finding the eigenvalues of the matrix \\(A^T A\\) (or \\(A A^T\\)), which are the squares of the singular values of \\(A\\).\n3. Extract the singular values from the eigenvalues.\n4. Round the singular values to the nearest 4th decimal place.\n5. Return the singular values as a tuple.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    # Compute A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Find the eigenvalues of A^T * A\n    eigenvalues = np.linalg.eigvals(ATA)\n    \n    # The singular values are the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Sort the singular values in descending order\n    singular_values = np.sort(singular_values)[::-1]\n    \n    # Round the singular values to the nearest 4th decimal place\n    singular_values = np.round(singular_values, 4)\n    \n    # Return the singular values as a tuple\n    return tuple(singular_values)\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(singular_values)\n```\n\nThis script defines the function `svd_2x2_singular_values` which computes the singular values of a 2x2 matrix `A` without directly using the `numpy.linalg.svd` function. Instead, it uses the eigenvalues of \\(A^T A\\) to find the singular values, which is a common approach to compute SVD. The singular values are then rounded to the nearest 4th decimal place as required."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix represented as a list of lists.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n    def determinant_3x3(minor: list[list[int|float]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        Args:\n        minor (list of list of int|float): A 3x3 matrix represented as a list of lists.\n        \n        Returns:\n        float: The determinant of the 3x3 matrix.\n        \"\"\"\n        return (minor[0][0] * (minor[1][1] * minor[2][2] - minor[1][2] * minor[2][1]) -\n                minor[0][1] * (minor[1][0] * minor[2][2] - minor[1][2] * minor[2][0]) +\n                minor[0][2] * (minor[1][0] * minor[2][1] - minor[1][1] * minor[2][0]))\n\n    def minor_matrix(matrix: list[list[int|float]], i: int, j: int) -> list[list[int|float]]:\n        \"\"\"\n        Returns the minor matrix of the given matrix by removing the ith row and jth column.\n        \n        Args:\n        matrix (list of list of int|float): The original matrix.\n        i (int): The row index to remove.\n        j (int): The column index to remove.\n        \n        Returns:\n        list of list of int|float: The minor matrix.\n        \"\"\"\n        return [row[:j] + row[j+1:] for row in (matrix[:i] + matrix[i+1:])]\n\n    det = 0\n    for j in range(4):\n        det += (-1) ** j * matrix[0][j] * determinant_3x3(minor_matrix(matrix, 0, j))\n    return det\n\n# Example usage\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix))  # Output should be 0\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Args:\n    X: A list of lists containing the feature values.\n    y: A list containing the target values.\n    \n    Returns:\n    A list of floats representing the coefficients of the linear regression model.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    \n    # Compute the normal equation: theta = (X^T * X)^(-1) * X^T * y\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert numpy array back to list for output\n    return theta_rounded.tolist()\n\n# Example usage\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 6]\nprint(linear_regression_normal_equation(X, y))\n```\n\nThis script defines a function that performs linear regression using the normal equation method, as requested. It includes the necessary import statement, the function definition with appropriate type hints, and an example usage that demonstrates how to call the function with a sample dataset. The output is rounded to the nearest 4th decimal place as specified."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix with a column of ones for the intercept.\n    y (np.ndarray): The target vector.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to perform.\n    \n    Returns:\n    list: The coefficients of the linear regression model.\n    \"\"\"\n    # Initialize coefficients\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    \n    for _ in range(iterations):\n        # Predictions\n        predictions = np.dot(X, theta)\n        \n        # Errors\n        errors = predictions - y\n        \n        # Gradient\n        gradient = np.dot(X.T, errors) / m\n        \n        # Update theta\n        theta -= alpha * gradient\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(theta, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 1], [1, 2], [1, 3]])\n    y = np.array([1, 2, 3])\n    alpha = 0.01\n    iterations = 1000\n    \n    # Call the function\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(coefficients)\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array and returns two 2D lists with features scaled by standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array where each row is a sample and each column is a feature.\n    \n    Returns:\n    tuple: A tuple containing two 2D lists. The first list contains data standardized, and the second list contains data normalized using min-max.\n    \"\"\"\n    # Standardization: (x - mean) / std\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Min-Max Normalization: (x - min) / (max - min)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    normalized_data = (data - min_vals) / (max_vals - min_vals)\n    \n    # Rounding to the nearest 4th decimal and converting to list\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    \n    return standardized_data, normalized_data\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating a random dataset\n    np.random.seed(0)\n    data = np.random.rand(5, 3) * 100  # 5 samples, 3 features, values between 0 and 100\n    \n    # Performing feature scaling\n    standardized, normalized = feature_scaling(data)\n    \n    print(\"Standardized Data:\", standardized)\n    print(\"Normalized Data:\", normalized)\n```\n\nThis script defines a function `feature_scaling` that takes a 2D NumPy array as input and returns two 2D lists: one with the data standardized and the other normalized using min-max normalization. The example usage section demonstrates how to use this function with a randomly generated dataset."}
{"task_id": 17, "completion_id": 0, "completion": "Here is a self-contained Python script that implements the k-Means clustering algorithm as described in the task:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a list of points.\n    \n    Parameters:\n    points (list[tuple[float, float]]): A list of points, where each point is a tuple of coordinates.\n    k (int): The number of clusters to form.\n    initial_centroids (list[tuple[float, float]]): A list of initial centroid points.\n    max_iterations (int): The maximum number of iterations to perform.\n    \n    Returns:\n    list[tuple[float, float]]: A list of the final centroids of the clusters.\n    \"\"\"\n    # Convert points and centroids to numpy arrays for easier manipulation\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the closest centroid\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis])**2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        \n        # Update centroids to be the mean of the points assigned to them\n        for i in range(k):\n            if np.any(closest_centroid == i):\n                centroids_array[i] = np.mean(points_array[closest_centroid == i], axis=0)\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    \n    return final_centroids\n\n# Example usage\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis script defines the `k_means_clustering` function that performs the k-Means clustering algorithm. It takes a list of points, the number of clusters `k`, a list of initial centroids, and the maximum number of iterations as inputs. The function returns the final centroids after performing the specified number of iterations, with each centroid rounded to the nearest fourth decimal."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    Parameters:\n    X (np.ndarray): The input features of the dataset.\n    y (np.ndarray): The target labels of the dataset.\n    k (int): The number of folds for cross-validation.\n    shuffle (bool): Whether to shuffle the dataset before splitting.\n    random_seed (int): The seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples containing train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    extra = X.shape[0] % k\n    start = 0\n    splits = []\n    \n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        splits.append((train_indices, test_indices))\n        start = end\n    \n    return splits\n\n# Example usage:\nX = np.random.rand(100, 5)  # 100 samples, 5 features\ny = np.random.randint(0, 2, 100)  # Binary target for 100 samples\nsplits = k_fold_cross_validation(X, y, k=5, shuffle=True, random_seed=42)\nfor i, (train_indices, test_indices) in enumerate(splits):\n    print(f\"Fold {i+1}: Train size: {len(train_indices)}, Test size: {len(test_indices)}\")\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    A list of lists, where each inner list represents a principal component (eigenvector).\n    \"\"\"\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Convert to list of lists and round to 4 decimal places\n    principal_components_list = [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]\n    \n    return principal_components_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate some random data\n    np.random.seed(0)\n    data = np.random.rand(100, 5)\n    \n    # Perform PCA\n    principal_components = pca(data, 2)\n    print(principal_components)\n```\n\nThis script defines a function `pca` that performs PCA on a given dataset, returning the top `k` principal components. The example usage demonstrates how to use this function with a randomly generated dataset."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree using the given examples and attributes.\n    \n    :param examples: A list of dictionaries, where each dictionary represents an example with attribute-value pairs.\n    :param attributes: A list of attribute names.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    # If all examples have the same target value, return that value\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    \n    # If no attributes left, return the majority target value\n    if not attributes:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    \n    # Choose the attribute with the highest information gain\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    \n    # Create a new decision tree/node with the best attribute and an empty dictionary to build the tree\n    tree = {best_attr: {}}\n    \n    # Create a new decision tree/sub-node for each of the values in the best attribute field\n    for value in set([example[best_attr] for example in examples]):\n        # Split the examples into two sets based on the value of the best attribute\n        examples_subset = [example for example in examples if example[best_attr] == value]\n        \n        # If no examples are left, return the majority target value\n        if not examples_subset:\n            tree[best_attr][value] = Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n        else:\n            # Build a subtree recursively\n            subtree = learn_decision_tree(examples_subset, [attr for attr in attributes if attr != best_attr], target_attr)\n            tree[best_attr][value] = subtree\n    \n    return tree\n\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Chooses the attribute with the highest information gain.\n    \n    :param examples: A list of dictionaries, where each dictionary represents an example with attribute-value pairs.\n    :param attributes: A list of attribute names.\n    :param target_attr: The name of the target attribute for classification.\n    :return: The name of the attribute with the highest information gain.\n    \"\"\"\n    base_entropy = calculate_entropy([example[target_attr] for example in examples])\n    best_gain = 0\n    best_attr = attributes[0]\n    \n    for attribute in attributes:\n        gain = base_entropy - calculate_weighted_entropy(examples, attribute, target_attr)\n        if gain > best_gain:\n            best_gain = gain\n            best_attr = attribute\n    \n    return best_attr\n\ndef calculate_entropy(values: list) -> float:\n    \"\"\"\n    Calculates the entropy of a list of values.\n    \n    :param values: A list of values.\n    :return: The entropy of the values.\n    \"\"\"\n    counter = Counter(values)\n    probabilities = [count / len(values) for count in counter.values()]\n    entropy = -sum(p * math.log2(p) for p in probabilities)\n    return entropy\n\ndef calculate_weighted_entropy(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Calculates the weighted entropy of a list of examples based on an attribute.\n    \n    :param examples: A list of dictionaries, where each dictionary represents an example with attribute-value pairs.\n    :param attribute: The name of the attribute to calculate the weighted entropy for.\n    :param target_attr: The name of the target attribute for classification.\n    :return: The weighted entropy of the examples based on the attribute.\n    \"\"\"\n    weighted_entropy = 0\n    for value in set([example[attribute] for example in examples]):\n        subset = [example for example in examples if example[attribute] == value]\n        subset_entropy = calculate_entropy([example[target_attr] for example in subset])\n        weighted_entropy += (len(subset) / len(examples)) * subset_entropy\n    return weighted_entropy\n\n# Example usage:\nexamples = [\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}\n]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'\n\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```\n\nThis script defines a function `learn_decision_tree` that implements a decision tree learning algorithm based on entropy and information gain. It also includes helper functions for calculating entropy and information gain, and an example usage of the function with a set of examples and attributes."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    \"\"\"Compute the kernel matrix based on the specified kernel type.\"\"\"\n    n_samples = data.shape[0]\n    kernel_matrix = np.zeros((n_samples, n_samples))\n    \n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                kernel_matrix[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma**2))\n    return kernel_matrix\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"Deterministic Pegasos Kernel SVM implementation.\"\"\"\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    kernel_mat = kernel_matrix(data, kernel, sigma)\n    \n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            if labels[i] * (np.sum(alpha * labels * kernel_mat[:, i]) - alpha[i]) < 1:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i] + lambda_t * labels[i]\n            else:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i]\n    \n    # Compute bias\n    support_indices = np.where(alpha > 0)[0]\n    bias = 0\n    for i in support_indices:\n        bias += labels[i] - np.sum(alpha * labels * kernel_mat[support_indices, i])\n    bias /= len(support_indices)\n    \n    return alpha.tolist(), round(bias, 4)\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# labels = np.array([1, 1, -1, -1])\n# alpha, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\n# print(\"Alpha coefficients:\", alpha)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `pegasos_kernel_svm` that implements a deterministic version of the Pegasos algorithm for training a kernel SVM. The function accepts a dataset, labels, kernel type, regularization parameter, number of iterations, and sigma for the RBF kernel. It returns the alpha coefficients and bias of the trained model, rounded to the nearest 4th decimal. The `kernel_matrix` function computes the kernel matrix based on the specified kernel type, which is used in the main SVM training function."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_value = 0.0\n    print(sigmoid(test_value))  # Expected output: 0.5\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of the given list of scores.\n    \n    Args:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: A list of softmax values for each score, rounded to four decimal places.\n    \"\"\"\n    # Calculate the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Sum of all exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax for each score\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_scores\n\n# Check function with provided data points\ndef check_function():\n    test_scores = [1.0, 2.0, 3.0]\n    expected_output = [0.0900, 0.2447, 0.6653]\n    assert softmax(test_scores) == expected_output, \"The softmax function does not return the expected output.\"\n    print(\"The softmax function works correctly with the provided data points.\")\n\ncheck_function()\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: List of feature vectors for each example.\n    :param labels: True binary labels for each example.\n    :param weights: Weights for each feature.\n    :param bias: Bias term for the neuron.\n    :return: A tuple containing the predicted probabilities and the mean squared error.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    features_np = np.array(features)\n    labels_np = np.array(labels)\n    weights_np = np.array(weights)\n    \n    # Compute the weighted sum of features and add the bias\n    weighted_sum = np.dot(features_np, weights_np) + bias\n    \n    # Apply the sigmoid activation function to get predicted probabilities\n    predicted_probabilities = np.array([sigmoid(x) for x in weighted_sum])\n    \n    # Calculate the mean squared error\n    mse = np.mean((predicted_probabilities - labels_np) ** 2)\n    \n    # Round the results to 4 decimal places and convert to list\n    predicted_probabilities_rounded = np.round(predicted_probabilities, 4).tolist()\n    mse_rounded = np.round(mse, 4)\n    \n    return predicted_probabilities_rounded, mse_rounded\n\n# Example usage\nfeatures = [[0.5, 0.3], [0.2, 0.8], [0.9, 0.1]]\nlabels = [1, 0, 1]\nweights = [0.7, 0.3]\nbias = 0.1\n\npredicted_probs, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predicted_probs)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines a function `single_neuron_model` that simulates a single neuron with a sigmoid activation function for binary classification. It takes multidimensional input features, true binary labels, neuron's weights, and bias as inputs. It returns the predicted probabilities after sigmoid activation and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    \"\"\"Derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef mse_loss(predictions, labels):\n    \"\"\"Mean Squared Error loss function.\"\"\"\n    return np.mean((predictions - labels) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        \n        # Compute loss\n        mse = mse_loss(predictions, labels)\n        mse_values.append(round(mse, 4))\n        \n        # Backward pass\n        error = predictions - labels\n        gradient_weights = np.dot(features.T, error * sigmoid_derivative(predictions))\n        gradient_bias = np.sum(error * sigmoid_derivative(predictions))\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    \n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Updated Weights:\", weights)\nprint(\"Updated Bias:\", bias)\nprint(\"MSE Values:\", mse_values)\n```\n\nThis script defines a function `train_neuron` that simulates a single neuron with a sigmoid activation function and implements backpropagation to update the neuron's weights and bias. The function takes feature vectors, labels, initial weights, initial bias, learning rate, and the number of epochs as input, and returns the updated weights, bias, and a list of MSE values for each epoch, all rounded to four decimal places."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a * b + c\ne = d.relu()\ne.backward()\n\nprint(f\"a.grad = {a.grad}, b.grad = {b.grad}, c.grad = {c.grad}\")\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in basis B.\n    C (list of list of int): Basis vectors in basis C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from basis B to C.\n    \"\"\"\n    # Convert the list of lists to numpy arrays for easier manipulation\n    B_np = np.array(B)\n    C_np = np.array(C)\n    \n    # Compute the inverse of C since we want to transform from B to C\n    C_inv = np.linalg.inv(C_np)\n    \n    # The transformation matrix P is given by C_inv * B\n    P = np.dot(C_inv, B_np)\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert the numpy array back to a list of lists and return\n    return P_rounded.tolist()\n\n# Example usage\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(transform_basis(B, C))\n```\n\nThis script defines a function `transform_basis` that computes the transformation matrix from one basis to another in R^3, given the basis vectors in both bases. The function uses numpy for matrix operations and ensures the output is rounded to the nearest 4th decimal as required."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the SVD of a 2x2 matrix A using eigenvalues and eigenvectors.\n    Returns U, S, V such that A = U * S * V.T.\n    \"\"\"\n    # Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Eigendecomposition of A^T A\n    eigenvalues, eigenvectors = np.linalg.eig(ATA)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Construct S from the square roots of the eigenvalues\n    S = np.diag(np.sqrt(eigenvalues))\n    \n    # Construct V from the eigenvectors of A^T A\n    V = eigenvectors\n    \n    # Compute U by normalizing A * v_i for each non-zero singular value\n    U = np.zeros_like(A)\n    for i in range(2):\n        if eigenvalues[i] > 1e-10:  # Check for non-zero singular value\n            U[:, i] = np.dot(A, V[:, i]) / np.sqrt(eigenvalues[i])\n        else:\n            # For zero singular values, we can choose any vector orthogonal to the existing columns of U\n            if i == 0:\n                U[:, i] = np.array([1, 0])\n            else:\n                U[:, i] = np.array([0, 1])\n            U[:, i] /= np.linalg.norm(U[:, i])  # Normalize\n    \n    # Round results to the nearest 4th decimal\n    U = np.round(U, decimals=4)\n    S = np.round(S, decimals=4)\n    V = np.round(V, decimals=4)\n    \n    # Convert to list for output\n    return U.tolist(), S.tolist(), V.tolist()\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines a function `svd_2x2` that computes the SVD of a 2x2 matrix using the specified method. It returns the matrices U, S, and V as lists, with all elements rounded to the nearest 4th decimal. The example usage demonstrates how to call the function and print the results."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, maintaining the corresponding order.\n    \n    Parameters:\n    - X: numpy array, feature dataset.\n    - y: numpy array, labels for the dataset.\n    - seed: int, optional random seed for reproducibility.\n    \n    Returns:\n    - Tuple of shuffled numpy arrays (X_shuffled, y_shuffled) as lists.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of samples\")\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Generate a random permutation of indices\n    permutation = np.random.permutation(X.shape[0])\n    \n    # Shuffle X and y using the permutation\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    \n    # Convert to list and return\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n    print(\"Shuffled X:\\n\", X_shuffled)\n    print(\"Shuffled y:\\n\", y_shuffled)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from X and y.\n    \n    Parameters:\n    X (np.ndarray): The input data.\n    y (np.ndarray, optional): The target data. Defaults to None.\n    batch_size (int): The size of each batch. Defaults to 64.\n    \n    Yields:\n    list or tuple: A batch of X, or a tuple of batches (X, y).\n    \"\"\"\n    assert type(X) == np.ndarray, \"X must be a numpy array\"\n    if y is not None:\n        assert type(y) == np.ndarray, \"y must be a numpy array\"\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n    \n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    \n    for start in range(0, n_samples, batch_size):\n        end = min(start + batch_size, n_samples)\n        batch_idx = indices[start:end]\n        if y is not None:\n            yield X[batch_idx].tolist(), y[batch_idx].tolist()\n        else:\n            yield X[batch_idx].tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[i, i+1] for i in range(100)])\n    y = np.array([i % 2 for i in range(100)])\n    for batch_X, batch_y in batch_iterator(X, y, batch_size=10):\n        print(f\"Batch X shape: {np.array(batch_X).shape}, Batch y shape: {np.array(batch_y).shape}\")\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X based on whether the value of the feature at index feature_i\n    is greater than or equal to the threshold.\n    \n    :param X: numpy.ndarray, the dataset to be divided.\n    :param feature_i: int, the index of the feature to divide the dataset on.\n    :param threshold: float, the threshold to divide the dataset.\n    :return: tuple, two lists representing the subsets of the dataset.\n    \"\"\"\n    # Check if the feature index is valid\n    if feature_i >= X.shape[1]:\n        raise ValueError(f\"Feature index {feature_i} is out of bounds for the dataset with {X.shape[1]} features.\")\n    \n    # Divide the dataset based on the feature threshold\n    subset_1 = X[X[:, feature_i] >= threshold].tolist()\n    subset_2 = X[X[:, feature_i] < threshold].tolist()\n    \n    return subset_1, subset_2\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random dataset\n    np.random.seed(0)\n    X = np.random.rand(10, 3)  # 10 samples, 3 features\n    \n    # Define the feature index and threshold\n    feature_index = 1\n    threshold_value = 0.5\n    \n    # Divide the dataset\n    subset_ge, subset_lt = divide_on_feature(X, feature_index, threshold_value)\n    \n    # Print the results\n    print(\"Subset with feature value >= threshold:\")\n    print(subset_ge)\n    print(\"\\nSubset with feature value < threshold:\")\n    print(subset_lt)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    Parameters:\n    X : np.ndarray\n        The input 2D numpy array.\n    degree : int\n        The degree of the polynomial features.\n        \n    Returns:\n    np.ndarray\n        The output 2D numpy array with polynomial features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, None]\n    \n    n_samples, n_features = X.shape\n    combinations = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combinations = [item for sublist in combinations for item in sublist]\n    n_output_features = len(combinations)\n    X_new = np.empty((n_samples, n_output_features))\n    \n    for i, index_com in enumerate(combinations):\n        X_new[:, i] = np.prod(X[:, index_com], axis=1)\n    \n    return X_new.tolist()\n\n# Example usage:\nX = np.array([[2, 3], [3, 5]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis script defines a function `polynomial_features` that generates polynomial features up to the specified degree for a given 2D numpy array. It uses `combinations_with_replacement` from the itertools module to generate all possible combinations of input features for the given degree. Then, it calculates the product of the features for each combination to form the polynomial features. Finally, it returns the result as a list of lists after reshaping the numpy array using `tolist()`."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of n_subsets random subsets of the dataset (X, y).\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the dataset features.\n    y (np.ndarray): 1D numpy array representing the dataset labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Seed for the random number generator.\n    \n    Returns:\n    list: A list of n_subsets tuples, each containing a subset of X and y.\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    n_subsets = 3\n    subsets = get_random_subsets(X, y, n_subsets, replacements=True)\n    for i, (X_subset, y_subset) in enumerate(subsets):\n        print(f\"Subset {i+1}:\\nX={X_subset}\\ny={y_subset}\\n\")\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (np.array): A 1D numpy array of integer values.\n    n_col (int, optional): The number of columns for the one-hot encoded array.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded matrix.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Example usage\nx = np.array([0, 1, 2, 2, 0])\nprint(to_categorical(x))\n```\n\nThis function `to_categorical` takes a 1D numpy array `x` and an optional integer `n_col`. It first checks if `n_col` is provided, and if not, it calculates it based on the maximum value in `x` plus one. It then uses numpy's `eye` function to create an identity matrix of size `n_col` and indexes into this matrix with `x` to perform one-hot encoding. Finally, it converts the resulting numpy array to a list of lists and returns it."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): 1D numpy array\n    \n    Returns:\n    list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    # Ensure x is a numpy array\n    x = np.array(x)\n    # Create a diagonal matrix using np.diag and the input array\n    diagonal_matrix = np.diag(x)\n    # Convert the numpy array to a python list and return\n    return diagonal_matrix.tolist()\n\n# Example usage\nx = np.array([1, 2, 3])\nprint(make_diagonal(x))\n```\n\nThis script defines a function `make_diagonal` that takes a 1D numpy array and returns a 2D list representing the diagonal matrix. The example usage demonstrates converting the array `[1, 2, 3]` into a diagonal matrix and printing the result."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"y_true and y_pred must be numpy arrays.\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape.\")\n    \n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 2, 2, 2])\n    y_pred = np.array([0, 0, 2, 2, 1])\n    print(accuracy_score(y_true, y_pred))\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array representing the dataset.\n    Y (numpy.ndarray, optional): A 2D numpy array representing another dataset. If not provided, X is used.\n    \n    Returns:\n    list: A 2D list representing the correlation matrix, rounded to the nearest 4th decimal.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Centering the data\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    \n    # Calculating the covariance matrix\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    \n    # Calculating the standard deviations\n    std_X = np.std(X, axis=0, keepdims=True).T\n    std_Y = np.std(Y, axis=0, keepdims=True)\n    \n    # Calculating the outer product of the standard deviations\n    std_outer_product = np.dot(std_X, std_Y)\n    \n    # Calculating the correlation matrix\n    correlation_matrix = cov_matrix / std_outer_product\n    \n    # Rounding to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Converting to list and returning\n    return correlation_matrix.tolist()\n\n# Example usage\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n\n# Test the function\nprint(calculate_correlation_matrix(X))\nprint(calculate_correlation_matrix(X, Y))\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    classifiers = []\n    \n    # Initialize sample weights\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    \n    for _ in range(n_clf):\n        best = {'error': 1, 'clf': None}\n        \n        # Find the best threshold for each feature\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.ones(n_samples)\n                pred[X[:, feature] < threshold] = -1\n                \n                # Calculate error\n                error = np.sum(sample_weights[y != pred])\n                \n                # Update if this is the best classifier so far\n                if error < best['error']:\n                    best['error'] = error\n                    best['clf'] = {'feature': feature, 'threshold': threshold, 'pred': pred}\n        \n        # Calculate alpha\n        alpha = 0.5 * np.log((1 - best['error']) / best['error'])\n        \n        # Update sample weights\n        sample_weights *= np.exp(-alpha * y * best['clf']['pred'])\n        sample_weights /= np.sum(sample_weights)\n        \n        # Save classifier\n        best['clf']['alpha'] = alpha\n        classifiers.append(best['clf'])\n    \n    return classifiers\n\n# Example usage\nX = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\ny = np.array([1, -1, -1, 1])\nn_clf = 2\nclassifiers = adaboost_fit(X, y, n_clf)\nprint(classifiers)\n```\n\nThis script defines a function `adaboost_fit` that implements the AdaBoost algorithm to train a classifier. It iteratively selects the best feature and threshold to split the data, calculates the error, and updates the sample weights. The classifiers are stored in a list and returned at the end."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax is computed as follows:\n    log_softmax(i) = log(exp(scores[i]) / sum_j(exp(scores[j])))\n    \n    Args:\n    scores (list): A list of scores.\n    \n    Returns:\n    list: A list of log-softmax values corresponding to the input scores.\n    \"\"\"\n    scores = np.array(scores)\n    max_score = np.max(scores)\n    scores -= max_score  # For numerical stability\n    exp_scores = np.exp(scores)\n    sum_exp_scores = np.sum(exp_scores)\n    log_softmax_scores = scores - np.log(sum_exp_scores)\n    return [round(score, 4) for score in log_softmax_scores.tolist()]\n\n# Example usage\nscores = [1.0, 2.0, 3.0]\nprint(log_softmax(scores))\n```\n\nThis script defines a function `log_softmax` that takes a list of scores as input and returns a list of log-softmax values. The function is designed to handle numerical stability issues by subtracting the maximum score from all scores before computing the exponential. This prevents overflow when the scores are large. The output is rounded to the nearest 4th decimal as requested."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        input_shape = self.input_shape\n        limit = 1 / math.sqrt(input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        W_grad = self.layer_input.T.dot(accum_grad)\n        self.optimizer.update([self.W, self.w0], [W_grad, w0_grad])\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a simple optimizer\n    class SimpleOptimizer:\n        def update(self, params, grads):\n            for param, grad in zip(params, grads):\n                param -= 0.01 * grad\n\n    # Create a Dense layer\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    dense_layer.initialize(SimpleOptimizer())\n\n    # Forward pass\n    input_data = np.array([[1, 2], [3, 4]])\n    output = dense_layer.forward_pass(input_data, training=True)\n    print(\"Output:\", output.round(4).tolist())\n\n    # Backward pass\n    accum_grad = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    grad_input = dense_layer.backward_pass(accum_grad)\n    print(\"Gradient Input:\", grad_input.round(4).tolist())\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution on the input_matrix using the given kernel, padding, and stride.\n    \n    Parameters:\n    input_matrix (np.ndarray): The input matrix to apply the convolution on.\n    kernel (np.ndarray): The convolutional kernel.\n    padding (int): The padding to be applied to the input matrix.\n    stride (int): The stride to be used for the convolution.\n    \n    Returns:\n    list: The result of the convolution as a list.\n    \"\"\"\n    # Add padding to the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Get the dimensions of the input matrix, kernel, and padded input\n    input_height, input_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    padded_height, padded_width = padded_input.shape\n    \n    # Calculate the dimensions of the output matrix\n    output_height = (input_height - kernel_height + 2 * padding) // stride + 1\n    output_width = (input_width - kernel_width + 2 * padding) // stride + 1\n    \n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Perform the convolution\n    for i in range(0, output_height):\n        for j in range(0, output_width):\n            row = i * stride\n            col = j * stride\n            output_matrix[i, j] = np.sum(padded_input[row:row + kernel_height, col:col + kernel_width] * kernel)\n    \n    # Round the output to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, 4)\n    \n    # Return the output as a list\n    return output_matrix.tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1\nprint(simple_conv2d(input_matrix, kernel, padding, stride))\n```\n\nThis script defines a function `simple_conv2d` that performs a 2D convolution on a given input matrix with a specified kernel, padding, and stride. The function adds padding to the input matrix, calculates the dimensions of the output matrix, and then iterates over the input matrix to apply the convolution. The result is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)\n\n# Test the relu function with some data points\ntest_values = [-10.0, -1.0, 0.0, 1.0, 10.0]\nfor value in test_values:\n    print(f\"relu({value}) = {relu(value)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss given the feature matrix, coefficients, true labels, and regularization parameter.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Predicted values\n    y_pred = X @ w\n    \n    # Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    \n    # Ridge loss\n    loss = mse + reg_term\n    \n    return round(loss, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, 1.0])\n    y_true = np.array([1, 2, 3])\n    alpha = 0.1\n    \n    # Calculate Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the Leaky ReLU function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Test the function with some example values\nprint(leaky_relu(-10))  # Should print -0.1\nprint(leaky_relu(-10, 0.1))  # Should print -1.0\nprint(leaky_relu(10))  # Should print 10\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    # Ensure that the inputs are numpy arrays\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    \n    # Compute the dot product of x1 and x2\n    kernel_value = np.dot(x1, x2)\n    \n    return kernel_value\n\n# Example usage\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nprint(kernel_function(x1, x2))  # Expected output: 32\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n    \n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    \n    if predicted_positives == 0:\n        return 1.0  # If no positive predictions, precision is 1 by definition\n    \n    precision_value = true_positives / predicted_positives\n    return precision_value\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(precision(y_true, y_pred))  # Output should be 0.75\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    X (np.ndarray): The input features.\n    y (np.ndarray): The target values.\n    weights (np.ndarray): The initial weights.\n    learning_rate (float): The learning rate for the gradient descent.\n    n_iterations (int): The number of iterations to perform.\n    batch_size (int): The size of the batch for mini-batch gradient descent.\n    method (str): The method to use for gradient descent ('batch', 'stochastic', 'mini-batch').\n    \n    Returns:\n    list: The final weights after performing gradient descent.\n    \"\"\"\n    n_samples, n_features = X.shape\n    \n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index+1]\n                yi = y[random_index:random_index+1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        \n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i+batch_size]\n                yi = y[i:i+batch_size]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient / batch_size\n        \n        elif method == 'batch':\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * X.T.dot(error)\n            weights -= learning_rate * gradient / n_samples\n    \n    return np.round(weights, 4).tolist()\n\n# Example usage\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.array([[1], [2], [2], [3]])\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 2\n\n# Batch Gradient Descent\nweights_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\nprint(\"Batch Gradient Descent Weights:\", weights_batch)\n\n# Stochastic Gradient Descent\nweights_stochastic = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\nprint(\"Stochastic Gradient Descent Weights:\", weights_stochastic)\n\n# Mini-Batch Gradient Descent\nweights_mini_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini-batch')\nprint(\"Mini-Batch Gradient Descent Weights:\", weights_mini_batch)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix into its Reduced Row Echelon Form (RREF).\n    \n    :param matrix: A 2D numpy array representing the matrix to be converted to RREF.\n    :return: A 2D list representing the RREF of the input matrix.\n    \"\"\"\n    # Convert the input matrix to a numpy array for easier manipulation\n    mat = np.array(matrix, dtype=float)\n    rows, cols = mat.shape\n    lead = 0\n    \n    for r in range(rows):\n        if cols <= lead:\n            return mat.tolist()\n        \n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if rows == i:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        \n        mat[[i, r]] = mat[[r, i]]\n        mat[r] = mat[r] / mat[r, lead]\n        \n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        \n        lead += 1\n    \n    return mat.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = np.array([[1, 2, -1, -4], [-2, -3, 4, 4], [-1, -2, 2, 2]])\n    print(rref(matrix))\n```\n\nThis script defines a function `rref` that takes a matrix as input and returns its Reduced Row Echelon Form (RREF). The function uses numpy for matrix operations and iterates through the matrix to perform the necessary row operations to achieve the RREF. The example usage at the bottom demonstrates how to use the function with a sample matrix."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm implementation.\n    \n    Parameters:\n    f (function): The objective function to be optimized.\n    grad (function): A function that computes the gradient of f.\n    x0 (numpy array): Initial parameter values.\n    learning_rate (float): The step size.\n    beta1 (float): Exponential decay rate for the first moment estimates.\n    beta2 (float): Exponential decay rate for the second moment estimates.\n    epsilon (float): A small constant for numerical stability.\n    num_iterations (int): Number of iterations to run the optimizer.\n    \n    Returns:\n    list: Optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    \n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return np.round(x0, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef quadratic_function(x):\n    return x[0]**2 + x[1]**2\n\ndef quadratic_gradient(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial parameters\nx0 = np.array([1.0, 1.0])\n\n# Optimize\noptimized_params = adam_optimizer(quadratic_function, quadratic_gradient, x0)\nprint(optimized_params)\n```\n\nThis script defines the `adam_optimizer` function that implements the Adam optimization algorithm. It takes an objective function `f`, its gradient `grad`, initial parameters `x0`, and several optional parameters that control the optimization process. The function returns the optimized parameters as a list, with values rounded to the nearest 4th decimal. An example usage is also provided, demonstrating how to optimize a simple quadratic function."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implement Lasso Regression using Gradient Descent with L1 regularization.\n    \n    Parameters:\n    X (np.array): The input features.\n    y (np.array): The target values.\n    alpha (float): The regularization parameter.\n    learning_rate (float): The learning rate for gradient descent.\n    max_iter (int): The maximum number of iterations.\n    tol (float): The tolerance for stopping criteria.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    m, n = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    prev_cost = np.inf\n    \n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        loss = y_pred - y\n        \n        # Compute gradients\n        dw = (1/m) * np.dot(X.T, loss) + alpha * np.sign(weights)\n        db = (1/m) * np.sum(loss)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        \n        # Compute cost\n        cost = (1/(2*m)) * np.sum(loss**2) + (alpha/m) * np.sum(np.abs(weights))\n        \n        # Check for convergence\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    return np.round(weights, 4).tolist(), np.round(bias, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating some sample data\n    np.random.seed(0)\n    X = np.random.rand(100, 1)\n    y = 2 + 3 * X + np.random.rand(100, 1).flatten()\n    \n    # Running the Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y)\n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n```\n\nThis script implements Lasso Regression using Gradient Descent with L1 regularization. The function `l1_regularization_gradient_descent` takes in the input features `X`, the target values `y`, and several hyperparameters for the gradient descent algorithm. It returns the weights and bias as a tuple, both rounded to the nearest 4th decimal and converted to a list using `tolist()`. The example usage section demonstrates how to use the function with some randomly generated data."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of edits (insert, delete, substitute, transpose)\n    required to transform one string into another.\n    \"\"\"\n    len_source, len_target = len(source), len(target)\n    # Create a 2D array to store the edit distances\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Initialize the base cases\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    \n    # Fill the dp table\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i-1] == target[j-1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i-1][j] + 1,        # Deletion\n                           dp[i][j-1] + 1,        # Insertion\n                           dp[i-1][j-1] + cost)   # Substitution\n            # Transposition\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                dp[i][j] = min(dp[i][j], dp[i-2][j-2] + 1)\n    \n    return dp[len_source][len_target]\n\n# Example usage\nsource = \"caper\"\ntarget = \"acer\"\nprint(OSA(source, target))  # Output: 2\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List[int], true binary labels (0 or 1).\n    - y_pred: List[int], predicted binary labels (0 or 1).\n    \n    Returns:\n    - float, recall value rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP) and False Negatives (FN)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    \n    # Return recall rounded to three decimal places\n    return round(recall_value, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(recall(y_true, y_pred))  # Output should be 0.75\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Parameters:\n    X (numpy.ndarray): Input sequence of shape (seq_len, d_model).\n    W_q (numpy.ndarray): Query weight matrix of shape (d_model, d_k).\n    W_k (numpy.ndarray): Key weight matrix of shape (d_model, d_k).\n    W_v (numpy.ndarray): Value weight matrix of shape (d_model, d_v).\n    \n    Returns:\n    numpy.ndarray: The output of the self-attention mechanism, rounded to 4 decimal places.\n    \"\"\"\n    # Compute Q, K, V matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T)\n    scaled_attention_scores = attention_scores / np.sqrt(K.shape[1])\n    \n    # Apply softmax to get attention weights\n    attention_weights = np.exp(scaled_attention_scores) / np.sum(np.exp(scaled_attention_scores), axis=-1, keepdims=True)\n    \n    # Compute the output of the attention mechanism\n    output = np.dot(attention_weights, V)\n    \n    # Round the output to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Return the output as a python list\n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input sequence and weight matrices\n    X = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\n    W_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    W_k = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])\n    W_v = np.array([[1.3, 1.4, 1.5], [1.6, 1.7, 1.8], [1.9, 2.0, 2.1]])\n    \n    # Call the self_attention function\n    output = self_attention(X, W_q, W_k, W_v)\n    print(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to process a sequence of input vectors and produce the final hidden state.\n    \n    Parameters:\n    input_sequence (list of list of float): Sequence of input vectors.\n    initial_hidden_state (list of float): Initial hidden state.\n    Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    b (list of float): Bias vector.\n    \n    Returns:\n    list of float: Final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state\n    hidden_state = initial_hidden_state\n    \n    # Process each input in the sequence\n    for x in input_sequence:\n        # Update the hidden state\n        hidden_state = np.tanh(np.dot(x, Wx) + np.dot(hidden_state, Wh) + b)\n    \n    # Round the final hidden state to 4 decimal places and convert back to list\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    \n    return final_hidden_state\n\n# Example usage\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]\n\nprint(rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b))\n```\n\nThis script defines a function `rnn_forward` that implements a simple RNN cell. It processes a sequence of input vectors and returns the final hidden state after processing the entire sequence, rounded to four decimal places. The function uses the tanh activation function for hidden state updates."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a list of points.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of [x, y] coordinates.\n    \"\"\"\n    # Convert the list of points to a numpy array for easier manipulation\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    # Note: In 2D, we need to use homogeneous coordinates to apply translation using matrix multiplication\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Convert points to homogeneous coordinates\n    homogeneous_points = np.column_stack((points_array, np.ones(points_array.shape[0])))\n    \n    # Apply the translation matrix\n    translated_points = np.dot(homogeneous_points, translation_matrix.T)\n    \n    # Convert back to 2D coordinates and return as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\nThis script defines a function `translate_object` that takes a list of points and translation distances in the x and y directions, and returns a new list of points after applying the translation. The translation is performed using a translation matrix in homogeneous coordinates to allow for the use of matrix multiplication. The result is then converted back to 2D coordinates and returned as a list."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of the first normal distribution P.\n    - sigma_p: Standard deviation of the first normal distribution P.\n    - mu_q: Mean of the second normal distribution Q.\n    - sigma_q: Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    - The KL divergence as a float.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    kl_div = term1 + term2 + term3\n    return kl_div\n\n# Example usage\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\nprint(f\"KL Divergence: {kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method to solve a system of linear equations.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (np.array, optional): Initial guess for the solution vector. Defaults to None.\n    \n    Returns:\n    list: The approximated solution vector after n iterations.\n    \"\"\"\n    # Get the size of the matrix A\n    size = len(b)\n    \n    # Initialize the solution vector x\n    if x_ini is None:\n        x = np.zeros_like(b, dtype='float')\n    else:\n        x = x_ini.astype('float')\n    \n    # Perform the Gauss-Seidel iterations\n    for _ in range(n):\n        for i in range(size):\n            # Initialize the sum to 0\n            s = 0\n            # Calculate the sum of the products of the coefficients and the most recent values of x\n            for j in range(size):\n                if j != i:\n                    s += A[i][j] * x[j]\n            # Update the value of x[i] using the most recent values\n            x[i] = (b[i] - s) / A[i][i]\n    \n    # Round the results to the nearest 4th decimal and return as a list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[4, -1, 0, 0],\n              [-1, 4, -1, 0],\n              [0, -1, 4, -1],\n              [0, 0, -1, 3]])\nb = np.array([12, 12, 12, 12])\nn = 20\nx_ini = np.array([0, 0, 0, 0])\n\nsolution = gauss_seidel(A, b, n, x_ini)\nprint(solution)\n```\n\nThis script defines a function `gauss_seidel` that implements the Gauss-Seidel method for solving a system of linear equations. The function takes a square matrix `A`, a right-hand side vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. It returns the approximated solution vector after the specified number of iterations, with each element rounded to the nearest 4th decimal place."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Constant terms.\n    \n    Returns:\n    list: Solution vector x as a list.\n    \"\"\"\n    n = len(A)\n    # Augment A with b\n    Ab = np.hstack([A, b.reshape(-1,1)])\n    \n    # Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        pivot_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]  # Swap rows\n        \n        pivot = Ab[i, i]\n        for row in range(i + 1, n):\n            factor = Ab[row, i] / pivot\n            Ab[row, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    \n    # Rounding and converting to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nprint(gaussian_elimination(A, b))\n```\n\nThis script defines a function `gaussian_elimination` that solves a system of linear equations using Gaussian Elimination with partial pivoting. The function takes two numpy arrays as input: `A` for the coefficients of the equations and `b` for the constants on the right-hand side of the equations. It returns the solution vector `x` as a list, with each element rounded to the nearest 4th decimal. The example usage demonstrates how to use the function with a 3x3 system of equations."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n        for t in range(len(x)):\n            # Concatenate the input and hidden state\n            concat_input = np.concatenate((x[t], hidden_state), axis=0).reshape(-1, 1)\n            \n            # Compute the gates\n            forget_gate = self.sigmoid(self.Wf @ concat_input + self.bf)\n            input_gate = self.sigmoid(self.Wi @ concat_input + self.bi)\n            candidate_cell_state = self.tanh(self.Wc @ concat_input + self.bc)\n            output_gate = self.sigmoid(self.Wo @ concat_input + self.bo)\n            \n            # Update the cell state and hidden state\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            \n            # Store the hidden state\n            hidden_states.append(hidden_state.flatten().tolist())\n        \n        # Return the hidden states, final hidden state, and final cell state\n        return hidden_states, hidden_state.flatten().tolist(), cell_state.flatten().tolist()\n\n# Example usage:\ninput_size = 3\nhidden_size = 2\nlstm = LSTM(input_size, hidden_size)\nx = [np.random.randn(input_size, 1) for _ in range(5)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\nprint(\"Hidden States:\", hidden_states)\nprint(\"Final Hidden State:\", final_hidden_state)\nprint(\"Final Cell State:\", final_cell_state)\n```\n\nThis script defines an LSTM class with an `__init__` method for initializing the weights and biases, and a `forward` method for processing a sequence of inputs. The `forward` method computes the forget gate, input gate, candidate cell state, and output gate at each time step to update the hidden state and cell state. The output is rounded to the nearest 4th decimal and returned as a list using numpy's `tolist()` method."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty.\")\n    \n    num_docs = len(corpus)\n    tf_idf_scores = []\n    \n    for doc in corpus:\n        doc_word_count = len(doc)\n        if doc_word_count == 0:\n            tf_idf_scores.append([0.0] * len(query))\n            continue\n        \n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term) / doc_word_count\n            df = sum(1 for d in corpus if term in d)\n            idf = math.log((num_docs + 1) / (df + 1)) + 1\n            tf_idf = tf * idf\n            doc_tf_idf.append(round(tf_idf, 4))\n        \n        tf_idf_scores.append(doc_tf_idf)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\"],\n    [\"the\", \"lazy\", \"dog\"],\n    [\"the\", \"quick\", \"dog\"]\n]\nquery = [\"the\", \"quick\", \"dog\"]\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script defines the `compute_tf_idf` function that computes the TF-IDF scores for a given query against a corpus of documents. It handles edge cases such as empty documents, terms not present in the corpus, and ensures that division by zero is avoided by adding smoothing to the IDF calculation. The output is rounded to the nearest 4th decimal place and returned as a list of lists."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision == 0 and recall == 0:\n        f_beta = 0\n    else:\n        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta, 3)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0\nprint(f_score(y_true, y_pred, beta))  # Output should be the F-Score\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hidden_states = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        last_inputs = []\n\n        for t in range(T):\n            x = np.array(input_sequence[t]).reshape(-1, 1)\n            last_inputs.append(x)\n            h_prev = hidden_states[-1]\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            hidden_states.append(h)\n            outputs.append(y)\n        \n        return outputs, last_inputs, hidden_states[1:]\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n\n        for t in reversed(range(T)):\n            y = outputs[t]\n            x = last_inputs[t]\n            h = hidden_states[t]\n            h_prev = hidden_states[t-1] if t > 0 else np.zeros_like(h)\n\n            dy = y - expected_output[t].reshape(-1, 1)\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - h**2) * dh\n            dW_xh += np.dot(dh_raw, x.T)\n            dW_hh += np.dot(dh_raw, h_prev.T)\n            db_h += dh_raw\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate=0.01, epochs=100):\n        for epoch in range(epochs):\n            outputs, last_inputs, hidden_states = self.forward(input_sequence)\n            self.backward(input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate)\n            loss = 0.5 * np.sum((np.array(expected_output) - np.array(outputs))**2)\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n\n# Example usage:\ninput_size = 2\nhidden_size = 3\noutput_size = 2\nrnn = SimpleRNN(input_size, hidden_size, output_size)\ninput_sequence = [[1, 2], [2, 3], [3, 4]]\nexpected_output = [[2, 3], [3, 4], [4, 5]]\nrnn.train(input_sequence, expected_output)\n```\n\nThis script defines a `SimpleRNN` class that can be used to train a simple RNN on a sequence prediction task. The `forward` method performs the forward pass, and the `backward` method performs the backward pass (BPTT). The `train` method is a convenience method that trains the RNN for a specified number of epochs. The example usage at the bottom demonstrates how to use the class to train the RNN on a simple sequence prediction task."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    \n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + (rsnew / rsold) * p\n        rsold = rsnew\n    \n    return np.round(x, 8).tolist()\n\n# Example usage:\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\nx0 = np.array([0, 0])\nsolution = conjugate_gradient(A, b, n, x0)\nprint(solution)\n```\n\nThis script implements the Conjugate Gradient method for solving a linear system `Ax = b`. The function `conjugate_gradient` takes a symmetric, positive-definite matrix `A`, a vector `b`, the maximum number of iterations `n`, an optional initial guess `x0`, and a tolerance `tol` for the stopping criteria. The solution vector `x` is returned, rounded to 8 decimal places and converted to a list."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each class\n    class_counts = np.bincount(y)\n    # Calculate the probabilities of each class\n    probabilities = class_counts / len(y)\n    # Calculate the Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)\n\n# Example usage:\n# This should print the Gini Impurity for the given class labels\nprint(gini_impurity([1, 1, 2, 2, 3, 3, 3, 3]))  # Example output: 0.633\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # The first row starts at index 0\n    \n    for row in dense_matrix:\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        row_pointers.append(len(values))  # End of the row is the current length of values\n    \n    return (values, column_indices, row_pointers)\n\n# Example usage:\ndense_matrix = [\n    [1, 0, 0, 2],\n    [0, 3, 0, 0],\n    [4, 0, 5, 0]\n]\n\nvalues, column_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", column_indices)\nprint(\"Row Pointers:\", row_pointers)\n```\n\nThis code snippet defines a function `compressed_row_sparse_matrix` that takes a dense matrix as input and returns its CSR representation. The example usage demonstrates how to use the function and prints the resulting CSR arrays."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    \n    # Calculate the projection using the formula: proj = (v\u00b7L / L\u00b7L) * L\n    proj = (np.dot(v, L) / np.dot(L, L)) * L\n    \n    # Round the result to three decimal places and convert to list\n    proj_rounded = np.round(proj, 3).tolist()\n    \n    return proj_rounded\n\n# Example usage:\n# v = [1, 2, 3]\n# L = [4, 5, 6]\n# print(orthogonal_projection(v, L))  # Output should be the projection of v onto L\n```\n\nThis code snippet defines a function `orthogonal_projection` that calculates the orthogonal projection of a vector `v` onto another vector `L`. It uses the numpy library for vector operations, such as dot product and rounding. The projection formula `(v\u00b7L / L\u00b7L) * L` is applied, where `\u00b7` denotes the dot product. The result is rounded to three decimal places and returned as a list."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    values = []\n    row_indices = []\n    col_ptr = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n\n    return values, row_indices, col_ptr\n\n# Example usage:\ndense_matrix = [\n    [1, 0, 0],\n    [0, 2, 0],\n    [0, 0, 3],\n    [4, 0, 0]\n]\n\nvalues, row_indices, col_ptr = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", col_ptr)\n```\n\nThis script defines the `compressed_col_sparse_matrix` function which takes a dense matrix as input and returns its CSC representation. The example usage demonstrates how to use this function with a sample matrix."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n    Returns the basis vectors that span the column space of A.\n    The returned matrix is rounded to 8 decimal places and converted to a list.\n    \"\"\"\n    # Convert the input to a numpy array for consistency\n    A = np.array(A)\n    # Get the row echelon form of A\n    A_rref, pivot_columns = row_echelon_form(A)\n    # Extract the pivot columns from the original matrix\n    basis_vectors = A[:, pivot_columns]\n    # Round the basis vectors to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n\ndef row_echelon_form(A):\n    \"\"\"\n    Computes the row echelon form of matrix A and returns the pivot columns.\n    \"\"\"\n    A = np.array(A, dtype=float)  # Ensure A is a float array for division\n    m, n = A.shape\n    i, j = 0, 0\n    pivot_columns = []\n    \n    while i < m and j < n:\n        # Find the index of the row with the largest absolute value in column j\n        max_row = np.argmax(np.abs(A[i:, j])) + i\n        if A[max_row, j] == 0:\n            # Move to the next column if all entries are zero\n            j += 1\n        else:\n            # Swap the current row with the row having the largest absolute value\n            A[[i, max_row]] = A[[max_row, i]]\n            pivot_columns.append(j)\n            # Make all rows below the current row zero in column j\n            for k in range(i + 1, m):\n                factor = A[k, j] / A[i, j]\n                A[k, j:] -= factor * A[i, j:]\n            i += 1\n            j += 1\n    return A, pivot_columns\n\n# Example usage\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix_image(A))\n```\n\nThis script defines a function `matrix_image(A)` that calculates the column space of a given matrix `A` by first finding its row echelon form and then extracting the pivot columns from the original matrix. The `row_echelon_form` helper function computes the row echelon form of the matrix and identifies the pivot columns. The result is rounded to 8 decimal places and converted to a list before being returned."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression model given true and predicted values.\n    \n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the total sum of squares (TSS) and the residual sum of squares (RSS)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    return round(r2, 3)\n\n# Example usage:\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(r_squared(y_true, y_pred))\n```\n\nThis script defines a function `r_squared` that calculates the R-squared value for a given set of true and predicted values, adhering to the provided guidelines and constraints."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n    \n    Parameters:\n    img (list of list of int): A 2D matrix representing the grayscale image.\n    \n    Returns:\n    float: The average brightness of the image rounded to two decimal places.\n           Returns -1 for invalid input.\n    \"\"\"\n    if not img or not all(isinstance(row, list) and len(row) == len(img[0]) for row in img):\n        # Check if the image is empty or if rows have inconsistent lengths\n        return -1\n    \n    total_brightness = 0\n    pixel_count = 0\n    \n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                # Check if pixel values are outside the valid range\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    \n    if pixel_count == 0:\n        # Handle case where the image is empty\n        return -1\n    \n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)\n\n# Example usage\nimage = [\n    [100, 150, 200],\n    [120, 130, 140],\n    [110, 160, 170]\n]\n\nprint(calculate_brightness(image))  # Example output: 138.89\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of actual values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have different shapes or are empty.\n    TypeError: If input is not of type np.ndarray.\n    \"\"\"\n    # Check if inputs are numpy arrays\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError(\"Inputs must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate RMSE\n    squared_errors = (y_true - y_pred) ** 2\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    \n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(rmse(y_true, y_pred))  # Output: 0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    - y_true: A binary numpy array of true labels.\n    - y_pred: A binary numpy array of predicted labels.\n    \n    Returns:\n    - The Jaccard Index as a float value rounded to three decimal places.\n    \"\"\"\n    # Ensure the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where there is no union (both arrays are all zeros)\n    if union == 0:\n        return 1.0  # Both arrays are all zeros, perfect overlap\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    return round(jaccard, 3)\n\n# Check function with provided data points\ny_true = np.array([1, 1, 0, 0, 1, 0])\ny_pred = np.array([1, 0, 0, 0, 1, 1])\nprint(jaccard_index(y_true, y_pred))  # Expected output: 0.5\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of true and predicted labels\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the number of elements in each set\n    elements_y_true = np.sum(y_true)\n    elements_y_pred = np.sum(y_pred)\n    \n    # Handle edge case where both sets are empty\n    if elements_y_true == 0 and elements_y_pred == 0:\n        return 1.0\n    \n    # Calculate the Dice Score\n    dice = (2.0 * intersection) / (elements_y_true + elements_y_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage and check function\ndef check_dice_score():\n    y_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 1, 1, 0, 0, 1])\n    expected_score = 0.8\n    calculated_score = dice_score(y_true, y_pred)\n    assert calculated_score == expected_score, f\"Expected {expected_score}, got {calculated_score}\"\n    print(\"Test passed, Dice Score is correct.\")\n\n# Run the check function\ncheck_dice_score()\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_hypervector(seed, dim):\n    \"\"\"Generate a hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.where(np.random.rand(dim) > 0.5, 1, -1)\n\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hypervectors(hvs):\n    \"\"\"Bundle a list of hypervectors by summing them up and normalizing.\"\"\"\n    bundled = np.sum(hvs, axis=0)\n    return np.where(bundled > 0, 1, -1)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row.\"\"\"\n    feature_hvs = []\n    for feature, value in row.items():\n        # Create hypervector for the feature name\n        feature_hv = create_hypervector(hash(feature), dim)\n        # Create hypervector for the value using the feature's seed\n        value_hv = create_hypervector(random_seeds[feature], dim)\n        # Bind the feature and value hypervectors\n        bound_hv = bind_hypervectors(feature_hv, value_hv)\n        feature_hvs.append(bound_hv)\n    # Bundle all feature hypervectors to create the composite hypervector\n    composite_hv = bundle_hypervectors(feature_hvs)\n    return composite_hv.tolist()\n\n# Example usage:\nrow = {'feature1': 10, 'feature2': 20, 'feature3': 30}\ndim = 1000\nrandom_seeds = {'feature1': 1, 'feature2': 2, 'feature3': 3}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\n\nThis script defines a function `create_row_hv` that generates a composite hypervector for a dataset row using Hyperdimensional Computing (HDC). It includes helper functions for creating, binding, and bundling hypervectors, ensuring the process is self-contained and clear."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List\n\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    Parameters:\n    data (List[List[int]]): A list of lists, where each inner list represents a pair [y_true, y_pred]\n    \n    Returns:\n    List[List[int]]: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    # Count occurrences of each combination\n    counts = Counter(tuple(pair) for pair in data)\n    \n    # Extract counts for each cell in the confusion matrix\n    tp = counts[(1, 1)]  # True Positives\n    tn = counts[(0, 0)]  # True Negatives\n    fp = counts[(0, 1)]  # False Positives\n    fn = counts[(1, 0)]  # False Negatives\n    \n    # Construct the confusion matrix\n    matrix = [\n        [tn, fp],\n        [fn, tp]\n    ]\n    \n    return matrix\n\n# Check function with provided data points\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]]\nprint(confusion_matrix(data))\n# Expected output: [[1, 1], [1, 2]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: Numpy array representing the first input vector.\n    - v2: Numpy array representing the second input vector.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    # Check if vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape.\")\n    \n    # Check if vectors are not empty and have non-zero magnitude\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Vectors cannot be empty or have zero magnitude.\")\n    \n    # Calculate dot product of v1 and v2\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate magnitude (norm) of v1 and v2\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Calculate cosine similarity\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    \n    # Return cosine similarity rounded to three decimal places\n    return round(cos_sim, 3)\n\n# Example usage\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    # Validate input\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n    \n    # Calculate confusion matrix\n    confusion_matrix = [[0, 0], [0, 0]]\n    for a, p in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n    \n    # Extract values from confusion matrix\n    tp, fp = confusion_matrix[1][1], confusion_matrix[0][1]\n    fn, tn = confusion_matrix[1][0], confusion_matrix[0][0]\n    \n    # Calculate metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if (tn + fp) > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if (tn + fn) > 0 else 0\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage\nactual = [1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\nprint(performance_metrics(actual, predicted))\n```\n\nThis script defines a function `performance_metrics` that calculates the performance metrics for a binary classification model based on the actual and predicted class labels. It returns a tuple containing the confusion matrix and the calculated metrics, with the numeric metrics rounded to three decimal places."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): The dataset for which to calculate statistics.\n    \n    Returns:\n    dict: A dictionary containing the calculated statistics.\n    \"\"\"\n    # Convert data to numpy array if it's not already\n    data = np.array(data)\n    \n    # Calculate statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]  # scipy.stats.mode returns mode and count\n    variance = np.var(data, ddof=1)  # ddof=1 for sample variance\n    standard_deviation = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    \n    # Prepare output dictionary\n    output = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    \n    return output\n\n# Example usage\ndata = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(descriptive_statistics(data))\n```\n\nThis script defines a function `descriptive_statistics` that calculates and returns a dictionary of various descriptive statistics for a given dataset. It uses NumPy for most calculations and scipy.stats for calculating the mode. The output is rounded to four decimal places where applicable, as per the requirements."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability of k successes\n    prob = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability rounded to five decimal places\n    return round(prob, 5)\n\n# Example usage\nn = 10  # number of trials\nk = 3   # number of successes\np = 0.5 # probability of success on each trial\nprint(binomial_probability(n, k, p))\n```\n\nThis script defines a function `binomial_probability` that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials, each with a probability `p` of success, using the Binomial distribution formula. The function uses the `math.comb` function to calculate the binomial coefficient, which represents the number of ways to choose `k` successes from `n` trials. The probability is then calculated and rounded to five decimal places before being returned."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\n\n# Example usage:\n# Calculate the PDF for a normal distribution with mean 0 and standard deviation 1 at x = 0\nprint(normal_pdf(0, 0, 1))  # Expected output: 0.39894\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    # Calculate the factorial of k\n    k_factorial = math.factorial(k)\n    # Calculate the Poisson probability\n    probability = (lam ** k) * (math.exp(-lam)) / k_factorial\n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage:\n# Calculate the probability of observing exactly 3 events when the average rate is 2\nprint(poisson_probability(3, 2))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError(\"Input must be a 2D numpy array.\")\n    \n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    contrast = max_pixel_value - min_pixel_value\n    return contrast\n\n# Example usage:\n# Create a sample 2D numpy array representing a grayscale image\nsample_img = np.array([[10, 20, 30],\n                       [40, 50, 60],\n                       [70, 80, 255]], dtype=np.uint8)\n\n# Calculate the contrast of the sample image\ncontrast = calculate_contrast(sample_img)\nprint(f\"The contrast of the image is: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\n\n# Calculate the dot product\nresult = calculate_dot_product(vector1, vector2)\nprint(\"The dot product is:\", result)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Reshape the data to be a 2D array with one feature column\n    data_reshaped = np.array(data).reshape(-1, 1)\n    \n    # Initialize the transformed data with the original data\n    transformed_data = data_reshaped.copy()\n    \n    # Generate polynomial features\n    for d in range(2, degree + 1):\n        transformed_data = np.hstack((transformed_data, data_reshaped ** d))\n    \n    # Convert to list of lists and round to 8 decimal places\n    transformed_data = [list(map(lambda x: round(x, 8), row)) for row in transformed_data]\n    \n    return transformed_data\n\n# Check function with provided data points\ndef check_phi_transform():\n    # Test cases\n    test_cases = [\n        ([1.0, 2.0, 3.0], 2, [[1.0, 1.0], [2.0, 4.0], [3.0, 9.0]]),\n        ([1.0, 2.0], 3, [[1.0, 1.0, 1.0], [2.0, 4.0, 8.0]]),\n        ([1.0, 2.0, 3.0], -1, []),\n    ]\n    \n    for data, degree, expected in test_cases:\n        result = phi_transform(data, degree)\n        assert result == expected, f\"Failed test with data={data}, degree={degree}. Expected {expected}, got {result}\"\n    \n    print(\"All tests passed!\")\n\n# Run the check function\ncheck_phi_transform()\n```\n\nThis code defines the `phi_transform` function that performs the Phi Transformation for polynomial features and includes a check function to validate the implementation against some test cases."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encodings for a given sequence length and model dimensionality.\n    If position is 0 or d_model is less than or equal to 0, returns -1.\n    \n    :param position: The length of the sequence.\n    :param d_model: The dimensionality of the model.\n    :return: A list of positional encodings or -1 if input is invalid.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    pe = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pe[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n            if i + 1 < d_model:\n                pe[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n    return pe.tolist()\n\n# Example check function to verify the correctness\ndef check_pos_encoding():\n    result = pos_encoding(3, 4)\n    expected = [[0.0, 1.0, 0.0, 1.0], [0.0, -1.0, 0.0, -1.0], [0.0, 1.0, 0.0, 1.0]]\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n    print(\"pos_encoding function works correctly with the provided data points.\")\n\n# Run the check function\ncheck_pos_encoding()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    # Check for underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # If neither overfitting nor underfitting, it's a good fit\n    else:\n        return 0\n\n# Test cases to verify the correctness of the function\nprint(model_fit_quality(0.9, 0.5))  # Overfitting, should return 1\nprint(model_fit_quality(0.6, 0.65)) # Underfitting, should return -1\nprint(model_fit_quality(0.8, 0.75)) # Good fit, should return 0\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first and second moment estimates\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Apply bias correction\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round and convert to list if necessary\n    if isinstance(parameter, np.ndarray):\n        parameter = np.round(parameter, 5).tolist()\n        m = np.round(m, 5).tolist()\n        v = np.round(v, 5).tolist()\n    else:\n        parameter = round(parameter, 5)\n        m = round(m, 5)\n        v = round(v, 5)\n    \n    return parameter, m, v\n\n# Example usage:\nparameter = np.array([0.1, 0.2, 0.3])\ngrad = np.array([0.01, 0.02, 0.03])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(updated_parameter, updated_m, updated_v)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt\n    token_ids = encoder.encode(prompt)\n    token_ids = token_ids[-hparams[\"n_ctx\"]:]  # Truncate to context size\n\n    # Token embeddings\n    token_embeddings = params[\"wte\"][token_ids]\n\n    # Positional embeddings\n    position_ids = np.arange(len(token_ids))\n    positional_embeddings = params[\"wpe\"][position_ids]\n\n    # Combine token and positional embeddings\n    x = token_embeddings + positional_embeddings\n\n    # Simplified multi-head attention (randomly selected embeddings)\n    # In a real model, this would involve calculating attention scores and applying them.\n    attention_output = np.random.rand(len(token_ids), x.shape[1])\n\n    # Feed-forward network (randomly generated output)\n    # In a real model, this would involve passing through dense layers.\n    feed_forward_output = np.random.rand(len(token_ids), x.shape[1])\n\n    # Layer normalization (simplified)\n    normalized_output = (feed_forward_output - np.mean(feed_forward_output, axis=1, keepdims=True)) / (np.std(feed_forward_output, axis=1, keepdims=True) + 1e-5)\n    normalized_output = normalized_output * params[\"ln_f\"][\"g\"] + params[\"ln_f\"][\"b\"]\n\n    # Generate new tokens (randomly selected from the encoder dictionary)\n    new_tokens = np.random.randint(0, len(encoder.encoder_dict), n_tokens_to_generate)\n    generated_text = encoder.decode(new_tokens)\n\n    return generated_text\n\n# Example usage\nprompt = \"hello world\"\ngenerated_text = gen_text(prompt, 5)\nprint(generated_text)\n```\n\nThis script provides a simplified version of the GPT-2 text generation process. It includes token and positional embeddings, a simplified multi-head attention mechanism, a feed-forward network, and layer normalization. The output is generated randomly for demonstration purposes."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    \n    # Initialize the weights for the self-attention mechanism\n    weights = np.random.rand(n, dimension)\n    \n    # Calculate the relationship between each crystal and every other crystal\n    attention_scores = np.dot(crystal_values, weights.T)\n    \n    # Apply softmax to get the attention scores\n    attention_scores = softmax(attention_scores)\n    \n    # Calculate the final weighted pattern for each crystal\n    weighted_patterns = np.dot(attention_scores, weights)\n    \n    # Return the final patterns, rounded to the 4th decimal place\n    return [round(pattern, 4) for pattern in weighted_patterns.flatten().tolist()]\n\n# Example usage:\nn = 3  # Number of crystals\ncrystal_values = np.array([1, 2, 3])  # Values of the crystals\ndimension = 2  # Dimension for the self-attention mechanism\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script defines a simplified self-attention mechanism for the given task. It initializes random weights for the self-attention mechanism, calculates the attention scores using the dot product between the crystal values and the weights, applies the softmax function to normalize these scores, and finally computes the weighted patterns for each crystal. The output is a list of floats representing the final patterns, each rounded to the 4th decimal place."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus given a query.\n    \n    Parameters:\n    corpus (list of str): The list of documents.\n    query (str): The query string.\n    k1 (float): Term frequency saturation parameter.\n    b (float): Document length normalization parameter.\n    \n    Returns:\n    list: BM25 scores for each document in the corpus.\n    \"\"\"\n    # Tokenize the query\n    query_terms = query.split()\n    \n    # Calculate average document length\n    avgdl = sum(len(doc.split()) for doc in corpus) / len(corpus)\n    \n    # Calculate idf for each term in the query\n    idf = {term: np.log((len(corpus) - sum(term in doc for doc in corpus) + 0.5) / (sum(term in doc for doc in corpus) + 0.5)) for term in query_terms}\n    \n    scores = []\n    for doc in corpus:\n        doc_terms = Counter(doc.split())\n        score = 0\n        for term in query_terms:\n            if term in doc_terms:\n                tf = doc_terms[term]\n                dl = len(doc.split())\n                score += idf[term] * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avgdl))\n        scores.append(round(score, 3))\n    \n    return scores\n\n# Example usage\ncorpus = [\n    \"the sky is blue\",\n    \"the sun is bright\",\n    \"the sun in the sky is bright\",\n    \"we can see the shining sun, the bright sun\"\n]\nquery = \"the sun bright\"\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines a function `calculate_bm25_scores` that calculates the BM25 scores for each document in a given corpus based on a query. The function uses parameters `k1` and `b` to adjust the influence of term frequency saturation and document length normalization, respectively. The example usage demonstrates how to call the function with a sample corpus and query, and it prints the resulting BM25 scores for each document."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize true positives, false positives, and false negatives\n    tp = fp = fn = 0\n    \n    # Convert lists to sets for easier calculation\n    set_true = set(y_true)\n    set_pred = set(y_pred)\n    \n    # Calculate true positives, false positives, and false negatives\n    for label in set_pred:\n        if label in set_true:\n            tp += min(y_true.count(label), y_pred.count(label))\n        else:\n            fp += y_pred.count(label)\n    \n    for label in set_true:\n        if label not in set_pred:\n            fn += y_true.count(label)\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1 score\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Return F1 score rounded to three decimal places\n    return round(f1, 3)\n\n# Example usage\ny_true = [0, 1, 2, 1, 0]\ny_pred = [0, 2, 1, 1, 0]\nprint(calculate_f1_score(y_true, y_pred))\n```\n\nThis script defines a function `calculate_f1_score` that calculates the F1 score based on the true and predicted labels provided. It handles the calculation of true positives, false positives, and false negatives, which are then used to compute precision and recall. Finally, the F1 score is calculated as the harmonic mean of precision and recall, rounded to three decimal places."}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # 1) Subtract the daily fluctuation (10 * sin(2\u03c0 * i / 10)) from each data point.\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    \n    # 2) Perform linear regression on the detrended data.\n    days = np.array(range(1, len(consumption_data) + 1)).reshape((-1, 1))\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # 3) Predict day 15's base consumption.\n    day_15 = np.array([15]).reshape((-1, 1))\n    base_consumption_day_15 = model.predict(day_15)[0]\n    \n    # 4) Add the day 15 fluctuation back.\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    \n    # 5) Round, then add a 5% safety margin (rounded up).\n    final_prediction = math.ceil(predicted_consumption_day_15 * 1.05)\n    \n    # 6) Return the final integer.\n    return final_prediction\n\n# Example usage:\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]  # Example data points\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script includes the necessary steps to forecast the power consumption for day 15, taking into account the linear growth trend and daily fluctuation, and adding a safety margin to ensure the power grid can handle the load."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Input arrays must be numpy.ndarray type\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape\")\n\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(mae(y_true, y_pred))  # Output: 0.5\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices for the self-attention mechanism.\n    :param X: Input data matrix.\n    :param W_q: Weight matrix for the Query.\n    :param W_k: Weight matrix for the Key.\n    :param W_v: Weight matrix for the Value.\n    :return: A tuple of Query, Key, and Value matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention output for a single head.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: The output of the self-attention mechanism.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), 1, scores)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention output.\n    :param X: Input data matrix.\n    :param W_q: Weight matrix for the Query.\n    :param W_k: Weight matrix for the Key.\n    :param W_v: Weight matrix for the Value.\n    :param n_heads: Number of attention heads.\n    :return: The output of the multi-head attention mechanism as a list.\n    \"\"\"\n    batch_size, seq_len, d_model = X.shape\n    d_k = d_model // n_heads\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    Q_heads = np.split(Q, n_heads, axis=-1)\n    K_heads = np.split(K, n_heads, axis=-1)\n    V_heads = np.split(V, n_heads, axis=-1)\n    \n    outputs = []\n    for Q_head, K_head, V_head in zip(Q_heads, K_heads, V_heads):\n        output = self_attention(Q_head, K_head, V_head)\n        outputs.append(output)\n    \n    concatenated_output = np.concatenate(outputs, axis=-1)\n    rounded_output = np.round(concatenated_output, 4)\n    return rounded_output.tolist()\n\n# Example usage\nX = np.random.rand(2, 4, 8)  # Batch size 2, sequence length 4, model dimension 8\nW_q = np.random.rand(8, 8)  # Model dimension 8, query dimension 8\nW_k = np.random.rand(8, 8)  # Model dimension 8, key dimension 8\nW_v = np.random.rand(8, 8)  # Model dimension 8, value dimension 8\nn_heads = 2  # Number of attention heads\n\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(output)\n```\n\nThis script provides a self-contained implementation of the multi-head attention mechanism, including the computation of Query, Key, and Value matrices, the self-attention mechanism for a single head, and the aggregation of multiple heads into a final output. The output is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Convert the lists into a contingency table\n    contingency_table = np.array([\n        [sum((xi == 0) & (yi == 0) for xi, yi in zip(x, y)),\n         sum((xi == 0) & (yi == 1) for xi, yi in zip(x, y))],\n        [sum((xi == 1) & (yi == 0) for xi, yi in zip(x, y)),\n         sum((xi == 1) & (yi == 1) for xi, yi in zip(x, y))]\n    ])\n    \n    # Chi-square test\n    chi2, _, _, _ = chi2_contingency(contingency_table)\n    \n    # Calculate the Phi coefficient\n    n = len(x)\n    phi = np.sqrt(chi2 / n)\n    \n    return round(phi, 4)\n\n# Example usage\nx = [0, 0, 1, 1, 0, 1, 0, 1, 1, 0]\ny = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\nprint(phi_corr(x, y))\n```\n\nThis code snippet defines a function `phi_corr` that calculates the Phi coefficient for two binary variables. It uses the `chi2_contingency` function from the `scipy.stats` module to perform a chi-square test on the contingency table derived from the input lists. The Phi coefficient is then calculated from the chi-square statistic and the number of observations, and the result is rounded to four decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Test the function with some example values\ntest_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\nfor value in test_values:\n    print(f\"hard_sigmoid({value}) = {hard_sigmoid(value)}\")\n```\n\nThis code defines the `hard_sigmoid` function and tests it with a set of example values to demonstrate its behavior across the range of possible inputs."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n        \n# Example usage and check function\ndef check_elu():\n    test_cases = [\n        (-1.0, 1.0, -0.6321),\n        (0.0, 1.0, 0.0),\n        (1.0, 1.0, 1.0),\n        (-2.0, 0.5, -0.3935),\n        (2.0, 0.5, 2.0)\n    ]\n    for x, alpha, expected in test_cases:\n        result = round(elu(x, alpha), 4)\n        assert result == expected, f\"elu({x}, {alpha}) = {result}, expected {expected}\"\n    print(\"All test cases passed.\")\n\n# Run the check function to verify the correctness of the elu function\ncheck_elu()\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage:\nif __name__ == \"__main__\":\n    input_value = -2.0\n    alpha_value = 0.25\n    output_value = prelu(input_value, alpha_value)\n    print(f\"PReLU({input_value}, alpha={alpha_value}) = {output_value}\")\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handling edge cases to prevent numerical overflow or underflow\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0.0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage\nprint(softplus(1.0))  # Expected output: 1.3133\nprint(softplus(-1000))  # Expected output: 0.0\nprint(softplus(1000))  # Expected output: 1000.0\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-10.0, -1.5, 0.0, 2.3, 100.0]\n    for val in test_values:\n        print(f\"softsign({val}) = {softsign(val)}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Clip likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the clipped objective\n    clipped_objective = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # Compute the KL divergence between old and reference policy\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Apply the KL divergence penalty\n    grpo_value = clipped_objective - beta * kl_divergence\n    \n    return round(grpo_value, 6)\n\n# Example usage\nrhos = [1.2, 0.9, 1.1, 0.8]\nA = [0.5, -0.2, 0.3, -0.1]\npi_theta_old = [0.7, 0.3, 0.8, 0.2]\npi_theta_ref = [0.6, 0.4, 0.7, 0.3]\nepsilon = 0.2\nbeta = 0.01\n\nprint(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon, beta))\n```\n\nThis script defines the `grpo_objective` function which calculates the GRPO objective value given the likelihood ratios, advantage estimates, old and reference policy probabilities, and applies a clipping mechanism and KL divergence penalty to ensure training stability. The function returns the computed GRPO objective value rounded to the nearest 6th decimal."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x / (1 + math.exp(-x))\n\n# Example usage and verification\ndef check_swish():\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    expected_results = [-0.2385, -0.2689, 0.0000, 0.7311, 1.7616]\n    for x, expected in zip(test_values, expected_results):\n        result = round(swish(x), 4)\n        assert result == expected, f\"Swish({x}) = {result}, expected {expected}\"\n    print(\"All tests passed!\")\n\n# Run the check function to verify the correctness of the swish function\ncheck_swish()\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Test the function with some values\ntest_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\nresults = [selu(x) for x in test_values]\n\n# Print results rounded to the nearest 4th decimal\nprint([round(result, 4) for result in results])\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights plus bias\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Convert probabilities to binary predictions (0 or 1)\n    predictions = (probabilities > 0.5).astype(int)\n    \n    # Return predictions as a list\n    return predictions.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    weights = np.array([0.5, -0.3])\n    bias = 0.2\n    \n    # Predict\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = len(np.unique(y))\n    B = np.zeros((n_features, n_classes))\n    losses = []\n\n    for _ in range(iterations):\n        # Compute softmax probabilities\n        exp_scores = np.exp(np.dot(X, B))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Compute gradient\n        y_one_hot = np.eye(n_classes)[y]\n        gradient = np.dot(X.T, (probs - y_one_hot)) / n_samples\n\n        # Update parameters\n        B -= learning_rate * gradient\n\n        # Compute cross entropy loss\n        correct_logprobs = -np.log(probs[range(n_samples), y])\n        data_loss = np.sum(correct_logprobs) / n_samples\n        losses.append(round(data_loss, 4))\n\n    return B.round(4).tolist(), [round(loss, 4) for loss in losses]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 1, 1, 0])\n    learning_rate = 0.01\n    iterations = 100\n\n    # Train softmax regression\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    print(\"Coefficients:\", B)\n    print(\"Losses:\", losses)\n```\n\nThis script defines the function `train_softmaxreg` which implements the gradient descent algorithm for training a Softmax regression model. It takes input features `X`, labels `y`, a learning rate, and the number of iterations as parameters. The function returns the optimized coefficients and a list of loss values for each iteration, both rounded to the nearest 4th decimal. The example usage at the end demonstrates how to use the function with a simple dataset."}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function to map predicted values to probabilities.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef compute_loss(y, y_pred):\n    \"\"\"\n    Computes the Binary Cross Entropy loss.\n    \"\"\"\n    epsilon = 1e-15  # To avoid division by zero or log(0)\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    m, n = X.shape\n    # Initialize weights and bias\n    weights = np.zeros(n)\n    bias = 0\n    losses = []\n\n    for _ in range(iterations):\n        # Predicted values\n        z = np.dot(X, weights) + bias\n        y_pred = sigmoid(z)\n\n        # Compute gradients\n        dw = (1 / m) * np.dot(X.T, (y_pred - y))\n        db = (1 / m) * np.sum(y_pred - y)\n\n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n\n        # Compute and store loss\n        loss = compute_loss(y, y_pred)\n        losses.append(round(loss, 4))\n\n    # Return optimized parameters and losses\n    return weights.tolist(), bias, losses\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([0, 0, 1, 1])\n# learning_rate = 0.1\n# iterations = 1000\n# weights, bias, losses = train_logreg(X, y, learning_rate, iterations)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n# print(\"Losses:\", losses)\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    scores = scores + mask\n    p_attn = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.matmul(p_attn, V)\n    return output.tolist()\n\n# Example usage\nX = np.random.rand(4, 16)  # Input matrix\nW_q = np.random.rand(16, 8)  # Query weight matrix\nW_k = np.random.rand(16, 8)  # Key weight matrix\nW_v = np.random.rand(16, 8)  # Value weight matrix\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\nmask = np.triu(np.ones((Q.shape[0], Q.shape[0])) * -1e9, k=1)  # Example mask for causal attention\n\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom math import log\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is calculated using the Shannon entropy formula.\n    \"\"\"\n    # Count the frequency of each color in the basket\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the probability of each color\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate the Shannon entropy as a measure of disorder\n    entropy = -sum(p * log(p, 2) for p in probabilities if p > 0)\n    \n    # Return the entropy rounded to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Test cases\nprint(disorder([0,0,0,0]))  # Expected output: 0.0\nprint(disorder([1,1,0,0]))  # Expected output: 1.0\nprint(disorder([0,1,2,3]))  # Expected output: 2.0\nprint(disorder([0,0,1,1,2,2,3,3]))  # Expected output: 2.0\nprint(disorder([0,0,0,0,0,1,2,3]))  # Expected output: 1.6204\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray, a 3D array of shape (batch_size, seq_length, feature_dim)\n    - gamma: np.ndarray, scaling parameter of shape (feature_dim,)\n    - beta: np.ndarray, shifting parameter of shape (feature_dim,)\n    - epsilon: float, a small constant for numerical stability\n    \n    Returns:\n    - np.ndarray, the normalized, scaled, and shifted tensor\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the tensor\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_scaled_shifted = gamma * X_norm + beta\n    \n    # Round to 5 decimal places and convert to list\n    return np.round(X_scaled_shifted, decimals=5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input tensor X, scaling parameter gamma, and shifting parameter beta\n    X = np.random.randn(2, 3, 4)  # Example 3D tensor with shape (batch_size=2, seq_length=3, feature_dim=4)\n    gamma = np.random.randn(4)    # Scaling parameter\n    beta = np.random.randn(4)     # Shifting parameter\n    \n    # Perform Layer Normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    # Print the normalized tensor\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"Compute the harmonic mean of precision and recall.\"\"\"\n    if precision + recall == 0:\n        return 0\n    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\ndef fragmentation_score(reference, candidate):\n    \"\"\"Compute the fragmentation penalty score.\"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n    \n    matches = 0\n    for word in cand_counter:\n        if word in ref_counter:\n            matches += min(cand_counter[word], ref_counter[word])\n    \n    ref_length = len(ref_words)\n    cand_length = len(cand_words)\n    if matches == 0:\n        return 0\n    return matches / max(ref_length, cand_length)\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"Compute the METEOR score for a given reference and candidate translation.\"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    \n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n    \n    matches = 0\n    for word in cand_counter:\n        if word in ref_counter:\n            matches += min(cand_counter[word], ref_counter[word])\n    \n    precision = matches / len(cand_words) if cand_words else 0\n    recall = matches / len(ref_words) if ref_words else 0\n    f_mean = harmonic_mean(precision, recall, beta)\n    fragmentation = fragmentation_score(reference, candidate)\n    score = (1 - alpha) * f_mean + alpha * fragmentation\n    score = score * (1 - np.exp(-gamma * matches))\n    \n    return round(score, 3)\n\n# Test the function with example data\nreference = \"The cat sat on the mat\"\ncandidate = \"The cat is sitting on the mat\"\nprint(meteor_score(reference, candidate))  # Expected output: METEOR score\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The number of times both events x and y occur together.\n    total_counts_x (int): The total number of occurrences of event x.\n    total_counts_y (int): The total number of occurrences of event y.\n    total_samples (int): The total number of samples in the data.\n    \n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate expected joint probability\n    expected_joint_prob = (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    \n    # Calculate actual joint probability\n    actual_joint_prob = joint_counts / total_samples\n    \n    # Compute PMI\n    pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    \n    return round(pmi, 3)\n\n# Example usage\njoint_counts = 100\ntotal_counts_x = 200\ntotal_counts_y = 250\ntotal_samples = 10000\n\nprint(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\nThis script defines a function `compute_pmi` that calculates the Pointwise Mutual Information based on the provided counts and total samples. It then rounds the result to three decimal places as required. An example usage of the function is also provided."}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    Each value is transformed using the formula: (x - min(x)) / (max(x) - min(x))\n    Returns a list of floats rounded to 4 decimal places.\n    \"\"\"\n    if not x or len(x) < 2:\n        raise ValueError(\"List must contain at least two elements for normalization.\")\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    if min_val == max_val:\n        raise ValueError(\"All elements in the list are identical; normalization is not possible.\")\n    \n    normalized = [(float(i - min_val) / (max_val - min_val)) for i in x]\n    return [round(i, 4) for i in normalized]\n\n# Example usage and check function\ndef check_min_max_normalization():\n    data_points = [[1, 2, 3, 4, 5], [10, 20, 30, 40, 50], [-5, 0, 5, 10]]\n    expected_results = [\n        [0.0, 0.25, 0.5, 0.75, 1.0],\n        [0.0, 0.25, 0.5, 0.75, 1.0],\n        [0.0, 0.25, 0.75, 1.0]\n    ]\n    \n    for data, expected in zip(data_points, expected_results):\n        result = min_max(data)\n        assert result == expected, f\"Failed for data: {data}. Expected: {expected}, got: {result}\"\n    print(\"All checks passed!\")\n\ncheck_min_max_normalization()\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Args:\n    x: np.ndarray - The input array.\n    w1: np.ndarray - The weight matrix for the first layer.\n    w2: np.ndarray - The weight matrix for the second layer.\n    \n    Returns:\n    np.ndarray - The output of the residual block, rounded to 4 decimal places.\n    \"\"\"\n    # First layer\n    x1 = np.matmul(x, w1)\n    x1 = np.maximum(x1, 0)  # ReLU activation\n    \n    # Second layer\n    x2 = np.matmul(x1, w2)\n    x2 = np.maximum(x2, 0)  # ReLU activation\n    \n    # Shortcut connection\n    out = x2 + x  # Add the input to the output of the second layer\n    \n    # Final ReLU activation\n    out = np.maximum(out, 0)\n    \n    # Return the output rounded to 4 decimal places\n    return np.round(out, 4).tolist()\n\n# Example usage\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n\noutput = residual_block(x, w1, w2)\nprint(output)\n```\n\nThis script defines a simple residual block function using NumPy, which processes the input through two weight layers with ReLU activations and includes a shortcut connection. The output is rounded to 4 decimal places and converted to a list before being returned."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels) representing feature maps.\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) where each element is the average of all values in the corresponding feature map.\n    \"\"\"\n    # Calculate the average of each feature map\n    return np.mean(x, axis=(0, 1))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random 3D array of shape (4, 4, 3) representing 3 feature maps\n    feature_maps = np.random.rand(4, 4, 3)\n    \n    # Perform Global Average Pooling\n    pooled_output = global_avg_pool(feature_maps)\n    \n    # Print the result\n    print(\"Pooled Output:\", pooled_output)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Applies batch normalization to a 4D input array in BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input array in BCHW format.\n    gamma (np.ndarray): Scale parameter for each channel.\n    beta (np.ndarray): Shift parameter for each channel.\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized input array, rounded to 4 decimal places.\n    \"\"\"\n    # Compute the mean and variance for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scale and shift\n    X_scaled = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_norm + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places\n    X_scaled_rounded = np.round(X_scaled, decimals=4)\n    \n    return X_scaled_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Parameters\n    np.random.seed(0)  # For reproducibility\n    batch_size, channels, height, width = 2, 3, 4, 4\n    X = np.random.rand(batch_size, channels, height, width)\n    gamma = np.random.rand(channels)\n    beta = np.random.rand(channels)\n    epsilon = 1e-5\n    \n    # Normalization\n    normalized_X = batch_normalization(X, gamma, beta, epsilon)\n    \n    # Output\n    print(normalized_X)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the power rule: d/dx(c * x^n) = c * n * x^(n-1)\n    derivative_value = c * n * (x ** (n - 1))\n    return round(derivative_value, 4)\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Test cases to verify the correctness of the function\n    test_cases = [\n        (3.0, 2.0, 2.0),  # c=3, x=2, n=2, expected derivative value is 12.0\n        (5.0, 3.0, 3.0),  # c=5, x=3, n=3, expected derivative value is 135.0\n        (1.5, 4.0, 1.0),  # c=1.5, x=4, n=1, expected derivative value is 1.5\n    ]\n    \n    for c, x, n in test_cases:\n        result = poly_term_derivative(c, x, n)\n        print(f\"Derivative of {c} * x^{n} at x={x} is: {result}\")\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    vectors (list of list of float): A list of 2D vectors.\n    tol (float): Tolerance value to determine linear independence.\n    \n    Returns:\n    list of list of float: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    # Convert list of vectors to a numpy array for easier manipulation\n    V = np.array(vectors)\n    num_vectors = V.shape[0]\n    \n    # Initialize an empty list to store the orthonormal basis\n    basis = []\n    \n    for i in range(num_vectors):\n        # Subtract the projection of V[i] onto the current basis\n        for b in basis:\n            V[i] -= np.dot(V[i], b) * b\n        \n        # Check if the resulting vector is linearly independent\n        if np.linalg.norm(V[i]) > tol:\n            # Normalize the vector and add it to the basis\n            basis.append(V[i] / np.linalg.norm(V[i]))\n    \n    # Convert numpy arrays to lists and round to 4 decimal places\n    basis = [np.round(np.array(b), 4).tolist() for b in basis]\n    \n    return basis\n\n# Example usage\nvectors = [[1, 2], [2, 4], [1, 3]]\nprint(orthonormal_basis(vectors))\n```\n\nThis script defines a function `orthonormal_basis` that computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process. The function takes a list of 2D vectors and a tolerance value to determine linear independence. It returns a list of orthonormal vectors that span the same subspace, with each vector rounded to 4 decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (list/tuple): The first 3D vector.\n    b (list/tuple): The second 3D vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    # Convert input lists/tuples to numpy arrays for vector operations\n    a_np = np.array(a)\n    b_np = np.array(b)\n    \n    # Compute the cross product using numpy's cross function\n    cross_prod = np.cross(a_np, b_np)\n    \n    # Round the result to 4 decimal places\n    cross_prod_rounded = np.round(cross_prod, 4)\n    \n    # Convert the numpy array back to a list for output\n    return cross_prod_rounded.tolist()\n\n# Example usage\nvector_a = (1, 2, 3)\nvector_b = (4, 5, 6)\nresult = cross_product(vector_a, vector_b)\nprint(result)  # Expected output: [-3.0, 6.0, -3.0]\n```\n\nThis code snippet defines a function `cross_product` that takes two 3D vectors as input and returns their cross product, rounded to 4 decimal places. The function uses the `numpy` library to perform the vector operations efficiently and accurately."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n    \n    Returns:\n    list: The solution vector x rounded to the nearest 4th decimal, or -1 if no unique solution exists.\n    \"\"\"\n    # Convert inputs to numpy arrays for consistency\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, return -1 indicating no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column of A to replace it with b and calculate the determinant\n    for i in range(len(A)):\n        # Replace the ith column of A with b\n        Ai = A.copy()\n        Ai[:, i] = b\n        # Calculate the determinant of the modified matrix\n        det_Ai = np.linalg.det(Ai)\n        # Calculate the ith component of the solution vector\n        xi = det_Ai / det_A\n        # Append the rounded value to the solution vector\n        x.append(round(xi, 4))\n    \n    # Return the solution vector as a list\n    return x\n\n# Example usage\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]\nsolution = cramers_rule(A, b)\nprint(solution)\n```\n\nThis script defines a function `cramers_rule` that takes a square matrix `A` and a vector `b` as inputs and solves the system of linear equations using Cramer's Rule. The solution is returned as a list of values rounded to the nearest 4th decimal, or `-1` if the system does not have a unique solution."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions p and q.\n    If the inputs have different lengths or are empty, returns 0.0.\n    The result is rounded to 4 decimal places.\n    \"\"\"\n    # Check if the inputs are valid\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(np.multiply(p, q)))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    return round(bd, 4)\n\n# Example usage\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]\nprint(bhattacharyya_distance(p, q))  # Example output: 0.1054\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n    \n    Returns:\n    list[int|float] | int: A new vector representing the element-wise sum of a and b,\n                            or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [x + y for x, y in zip(a, b)]\n    \n    return result\n\n# Check function to verify the correctness of the solution\ndef check_vector_sum():\n    assert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9], \"Test case 1 failed\"\n    assert vector_sum([1, 2], [3, 4, 5]) == -1, \"Test case 2 failed\"\n    assert vector_sum([0.5, 1.5], [2.5, 3.5]) == [3.0, 5.0], \"Test case 3 failed\"\n    assert vector_sum([], []) == [], \"Test case 4 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the solution\ncheck_vector_sum()\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        returns = np.zeros(len(episode))\n        returns[-1] = episode[-1][2]\n        for t in reversed(range(len(episode)-1)):\n            returns[t] = episode[t][2] + returns[t+1]\n        \n        for t, (state, action, _) in enumerate(episode):\n            state_policy = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            gradient[state, :] += state_policy * returns[t]\n            gradient[state, action] -= returns[t]\n    \n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()\n\n# Example usage\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [\n    [(0, 1, 1.0), (1, 0, 1.0)],\n    [(0, 0, 0.5), (1, 1, 0.5)]\n]\nprint(compute_policy_gradient(theta, episodes))\n```\n\nThis script defines a function `compute_policy_gradient` that computes the average gradient of the log-policy multiplied by the return at each time step for a given set of episodes and a policy parameterized by `theta`. The function iterates over each episode to calculate the returns and then computes the gradient for each state-action pair. Finally, it averages the gradient over all episodes and returns the result as a list of lists, rounded to the nearest 4th decimal."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts in the MoE layer.\n    d_in (int): The input dimension.\n    d_out (int): The output dimension.\n    \n    Returns:\n    float: The percentage of computational cost savings rounded to the nearest 1th decimal.\n    \"\"\"\n    # FLOPs for dense layer: 2 * d_in * d_out (multiplication and addition)\n    flops_dense = 2 * d_in * d_out\n    \n    # FLOPs for MoE layer: 2 * d_in * d_out * k_active (for active experts) + 2 * d_in * n_experts (for gating)\n    flops_moe = 2 * d_in * d_out * k_active + 2 * d_in * n_experts\n    \n    # Calculate savings\n    savings = flops_dense - flops_moe\n    \n    # Calculate savings percentage\n    savings_percentage = (savings / flops_dense) * 100\n    \n    # Round to the nearest 1th decimal\n    return round(savings_percentage, 1)\n\n# Example usage\nn_experts = 128\nk_active = 4\nd_in = 1024\nd_out = 1024\n\nprint(compute_efficiency(n_experts, k_active, d_in, d_out))\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer, based on the provided parameters. It then prints the savings percentage, rounded to the nearest 1th decimal."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K Gating Function for Mixture-of-Experts models.\n    \n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for gating.\n    W_noise (np.ndarray): Weight matrix for noise.\n    N (np.ndarray): Pre-sampled noise.\n    k (int): Sparsity constraint, number of experts to select.\n    \n    Returns:\n    list: Final gating probabilities matrix as a list, rounded to 4 decimal places.\n    \"\"\"\n    # Compute the logits for gating\n    logits = X @ W_g\n    \n    # Add noise to the logits\n    noisy_logits = logits + N * W_noise\n    \n    # Apply softmax to get probabilities\n    exp_logits = np.exp(noisy_logits - noisy_logits.max(axis=-1, keepdims=True))\n    probs = exp_logits / exp_logits.sum(axis=-1, keepdims=True)\n    \n    # Select top-k probabilities\n    topk_indices = np.argpartition(probs, -k, axis=-1)[:, -k:]\n    topk_probs = np.take_along_axis(probs, topk_indices, axis=-1)\n    \n    # Normalize the top-k probabilities\n    topk_probs /= topk_probs.sum(axis=-1, keepdims=True)\n    \n    # Create the final gating matrix\n    gating_matrix = np.zeros_like(probs)\n    np.put_along_axis(gating_matrix, topk_indices, topk_probs, axis=-1)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(gating_matrix, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [3, 4]])\n    W_g = np.array([[0.1, 0.2], [0.3, 0.4]])\n    W_noise = np.array([[0.05, 0.05], [0.05, 0.05]])\n    N = np.array([[0.1, -0.1], [-0.1, 0.1]])\n    k = 1\n    \n    # Call the function\n    result = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(result)\n```\n\nThis script defines the `noisy_topk_gating` function that implements the Noisy Top-K gating mechanism used in Mixture-of-Experts models. It takes as input an input matrix `X`, weight matrices `W_g` and `W_noise`, pre-sampled noise `N`, and a sparsity constraint `k`. The function computes the final gating probabilities matrix, rounding the results to the nearest 4th decimal and returning them as a list."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n    \n    Returns:\n    np.ndarray: Output tensor after applying the MoE layer.\n    \"\"\"\n    # Compute gating scores\n    gating_scores = np.dot(x, Wg)\n    # Apply softmax to get gating probabilities\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1, keepdims=True)\n    \n    # Select top-k experts for each token\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    # Normalize the top-k probabilities\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    \n    # Compute expert outputs\n    expert_outputs = np.dot(x[:, np.newaxis, :], We).squeeze(1)\n    # Select top-k expert outputs\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, np.newaxis], axis=1)\n    \n    # Weighted sum of top-k expert outputs\n    final_output = np.sum(top_k_expert_outputs * top_k_probs_normalized[:, :, np.newaxis], axis=1)\n    \n    # Round to the nearest 4th decimal and return as a list\n    return np.round(final_output, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    batch_size, input_dim, output_dim = 2, 3, 4\n    n_experts, top_k = 5, 2\n    \n    x = np.random.rand(batch_size, input_dim)\n    We = np.random.rand(n_experts, input_dim, output_dim)\n    Wg = np.random.rand(input_dim, n_experts)\n    \n    # Compute MoE output\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    G = num_groups\n    \n    # Check if the number of channels is divisible by the number of groups\n    assert C % G == 0, \"Number of channels must be divisible by the number of groups\"\n    \n    # Reshape the input tensor to group channels\n    X = X.reshape(B, G, -1)\n    \n    # Calculate the mean and variance for each group\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    \n    # Normalize the input tensor\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to the original shape\n    X_norm = X_norm.reshape(B, C, H, W)\n    \n    # Apply scale and shift\n    X_out = X_norm * gamma + beta\n    \n    # Round to the nearest 4th decimal\n    X_out = np.round(X_out, decimals=4)\n    \n    # Return the result as a list\n    return X_out.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input tensor with shape (2, 4, 2, 2)\n    X = np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]], [[13, 14], [15, 16]]],\n                  [[[17, 18], [19, 20]], [[21, 22], [23, 24]], [[25, 26], [27, 28]], [[29, 30], [31, 32]]]])\n    # Gamma and Beta with shape (4,)\n    gamma = np.array([1.0, 1.0, 1.0, 1.0])\n    beta = np.array([0.0, 0.0, 0.0, 0.0])\n    # Number of groups\n    num_groups = 2\n    \n    # Perform Group Normalization\n    result = group_normalization(X, gamma, beta, num_groups)\n    print(result)\n```"}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    \n    Parameters:\n    - start_x: Initial guess for x.\n    - learning_rate: Step size for gradient descent.\n    - tolerance: Threshold to determine convergence.\n    - max_iters: Maximum number of iterations to perform.\n    \n    Returns:\n    - The value of x that minimizes f(x), rounded to 4 decimal places.\n    \"\"\"\n    \n    x = start_x\n    for _ in range(max_iters):\n        # Calculate the derivative of f(x)\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Update x using the gradient descent method\n        x_new = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(x_new - x) < tolerance:\n            break\n        \n        x = x_new\n    \n    return round(x, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    start_x = np.random.uniform(-10, 10)\n    print(f\"Starting position: {start_x}\")\n    treasure_location = find_treasure(start_x)\n    print(f\"Treasure found at: {treasure_location}\")\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which is a normalization-free transformation.\n    This function takes an array of inputs and applies the DyT transformation with given parameters.\n    \n    Parameters:\n    x (np.ndarray): Input array.\n    alpha (float): Scaling factor for the input.\n    gamma (float): Scaling factor for the output.\n    beta (float): Bias term for the output.\n    \n    Returns:\n    list[float]: The transformed output as a list of floats, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the Dynamic Tanh transformation\n    transformed = gamma * np.tanh(alpha * x) + beta\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    result = np.round(transformed, 4).tolist()\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    x = np.array([0.5, -1.0, 2.0, -2.0])\n    alpha = 1.5\n    gamma = 0.8\n    beta = 0.2\n    \n    # Apply the dynamic tanh function\n    output = dynamic_tanh(x, alpha, gamma, beta)\n    print(output)\n```\n\nThis script defines the `dynamic_tanh` function as per the given task and demonstrates its usage with an example. The function applies a transformation inspired by the Tanh function, with additional scaling and bias parameters, to an input array. The output is rounded to the nearest 4th decimal and returned as a list of floats."}
