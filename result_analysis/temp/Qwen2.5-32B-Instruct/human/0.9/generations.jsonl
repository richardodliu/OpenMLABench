{"task_id": 1, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    Returns a list representing the resulting vector if the operation is valid.\n    Returns -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    # Check if the dot product is possible\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        result.append(sum(x * y for x, y in zip(row, b)))\n    \n    return result\n\n# Function to check the correctness of the matrix_dot_vector function\ndef check(a, b):\n    print(f\"Input Matrix: {a}\\nInput Vector: {b}\")\n    output = matrix_dot_vector(a, b)\n    if isinstance(output, list):\n        print(f\"Output Vector: {output}\\n\")\n    else:\n        print(f\"Incompatible dimensions. Output: {output}\\n\")\n\n# Correct usage\ncheck([[1, 2, 3], [4, 5, 6]], [7, 8, 9])\n# Incompatible dimensions\ncheck([[1, 2], [3, 4]], [1, 2, 3])\n```\n\nIn the provided code snippet, the `matrix_dot_vector` function calculates the dot product between a matrix `a` and a vector `b`, as per the task's requirements. Additionally, the `check` function is used to verify the correctness of the `matrix_dot_vector` function with two different sets of inputs, one that will compute a result and another that will result in an error due to incompatible dimensions."}
{"task_id": 2, "completion_id": 0, "completion": "Below is the self-contained Python script that solves the given task, which involves computing the transpose of a given matrix. The function `transpose_matrix` takes a list of lists (where each inner list represents a row of the matrix) and returns another list of lists representing the transposed matrix.\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transposes a given matrix.\n    \n    Parameters:\n    a (list of list of int|float): The matrix to be transposed.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    # Check if the input list is empty\n    if not a:\n        return []\n    \n    # Calculate the number of rows and columns in the original matrix\n    num_rows = len(a)\n    num_cols = len(a[0])\n    \n    # Initialize the transposed matrix with zeros\n    transposed_matrix = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    \n    # Fill the transposed matrix\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    \n    return transposed_matrix\n\n# Example usage\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\ntransposed = transpose_matrix(matrix)\nfor row in transposed:\n    print(row)\n```\n\nThis script defines the `transpose_matrix` function and provides an example of how to use it. The `transpose_matrix` function works by iterating over each element of the input matrix `a` and placing it in the appropriate position in the transposed matrix. The example matrix and its transposed output are printed to show the function works as expected."}
{"task_id": 3, "completion_id": 0, "completion": "Below is a self-contained Python script that defines the `reshape_matrix` function. This function attempts to reshape a given matrix into a new specified shape and returns the reshaped matrix as a Python list. If the reshaping is not possible, it returns an empty list.\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape a given matrix into a specified shape using numpy. If the reshaping is not possible, return an empty list.\n    \n    Parameters:\n    a (list[list[int|float]]): The matrix to reshape.\n    new_shape (tuple[int, int]): The new shape of the matrix.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix as a Python list, or an empty list if reshaping is not possible.\n    \"\"\"\n    # Convert the list to a numpy array\n    arr = np.array(a)\n    \n    # Check if the new shape is valid\n    if arr.size == new_shape[0] * new_shape[1]:\n        # Reshape the array\n        reshaped_arr = arr.reshape(new_shape)\n        # Convert the reshaped array back to a Python list and return\n        return reshaped_arr.tolist()\n    else:\n        # Return an empty list if reshaping is not possible\n        return []\n\n# Example usage:\nmat = [[1, 2], [3, 4]]\nnew_shape = (4, 1)\nreshaped_matrix = reshape_matrix(mat, new_shape)\nprint(reshaped_matrix)\n```\n\nThis script includes a basic example usage that reshapes a 2x2 matrix into a 4x1 matrix if possible. Adjust the `mat` and `new_shape` variables to test with different inputs."}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists representing the matrix with numerical values.\n    :param mode: A string, either 'row' or 'column', defining the axis for calculating the mean.\n    :return: A list of means.\n    \"\"\"\n    if not matrix or not matrix[0]:\n        raise ValueError(\"Matrix cannot be empty\")\n        \n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        return [sum(matrix[row][col] for row in range(len(matrix))) / len(matrix) for col in range(num_columns)]\n    else:\n        raise ValueError(\"Mode should either be 'row' or 'column'\")\n    \n# Example usage:\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\nprint(\"Row Means:\", calculate_matrix_mean(matrix, 'row'))\nprint(\"Column Means:\", calculate_matrix_mean(matrix, 'column'))\n```\n\nThis solution includes the `numpy` import statement as initially prescribed, however, the implemented function `calculate_matrix_mean` does not directly rely on `numpy`, using pure Python for calculating the mean. It checks for the empty matrix scenario and validates the mode parameter to ensure it is either 'row' or 'column'. The function computes the mean for each row or column as specified and handles the calculation through list comprehension for efficiency and readability."}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the given 2D matrix by the scalar value and returns the resulting matrix.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar value to multiply the matrix by.\n    :return: A 2D list representing the matrix after scalar multiplication.\n    \"\"\"\n    # Multiply each element by the scalar\n    return [[elem * scalar for elem in row] for row in matrix]\n\n# Example check function to verify the correctness of the scalar_multiply function.\ndef check_function():\n    test_matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar = 2\n    expected_result = [[2, 4, 6], [8, 10, 12], [14, 16, 18]]\n    assert scalar_multiply(test_matrix, scalar) == expected_result, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Run the check function to verify the solution works as expected\ncheck_function()\n```"}
{"task_id": 6, "completion_id": 0, "completion": "Here is a self-contained Python script that defines and uses the `calculate_eigenvalues` function to calculate the eigenvalues of a 2x2 matrix and returns them sorted from highest to lowest.\n\n```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix given as a list of lists.\n    :return: A list of eigenvalues sorted in descending order.\n    \"\"\"\n    # Convert the matrix to a numpy array for easier manipulation\n    matrix_np = np.array(matrix)\n    \n    # Calculate the eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    \n    # Sort the eigenvalues in descending order and return\n    return sorted(eigenvalues, reverse=True)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a 2x2 matrix\n    matrix_example = [[4, 2], [3, 1]]\n    \n    # Calculate and print the eigenvalues\n    eigenvalues = calculate_eigenvalues(matrix_example)\n    print(\"Eigenvalues (sorted from highest to lowest):\", eigenvalues)\n```\n\nBefore running the script, ensure the `numpy` package is installed in your Python environment. If not, you can install it using pip:\n\n```bash\npip install numpy\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    try:\n        # Convert input lists to numpy arrays\n        A_matrix = np.array(A)\n        T_matrix = np.array(T)\n        S_matrix = np.array(S)\n        \n        # Validate that T and S are square and invertible\n        if T_matrix.shape[0] != T_matrix.shape[1] or S_matrix.shape[0] != S_matrix.shape[1]:\n            return -1\n        \n        # Calculate the determinants to check for invertibility\n        T_det = np.linalg.det(T_matrix)\n        S_det = np.linalg.det(S_matrix)\n        \n        if T_det == 0 or S_det == 0:\n            return -1\n        \n        # Calculate the inverse of T (T^{-1})\n        T_inv = np.linalg.inv(T_matrix)\n        # Calculate the multiplication T^{-1}AS\n        transformed_matrix = np.round(T_inv @ A_matrix @ S_matrix, 4)\n        # Convert the numpy array back to a list\n        return transformed_matrix.tolist()\n    except Exception as e:\n        return -1\n\n# Example usage\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]  # This T is not invertible because its determinant is 0\nS = [[1, 0], [0, 1]]  # Identity matrix\n    \nresult = transform_matrix(A, T, S)\nprint(result)\n\n# Expected output: -1 because T is not invertible\n```\n\nThis Python script provides a function `transform_matrix` that takes three matrices as input and returns a transformed matrix according to the $T^{-1}AS$ operation, validating that the $T$ and $S$ matrices are invertible. If the matrices are not invertible or any other error occurs, the function returns `-1`. The example usage demonstrates checking with a non-invertible matrix, so it should output `-1`."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    - matrix: A list of two lists, each containing two float elements.\n    \n    Returns:\n    - The inverse matrix as a list of two lists, each containing two float elements.\n    - Returns None if the matrix is not invertible.\n    \"\"\"\n    # Extracting elements from the matrix\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Calculate determinant\n    det = a * d - b * c\n    \n    # Check if determinant is zero\n    if det == 0:\n        return None\n    \n    # Calculate inverse\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    \n    return inv_matrix\n\n# Check function to verify the correctness of the solution\ndef check_inverse():\n    # Test matrices and their expected inverses\n    test_matrices = [\n        ([[2.0, 4.0], [3.0, 1.0]], [[-0.09090909090909091, 0.36363636363636365],\n                                    [0.2727272727272727, -0.18181818181818182]]),\n        ([[1.0, 2.0], [2.0, 1.0]], [[-0.3333333333333333, 0.6666666666666666],\n                                    [0.6666666666666666, -0.3333333333333333]]),\n        ([[1.0, 0.0], [0.0, 1.0]], [[1.0, 0.0],\n                                    [0.0, 1.0]])\n    ]\n    \n    for matrix, expected in test_matrices:\n        result = inverse_2x2(matrix)\n        # Using numpy to compare matrices approximately\n        if not np.allclose(result, expected):\n            print(f\"Failed for matrix: {matrix}. Expected: {expected}, but got: {result}\")\n            return False\n    print(\"All tests passed!\")\n    return True\n\n# Running the check\ncheck_inverse()\n```\n\nIn this solution, we created a function called `inverse_2x2` to compute the inverse of a 2x2 matrix in Python. We also implemented a `check_inverse` function to validate the correctness of our solution with test matrices. We use numpy's `allclose` method to account for potential floating point arithmetic issues when comparing the calculated inverse with the expected result."}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int|float]], b: list[list[int|float]]) -> list[list[int|float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b. Returns -1 if the matrices cannot be multiplied.\n    \n    Parameters:\n    a (list of list of int/float): The first matrix.\n    b (list of list of int/float): The second matrix.\n    \n    Returns:\n    list of list of int/float or int: The product matrix or -1 if multiplication is not possible.\n    \"\"\"\n    # Check if multiplication is possible\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the product matrix with zeros\n    product = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                product[i][j] += a[i][k] * b[k][j]\n    \n    return product\n\n# Example usage\nif __name__ == \"__main__\":\n    a = [[1, 2], [3, 4]]\n    b = [[2, 0], [1, 2]]\n    print(matrixmul(a, b))  # Output: [[4, 4], [10, 8]]\n\n    a = [[1, 2, 3], [4, 5, 6]]\n    b = [[1, 2], [3, 4], [5, 6]]\n    print(matrixmul(a, b))  # Output: [[22, 28], [49, 64]]\n\n    a = [[1, 2], [3, 4]]\n    b = [[1, 2, 3], [4, 5, 6]]\n    print(matrixmul(a, b))  # Output: -1 because dimensions are incompatible for multiplication\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of lists of float): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of lists of float: A covariance matrix.\n    \"\"\"\n    # Convert the list of lists to a numpy array for easier manipulation\n    data = np.array(vectors)\n    \n    # Calculate the covariance matrix using numpy\n    cov_matrix = np.cov(data)\n    \n    # Convert back to a list of lists for the return value\n    return cov_matrix.tolist()\n\n# Test cases\ndef test_calculate_covariance_matrix():\n    vectors = [\n        [1, 2, 3, 4, 5],\n        [5, 4, 3, 2, 1],\n        [2, 4, 6, 8, 10]\n    ]\n    \n    expected_cov_matrix = [\n        [2.0, -2.0, 4.0],\n        [-2.0, 2.0, -4.0],\n        [4.0, -4.0, 8.0]\n    ]\n    \n    calculated_cov_matrix = calculate_covariance_matrix(vectors)\n    \n    # Check if the shape is correct and the values are close enough\n    assert len(calculated_cov_matrix) == len(expected_cov_matrix) and \\\n           all(len(row) == len(expected_cov_matrix[i]) for i, row in enumerate(calculated_cov_matrix)), \"Shape mismatch\"\n    \n    # Allow for some floating point tolerance\n    tolerance = 1e-5\n    for i in range(len(calculated_cov_matrix)):\n        for j in range(len(calculated_cov_matrix[i])):\n            assert abs(calculated_cov_matrix[i][j] - expected_cov_matrix[i][j]) < tolerance, f\"Covariance mismatch at position ({i},{j})\"\n    \n    print(\"All test cases passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\nIn the code above, a function `calculate_covariance_matrix` is defined to calculate the covariance matrix for a given set of vectors. The vectors are provided as a list of lists. After calculation, the result is returned as a list of lists. The `test_calculate_covariance_matrix` function checks the correctness of the implemented function using a predefined set of vectors and verifying the output against an expected result."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Uses the Jacobi method to solve a system of linear equations given by Ax = b.\n    \n    Parameters:\n    A (np.ndarray): The coefficient matrix of the system.\n    b (np.ndarray): The target vector.\n    n (int): The number of iterations to perform.\n    \n    Returns:\n    list: The approximate solution, rounded to four decimal places.\n    \"\"\"\n    # Check if the input matrix is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Input matrix must be a square matrix.\")\n    \n    # Initialize the solution vector\n    x = np.zeros_like(b)\n    \n    # Iterate n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        \n        # Update the solution vector\n        x = x_new\n        \n        # Round to four decimal places and cast to list\n        x = np.round(x, 4)\n    \n    return x.tolist()\n\n# Example Usage\nA = np.array([[4, -1, 0, 0],\n              [-1, 4, -1, 0],\n              [0, -1, 4, -1],\n              [0, 0, -1, 3]])\nb = np.array([24, -10, -10, -10])\n\nn = 25\nsolution = solve_jacobi(A, b, n)\nprint(\"Solution after 25 iterations:\", solution)\n```\n\nThis script implements the Jacobi method for solving a given system of linear equations. It ensures the matrix is square and initializes the solution vector. Each iteration updates the solution vector based on the Jacobi formula, rounding the values to four decimal places to comply with the output constraints."}
{"task_id": 12, "completion_id": 0, "completion": "The task involves calculating the singular values of a 2x2 matrix using a method that approximates SVD without directly calling the `numpy.linalg.svd` function. One common approach to approximate SVD involves iterative methods like the Jacobian method, but since the Jacobian method is more often used for finding roots and not quite directly for SVD, we will instead implement an iterative approach that could be considered as a simplified and direct method for 2x2 matrices.\n\nThe singular values of a matrix \\(A\\) are the square roots of the eigenvalues of \\(A^TA\\) (or \\(AA^T\\)), where \\(A^T\\) is the transpose of \\(A\\). For a 2x2 matrix, we can directly solve for the eigenvalues using the characteristic equation.\n\nHere is a self-contained Python script to calculate the singular values, based on the described method, rounded to the nearest 4th decimal:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Calculates the singular values of a 2x2 matrix A using an iterative method inspired by SVD,\n    without using numpy's direct svd function.\n    Returns the singular values rounded to 4 decimal places.\n    \"\"\"\n    # Compute the transpose of A\n    A_T = A.T\n    \n    # Calculate A^T * A\n    ATA = np.dot(A_T, A)\n    \n    # Get the trace and determinant of ATA\n    trace = np.trace(ATA)\n    det = np.linalg.det(ATA)\n    \n    # Calculate the eigenvalues (sigma1^2, sigma2^2)\n    # Use the quadratic formula to solve for the roots\n    eigenvalues = sorted((trace + np.sqrt(trace**2 - 4 * det)) / 2, (trace - np.sqrt(trace**2 - 4 * det)) / 2, reverse=True)\n    \n    # Calculate the singular values (square roots of the eigenvalues)\n    singular_values = [np.sqrt(val) for val in eigenvalues]\n    \n    # Round to 4 decimal places\n    rounded_singular_values = [round(val, 4) for val in singular_values]\n    \n    return tuple(rounded_singular_values)\n\n# Example usage:\nif __name__ == \"__main__\":\n    A = np.array([[1, 2], [3, 4]], dtype=float)\n    sv = svd_2x2_singular_values(A)\n    print(f\"The singular values of the matrix are: {sv}\")\n```\n\nThis script starts by calculating \\(A^TA\\), then finds the trace and determinant of \\(A^TA\\) to solve the characteristic equation for the eigenvalues. These eigenvalues are the squares of the singular values, so we take their square roots to find the singular values, rounding them to 4 decimal places as requested."}
{"task_id": 13, "completion_id": 0, "completion": "```python\nfrom typing import List, Union\n\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n    def determinant_3x3(submatrix: List[List[Union[int, float]]]) -> float:\n        \"\"\"\n        Helper function to calculate the determinant of a 3x3 matrix.\n        \n        :param submatrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the submatrix as a float.\n        \"\"\"\n        return (\n            submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) -\n            submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) +\n            submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n        )\n    \n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input matrix must be a 4x4 list of lists\")\n    \n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i+1:] for row in matrix[1:]]\n        det += ((-1) ** i) * matrix[0][i] * determinant_3x3(minor)\n    \n    return det\n\n# Test code to verify the solution\nif __name__ == \"__main__\":\n    test_matrix = [\n        [1, 3, 5, 9],\n        [1, 3, 1, 7],\n        [4, 3, 9, 7],\n        [5, 2, 0, 9]\n    ]\n    print(determinant_4x4(test_matrix))  # Expected output is the determinant of test_matrix\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    X is a list of lists, where each sublist represents one data point's features.\n    y is a list of floats representing the target values.\n    The function returns the coefficients of the linear regression model.\n    \"\"\"\n    # Convert X and y to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X to account for the intercept term\n    X_np = np.column_stack((np.ones(X_np.shape[0]), X_np))\n    \n    # Calculate the coefficients using the normal equation\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    \n    # Round the coefficients to the fourth decimal place\n    theta_rounded = np.round(theta, decimals=4)\n    \n    # Convert numpy array back to list and return\n    return theta_rounded.tolist()\n\n# Check function to test the correctness of the code\ndef check_linear_regression_normal_equation():\n    X = [[1, 2], [3, 4], [5, 6]]\n    y = [3, 5, 7]\n    expected_theta = [-0.0, 1.0, 1.0]  # Assuming a model of y = x1 + x2, based on data pattern.\n    # We should get coefficients close to [0, 1, 1] because the equation is roughly y = 0 + 1*x1 + 1*x2\n    result_theta = linear_regression_normal_equation(X, y)\n    assert np.allclose(result_theta, expected_theta), \"The calculated coefficients do not match the expected values.\"\n    print(\"Check passed, calculated theta is:\", result_theta)\n\n# Run the check function\ncheck_linear_regression_normal_equation()\n```"}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    - X: np.ndarray, the feature matrix with a column of ones for the intercept.\n    - y: np.ndarray, the target variable.\n    - alpha: float, learning rate.\n    - iterations: int, number of iterations.\n    \n    Returns:\n    - list, the coefficients of the linear regression model rounded to four decimal places.\n    \"\"\"\n    m, n = X.shape  # m is number of examples, n is number of features\n    theta = np.zeros(n)  # initialize parameters\n    \n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / m\n        theta -= alpha * gradient\n    \n    # Return coefficients rounded to 4 decimal places\n    return (np.around(theta, decimals=4)).tolist()\n\n# Example check function\ndef check_solution():\n    X = np.array([[1, 1], [1, 2], [1, 3]])\n    y = np.array([1, 2, 3])\n    alpha = 0.01\n    iterations = 1000\n    result = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(f\"Coefficients: {result}\")\n\n# Running the check function\ncheck_solution()\n```\n\nThis script defines a function that performs linear regression using gradient descent, adhering to the specified input and output requirements. It also includes a simple check to demonstrate how to use the function with a small dataset."}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array as input, and returns two 2D lists:\n    one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    # Standardization: (data - mean) / std_dev\n    std_scaled = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    # Min-Max Normalization: (data - min) / (max - min)\n    min_max_scaled = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    \n    # Ensure no division by zero in min_max_scaled\n    min_max_scaled = np.nan_to_num(min_max_scaled)\n    \n    # Round to the nearest 4th decimal and convert to list\n    std_scaled_list = np.round(std_scaled, 4).tolist()\n    min_max_scaled_list = np.round(min_max_scaled, 4).tolist()\n    \n    return std_scaled_list, min_max_scaled_list\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Sample data: 3 samples, 2 features (e.g., height and weight)\n    sample_data = np.array([[1.65, 65],\n                            [1.75, 75],\n                            [1.80, 80]])\n    \n    standardized_data, minmax_data = feature_scaling(sample_data)\n    \n    print(\"Standardized Data:\", standardized_data)\n    print(\"Min-Max Normalized Data:\", minmax_data)\n```\n\nThis script provides a function `feature_scaling` that accepts a 2D NumPy array and returns a tuple with two lists: one with data standardized (scaled by z-score) and one with data normalized using min-max scaling. Both the standardized and normalized datasets are rounded to the nearest 4th decimal place for consistency and precision control."}
{"task_id": 17, "completion_id": 0, "completion": "Based on the task provided, here is a Python script that implements the k-Means clustering algorithm:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert to numpy arrays for easier manipulation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        distances = np.sqrt(((points - centroids[:, np.newaxis])**2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        \n        # Move centroids to the average of the points that are closest to it\n        new_centroids = np.array([points[closest_centroid == k].mean(axis=0) for k in range(centroids.shape[0])])\n        # Check if centroids have converged\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    \n    # Round to 4th decimal and convert back to list of tuples\n    final_centroids = [(round(x, 4), round(y, 4)) for x, y in centroids]\n    return final_centroids\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(6.0, 2.0), (1.0, 2.0), (3.0, 4.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis script defines a `k_means_clustering` function that takes the specified inputs and outputs the final centroids of the clusters as a list of tuples. The script includes example usage that demonstrates how to call the function with a list of points and other parameters."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    :param X: Numpy array of input features.\n    :param y: Numpy array of target variable.\n    :param k: Number of folds for cross-validation.\n    :param shuffle: Whether to shuffle the data before splitting.\n    :param random_seed: Seed for random number generator for reproducibility.\n    :return: List of tuples, each containing train and test indices for k folds.\n    \"\"\"\n    # Set the random seed for reproducibility\n    if random_seed:\n        np.random.seed(random_seed)\n    \n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    # Create folds\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    \n    start_idx = 0\n    splits = []\n    \n    for i in range(k):\n        fold_end = start_idx + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start_idx:fold_end]\n        train_indices = np.concatenate((indices[:start_idx], indices[fold_end:]))\n        start_idx = fold_end\n        splits.append((train_indices, test_indices))\n    \n    return splits\n\n# Example usage\nX = np.random.rand(100, 5)  # 100 samples, 5 features\ny = np.random.randint(0, 2, size=(100,))  # Binary target\n\n# Call the function with our data\nk = 5\nrandom_seed = 42\nsplits = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=random_seed)\n\n# Print train and test indices for each fold\nfor i, (train_idx, test_idx) in enumerate(splits):\n    print(f\"Fold {i+1}:\")\n    print(f\"  Train indices: {train_idx}\")\n    print(f\"  Test indices: {test_idx}\")\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data and return the top k principal components.\n    \n    Parameters:\n    data (np.ndarray): The input dataset as a 2D numpy array where each row is a data sample and each column is a feature.\n    k (int): The number of principal components to return.\n    \n    Returns:\n    list[list[float]]: A list of lists representing the top k principal components.\n    \"\"\"\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n\n    # Compute eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n\n    # Sort eigenvalues and eigenvectors in decreasing order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Select the top k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Return the principal components as a list of lists, rounded to 4 decimal places\n    return [list(principal_components[:, i]) for i in range(k)]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random dataset for testing\n    np.random.seed(0)  # For reproducibility\n    data = np.random.rand(100, 5)\n    k = 3\n\n    # Perform PCA\n    principal_components = pca(data, k)\n\n    # Display the principal components\n    for i, pc in enumerate(principal_components):\n        print(f\"Principal Component {i+1}: {pc}\")\n```"}
{"task_id": 20, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the decision tree learning algorithm for classification based on the description you've provided. This script includes the calculation of entropy and information gain to decide the best attribute for splitting at each node of the tree.\n\n```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Construct a decision tree from given examples and attributes.\n    \n    :param examples: A list of dictionaries representing the examples, each with attribute-value pairs.\n    :param attributes: A list of attribute names to consider for splitting.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    # Base case: If all examples belong to the same class\n    unique_classes = set(example[target_attr] for example in examples)\n    if len(unique_classes) == 1:\n        return unique_classes.pop()\n    \n    # If there are no attributes left to split on, return the most common class\n    if not attributes:\n        return Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n    \n    # Choose the best attribute to split on based on information gain\n    best_attr = max(attributes, key=lambda attr: info_gain(examples, attr, target_attr))\n    \n    # Initialize the tree with the best attribute\n    tree = {best_attr: {}}\n    \n    # Recursively build the tree for each value of the best attribute\n    for value in set(example[best_attr] for example in examples):\n        subtree_examples = [example for example in examples if example[best_attr] == value]\n        if subtree_examples:\n            subattributes = [attr for attr in attributes if attr != best_attr]\n            tree[best_attr][value] = learn_decision_tree(subtree_examples, subattributes, target_attr)\n        else:\n            tree[best_attr][value] = Counter(example[target_attr] for example in examples).most_common(1)[0][0]\n    \n    return tree\n\ndef entropy(s: list[str]) -> float:\n    \"\"\"\n    Calculate the entropy of a list of values.\n    \n    :param s: A list of values.\n    :return: The entropy of the list.\n    \"\"\"\n    counter = Counter(s)\n    probs = [count / len(s) for count in counter.values()]\n    return -sum(p * math.log(p, 2) for p in probs if p > 0)\n\ndef info_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain from splitting by a given attribute.\n    \n    :param examples: A list of examples.\n    :param attribute: The attribute to split by.\n    :param target_attr: The target attribute to classify.\n    :return: The information gain of the split.\n    \"\"\"\n    # Entropy of the entire set\n    entropy_s = entropy([example[target_attr] for example in examples])\n    \n    # Entropy of the set, split by the attribute\n    entropy_splits = sum(\n        (len([example for example in examples if example[attribute] == value]) / len(examples)) *\n        entropy([example[target_attr] for example in examples if example[attribute] == value])\n        for value in set(example[attribute] for example in examples)\n    )\n    \n    return entropy_s - entropy_splits\n```\n\n### Explanation\n- **Entropy**: This is a measure of impurity. It is used to determine the homogeneity of a sample. The entropy of a set of samples is 0 if all samples belong to the same class, and it is highest when the classes are perfectly mixed.\n- **Information Gain**: This is the expected reduction in entropy caused by partitioning the examples according to a particular attribute. The attribute with the highest information gain is chosen to split the data.\n\n### Usage\nTo test the `learn_decision_tree` function, you can pass in a list of examples and the attributes you want to consider for splitting. The target attribute should also be specified. The function will return a decision tree that represents the learned classification rule based on the provided examples."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_linear(x, y):\n    return np.dot(x, y.T)\n\ndef kernel_rbf(x, y, sigma):\n    return np.exp(-np.sum((x[:, np.newaxis] - y)**2, axis=2) / (2 * sigma**2))\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0\n\n    for t in range(1, iterations + 1):\n        eta_t = 1 / (lambda_val * t)\n        \n        if kernel == 'linear':\n            k = kernel_linear(data, data)\n        elif kernel == 'rbf':\n            k = kernel_rbf(data, data, sigma)\n        else:\n            raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n        \n        pred = np.sum(alpha[:, np.newaxis] * labels[:, np.newaxis] * k, axis=0) + b\n        pred_sign = np.sign(pred)\n        incorrect = pred_sign != labels\n\n        if kernel == 'linear':\n            grad = (labels[incorrect] * (data[incorrect])[:, np.newaxis]).sum(axis=0)\n            step = eta_t * (grad / (t))\n        elif kernel == 'rbf':\n            step = eta_t * (k[:, incorrect] @ labels[incorrect]) / (t)\n\n        alpha += (step * labels) / (t)\n        alpha[alpha < 0] = 0\n        b += eta_t * labels[(pred * labels) < 1].mean()\n\n    alpha = np.round(alpha, 4).tolist()\n    b = np.round(b, 4)\n    return alpha, b\n\n# Example usage\ndata = np.random.rand(100, 5)  # 100 samples, 5 features\nlabels = 2 * (np.random.rand(100) > 0.5) - 1  # binary labels: -1 or 1\nalpha, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.1, iterations=1000)\nprint(\"Alpha coefficients:\", alpha)\nprint(\"Bias:\", bias)\n```\n\nThis script implements a deterministic version of the Pegasos kernel SVM algorithm as per the task specifications. It includes the option for a linear or RBF kernel, updates the alpha coefficients and bias for each iteration, and constraints on the output as requested."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example check function to test the sigmoid function with some data points\ndef check_sigmoid():\n    test_values = [-5, -1, 0, 1, 5]\n    expected_outputs = [0.0067, 0.2689, 0.5, 0.7311, 0.9933]\n    \n    for value, expected in zip(test_values, expected_outputs):\n        actual_output = sigmoid(value)\n        assert actual_output == expected, f\"Test failed for input {value}. Expected: {expected}, Got: {actual_output}\"\n        print(f\"Test passed for input {value}: Expected {expected}, Got {actual_output}\")\n\n# Running the check function to verify the correctness of the sigmoid function\ncheck_sigmoid()\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a given list of scores.\n    \n    The softmax function transforms each score in a list to a value between 0 and 1,\n    and normalizes the scores so that their sum is 1. This function is often used in\n    machine learning for converting a vector of arbitrary real values to a probability distribution.\n    \n    Args:\n    scores (list of float): The input scores.\n    \n    Returns:\n    list of float: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    # Apply the exponent to all elements in the list to ensure all elements are positive\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Sum of all exponentiated scores\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax values by dividing each exponentiated score by the sum of all scores\n    # and round each to four decimal places\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    \n    return softmax_values\n\n# Function to check the correctness of the softmax function using given data points\ndef check_softmax():\n    test_scores = [1.0, 2.0, 3.0]\n    expected_output = [0.0900, 0.2447, 0.6653]\n    assert softmax(test_scores) == expected_output, \"Test case failed!\"\n\n    print(\"All test cases passed!\")\n\n# Run check function to verify the correctness of the softmax function\ncheck_softmax()\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    # Convert list of list into numpy array for easier manipulation\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    \n    # Compute the net input\n    net_input = features_array.dot(weights_array) + bias\n    \n    # Apply activation function\n    predicted_probabilities = sigmoid(net_input)\n    \n    # Compute the mean squared error\n    mse = ((predicted_probabilities - np.array(labels)) ** 2).mean()\n    \n    # Return results, rounding to 4 decimal places\n    return predicted_probabilities.round(4).tolist(), round(mse, 4)\n\n# Example use case\nfeatures = [[0, 0], [0, 1], [1, 0], [1, 1]]\nlabels = [0, 1, 1, 0]\nweights = [0.7, 0.3]\nbias = 0.5\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(predictions)\nprint(mse)\n```\nIn this solution, we simulate a single neuron's function including the forward pass with binary classification in mind. The provided example demonstrates its usage for a simple XNOR gate with two inputs, showcasing its ability to interact with multidimensional input features."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef calculate_mse_loss(predictions: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Calculate the mean squared error loss.\"\"\"\n    return np.mean((predictions - labels) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_losses = []\n\n    for _ in range(epochs):\n        # Forward pass\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n\n        # Calculate and store mse_loss\n        mse_loss = calculate_mse_loss(predictions, labels)\n        mse_losses.append(round(mse_loss, 4))\n\n        # Backward pass\n        error = predictions - labels\n        derivative = sigmoid_derivative(linear_output)\n        gradient = error * derivative\n\n        # Compute gradients\n        weights_gradient = np.dot(features.T, gradient)\n        bias_gradient = np.sum(gradient)\n\n        # Update weights and bias\n        weights = weights - learning_rate * weights_gradient\n        bias = bias - learning_rate * bias_gradient\n\n    mse_losses = [round(loss, 4) for loss in mse_losses]\n    return weights.tolist(), round(bias, 4), mse_losses\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])  # XOR gate labels\ninitial_weights = np.array([0.1, 0.2])\ninitial_bias = 0.3\nlearning_rate = 0.5\nepochs = 1000\nweights, bias, mse_losses = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(f\"Final Weights: {weights}\")\nprint(f\"Final Bias: {bias}\")\nprint(f\"MSE Losses over epochs: {mse_losses[:5]} (only first 5 shown)\")\n```\n\nThis Python script simulates a single neuron with a sigmoid activation function and trains this neuron using backpropagation for gradient descent to minimize the mean squared error loss. The script includes example usage that trains the neuron on a toy dataset simulating an XOR gate, which is a non-linearly separable problem to emphasize the need for non-linear activation functions like sigmoid."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage\nx = Value(2.0)\ny = Value(3.0)\nz = x * y\nq = z.relu() + 4\n\nq.backward()\n\nprint(q, q.grad)\nprint(z, z.grad)\nprint(x, x.grad)\nprint(y, y.grad)\n```\n\nThis script defines a class `Value` which can be used for automatic differentiation. It supports basic operations such as addition, multiplication, and ReLU activation. The `backward()` method computes gradients through these operations. The example usage at the bottom demonstrates how to use the class for a simple computation graph and how to compute gradients for the variables."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    B (list of lists of int): The basis vectors of B as rows in a matrix.\n    C (list of lists of int): The basis vectors of C as rows in a matrix.\n    \n    Returns:\n    list of lists of float: The transformation matrix from B to C, rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    B_array = np.array(B)\n    C_array = np.array(C)\n    \n    # Calculate the inverse of B\n    B_inv = np.linalg.inv(B_array)\n    \n    # Calculate the transformation matrix from B to C: P = C * B_inv\n    P = np.dot(C_array, B_inv)\n    \n    # Round the matrix to the nearest 4th decimal place\n    P_rounded = np.round(P, decimals=4)\n    \n    # Convert numpy array back to list and return\n    return P_rounded.tolist()\n\n# Check function to test the transform_basis function with provided data points\ndef check():\n    B = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    C = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n    expected_result = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n    \n    assert transform_basis(B, C) == expected_result, \"Test failed!\"\n    print(\"Transformation matrix computed correctly for the identity and a swapped axis basis.\")\n\n# Run the check function\nif __name__ == \"__main__\":\n    check()\n```"}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) for a 2x2 matrix A.\n    Returns U, S, V such that A \u2248 U*S*V.T.\n    \n    :param A: A 2x2 numpy array\n    :return: Tuple of U, S, V where U and V are orthogonal matrices and S is a diagonal matrix of singular values.\n    \"\"\"\n    # Calculate A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Eigendecomposition of A^T A\n    eigenvalues, eigenvectors = np.linalg.eig(ATA)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Form the S diagonal matrix and U matrix\n    S = np.diag(np.sqrt(eigenvalues))\n    V = eigenvectors\n    U = np.zeros_like(V)\n    for i in range(S.shape[0]):\n        if S[i, i] > 1e-8:  # Avoid division by zero\n            U[:, i] = np.dot(A, V[:, i]) / S[i, i]\n    \n    # Round to 4 decimal places\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to list for output\n    return U.tolist(), S.tolist(), V.T.tolist()\n\n# Example usage\nA = np.array([[3, 2], [2, 6]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nPlease ensure that you run this script with an appropriate Python environment where `numpy` is installed to test the `svd_2x2` function. The provided example usage will compute the SVD for a 2x2 matrix and print the resulting U, S, and V matrices."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two numpy arrays, X and y, maintaining their correspondence.\n    \n    :param X: A 2D numpy array where rows are samples.\n    :param y: A 1D/2D numpy array where elements correspond to the labels of the samples in X.\n    :param seed: An optional seed for the random number generator to ensure reproducibility.\n    :return: Two numpy arrays (X_shuffled, y_shuffled) with the same data but in a random order.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    n_samples = X.shape[0]\n    if y.shape[0] != n_samples:\n        raise ValueError(\"X and y must have the same number of samples\")\n    \n    # Generate a random permutation of indices\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    # Ensure the output is a list by using tolist()\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\nX = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9],\n    [10, 11, 12]\n])\ny = np.array([0, 1, 0, 1])\n\n# Shuffle with a seed for reproducibility\nX_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X:\\n\", X_shuffled)\nprint(\"Shuffled y:\\n\", y_shuffled)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Iterates over numpy arrays X and y in batches of specified size.\n    \n    Parameters:\n    - X: numpy array of input data\n    - y: optional numpy array of target labels\n    - batch_size: size of each batch to yield\n    \n    Yields:\n    - If y is provided, yields (batch_X, batch_y) pairs as lists.\n    - If y is not provided, yields batch_X as lists.\n    \"\"\"\n    if y is not None:\n        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n        # If y is provided, reshape to match X for stacking\n        if y.ndim == 1:\n            y = y[:, np.newaxis]\n        data = np.hstack((X, y))\n        for i in range(0, data.shape[0], batch_size):\n            batch = data[i:i + batch_size]\n            batch_X, batch_y = np.hsplit(batch, [-1])  # Split based on the last column\n            yield batch_X.tolist(), batch_y.flatten().tolist()\n    else:\n        for i in range(0, X.shape[0], batch_size):\n            yield X[i:i + batch_size].tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Sample data creation\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100, 1)\n    \n    # Testing batch_iterator without y\n    print(\"Batching X only:\")\n    for batch in batch_iterator(X, batch_size=20):\n        print(np.array(batch))\n    \n    # Testing batch_iterator with y\n    print(\"\\nBatching X and y together:\")\n    for batch_X, batch_y in batch_iterator(X, y, batch_size=20):\n        print(np.array(batch_X), np.array(batch_y))\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset X based on whether the feature at index feature_i is greater than or equal to the threshold.\n\n    Parameters:\n    - X: numpy.ndarray, the dataset to be divided, consisting of samples and features.\n    - feature_i: int, the index of the feature to divide the dataset with.\n    - threshold: float, the threshold value to compare the feature against.\n\n    Returns:\n    - Two lists containing the subset of the dataset where the feature is greater than or equal to the threshold,\n      and the subset where the feature is less than the threshold, respectively.\n    \"\"\"\n    # Create a boolean mask where the feature is greater than or equal to the threshold.\n    above_threshold = X[:, feature_i] >= threshold\n    \n    # Use the mask to divide the dataset.\n    above = X[above_threshold].tolist()\n    below = X[~above_threshold].tolist()\n    \n    return above, below\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random dataset\n    np.random.seed(42)\n    X = np.random.rand(100, 5)  # 100 samples, 5 features each\n    \n    # Divide the dataset\n    feature_index = 3\n    feature_threshold = 0.5\n    above_threshold_samples, below_threshold_samples = divide_on_feature(X, feature_index, feature_threshold)\n    \n    # Output the sizes of the subsets to check the function\n    print(f\"Samples above threshold: {len(above_threshold_samples)}\")\n    print(f\"Samples below threshold: {len(below_threshold_samples)}\")\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features\n    with degree less than or equal to the specified degree. For example, if an input sample is\n    two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    - X: numpy array of shape [n_samples, n_features]\n    - degree: integer, the degree of the polynomial features\n    \n    Returns:\n    - numpy array containing the new features after expanding with polynomial features\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    n_samples, n_features = X.shape\n    new_features = []\n\n    # Generate all combination of features for each degree\n    for deg in range(degree + 1):\n        for indices in combinations_with_replacement(range(n_features), deg):\n            if indices:\n                new_feature = np.prod(X[:, indices], axis=1)\n                new_features.append(new_feature)\n    \n    # Insert the bias term (constant feature) at the beginning\n    new_features.insert(0, np.ones(n_samples))\n    return np.column_stack(new_features).tolist()\n\n# Example usage\nX = np.array([[1, 2], [3, 4]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nIn this script, the `polynomial_features` function computes all combinations of polynomial features up to the given degree for a given input array `X`. The combinations of features are computed using `combinations_with_replacement` from itertools, which allows for dealing with any dimensionality and any degree in a more generalized manner. The result is then provided as a list for compatibility with the output constraints."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of n_subsets random subsets of the dataset.\n    \n    Parameters:\n    X (numpy.ndarray): The feature matrix of the dataset.\n    y (numpy.ndarray): The labels of the dataset.\n    n_subsets (int): The number of subsets to generate.\n    replacements (bool): Whether sampling should be with or without replacements.\n    seed (int): The seed for the random number generator.\n    \n    Returns:\n    list: A list of tuples, each representing a random subset of the dataset. \n    \"\"\"\n    rng = np.random.default_rng(seed)\n    all_indices = np.arange(X.shape[0])\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = rng.choice(all_indices, size=all_indices.shape, replace=True)\n        else:\n            indices = rng.permutation(all_indices)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\n\n# Example Usage:\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ny = np.array([0, 1, 2])\nn_subsets = 3\nnp.random.seed(42)  # Setting seed for reproducibility in example usage\nresult = get_random_subsets(X, y, n_subsets, replacements=True)\nfor i, subset in enumerate(result):\n    print(f\"Subset {i+1}: {subset}\")\n```\nIn this script, the `get_random_subsets` function creates a list of random subsets based on the parameters given. It ensures that the subsets are returned as lists of tuples, meeting the output constraints requirement. The usage example demonstrates how to call this function with a sample dataset and parameters."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers to a 2D numpy array with one-hot encoding.\n    \n    Parameters:\n    x (numpy.array): A 1D numpy array of integers.\n    n_col (int, optional): The number of columns for the one-hot encoding. If None, it will be determined by the max value in x.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded matrix.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Example usage\nx = np.array([1, 2, 3, 2, 0])\nprint(to_categorical(x))\n```\n\nThis script provides a function `to_categorical` which performs one-hot encoding for a given 1D numpy array of integers. The example usage at the bottom demonstrates how to use the function with a sample input array `x`."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x: A 1D numpy array representing the diagonal elements.\n    \n    Returns:\n    - A 2D numpy array representing the diagonal matrix converted from the 1D array.\n    \"\"\"\n    # Ensures x is a numpy array\n    x = np.array(x)\n    \n    # Creates a 2D diagonal matrix from the 1D array\n    diag_matrix = np.diag(x)\n    \n    # Converts to a python list for output\n    return diag_matrix.tolist()\n\n# Example usage:\nx = np.array([1, 2, 3])\nprint(make_diagonal(x))\n```\n\nThis script defines a function `make_diagonal` that takes a 1D numpy array and converts it into a diagonal matrix, returning the result as a Python list, which meets the requirement of using numpy's `tolist()` method for the output."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (np.array): 1D array containing the true labels.\n    y_pred (np.array): 1D array containing the predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The lengths of y_true and y_pred must match.\")\n    \n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\n\n# Example Usage\ny_true = np.array([0, 1, 2, 0, 2, 1])\ny_pred = np.array([0, 2, 2, 0, 2, 1])\nprint(accuracy_score(y_true, y_pred))\n```\n\nThis Python script contains a function called `accuracy_score` which takes two parameters: `y_true` and `y_pred`. These parameters are expected to be 1D numpy arrays containing the true and predicted labels, respectively. The function calculates the accuracy score of the predictions by comparing the true labels with the predicted labels and computes the mean of the equality comparison result. The computed accuracy score is then rounded to the nearest 4th decimal place as required by the task."}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array, dataset.\n    Y (np.ndarray, optional): 2D numpy array, another dataset to calculate the correlation with X.\n    \n    Returns:\n    list: The correlation matrix as a list of lists, with values rounded to 4 decimal places.\n    \"\"\"\n    # Ensure the array is a numpy array for operation\n    X = np.array(X)\n    if Y is not None:\n        Y = np.array(Y)\n        # Centering and scaling the data\n        mean_X = np.mean(X, axis=0)\n        mean_Y = np.mean(Y, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        std_Y = np.std(Y, axis=0, ddof=1)\n        X = (X - mean_X) / std_X\n        Y = (Y - mean_Y) / std_Y\n        # Calculating the correlation matrix\n        correlation_matrix = np.dot(X.T, Y) / (X.shape[0] - 1)\n    else:\n        # Centering and scaling the data\n        mean_X = np.mean(X, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        X = (X - mean_X) / std_X\n        # Calculating the correlation matrix\n        correlation_matrix = np.dot(X.T, X) / (X.shape[0] - 1)\n    \n    # Return the correlation matrix as a list of lists with values rounded to 4 decimal places\n    return np.round(correlation_matrix, 4).tolist()\n\n# Data Points for checking\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n\n# Function call with X only\nprint(calculate_correlation_matrix(X))\n\n# Function call with both X and Y\nprint(calculate_correlation_matrix(X, Y))\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the AdaBoost classifier fit method.\n    \n    Parameters:\n    X (numpy.ndarray): The training data, shape (n_samples, n_features).\n    y (numpy.ndarray): The target labels, shape (n_samples,).\n    n_clf (int): The number of classifiers to use.\n    \n    Returns:\n    list: A list of dictionaries, each containing the classifier parameters.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Initialize weights\n    w = np.full(n_samples, (1 / n_samples))\n    clfs = []\n\n    for _ in range(n_clf):\n        best = {'clf': None, 'error': 1, 'alpha': -1}\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.ones(n_samples)\n                pred[X[:, feature] < threshold] = -1\n                error = np.sum(w[y != pred])\n                \n                if error < best['error']:\n                    best['clf'] = {'feature': feature, 'threshold': threshold}\n                    best['error'] = error\n\n        clf = best['clf']\n        pred = np.ones(n_samples)\n        pred[X[:, clf['feature']] < clf['threshold']] = -1\n        \n        # Calculate alpha\n        best['alpha'] = 0.5 * np.log((1 - best['error']) / best['error'])\n        # Update weights\n        w *= np.exp(-best['alpha'] * y * pred)\n        w /= np.sum(w)\n        \n        clfs.append({\n            'feature': clf['feature'],\n            'threshold': clf['threshold'],\n            'alpha': np.round(best['alpha'], 4)\n        })\n    \n    return clfs\n\n# Test\nX_test = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\ny_test = np.array([-1, -1, -1, 1, 1, 1])\nn_clf = 2\nprint(adaboost_fit(X_test, y_test, n_clf))\n```\n\nThis script implements the AdaBoost classifier fitting function as requested. Note that the provided test data `X_test` and `y_test` is for demonstration purposes and might not represent a real-world dataset. The output will be a list of classifiers' parameters, with each classifier being represented as a dictionary containing the feature used for the split, the threshold for that feature, and the weight or alpha value assigned to the classifier for voting in the final prediction."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a given list of scores.\n    \n    Parameters:\n    scores (list): A list of numerical scores.\n    \n    Returns:\n    list: The log-softmax of the scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores_np = np.array(scores)\n    # Numerically stable computation of log softmax\n    log_softmax_values = scores_np - np.max(scores_np)\n    log_softmax_values = log_softmax_values - np.log(np.sum(np.exp(log_softmax_values)))\n    # Rounding to the nearest 4th decimal\n    log_softmax_values = np.around(log_softmax_values, decimals=4)\n    return log_softmax_values.tolist()\n\n# Test cases to verify the correctness\ndef check_log_softmax():\n    test_scores = [2.0, 1.0, 0.1]\n    result = log_softmax(test_scores)\n    expected = [-1.4176, -2.4176, -3.4176]\n    assert result == expected, f\"Expected {expected}, but got {result}\"\n    print(\"Test passed!\")\n\ncheck_log_softmax()\n```\nThis script includes a function `log_softmax` that takes a list of numerical scores and returns the log-softmax of those scores, rounded to the nearest 4th decimal. The function employs a numerically stable approach for computing the log softmax values. Additionally, a test case is provided to ensure the correctness of the implementation."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.input_shape = input_shape\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        # weights initialization\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # accum_grad is the gradient of the loss function w.r.t. the output of this layer\n        W = self.W\n\n        if self.trainable:\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            grad_W = self.layer_input.T.dot(accum_grad)\n            self.W = self.optimizer_W.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage:\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the dataset\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Define an optimizer (can define any optimizer here as an example)\nfrom sklearn.linear_model import SGDClassifier\n\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n    \n    def update(self, param, grad):\n        return param - self.learning_rate * grad\n\n# Initialize Dense layer with 10 units and an optimizer\ndense_layer = Dense(n_units=10, input_shape=(X_train.shape[1],))\ndense_layer.initialize(optimizer=SGD(learning_rate=0.01))\n\n# Forward and Backward pass example for a single batch of input data\noutput = dense_layer.forward_pass(X_train[0:5], training=True)\naccum_grad = np.ones_like(output)  # dummy accum_grad for demonstration\nback_grad = dense_layer.backward_pass(accum_grad)\n\nprint(f\"Output of Dense Layer:\\n{np.round(output, decimals=4).tolist()}\")\nprint(f\"Backward Pass Gradient:\\n{np.round(back_grad, decimals=4).tolist()}\")\n```\n\nThis code snippet defines a base `Layer` class, a `Dense` subclass which implements a dense neural network layer, and an optimizer class for demonstration. Methods are implemented as asked: `initialize`, `parameters`, `forward_pass`, `backward_pass`, `output_shape`. It is accompanied by an example usage that includes data preparation, a dummy optimizer, and the application of the dense layer to perform a forward and backward pass."}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution between an input matrix and a convolutional kernel.\n    \n    :param input_matrix: 2D numpy array representing the input matrix.\n    :param kernel: 2D numpy array representing the convolutional kernel.\n    :param padding: Integer value for padding the input matrix.\n    :param stride: Integer value for stride size during convolution.\n    :return: 2D numpy array representing the resultant feature map.\n    \"\"\"\n    # Pad the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Extract dimensions\n    in_height, in_width = input_matrix.shape\n    kernel_height, kernel_width = kernel.shape\n    out_height = (in_height - kernel_height + 2 * padding) // stride + 1\n    out_width = (in_width - kernel_width + 2 * padding) // stride + 1\n    \n    # Initialize output matrix\n    output = np.zeros((out_height, out_width))\n    \n    # Perform convolution\n    for y in range(0, out_height):\n        for x in range(0, out_width):\n            output[y, x] = np.sum(padded_input[y*stride:y*stride+kernel_height, x*stride:x*stride+kernel_width] * kernel)\n    \n    # Ensure all results are rounded to the nearest 4th decimal\n    output = np.round(output, 4)\n    \n    # Return the output as a list\n    return output.tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1\nprint(simple_conv2d(input_matrix, kernel, padding, stride))\n```\n\nThis script defines a function `simple_conv2d` that performs a simple 2D convolution using the provided parameters and constraints."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A single input value.\n    \n    Returns:\n    float: The input value if it's greater than 0, otherwise 0.\n    \"\"\"\n    return max(0.0, z)\n\n# Test the relu function with given data points\ndef check_function():\n    test_cases = [(-1.0, 0.0), (0.0, 0.0), (2.5, 2.5), (-15.7, 0.0)]\n    for input_value, expected_output in test_cases:\n        assert relu(input_value) == expected_output, f\"Failed for input {input_value}\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the correctness of the relu function\ncheck_function()\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Function to calculate Ridge Regression loss.\n    \n    Parameters:\n    X (np.ndarray): 2D array, the feature matrix.\n    w (np.ndarray): 1D array, the coefficients.\n    y_true (np.ndarray): 1D array, the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The calculated Ridge loss.\n    \"\"\"\n    predictions = X @ w  # Compute predictions using dot product\n    mse = np.mean((predictions - y_true) ** 2)  # Compute Mean Squared Error\n    regularization = 0.5 * alpha * w.T @ w  # Compute regularization term\n    loss = mse + regularization  # Calculate total Ridge loss\n    return round(loss, 4)  # Round to the nearest 4th decimal\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, -0.2])\n    y_true = np.array([2, 4, 6])\n    alpha = 0.1  # Regularization parameter\n    \n    # Calculate Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value to the Leaky ReLU function.\n    - alpha (float): The slope of the function for z < 0. Default is 0.01.\n    \n    Returns:\n    - float: The output of the Leaky ReLU function, which is z if z >= 0, and alpha*z if z < 0.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Test cases\nprint(leaky_relu(0.5))          # Expected output: 0.5\nprint(leaky_relu(-0.5))         # Expected output: -0.005\nprint(leaky_relu(-1.0, 0.1))    # Expected output: -0.1\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Args:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\n\n# Example usage\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nprint(kernel_function(x1, x2))  # Expected output: 32\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision, given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: Precision value.\n    \n    Raises:\n    ValueError: If the input arrays are not of the same length.\n    \"\"\"\n    \n    if len(y_true) != len(y_pred):\n        raise ValueError(\"y_true and y_pred must be the same length.\")\n    \n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    \n    if predicted_positives == 0:\n        return 1.0  # If there are no positive predictions, precision is 1.\n    else:\n        return true_positives / predicted_positives\n\n# Function to check the solution with example data\ndef check_precision():\n    # Example 1: Basic case with some TP, FP, TN, FN\n    y_true_example_1 = np.array([0, 1, 1, 1, 0, 1])\n    y_pred_example_1 = np.array([0, 1, 0, 1, 0, 1])\n    print(f\"Precision Example 1: {precision(y_true_example_1, y_pred_example_1)} (Expected: 0.75)\")\n\n    # Example 2: Perfect prediction\n    y_true_example_2 = np.array([0, 1, 0, 1, 0, 1])\n    y_pred_example_2 = np.array([0, 1, 0, 1, 0, 1])\n    print(f\"Precision Example 2: {precision(y_true_example_2, y_pred_example_2)} (Expected: 1.0)\")\n\n    # Example 3: No positive predictions\n    y_true_example_3 = np.array([0, 0, 0, 0])\n    y_pred_example_3 = np.array([0, 0, 0, 0])\n    print(f\"Precision Example 3: {precision(y_true_example_3, y_pred_example_3)} (Expected: 1.0)\")\n\n    # Example 4: No true positives, but some predicted positives\n    y_true_example_4 = np.array([0, 0, 0, 0])\n    y_pred_example_4 = np.array([1, 0, 1, 0])\n    print(f\"Precision Example 4: {precision(y_true_example_4, y_pred_example_4)} (Expected: 0.0)\")\n\n# Checking the solution\ncheck_precision()\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements a flexible gradient descent function that can perform SGD, Batch GD, and Mini-batch GD.\n    \n    Parameters:\n    X: Feature matrix with shape (n_samples, n_features) including a column of ones for the intercept.\n    y: Target vector with shape (n_samples,).\n    weights: Initial weights array of shape (n_features,).\n    learning_rate: The step size at each iteration while moving toward a minimum of a loss function.\n    n_iterations: Number of iterations of the algorithm.\n    batch_size: Number of samples to work through before updating the internal model parameters.\n    method: A string indicating which variant of gradient descent to use: 'batch', 'stochastic', or 'mini-batch'.\n    \n    Returns:\n    weights: The final weights array after completing the specified number of iterations.\n    \"\"\"\n    \n    n_samples, n_features = X.shape\n    \n    if method == 'batch':\n        batch_size = n_samples\n        \n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index+1]\n                yi = y[random_index:random_index+1]\n                gradient = 2 * xi.T.dot(xi.dot(weights) - yi)\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            for i in range(0, n_samples, batch_size):\n                X_i = X_shuffled[i:i+batch_size]\n                y_i = y_shuffled[i:i+batch_size]\n                gradient = 2 * X_i.T.dot(X_i.dot(weights) - y_i)\n                weights = weights - learning_rate * gradient\n        else:  # 'batch'\n            predictions = X.dot(weights)\n            error = predictions - y\n            gradient = 2 * X.T.dot(error) / n_samples\n            weights = weights - learning_rate * gradient\n            \n    return np.round(weights, 4).tolist()\n\n# Example usage of the function\nX = np.c_[np.ones((100, 1)), np.random.rand(100, 2)]  # Including intercept\ny = np.dot(X, np.array([1, 2, 3])) + np.random.randn(100)\nweights = np.zeros(3)\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 32  # Can be adjusted\n\n# Batch GD\nweights_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\nprint(\"Weights using Batch GD:\", weights_batch)\n\n# Stochastic GD\nweights_stochastic = gradient_descent(X, y, weights, learning_rate, n_iterations, method='stochastic')\nprint(\"Weights using Stochastic GD:\", weights_stochastic)\n\n# Mini-batch GD\nweights_mini_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method='mini-batch')\nprint(\"Weights using Mini-batch GD:\", weights_mini_batch)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the matrix into a numpy array\n    matrix = np.array(matrix)\n    num_rows, num_cols = matrix.shape\n    r = 0  # row index\n    c = 0  # column index\n    \n    while r < num_rows and c < num_cols:\n        # Find the row with the maximum value in the current column\n        swap_row = r\n        for i in range(r + 1, num_rows):\n            if abs(matrix[i, c]) > abs(matrix[swap_row, c]):\n                swap_row = i\n        \n        # If there's a value in the current column, start elimination\n        if matrix[swap_row, c] != 0:\n            # Swap the current row with the swap row\n            matrix[[r, swap_row]] = matrix[[swap_row, r]]\n            \n            # Divide to make pivot = 1\n            pivot = matrix[r, c]\n            matrix[r, :] /= pivot\n            \n            # Eliminate the current column for all other rows\n            for i in range(num_rows):\n                if i != r:\n                    factor = matrix[i, c]\n                    matrix[i, :] -= factor * matrix[r, :]\n            \n            # Move to the next row and column\n            r += 1\n            c += 1\n        else:\n            # Move to the next column if the current column doesn't have a pivot\n            c += 1\n    \n    return matrix.tolist()\n\n# Example usage\nmatrix = [[2, 1, -1, 3], [-3, -1, 2, -1], [-2, 1, 2, -1]]\nrref_matrix = rref(matrix)\nprint(rref_matrix)\n```\n\nThis function takes a matrix as input and returns its Reduced Row Echelon Form (RREF). It deals with the mentioned constraints by finding the row with the largest value in each column to minimize numerical inaccuracies, ensuring pivot entries are 1, and making all other entries in the same column 0. It also allows for zero pivot values when the corresponding row is an all-zero row."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0)\n    t = 0\n    \n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        \n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        mt = m / (1 - beta1 ** t)\n        vt = v / (1 - beta2 ** t)\n        \n        x = x - learning_rate * mt / (np.sqrt(vt) + epsilon)\n    \n    return x.round(4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    def f(x):\n        return x[0] ** 2 + 3 * x[1] ** 2\n    \n    def grad_f(x):\n        return np.array([2 * x[0], 6 * x[1]])\n    \n    x0 = [2.5, 2.5]\n    optimized_params = adam_optimizer(f, grad_f, x0)\n    print(optimized_params)\n```\n\nThis code snippet defines and implements the Adam optimization algorithm. The example usage demonstrates how to optimize a simple function using the `adam_optimizer` function."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implementation of Lasso Regression using Gradient Descent.\n\n    Parameters:\n    -----------\n    X : np.array\n        Input features.\n    y : np.array\n        Target values.\n    alpha : float, optional\n        Regularization parameter. The default is 0.1.\n    learning_rate : float, optional\n        Learning rate for gradient descent. The default is 0.01.\n    max_iter : int, optional\n        Maximum number of iterations for gradient descent. The default is 1000.\n    tol : float, optional\n        Tolerance for stopping criteria. The default is 1e-4.\n\n    Returns:\n    --------\n    tuple\n        A tuple containing the weights and bias.\n    \"\"\"\n    m, n = X.shape\n    w = np.zeros(n)\n    b = 0\n    prev_cost = np.inf\n    \n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        loss = y_pred - y\n        gradient_w = (1/m) * np.dot(X.T, loss) + alpha * np.sign(w)\n        gradient_b = (1/m) * np.sum(loss)\n        \n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        \n        cost = (1/(2*m)) * np.sum(loss**2) + (alpha/m) * np.sum(np.abs(w))\n        \n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    return np.round(w, 4).tolist(), np.round(b, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generating some example data\n    np.random.seed(42)\n    X = np.random.rand(100, 5)\n    y = np.random.rand(100)\n    \n    # Running the Lasso Regression\n    weights, bias = l1_regularization_gradient_descent(X, y)\n    print(\"Weights:\", weights)\n    print(\"Bias:\", bias)\n```\nThis script defines a function `l1_regularization_gradient_descent`, which implements Lasso Regression using Gradient Descent, and includes an example usage that demonstrates how to generate some synthetic data and run the function to obtain the weights and bias."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    - source (str): The source string.\n    - target (str): The target string.\n    \n    Returns:\n    - int: The minimum number of edits required to convert the source into the target string.\n    \"\"\"\n    \n    # Initialize the matrix with dimensions (len(source)+1) x (len(target)+1)\n    matrix = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n   \n    # Fill the first column\n    for i in range(len(source) + 1):\n        matrix[i][0] = i\n    \n    # Fill the first row\n    for j in range(len(target) + 1):\n        matrix[0][j] = j\n    \n    # Fill the matrix with the minimum cost of edit operations\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            if source[i-1] == target[j-1]:\n                cost = 0\n            else:\n                cost = 1\n            \n            # Calculate the cost of each possible operation\n            substitution_cost = matrix[i-1][j-1] + cost\n            deletion_cost = matrix[i-1][j] + 1\n            insertion_cost = matrix[i][j-1] + 1\n            \n            # Transposition cost only if the characters preceding the mismatch are the same and at least one is two characters away from the start of the string\n            transposition_cost = float('inf')\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                transposition_cost = matrix[i-2][j-2] + cost\n            \n            # Choose the minimum cost of operation\n            matrix[i][j] = min(substitution_cost, deletion_cost, insertion_cost, transposition_cost)\n    \n    # The minimum edits required is in the bottom-right corner of the matrix\n    return matrix[len(source)][len(target)]\n\n# Example use cases\nprint(OSA(\"caper\", \"acer\"))  # Output: 2\nprint(OSA(\"intention\", \"execution\"))  # Output: 5\n```"}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall of a binary classification model.\n    \n    Parameters:\n    y_true (list of int): List of true binary labels.\n    y_pred (list of int): List of predicted binary labels.\n    \n    Returns:\n    float: The recall value rounded to 3 decimal places. Returns 0.0 if TP + FN == 0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # True Positive (TP): True labels are 1 and predicted labels are also 1\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n\n    # False Negative (FN): True labels are 1 but predicted labels are 0\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # If there are no positive cases in the true labels, recall is 0\n    if TP + FN == 0:\n        return 0.0\n    \n    recall_val = TP / (TP + FN)\n    return round(recall_val, 3)\n\n# Example use cases\ny_true = [0, 1, 1, 1, 0, 1]\ny_pred = [0, 1, 0, 1, 0, 1]\nprint(recall(y_true, y_pred))  # Output: 0.75\n\ny_true = [0, 1, 0, 1, 0, 1]\ny_pred = [0, 0, 1, 1, 0, 1]\nprint(recall(y_true, y_pred))  # Output: 0.667\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Args:\n    X: Input sequence represented as 2D numpy array of shape (sequence_length, input_dim).\n    W_q: Query matrix of shape (input_dim, dim_k).\n    W_k: Key matrix of shape (input_dim, dim_k).\n    W_v: Value matrix of shape (input_dim, dim_v).\n    \n    Returns:\n    The self-attention output as a list after reshaping into (sequence_length, dim_v) and rounding to 4 decimal places.\n    \"\"\"\n    # Compute the query, key, and value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the attention scores\n    attention_scores = np.dot(Q, K.T)\n    # Scale the scores to improve stability\n    dim_k = W_k.shape[1]\n    scaled_scores = attention_scores / np.sqrt(dim_k)\n    # Apply softmax to the scaled scores to get the attention weights\n    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n    # Calculate the self-attention output\n    self_attn_output = np.dot(attention_weights, V)\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(self_attn_output, 4).tolist()\n\n# Test the function with some sample inputs\nX = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0]])\nW_v = np.array([[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1]])\n\n# Call the function with the sample inputs\noutput = self_attention(X, W_q, W_k, W_v)\nprint(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass of an RNN cell with tanh activation.\n\n    Parameters:\n    input_sequence (list of list of float): List of input vectors for each time step.\n    initial_hidden_state (list of float): The initial hidden state.\n    Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    b (list of float): Bias vector for the hidden state updates.\n    \n    Returns:\n    list of float: The final hidden state after processing the entire sequence, rounded to 4 decimals.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence, dtype=np.float32)\n    initial_hidden_state = np.array(initial_hidden_state, dtype=np.float32)\n    Wx = np.array(Wx, dtype=np.float32)\n    Wh = np.array(Wh, dtype=np.float32)\n    b = np.array(b, dtype=np.float32)\n    \n    # Initialize hidden state\n    h = initial_hidden_state\n    \n    # Process each input in the sequence\n    for x_t in input_sequence:\n        h = np.tanh(Wx @ x_t + Wh @ h + b)  # Hidden state update with tanh activation\n    \n    # Return final hidden state, rounded to 4 decimals\n    return np.round(h, 4).tolist()\n\n# Example usage\ninput_sequence = [[1, 0, 2], [0, 1, 3], [2, 1, 1]]\ninitial_hidden_state = [0, 0, 0]\nWx = [[0.1, 0.2, 0.3], [0.1, 0.2, 0.3], [0.1, 0.2, 0.3]]\nWh = [[0.4, 0.5, 0.6], [0.4, 0.5, 0.6], [0.4, 0.5, 0.6]]\nb = [0.1, 0.1, 0.1]\n\nprint(rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b))\n```\n\nThis code snippet is designed as per the request and implements a simple RNN cell based on the task description. The function processes a sequence of input vectors and updates the hidden state at each step using tanh activation. The final hidden state, after processing the entire sequence, is returned and rounded to 4 decimal points as required."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of 2D points by a specified distance in the x and y directions.\n    \n    Parameters:\n    points (list of list): A list of [x, y] coordinates.\n    tx (float): The translation distance in the x direction.\n    ty (float): The translation distance in the y direction.\n    \n    Returns:\n    list: The list of translated [x, y] coordinates.\n    \"\"\"\n    # Convert the list of points to a numpy array for easier manipulation\n    points_array = np.array(points)\n    # Create a translation matrix using numpy\n    # The matrix is a 2x2 identity, followed by the translation vector [tx, ty]\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n    # Add a row of ones to the points array for matrix multiplication\n    ones = np.ones((points_array.shape[0], 1))\n    points_array = np.hstack((points_array, ones))\n    # Multiply points with translation matrix\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    # Return the new points as a list, excluding the last column of ones\n    return translated_points[:, :2].tolist()\n\n# Example check\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(\"Original Points:\", points)\nprint(\"Translated Points:\", translated_points)\n```\n\nIn this solution the `translate_object` function utilizes a 2D translation matrix in homogeneous coordinates to move a collection of points in the x and y directions. This employs the usage of numpy for the mathematical operations and converting between different data structures."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n    \n    Returns:\n    float: KL divergence between the two distributions.\n    \"\"\"\n    # KL Divergence Formula for Normal Distributions\n    term1 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    term2 = np.log(sigma_q / sigma_p)\n    kl_div = term1 + term2 - 0.5\n    return kl_div\n\n# Example usage\nmu_p = 1.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 1.0\nresult = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(result)\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method for solving a system of linear equations.\n    \n    Parameters:\n    - A: np.array: A square matrix of coefficients.\n    - b: np.array: The right-hand side vector.\n    - n: int: Number of iterations.\n    - x_ini: np.array, optional: Initial guess for the solution vector.\n    \n    Returns:\n    - x: list: The approximated solution vector after n iterations.\n    \"\"\"\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square\")\n    \n    # Initialize the solution vector x\n    if x_ini is None:\n        x = np.zeros_like(b, dtype='float')\n    else:\n        x = x_ini.astype(float)\n    \n    # Number of equations\n    n_eq = A.shape[0]\n    \n    # Gauss-Seidel iteration\n    for it_count in range(n):\n        for i in range(n_eq):\n            # Initialize s to 0 for each row i\n            s = 0\n            # Sum Ax for solved values from the current iteration\n            # and x for values from the previous iteration\n            for j in range(n_eq):\n                if j != i:\n                    s += A[i, j] * x[j]\n            # Update the solution\n            x[i] = (b[i] - s) / A[i, i]\n    \n    # Round answers to 4 decimal places and return as list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[10., -1., 2., 0.],\n              [-1., 11., -1., 3.],\n              [2., -1., 10., -1.],\n              [0.0, 3., -1., 8.]])\nb =  np.array([6., 25., -11., 15.])\nn = 20 # Number of iterations\n\n# Call the function\nx = gauss_seidel(A, b, n)\nprint(\"Approximate solution:\", x)\n```\nThe provided script defines the `gauss_seidel` function that implements the iterative Gauss-Seidel method for solving a system of linear equations, specified by the square matrix `A` and the right-hand side vector `b`. The function accepts an optional initial guess `x_ini` and iterates `n` times to find the approximate solution vector `x`. The solution vector is returned as a list, with each element rounded to four decimal places."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting\n    \n    Parameters:\n    A : np.ndarray\n        Coefficient matrix of the system\n    b : np.ndarray\n        Constant terms of the system\n        \n    Returns:\n    x : list\n        Solution vector x, rounded to 4 decimal places\n    \"\"\"\n    # Combine A and b into an augmented matrix\n    n = len(A)\n    aug_matrix = np.hstack((A, b.reshape(-1, 1)))\n    \n    # Function to perform forward elimination\n    def forward_elimination():\n        for i in range(n):\n            # Partial pivoting\n            max_row = i + np.argmax(np.abs(aug_matrix[i:, i]))\n            aug_matrix[[i, max_row]] = aug_matrix[[max_row, i]]\n            \n            for j in range(i+1, n):\n                factor = aug_matrix[j, i] / aug_matrix[i, i]\n                aug_matrix[j, i:] -= factor * aug_matrix[i, i:]\n    \n    # Perform forward elimination\n    forward_elimination()\n    \n    # Backward substitution\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n-1, -1, -1):\n        x[i] = (aug_matrix[i, -1] - np.dot(aug_matrix[i, :-1], x)) / aug_matrix[i, i]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n\n# Example\nA = np.array([[2, -1, 0], [2, 1, 4], [1, 1, -1]])\nb = np.array([-2, 6, 0])\nprint(gaussian_elimination(A, b))  # Expected output: [1.0, 4.0, -1.0]\n```"}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x):\n    return np.tanh(x)\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n\n        for t in range(x.shape[1]):  # Assuming x is of shape [input_size, time_steps]\n            xt = x[:, t].reshape(-1, 1)\n            combined = np.vstack((hidden_state, xt))\n\n            ft = sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = sigmoid(np.dot(self.Wi, combined) + self.bi)\n            cct = tanh(np.dot(self.Wc, combined) + self.bc)\n            ot = sigmoid(np.dot(self.Wo, combined) + self.bo)\n\n            cell_state = ft * cell_state + it * cct\n            hidden_state = ot * tanh(cell_state)\n\n            hidden_states.append(hidden_state.round(4).tolist())\n\n        final_hidden_state = np.array(hidden_states[-1], dtype=np.float32).round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage\nlstm = LSTM(input_size=3, hidden_size=2)\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\ninitial_hidden_state = np.zeros((2, 1), dtype=np.float32)\ninitial_cell_state = np.zeros((2, 1), dtype=np.float32)\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\nprint(\"Hidden States:\", hidden_states)\nprint(\"Final Hidden State:\", final_hidden_state)\nprint(\"Final Cell State:\", final_cell_state)\n```\n\nThis code defines an LSTM class with randomized weights and biases for the forget gate, input gate, cell state, and output gate. The forward method processes the input sequence `x` and returns the hidden states for each time step as well as the final hidden state and cell state, adhering to the output format constraints."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus cannot be empty.\")\n    \n    num_docs = len(corpus)\n    query = set(query)  # Remove duplicates in query for efficiency\n    \n    # Calculate Term Frequency (TF)\n    tf_scores = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        if doc_word_count == 0:\n            tf_scores.append({})\n        else:\n            tf = {}\n            for word in doc:\n                if word in query:\n                    tf[word] = tf.get(word, 0) + 1 / doc_word_count\n            tf_scores.append(tf)\n\n    # Calculate Inverse Document Frequency (IDF)\n    idf_scores = {}\n    for q_word in query:\n        df = sum(q_word in doc for doc in corpus)\n        idf_scores[q_word] = math.log((1 + num_docs) / (1 + df))\n    \n    # Combine TF and IDF to get TF-IDF\n    tf_idf_scores = []\n    for doc_tf in tf_scores:\n        tf_idf = {}\n        for q_word in query:\n            tf_idf[q_word] = doc_tf.get(q_word, 0) * idf_scores[q_word]\n        tf_idf_scores.append(list(tf_idf.values()))\n    \n    return np.round(np.array(tf_idf_scores, dtype=float), 4).tolist()\n```"}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives (tp), false positives (fp), and false negatives (fn)\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n\n    # Precision\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    \n    # Recall\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # F-Score\n    bb = beta ** 2\n    fscore = (1 + bb) * (precision * recall) / ((bb * precision) + recall) if (precision + recall) > 0 else 0\n    return round(fscore, 3)\n\n# Check function with provided data points\ny_true_example = np.array([0, 1, 1, 0, 1, 1])\ny_pred_example = np.array([0, 1, 0, 0, 1, 1])\nbeta_example = 1.0\n\nprint(f_score(y_true_example, y_pred_example, beta_example))  # Expected F1-Score output, may vary depending on input\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        last_inputs = []\n        last_hiddens = []\n        outputs = []\n        for t in range(len(x)):\n            h = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h) + self.b_h)\n            y = self.sigmoid(np.dot(self.W_hy, h) + self.b_y)\n            last_inputs.append(x[t])\n            last_hiddens.append(h)\n            outputs.append(y)\n        return outputs, last_inputs, last_hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN with respect to the sequence of inputs and expected output.\n        \"\"\"\n        dh_next = np.zeros_like(last_hiddens[0])\n        dW_xh, dW_hh, dW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n\n        for t in reversed(range(len(input_sequence))):\n            h = last_hiddens[t]\n            y = outputs[t]\n            x = last_inputs[t]\n            dy = y - expected_output[t]  # Derivative of 1/2 MSE loss wrt output\n            dh = np.dot(self.W_hy.T, dy) + dh_next  # Backprop into h\n            dhrec = (1 - h**2) * dh  # Backprop through tanh nonlinearity\n\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n            dW_xh += np.dot(dhrec, x.reshape(1, -1))\n            dW_hh += np.dot(dhrec, last_hiddens[t-1].T) if t > 0 else np.zeros_like(self.W_hh)\n            db_h += dhrec\n            dh_next = np.dot(self.W_hh.T, dhrec)\n\n        # Clip to mitigate exploding gradients\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        # Perform parameter update\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 3\n    hidden_size = 4\n    output_size = 3\n    rnn = SimpleRNN(input_size, hidden_size, output_size)\n\n    input_sequence = [np.random.rand(input_size, 1) for _ in range(10)]\n    expected_output = [np.random.rand(output_size, 1) for _ in range(10)]\n\n    outputs, last_inputs, last_hiddens = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate=0.01)\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to a list)\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros(len(b))\n    x = np.array(x0)\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r, r)\n\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n\n    return np.round(x, decimals=8).tolist()\n\n# Example usage and a check function to verify the correctness of the generated function\ndef check_solution():\n    A = np.array([[1., 0.5], [0.5, 1.]])\n    b = np.array([1., 2.])\n    tol = 1e-8\n    solution = conjugate_gradient(A, b, n=1000, tol=tol)\n    expected = np.linalg.solve(A, b)  # Using numpy's built-in solver for comparison\n    assert np.allclose(solution, expected, atol=tol), \"The solution did not match the expected result.\"\n\ncheck_solution()\n```\n\nThe provided script solves the given task by implementing the conjugate gradient method in Python. The `check_solution` function is included to demonstrate how the `conjugate_gradient` function may be used and to verify its correctness against numpy's built-in linear system solver for a matrix that is known to be symmetric and positive-definite."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Calculate the total number of samples\n    total_samples = len(y)\n    if total_samples == 0:\n        return 0.0\n\n    # Count the occurrences of each class\n    classes, counts = np.unique(y, return_counts=True)\n    class_frequencies = counts / total_samples\n\n    # Calculate the Gini Impurity: 1 - \u03a3(pi^2) for each class i\n    gini = 1.0 - np.sum(class_frequencies ** 2)\n    return round(gini, 3)\n\n# Check function to verify the correctness of the gini_impurity function\ndef check_gini_impurity():\n    assert gini_impurity([1, 0, 1, 0, 1, 1, 0, 1, 1]) == 0.494, \"Test case 1 failed\"\n    assert gini_impurity([0, 0, 0, 0, 1]) == 0.320, \"Test case 2 failed\"\n    assert gini_impurity([1, 1, 1, 1]) == 0.0, \"Test case 3 failed\"\n    assert gini_impurity([]) == 0.0, \"Test case 4 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function\ncheck_gini_impurity()\n```\n\nThis Python script defines and tests the `gini_impurity` function, using `numpy` for efficient calculations of class frequencies and Gini impurity. The check function `check_gini_impurity` confirms the function works as expected with a few test cases."}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        return ([], [], [])\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_pointers = [0]  # The first row starts from index 0\n\n    for row in dense_matrix:\n        non_zero_count = 0  # Count of non-zero elements in this row\n        for col, item in enumerate(row):\n            if item != 0:\n                values.append(item)\n                col_indices.append(col)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)  # Append the cumulative count\n\n    return (values, col_indices, row_pointers[:-1])  # Exclude the last cumulative count\n\n# Check function to test the conversion\ndef check_conversion(test_matrix):\n    print(f\"Original matrix: {test_matrix}\")\n    csr_representation = compressed_row_sparse_matrix(test_matrix)\n    print(f\"CSR representation: Values: {csr_representation[0]}, Column Indices: {csr_representation[1]}, Row Pointers: {csr_representation[2]}\")\n    print()\n\n# Provided data points for verification\ncheck_conversion([[1, 0, 0, 2], [0, 0, 3, 0], [4, 0, 0, 0]])\ncheck_conversion([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\ncheck_conversion([[5, 0], [0, 6], [7, 0]])\n```\n\nThis python code snippet satisfies the task by implementing a conversion function from a dense matrix to the Compressed Row Sparse (CSR) format, followed by a check function to validate the behavior with provided data points."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v_np = np.array(v)\n    L_np = np.array(L)\n    \n    # Calculate the unit vector in the direction of L\n    unit_L = L_np / np.linalg.norm(L_np)\n    \n    # Formula for the projection of v onto L is: proj = (v\u00b7L/||L||^2) * L\n    # Since we divide L by its norm to get the unit vector directly use v\u00b7unit_L as scaler\n    proj = np.dot(v_np, unit_L) * unit_L\n    \n    # Convert the result back to a list and round to three decimal places\n    proj_list = [round(x, 3) for x in proj.tolist()]\n    \n    return proj_list\n\n# Example checks\nprint(orthogonal_projection([1, 2, 3], [1, 1, 1]))  # Output: [2.0, 2.0, 2.0]\nprint(orthogonal_projection([4, 5, 6], [1, 0, 0]))  # Output: [4.0, 0.0, 0.0]\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    values = []\n    row_indices = []\n    column_pointer = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\n# Example usage\ndense_matrix = [\n    [1, 0, 0],\n    [0, 2, 0],\n    [3, 0, 4]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\nThis Python code provides a function `compressed_col_sparse_matrix` that implements the conversion of a dense matrix into its Compressed Column Sparse (CSC) representation as described in the task."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n    \n    Args:\n    - A (numpy.ndarray): The input matrix.\n    \n    Returns:\n    - numpy.ndarray: The basis vectors (from the original matrix) that span the column space of A.\n    \"\"\"\n    \n    # Convert A to row echelon form\n    A = A.astype('float')\n    (h, w) = A.shape\n    lead = 0\n    row_count = h\n\n    for r in range(row_count):\n        if lead >= w:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == row_count:\n                i = r\n                lead += 1\n                if lead == w:\n                    break\n        A[[i, r]] = A[[r, i]]\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        for i in range(row_count):\n            if i != r:\n                lv = A[i, lead]\n                A[i] = A[i] - lv * A[r]\n        lead += 1\n    \n    # Extract the columns with leading 1s\n    independent_columns = [col for col in range(w) if np.any(A[:, col] == 1)]\n    # Fetch corresponding columns from the original matrix\n    basis_vectors = A[:, independent_columns]\n    \n    return np.round(basis_vectors, 8).tolist()\n\n# Example usage\nA = np.array([[1, -2, 3, -4, 5],\n              [2, -3, 1,  7, -1],\n              [3, -4, 2,  8,  1]])\n\nprint(matrix_image(A))\n```\n\nThis script first takes a matrix `A`, then converts it to its row echelon form to identify the columns that are linearly independent. These columns are chosen to be the basis vectors for the column space (image) of `A`. The result is rounded to 8 decimal places and returned as a list."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) between the true and predicted values.\n    \n    Parameters:\n    y_true (np.array): Array of true values.\n    y_pred (np.array): Array of predicted values.\n    \n    Returns:\n    float: Rounded R-squared value.\n    \"\"\"\n    # Calculate the total sum of squares\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Calculate the residual sum of squares\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate the R-squared value\n    r2 = 1 - (ss_res / ss_tot)\n    \n    # Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\n\n# Calculate R-squared\nr2_value = r_squared(y_true, y_pred)\nprint(f'R-squared: {r2_value}')\n\n# Check function to verify the correctness of the generated function\ndef check_function():\n    assert r_squared(np.array([3, -0.5, 2, 7]), np.array([2.5, 0.0, 2, 8])) == 0.948\n    assert r_squared(np.array([1, -1, 1, 2]), np.array([2, -2, 2, 2.5])) == 0.0\n    print(\"All tests passed!\")\n\ncheck_function()\n```"}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented as a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): 2D matrix representing the image.\n    \n    Returns:\n    float: Average brightness of the image rounded to two decimal places, or -1 for invalid input.\n    \"\"\"\n    if not img or not all(len(row) == len(img[0]) for row in img):\n        # Check for empty image or inconsistent row lengths\n        return -1\n    \n    total_brightness = 0\n    num_pixels = 0\n    \n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                # Check for invalid pixel values\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    \n    if num_pixels == 0:\n        # Handle potential divide by zero error\n        return -1\n    \n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)\n\n# Example usage\nexample_img = [\n    [128, 64, 192],\n    [0, 255, 128],\n    [127, 128, 129]\n]\n\nprint(calculate_brightness(example_img))  # Expected output: 120.56\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate and return the Root Mean Square Error (RMSE) between the two numpy arrays\n    representing true values and predicted values. The RMSE is rounded to three decimal places.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have mismatched shapes, are empty, or not arrays.\n    TypeError: If input types are not numpy arrays.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError(\"Inputs must be numpy arrays.\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    if y_true.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    squared_diffs = (y_true - y_pred) ** 2\n    rmse_value = np.sqrt(np.mean(squared_diffs))\n    \n    return round(rmse_value, 3)\n\n# Test the function with example data\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(rmse(y_true, y_pred))  # Should print a RMSE value based on the provided data points\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index for given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where union is 0 to avoid division by zero\n    if union == 0:\n        return 0.0\n    else:\n        # Calculate Jaccard Index\n        jaccard = intersection / union\n        return round(float(jaccard), 3)\n\n# Example usage\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])\nprint(jaccard_index(y_true, y_pred))  # Output should be 0.333\n\n# Check function to verify correctness\ndef check_jaccard_index():\n    assert jaccard_index([1, 1, 0, 0], [1, 1, 1, 1]) == 0.500\n    assert jaccard_index([1, 0, 0, 0], [1, 0, 0, 0]) == 1.000\n    assert jaccard_index([0, 0, 0, 0], [1, 1, 1, 1]) == 0.000\n    assert jaccard_index([0, 0, 0, 0], [0, 0, 0, 0]) == 0.000\n    print(\"All checks passed.\")\n\ncheck_jaccard_index()\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true binary labels.\n    y_pred (np.ndarray): 1D array of predicted binary labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of true and predicted labels\n    numerator = 2 * np.sum(y_true * y_pred)\n    \n    # Calculate the sum of true and predicted labels separately\n    denominator = np.sum(y_true) + np.sum(y_pred)\n    \n    # If both are empty, return 1.0 indicating perfect overlap\n    if denominator == 0:\n        return float('nan')\n    \n    # Calculate the Dice Score\n    dice = numerator / denominator\n    \n    # Round the result to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\n# y_true = np.array([1, 0, 1, 1, 0, 1])\n# y_pred = np.array([1, 1, 1, 0, 0, 1])\n# print(dice_score(y_true, y_pred))  # Expected output: 0.8\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_binary_hv(seed, dim):\n    \"\"\"Create a binary hypervector using the given seed.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\n\ndef bind_hv(hv1, hv2):\n    \"\"\"Element-wise multiplication of two hypervectors to bind them.\"\"\"\n    return hv1 * hv2\n\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors through element-wise sum and normalization.\"\"\"\n    bundled = np.sum(hvs, axis=0)\n    # Sign function to simulate the XOR-like bundling in binary HDC\n    return np.sign(bundled)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row.\"\"\"\n    feature_hvs = []\n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_name_hv = create_binary_hv(hash(feature), dim)\n        # Generate hypervector for the value using the feature-specific seed\n        feature_value_hv = create_binary_hv(random_seeds[feature], dim)\n        # Bind the hypervectors for the feature name and its value\n        bound_hv = bind_hv(feature_name_hv, feature_value_hv)\n        feature_hvs.append(bound_hv)\n    # Bundle all feature hypervectors to form the composite row hypervector\n    composite_hv = bundle_hvs(feature_hvs)\n    return composite_hv.tolist()\n\n# Example usage\ndim = 10\nrow = {'age': 30, 'income': 50000, 'height': 175}\nrandom_seeds = {'age': 3, 'income': 22, 'height': 101}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\nThis script includes the implementation of the `create_row_hv` function, which uses helper functions to create binary hypervectors (`create_binary_hv`), bind hypervectors (`bind_hv`), and bundle hypervectors (`bundle_hvs`). The result is a composite hypervector representing the entire row given as input, returned as a Python list."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    :param data: list of lists where each inner list is [y_true, y_pred]\n    :return: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    # Possible states for binary classification\n    states = {'TP': (1, 1), 'TN': (0, 0), 'FP': (0, 1), 'FN': (1, 0)}\n    \n    # Count occurrences of each state\n    counts = Counter(tuple(item) for sublist in data for item in [tuple(sublist)])\n    \n    # Extract counts for each state, defaulting to 0 if the state is not present\n    TP = counts[states['TP']]\n    TN = counts[states['TN']]\n    FP = counts[states['FP']]\n    FN = counts[states['FN']]\n    \n    # Create the confusion matrix\n    matrix = [\n        [TN, FP],\n        [FN, TP]\n    ]\n    \n    return matrix\n\n# Test\ndata = [[1, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 0], [0, 1]]\nprint(confusion_matrix(data))\n```\n\nThis code block defines a function `confusion_matrix(data)` that generates a 2x2 confusion matrix for a binary classification task based on the given input data. It uses the `collections.Counter` class to count occurrences of each combination of actual and predicted labels and then arranges these counts into the confusion matrix structure. A short test case is provided for verification."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: numpy array representing the first vector.\n    - v2: numpy array representing the second vector.\n    \n    Returns:\n    - The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    # Check input vector shapes\n    if v1.shape != v2.shape:\n        raise ValueError(\"Input vectors must have the same shape.\")\n    \n    # Check for zero magnitude\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Input vectors cannot be of zero magnitude.\")\n    \n    # Calculate dot product of v1 and v2\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate magnitudes of v1 and v2\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Calculate cosine similarity\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return cosine similarity rounded to three decimal places\n    return round(similarity, 3)\n\n# Example usage\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\n\n# Calculate and print the cosine similarity\nprint(cosine_similarity(v1, v2))\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    # Validating input constraints\n    if len(actual) != len(predicted):\n        raise ValueError(\"The actual and predicted lists must have the same length.\")\n    if not all(x in [0, 1] for x in actual) or not all(x in [0, 1] for x in predicted):\n        raise ValueError(\"All elements in the actual and predicted lists must be either 0 or 1.\")\n    \n    # Calculating Confusion Matrix\n    confusion_matrix = np.zeros((2, 2), dtype=int)\n    for true, pred in zip(actual, predicted):\n        confusion_matrix[true, pred] += 1\n    \n    # Extracting counts for each quadrant in the confusion matrix\n    true_pos = confusion_matrix[1, 1]\n    true_neg = confusion_matrix[0, 0]\n    false_pos = confusion_matrix[0, 1]\n    false_neg = confusion_matrix[1, 0]\n    \n    # Calculating metrics\n    accuracy = round(float((true_pos + true_neg) / len(actual)), 3)\n    precision = round(float(true_pos / (true_pos + false_pos)), 3) if (true_pos + false_pos) > 0 else 0\n    recall = round(float(true_pos / (true_pos + false_neg)), 3) if (true_pos + false_neg) > 0 else 0\n    f1_score = round(2 * ((precision * recall) / (precision + recall)), 3) if (precision + recall) > 0 else 0\n    specificity = round(float(true_neg / (true_neg + false_pos)), 3) if (true_neg + false_pos) > 0 else 0\n    negative_predictive_value = round(float(true_neg / (true_neg + false_neg)), 3) if (true_neg + false_neg) > 0 else 0\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example Usage\nactual = [1, 0, 1, 0, 1, 0, 0, 1]\npredicted = [1, 0, 0, 0, 1, 1, 0, 1]\nresult = performance_metrics(actual, predicted)\nprint(result)\n```\n\nThis script defines the `performance_metrics` function which calculates the required performance metrics for a binary classification model. It begins by validating the constraints on the input data and then builds the confusion matrix. After that, it calculates each metric based on the values found in the confusion matrix. Finally, values for accuracy, F1 score, specificity, and negative predictive value are rounded to three decimal places as required."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for the provided data.\n    \n    Parameters:\n    data (list or numpy.ndarray): A list or array of numerical data.\n    \n    Returns:\n    dict: A dictionary of descriptive statistics for the data.\n    \"\"\"\n    # Convert the list to numpy array if it is not already one\n    data = np.array(data) if not isinstance(data, np.ndarray) else data\n    \n    statistics = {}\n    statistics['mean'] = round(np.mean(data), 4)\n    statistics['median'] = round(np.median(data), 4)\n    statistics['mode'] = round(float(stats.mode(data)[0]), 4) if len(data) > 1 else float(stats.mode(data)[0])\n    statistics['variance'] = round(np.var(data, ddof=1), 4) # ddof=1 for sample variance\n    statistics['standard_deviation'] = round(np.std(data, ddof=1), 4) # ddof=1 for sample standard deviation\n    statistics['25th_percentile'] = round(np.percentile(data, 25), 4)\n    statistics['50th_percentile'] = round(np.percentile(data, 50), 4)\n    statistics['75th_percentile'] = round(np.percentile(data, 75), 4)\n    statistics['interquartile_range'] = round(np.percentile(data, 75) - np.percentile(data, 25), 4)\n    \n    return statistics\n\n# Check function correctness\ndata_points = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9]\ncalculate_stats = descriptive_statistics(data_points)\nprint(calculate_stats)\n```\n\nNote: To ensure the functionality works correctly, the usage of `stats.mode` from `scipy.stats` package might be necessary due to its handling of single-value and multi-value arrays, but the import statement for `scipy.stats` (as `stats`) is missing in the provided and executed code block above. Please prepend `from scipy.stats import mode as stats_mode` and replace `stats.mode(data)[0]` with `stats_mode(data)[0]` if using `scipy`'s mode function. However, for simplicity and sticking strictly to the requirements without assuming extra imports, use of numpy or built-in Python operations might be more direct."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate binomial coefficient\n    binom_coeff = math.comb(n, k)\n    # Calculate probability\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example check function to verify the correctness\ndef check_binomial_probability():\n    print(\"Checking a few scenarios...\")\n    print(f\"binomial_probability(10, 5, 0.5): {binomial_probability(10, 5, 0.5)} (Expected: 0.24610)\")\n    print(f\"binomial_probability(20, 15, 0.8): {binomial_probability(20, 15, 0.8)} (Expected: 0.17974)\")\n    print(f\"binomial_probability(4, 2, 0.25): {binomial_probability(4, 2, 0.25)} (Expected: 0.21094)\")\n\n# Uncomment the following line to run the check function\n# check_binomial_probability()\n```"}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value of the normal distribution at x, rounded to 5 decimal places.\n    \"\"\"\n    power = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = (1 / denominator) * math.exp(power)\n    return round(pdf_value, 5)\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    print(normal_pdf(0, 0, 1))  # Expected: 0.39894\n    print(normal_pdf(1, 0, 1))  # Expected: 0.24197\n    print(normal_pdf(0, 1, 1))  # Expected: 0.24197\n    print(normal_pdf(1, 1, 0.5))  # Expected: 0.79788\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events (float rounded to 5 decimal places)\n    \"\"\"\n    # Calculate the factorial of k\n    factorial_k = math.factorial(k)\n    \n    # Calculate e to the power of -lam\n    e_power = math.exp(-lam)\n    \n    # Calculate lam to the power of k\n    lam_power_k = lam ** k\n    \n    # Poisson distribution formula\n    probability = (lam_power_k * e_power) / factorial_k\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage\nprint(poisson_probability(3, 2))  # Example: Calculate the probability of 3 events with a rate of 2\n```\n\nThis script defines a function `poisson_probability` that calculates the probability of exactly `k` events occurring given the mean rate of occurrences `lam`, based on the Poisson distribution formula. It uses the `math` library to handle the exponential function and factorial calculation. The probability is returned to the user rounded to 5 decimal places."}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D numpy array.\")\n    \n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    contrast = max_pixel_value - min_pixel_value\n    return contrast\n\n# Example usage:\n# Create a sample grayscale image with numpy\nimg = np.array([[20, 50, 30],\n                [40, 70, 60],\n                [80, 100, 90]])\n\n# Calculate the contrast\ncontrast = calculate_contrast(img)\nprint(f\"The contrast of the image is: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example Usage\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])\n\nresult = calculate_dot_product(vec1, vec2)\nprint(\"Dot product:\", result)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    transformed_data = []\n    \n    for x in data:\n        transformed_features = [x ** i for i in range(1, degree + 1)]\n        transformed_data.append(transformed_features)\n    \n    return np.round(transformed_data, 8).tolist()\n\n# Check function to test the phi_transform function\ndef check_phi_transform():\n    test_data = [1.0, 2.0, 3.0]\n    test_degree = 3\n    expected_output = [[1.0, 1.0, 1.0], [2.0, 4.0, 8.0], [3.0, 9.0, 27.0]]\n    actual_output = phi_transform(test_data, test_degree)\n    assert actual_output == expected_output, f\"Expected {expected_output}, but got {actual_output}\"\n\n    test_degree = 0\n    expected_output = []\n    actual_output = phi_transform(test_data, test_degree)\n    assert actual_output == expected_output, f\"Expected {expected_output}, but got {actual_output}\"\n\n    print(\"All tests passed.\")\n\ncheck_phi_transform()\n```"}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a Transformer model.\n    \n    :param position: Length of the sequence (int)\n    :param d_model: Model dimensionality (int)\n    :return: If position or d_model is invalid, returns -1. Otherwise, returns a list of positional encodings.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n    \n    for pos in range(position):\n        pe[pos, 0::2] = np.sin(pos * div_term)\n        pe[pos, 1::2] = np.cos(pos * div_term)\n\n    return pe.tolist()\n\n# Check function to test the pos_encoding function\ndef check_pos_encoding():\n    expected = [[0.0, 1.0, 0.0, -1.0], [1.0, 0.0, 1.0, 0.0]]  # This is a hypothetical example and might not match the actual output\n    result = pos_encoding(2, 4)  # Keep in mind that due to np.float16 conversion, results might slightly vary.\n\n    # Due to the nature of np.float16, direct comparison might not be reliable. Here we check if the structure matches.\n    if result == -1 or len(result) != 2 or len(result[0]) != 4:\n        print(\"The pos_encoding function does not return the expected structure.\")\n    else:\n        print(\"The pos_encoding function seems to be working correctly with the test case.\")\n\n# Running the check function\ncheck_pos_encoding()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        # Overfitting\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        # Underfitting\n        return -1\n    else:\n        # Good fit\n        return 0\n\n# Test cases to verify the correctness of the function\nassert model_fit_quality(0.9, 0.5) == 1, \"The model is overfitting because the difference between training accuracy and test accuracy is more than 0.2\"\nassert model_fit_quality(0.6, 0.65) == -1, \"The model is underfitting because both training and test accuracy are below 0.7\"\nassert model_fit_quality(0.8, 0.75) == 0, \"The model has a good fit\"\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first and second moment estimates\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Bias correction\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameters\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist()\n\n# Test the function\nparameter = np.array([0.5])\ngrad = np.array([0.1])\nm = np.array([0.0])\nv = np.array([0.0])\nt = 1  # Timestep\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(\"Updated Parameter:\", updated_parameter)\nprint(\"Updated m:\", updated_m)\nprint(\"Updated v:\", updated_v)\n```\n\nIn the above script, we implemented the Adam optimizer update step function incorporating moving averages `m` and `v` of the gradient and the squared gradient, and performing bias corrections. We also ensured that the function handles inputs as numpy arrays and outputs lists of values that represent the updated parameter and moving averages, rounded to 5 decimal places."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [{\"attn\": {\"c_attn\": np.random.rand(10, 30), \"c_proj\": np.random.rand(30, 10)},\n                    \"ln_1\": {\"g\": np.ones(10), \"b\": np.zeros(10)},\n                    \"mlp\": {\"c_fc\": np.random.rand(10, 20), \"c_proj\": np.random.rand(20, 10)},\n                    \"ln_2\": {\"g\": np.ones(10), \"b\": np.zeros(10)}}] * 2,  # Simplified to 2 blocks\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10)\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef layer_norm(x, g, b, eps=1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\n\ndef attention(q, k, v):\n    scores = np.matmul(q, k.T) / np.sqrt(k.shape[-1])\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    return np.matmul(weights, v)\n\ndef feed_forward(x, c_fc, c_proj):\n    return np.matmul(np.maximum(0, np.matmul(x, c_fc)), c_proj)\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode input tokens\n    input_token_ids = encoder.encode(prompt)\n    output = []\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params[\"wte\"][input_token_ids]\n        pos_embeddings = params[\"wpe\"][np.arange(len(input_token_ids))]\n        x = token_embeddings + pos_embeddings\n\n        for block in params[\"blocks\"]:\n            attn_in = x\n            qkv = np.matmul(x, block[\"attn\"][\"c_attn\"])\n            q, k, v = np.split(qkv, [10, 20], axis=-1)\n            attn_out = attention(q, k, v)\n\n            attn_out = layer_norm(attn_out + x, block[\"ln_1\"][\"g\"], block[\"ln_1\"][\"b\"])\n            x = attn_out\n\n            mlp_in = attn_out\n            mlp_out = feed_forward(mlp_in, block[\"mlp\"][\"c_fc\"], block[\"mlp\"][\"c_proj\"])\n            x = layer_norm(mlp_out + attn_out, block[\"ln_2\"][\"g\"], block[\"ln_2\"][\"b\"])\n\n        x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        last_token = x[-1]  # Select last token as the generated token\n        new_token_id = np.argmax(last_token)  # Simplified token generation approach\n        output.append(new_token_id)\n        input_token_ids = input_token_ids[1:] + [new_token_id]  # Shift token_ids and add new token\n\n    return encoder.decode(output)\n\n# Example usage\nprint(gen_text(\"hello world\", 5))\n```\n\n### Notes:\nThis script is a simplified version of GPT-2 text generation. It starts with input tokens encoded by the `DummyBPE`, which simulates tokenization and encoding. The core of the script performs token embeddings, positional embeddings, multi-head attention, and feed-forward networks, then feeds the result through layer normalization. The core loop of GPT-2 is simplified into a single-token generation loop which adds a new token to the sequence and shifts the input token IDs accordingly. The token generation process is rudimentary and would not work accurately on a large vocabulary or real data, as it selects the index with maximum value from the last token."}
{"task_id": 89, "completion_id": 0, "completion": "Below is the Python script as described:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function to handle numerical stability\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n\n    # Convert the crystal values to a numpy array for operations\n    crystals = np.array(crystal_values).reshape(n, dimension)\n    \n    # Initialize weights for simplicity (use a random initialization for a more realistic scenario)\n    weights = np.random.rand(n, dimension)\n    \n    # Calculate the weighted values of the crystals\n    weighted_crystals = np.tanh(np.dot(crystals, weights.T))\n    \n    # Compute the attention scores between each crystal\n    attention_scores = np.dot(weighted_crystals, weighted_crystals.T)\n    \n    # Apply softmax to the scores to get the attention weights\n    attention_weights = softmax(attention_scores)\n    \n    # Calculate the final weighted patterns\n    weighted_patterns = np.dot(attention_weights, weighted_crystals)\n    \n    # Flatten the result and round to the 4th decimal place\n    final_patterns = np.round(weighted_patterns.flatten(), 4)\n    \n    return final_patterns.tolist()\n\n# Example usage:\nn = 4\ncrystal_values = [1, 2, 3, 4]\ndimension = 1\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\n**Explanation of the Code:**\n\n- The `pattern_weaver` function takes in the number of crystals `n`, the values of these crystals, and the dimension `dimension`.\n- Inside the function, a `softmax` function is defined first to allow for scaling the attention scores nicely, ensuring numerical stability by subtracting the maximum value before the exponential operation.\n- The crystal values are reshaped into a matrix if necessary and then multiplied by random weights to simulate the effect of learning the weights through training. Here, a simple tanh activation is used to produce non-linear transformations.\n- The dot product of the weighted crystals is used to compute attention scores between every pair of crystals.\n- The softmax function is applied to these scores to get the attention weights.\n- Finally, the attention weights are used to compute the final weighted patterns by multiplying the attention weights with the weighted crystals and returning the flattened, rounded result.\n\nThis code provides a simplified, illustrative version of the core self-attention mechanism and how it might be applied in a unique scenario such as the Pattern Weaver's task in the Crystal Cave."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate the BM25 scores for documents in the corpus based on the query.\n\n    :param corpus: List of documents, where each document is a list of terms.\n    :param query: A list of terms representing the query.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    # Preprocess the input to get data in a proper form\n    corpus = [Counter(doc) for doc in corpus]\n    query = Counter(query)\n    avgdl = np.mean([sum(doc.values()) for doc in corpus])\n    N = len(corpus)\n    doc_len = np.array([sum(doc.values()) for doc in corpus])\n\n    # Calculate idf for each term in the query\n    idf = {}\n    for term in query:\n        df = sum(1 for doc in corpus if doc.get(term, 0) > 0)\n        idf[term] = np.log((N - df + 0.5) / (df + 0.5))\n\n    # Calculate bm25 score for each document\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = doc[term]\n                score += idf[term] * ((tf * (k1 + 1)) /\n                                      (tf + k1 * (1 - b + b * (doc_len / avgdl))))\n        scores.append(round(score, 3))\n\n    return scores\n\n# Example usage\ncorpus = [\n    [\"information\", \"retrieval\", \"system\"],\n    [\"information\", \"retrieval\", \"system\", \"design\"],\n    [\"design\", \"information\", \"retrieval\"],\n    [\"machine\", \"learning\", \"and\", \"information\", \"retrieval\"]\n]\n\nquery = [\"information\", \"retrieval\"]\n\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nIn this Python script, the `calculate_bm25_scores` function computes the BM25 ranking scores for each document in the corpus with respect to the given query. The constants `k1` and `b` are set according to the guideline provided, where `k1` controls term frequency saturation, and `b` is the parameter that determines the importance of the document length normalization effect. The average document length (`avgdl`) and the number of documents (`N`) are then computed, followed by calculating the inverse document frequency (`idf`) for every term in the query. Lastly, the BM25 score for each document is calculated considering the term frequency, idf values, document length, and the adjustable parameters."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Convert to sets\n    y_true_set = set(y_true)\n    y_pred_set = set(y_pred)\n    \n    # Calculate TP, FP, FN\n    tp = len(y_true_set.intersection(y_pred_set))\n    fp = len(y_pred_set - y_true_set)\n    fn = len(y_true_set - y_pred_set)\n    \n    # Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Return F1 score rounded to three decimal places\n    return round(f1_score, 3)\n\n# Check function with provided data points\ny_true = [1, 2, 3, 4, 5]\ny_pred = [2, 4, 6, 8, 10]\nprint(calculate_f1_score(y_true, y_pred))  # Expected output: 0.286\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    This function takes a list of daily power consumption data and predicts the power consumption for day 15,\n    after removing the daily fluctuation and fitting a linear regression model on the detrended data.\n    A 5% safety margin is added to the prediction, and the final result is an integer.\n    \"\"\"\n    # Calculate the daily fluctuation for given data\n    fluctuation = np.array([10 * math.sin(2 * PI * i / 10) for i in range(1, len(consumption_data) + 1)])\n    # Detrend the data by subtracting the fluctuation\n    detrended_data = np.array(consumption_data) - fluctuation\n\n    # Perform linear regression\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # For day 15, predict the base consumption\n    day_15 = np.array([[15]])\n    base_prediction = model.predict(day_15)[0]\n\n    # Calculate the fluctuation for day 15\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10)\n    \n    # Add back the fluctuation for day 15 to the base consumption\n    final_prediction = base_prediction + fluctuation_15\n    \n    # Apply safety margin and round up to the nearest integer\n    final_prediction = math.ceil(final_prediction * 1.05)\n    \n    return final_prediction\n\n# Example usage\nconsumption_data = [120, 125, 130, 135, 138, 140, 143, 145, 150, 155]\nforecast = power_grid_forecast(consumption_data)\nprint(f\"Day 15 Power Consumption Forecast: {forecast}\")\n```\nPlease replace the example usage with the actual data when applying this script."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError(\"Both y_true and y_pred should be of type numpy.ndarray.\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred should have the same shape.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n    print(mae(y_true, y_pred))  # Output: 0.5\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the query, key, and value matrices.\n    :param X: Input tensor of shape (batch_size, seq_len, d_model).\n    :param W_q: Weight matrix for query of shape (d_model, d_k).\n    :param W_k: Weight matrix for key of shape (d_model, d_k).\n    :param W_v: Weight matrix for value of shape (d_model, d_v).\n    :return: A tuple of query, key, and value matrices each of shape (batch_size, seq_len, d_k or d_v).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes self-attention using the query, key, and value matrices.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: A tensor of shape (batch_size, seq_len, d_v).\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Compute attention scores\n    attention = np.dot(np.softmax(scores, axis=-1), V)  # Apply softmax and compute weighted sum\n    return attention\n\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism.\n    :param X: Input tensor of shape (batch_size, seq_len, d_model).\n    :param W_q: Weight matrix for query of shape (n_heads, d_model, d_k).\n    :param W_k: Weight matrix for key of shape (n_heads, d_model, d_k).\n    :param W_v: Weight matrix for value of shape (n_heads, d_model, d_v).\n    :param n_heads: The number of attention heads.\n    :return: A python list of the multi-head attention output rounded to the nearest 4th decimal.\n    \"\"\"\n    batch_size, seq_len, _ = X.shape\n    head_dim = W_q.shape[-1]\n    d_v = W_v.shape[-1]\n    \n    # Compute Q, K, V for each head\n    Q_heads = []\n    K_heads = []\n    V_heads = []\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        Q_heads.append(Q)\n        K_heads.append(K)\n        V_heads.append(V)\n    \n    # Concatenate outputs of each head and apply linear transformation\n    attention_heads = []\n    for i in range(n_heads):\n        attention_head = self_attention(Q_heads[i], K_heads[i], V_heads[i])\n        attention_heads.append(attention_head)\n    \n    # Concatenate and reshape for output\n    attention = np.concatenate(attention_heads, axis=-1).reshape(batch_size, seq_len, n_heads, d_v)\n    attention = np.mean(attention, axis=2)  # Simplified linear transformation with mean\n    \n    # Round and convert to list\n    attention_rounded = np.round(attention, decimals=4).tolist()\n    return attention_rounded\n\n\n# Example usage\nX = np.random.rand(1, 5, 128)  # Example input (batch_size, seq_len, d_model)\nW_q = np.random.rand(8, 128, 16)  # Example W_q (n_heads, d_model, d_k)\nW_k = np.random.rand(8, 128, 16)  # Example W_k (n_heads, d_model, d_k)\nW_v = np.random.rand(8, 128, 16)  # Example W_v (n_heads, d_model, d_v)\nn_heads = 8  # Number of heads\n\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(output)\n```\n\nThis Python script implements the multi-head attention mechanism, a critical component of the Transformer model. The key functions `compute_qkv`, `self_attention`, and `multi_head_attention` are defined to handle the computation for query, key, and value; the dot-product attention; and the multi-head attention mechanism, respectively. It starts with the provided function signature and ensures all outputs are rounded to the nearest fourth decimal, conforming to the given constraints."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nfrom typing import List\nimport numpy as np\n\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    x_array = np.array(x)\n    y_array = np.array(y)\n\n    # Calculate contingency table\n    n11 = np.sum(x_array * y_array)  # Both x and y are 1\n    n00 = np.sum((1 - x_array) * (1 - y_array))  # Both x and y are 0\n    n10 = np.sum(x_array * (1 - y_array))  # x is 1 and y is 0\n    n01 = np.sum((1 - x_array) * y_array)  # x is 0 and y is 1\n\n    # Calculate Phi coefficient\n    numerator = n11 * n00 - n10 * n01\n    denominator = (n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)\n\n    if denominator == 0:\n        return 0.0  # Avoid division by zero\n\n    phi = numerator / np.sqrt(denominator)\n\n    return round(phi, 4)\n\n# Example usage\nx = [0, 0, 0, 1, 1, 1, 1, 1]\ny = [0, 1, 1, 0, 0, 1, 1, 1]\nprint(phi_corr(x, y))\n```\n\nIn this solution, the phi_corr function calculates the Phi coefficient by first building a contingency table from the input binary variables, and then applying the formula for the Phi coefficient. The result is rounded to 4 decimal places as required."}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Test the function with a few sample values\nif __name__ == \"__main__\":\n    import numpy as np\n\n    test_values = np.linspace(-10, 10, 100)\n    for value in test_values:\n        print(f\"Input: {value:.3f}, Hard Sigmoid: {hard_sigmoid(value):.3f}\")\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n\n# Example usage and test cases\nprint(f\"elu(1.0) = {elu(1.0):.4f}\")       # Should output 1.0000\nprint(f\"elu(-1.0) = {elu(-1.0):.4f}\")     # Should output -0.6321\nprint(f\"elu(-1.0, 0.5) = {elu(-1.0, 0.5):.4f}\")  # Should output -0.3161\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [1.0, -1.0, 0.0, -2.0, 2.0]\n    alpha = 0.25\n\n    for val in test_values:\n        print(f\"PReLU({val}) with alpha={alpha} is {prelu(val, alpha)}\")\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place\n    \"\"\"\n    if x > 100:\n        return round(x, 4)  # For large x, softplus approaches x\n    elif x < -100:\n        return round(0, 4)  # For very negative x, softplus approaches 0\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example checks:\nprint(softplus(1.0))  # Expected output: 1.3133\nprint(softplus(100.0))  # Expected output: 100.0\nprint(softplus(-100.0))  # Expected output: 0.0\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1.0 + abs(x))\n    return round(result, 4)\n\n# Example usage and verification:\ndef check():\n    test_cases = [0.0, 1.0, -1.0, 100.0, -100.0]\n    expected_results = [0.0, 0.5, -0.5, 1.0, -1.0]\n    for x, expected in zip(test_cases, expected_results):\n        result = softsign(x)\n        assert result == expected, f\"Failed for input {x}: expected {expected}, got {result}\"\n    print(\"All test cases passed!\")\n\ncheck()\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for easier computation\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Clipping the likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    # Compute the clipped surrogate objective\n    surrogate_objective = np.minimum(rhos * A, clipped_rhos * A)\n\n    # Compute the KL divergence\n    kl_divergence = np.mean(np.log(pi_theta_old / pi_theta_ref))\n\n    # Apply the KL divergence penalty\n    grpo_value = np.mean(surrogate_objective) - beta * kl_divergence\n\n    # Round the result to 6 decimal places\n    return round(grpo_value, 6)\n\n# Example usage\nrhos = [1.2, 0.8, 1.5, 0.9, 1.05]\nA = [0.3, 0.1, -0.8, 0.5, 0.1]\npi_theta_old = [0.7, 0.3, 0.8, 0.4, 0.5]\npi_theta_ref = [0.6, 0.2, 0.7, 0.3, 0.45]\n\n# Calculate the GRPO objective value\ngrpo_val = grpo_objective(rhos, A, pi_theta_old, pi_theta_ref)\nprint(f\"The calculated GRPO objective value is: {grpo_val}\")\n```\n\nThis script defines the `grpo_objective` function that computes the Group Relative Policy Optimization (GRPO) objective, adhering to the description and constraints provided."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))\n\n# Test the function with some values\ntest_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\nresults = [round(swish(x), 4) for x in test_values]\nprint(results)\n\n# The following lines should output the Swish activation values for the test_values\n# Rounded to 4 decimal places for comparison and accuracy checking\n# Expected output (approximate): [-0.2384, -0.2689, 0.0, 0.7311, 1.7616]\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    results = [selu(x) for x in test_values]\n    # Rounding the results to the nearest 4th decimal\n    rounded_results = [round(result, 4) for result in results]\n    print(rounded_results)\n```\n\nThis script provides a function `selu` to compute the SELU activation of a given input number `x` with the specified scale and alpha constants, ensuring numerical stability and rounding the final result to the 4th decimal place."}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Sigmoid function to compute probabilities\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    # Compute the logits (linear combination of inputs and weights)\n    logits = np.dot(X, weights) + bias\n    # Compute the probabilities using the sigmoid function\n    probabilities = sigmoid(logits)\n    # Convert probabilities to binary predictions (threshold at 0.5)\n    predictions = (probabilities > 0.5).astype(int)\n\n    return predictions.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Assuming we have N samples and D features\n    N, D = 4, 3\n    X = np.random.rand(N, D)  # Feature matrix\n    weights = np.random.rand(D)  # Model weights\n    bias = np.random.rand()  # Model bias\n\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    m, n = X.shape\n    K = len(np.unique(y))\n    X = np.insert(X, 0, 1, axis=1)  # Adding bias column to X\n    B = np.zeros((n+1, K))  # Initialize parameters\n    \n    def softmax(S):\n        exp_scores = np.exp(S - np.max(S, axis=1, keepdims=True))\n        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n    def to_onehot(y):\n        return np.eye(K)[y]\n    \n    Y_onehot = to_onehot(y)\n    losses = []\n\n    for _ in range(iterations):\n        S = X @ B\n        predictions = softmax(S)\n        gradient = X.T @ (predictions - Y_onehot) / m\n        B -= learning_rate * gradient\n        \n        # Compute Cross Entropy loss and append to losses\n        entropy = Y_onehot * np.log(predictions + 1e-10)  # Adding small value to prevent NaN\n        loss = -np.sum(entropy) / m\n        losses.append(loss.round(4))\n    \n    B = B.round(4)\n    \n    return B.tolist(), [loss for loss in np.round(losses, 4)]\n\n# Example usage\n# X = np.random.randn(100, 2)  # 100 samples, 2 features\n# y = np.random.randint(0, 2, 100)  # 2 classes\n# learning_rate = 0.01\n# iterations = 1000\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(B, losses)\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add intercept term to X\n    intercept = np.ones((X.shape[0], 1))\n    X = np.concatenate((intercept, X), axis=1)\n    \n    # Initialize coefficients\n    coefficients = np.zeros(X.shape[1])\n    \n    # Training process\n    loss_history = []\n    for i in range(iterations):\n        # Predictions\n        z = np.dot(X, coefficients)\n        predictions = 1 / (1 + np.exp(-z))\n        \n        # Calculate loss\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_history.append(round(loss, 4))\n        \n        # Gradient calculation\n        gradient = np.dot(X.T, (predictions - y)) / y.size\n        \n        # Update coefficients\n        coefficients -= learning_rate * gradient\n    \n    # Return coefficients and loss history\n    return coefficients.tolist(), np.array(loss_history).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X_data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y_data = np.array([0, 0, 1, 1])\n    \n    # Training parameters\n    learning_rate = 0.1\n    iterations = 1000\n    \n    # Train the logistic regression model\n    coef, loss_history = train_logreg(X_data, y_data, learning_rate, iterations)\n    print(\"Coefficients:\", coef)\n    print(\"Loss history:\", loss_history)\n```\n\nThis code implements the Logistic Regression model with a Gradient Descent optimizer, using Binary Cross Entropy as the loss function. The coefficients of the model and the loss values over the iterations are returned after the specified number of iterations."}
{"task_id": 107, "completion_id": 0, "completion": "Certainly! Below is a self-contained Python script that implements the masked self-attention mechanism using NumPy:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q (np.ndarray): Query matrix.\n        K (np.ndarray): Key matrix.\n        V (np.ndarray): Value matrix.\n        mask (np.ndarray): Mask matrix, used to mask out some elements.\n        \n    Returns:\n        np.ndarray: Output of the attention mechanism after applying the mask.\n    \"\"\"\n    # Compute the dot product of Q and K.T (transpose of K)\n    attention_scores = np.dot(Q, K.T)\n    \n    # Apply the mask, which typically assigns a large negative value to elements to be masked\n    # This prevents those elements from contributing to the output\n    attention_scores = np.where(mask, attention_scores, -np.inf)\n    \n    # Compute the softmax across the last dimension\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the value matrix\n    output = np.dot(attention_weights, V)\n    \n    # Convert to list for final output\n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example matrices\n    X = np.random.randn(3, 6)\n    W_q = np.random.randn(6, 5)\n    W_k = np.random.randn(6, 5)\n    W_v = np.random.randn(6, 5)\n    \n    # Mask to simulate a causal mask (used in auto-regressive models)\n    mask = np.tril(np.ones((3, 3)))  # Lower triangular mask (3x3)\n    \n    # Compute Q, K, V matrices\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Compute masked attention\n    output = masked_attention(Q, K, V, mask)\n    \n    print(output)\n```\n\nYou can run the above script in a Python environment with numpy installed to test the masked self-attention mechanism. The `masked_attention` function applies the mask to the attention scores and then computes the softmax to get the attention weights, which are then used to weigh the value matrix for the final output."}
{"task_id": 108, "completion_id": 0, "completion": "Here's a self-contained Python script that defines a function to calculate the disorder in a basket of apples based on their colors, consistent with the provided specifications:\n\n```python\nfrom typing import List\nfrom collections import Counter\n\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The more evenly distributed the colors, the higher the measure of disorder.\n    \n    Args:\n    apples (List[int]): A list of integers representing the colors of apples.\n    \n    Returns:\n    float: A float value representing the disorder in the basket of apples.\n    \"\"\"\n    frequency = Counter(apples)  # Count the frequency of each color\n    total_apples = len(apples)   # Total number of apples\n    probabilities = [count / total_apples for count in frequency.values()]  # Calculate the probability of each color\n    \n    # Calculate the Shannon entropy, which is a measure of disorder (or uncertainty)\n    entropy = -sum(p * math.log(p, 2) for p in probabilities if p > 0)\n    \n    # Round the result to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test cases to verify the correctness\n    print(disorder([0,0,0,0])) # Expected: 0.0\n    print(disorder([1,1,0,0])) # Expected: higher than 0.0\n    print(disorder([0,1,2,3])) # Expected: higher than [1,1,0,0]\n    print(disorder([0,0,1,1,2,2,3,3])) # Expected: higher than [0,0,0,0,0,1,2,3]\n```\n\nThis function uses Shannon entropy as a measure of disorder. The entropy will be highest when all colors are equally represented and lowest when the basket contains apples of a single color, thus satisfying the given criteria. The probabilities of each color are calculated, and then the entropy is computed. The result is rounded to the nearest 4th decimal as required."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Applies layer normalization on a 3D array X with scaling and shifting parameters.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, seq_length, feature_num)\n    - gamma: np.ndarray of scaling factors, should match feature_num\n    - beta: np.ndarray of shifting factors, should match feature_num\n    - epsilon: float, a small constant for numerical stability\n    \n    Returns:\n    - np.ndarray: Layer-normalized X, rounded to 5 decimal places and converted to list\n    \"\"\"\n    # Calculate the mean across the feature dimension\n    mean = np.mean(X, axis=2, keepdims=True)\n    # Calculate the variance across the feature dimension\n    var = np.var(X, axis=2, keepdims=True)\n    # Normalize the input tensor\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    # Scale and shift the normalized input\n    X_scaled_shifted = gamma * X_norm + beta\n    # Round the output to 5 decimal places\n    X_scaled_shifted_rounded = np.round(X_scaled_shifted, 5)\n    # Convert to list and return\n    return X_scaled_shifted_rounded.tolist()\n\n# Example parameters\nX_example = np.array([[[1, 2, 3], [2, 3, 4]], [[2, 3, 4], [1, 2, 3]]])\ngamma_example = np.array([1.5, 2.0, 2.5])\nbeta_example = np.array([0.5, 0.1, -0.3])\n\n# Check function with the provided data points\noutput_example = layer_normalization(X_example, gamma_example, beta_example)\nprint(output_example)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef get_precision_recall(reference, candidate):\n    ref_counter = Counter(reference)\n    cand_counter = Counter(candidate)\n    \n    match_counter = ref_counter & cand_counter\n    matches = sum(match_counter.values())\n    \n    precision = matches / len(candidate) if len(candidate) > 0 else 0\n    recall = matches / len(reference) if len(reference) > 0 else 0\n    \n    return precision, recall\n\ndef fragmentation_cost(reference, candidate, beta):\n    ref_words = set(reference)\n    cand_words = set(candidate)\n    common_words = ref_words.intersection(cand_words)\n    frag_cost = 0\n    \n    for word in common_words:\n        ref_positions = [i for i, x in enumerate(reference) if x == word]\n        cand_positions = [i for i, x in enumerate(candidate) if x == word]\n        n_frags = abs(len(ref_positions) - len(cand_positions)) + sum(abs(a-b) for a, b in zip(ref_positions, cand_positions))\n        frag_cost += n_frags\n    \n    return frag_cost / (beta * len(reference))\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.split()\n    candidate = candidate.split()\n    \n    precision, recall = get_precision_recall(reference, candidate)\n    f_mean = ((1 + alpha**2) * precision * recall) / (alpha**2 * precision + recall) if (alpha**2 * precision + recall) > 0 else 0\n    \n    frag_cost = fragmentation_cost(reference, candidate, beta)\n    penalty = np.exp(-gamma * frag_cost) if frag_cost > 0 else 1\n    \n    meteor = f_mean * penalty\n    return round(meteor, 3)\n\n# Test cases\nref = \"the cat sat on the mat\"\ncand = \"the cat is sitting on the mat\"\nprint(meteor_score(ref, cand))  # Expected output depends on the calculation, but it should be a float value within [0,1]\ncand = \"cat the sat on mat the\"\nprint(meteor_score(ref, cand))  # Expected output also depends on the calculation, but it should be a float value within [0,1]\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total individual occurrence count of event x.\n    total_counts_y (int): The total individual occurrence count of event y.\n    total_samples (int): The total number of samples.\n    \n    Returns:\n    float: PMI rounded to 3 decimal places.\n    \"\"\"\n    # Avoiding division by zero\n    if total_counts_x == 0 or total_counts_y == 0 or joint_counts == 0:\n        return 0.0\n    \n    # Computing PMI\n    pmi = np.log2(\n        (joint_counts / total_samples) / \n        ((total_counts_x / total_samples) * (total_counts_y / total_samples))\n    )\n    \n    return round(pmi, 3)\n\n# Example Usage\nif __name__ == \"__main__\":\n    joint_counts = 20\n    total_counts_x = 100\n    total_counts_y = 150\n    total_samples = 1000\n    print(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```"}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    The normalized values are rounded to 4 decimal places.\n    \n    :param x: List of integers.\n    :return: List of normalized floats.\n    \"\"\"\n    if not x or max(x) == min(x):  # Handling edge cases of empty list or list with identical elements.\n        return [0.0 for _ in x]\n    \n    minimum = min(x)\n    maximum = max(x)\n    normalized = [round((val - minimum) / (maximum - minimum), 4) for val in x]\n    return normalized\n\n# Test the function to verify its correctness\ndef check_function():\n    print(min_max([1, 2, 3, 4, 5]))  # Expected output: [0.0, 0.25, 0.5, 0.75, 1.0]\n    print(min_max([10, 20, 30]))     # Expected output: [0.0, 0.5, 1.0]\n    print(min_max([5]))              # Expected output: [0.0] (Edge case handling)\n    print(min_max([]))               # Expected output: [] (Edge case handling)\n\ncheck_function()\n```\n\nThe provided Python script defines the `min_max` function for performing Min-Max Normalization on a list of integers, adhering to the task guidelines. It also includes a basic check function to test the correctness of the implementation with given data points."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Args:\n    x (np.ndarray): Input array of shape (batch_size, input_dim).\n    w1 (np.ndarray): Weight matrix of the first layer of shape (input_dim, hidden_dim).\n    w2 (np.ndarray): Weight matrix of the second layer of shape (hidden_dim, input_dim).\n    \n    Returns:\n    np.ndarray: Output array of the residual block after a final ReLU application, \n                rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Apply first layer and activation\n    hidden = np.maximum(0, np.dot(x, w1))\n    # Apply second layer and add shortcut\n    out = np.dot(hidden, w2) + x\n    # Apply final activation\n    output = np.maximum(0, out)\n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    # Generate random input and weight matrices\n    x_input = np.random.rand(5, 3)  # Batch size 5, input dim 3\n    weight1 = np.random.rand(3, 4)  # Hidden dim 4\n    weight2 = np.random.rand(4, 3)  # Back to input dim 3\n    \n    # Test the residual block function and print the result\n    result = residual_block(x_input, weight1, weight2)\n    print(result)\n```"}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels) representing feature maps.\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) where each element is the mean of the \n                values in the corresponding feature map from the input array.\n    \"\"\"\n    # Calculate the average of each channel across height and width\n    return np.mean(x, axis=(0, 1))\n\n# Example usage:\n# A 3D array with shape (4, 4, 2), simulating 2 feature maps with 4x4 each\nfeature_maps = np.array([[[1, 2], [3, 4], [5, 6], [7, 8]],\n                         [[9, 10], [11, 12], [13, 14], [15, 16]],\n                         [[17, 18], [19, 20], [21, 22], [23, 24]],\n                         [[25, 26], [27, 28], [29, 30], [31, 32]]])\n\n# Perform Global Average Pooling\npooled_output = global_avg_pool(feature_maps)\nprint(pooled_output)  # Expected output: [16. 17.]\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Performs Batch Normalization on a 4D input array X with scale (gamma) and shift (beta) parameters.\n    The input X is expected to be in BCHW format.\n    \n    Parameters:\n    X:          Input array in BCHW format (batch, channels, height, width)\n    gamma:      Scale parameter (per channel)\n    beta:       Shift parameter (per channel)\n    epsilon:    Small constant to avoid division by zero\n    \n    Returns:\n    A list representation of the normalized array rounded to 4 decimal places.\n    \"\"\"\n    # Get the dimensions\n    batch_size, channels, height, width = X.shape\n    \n    # Calculate mean and variance over the batch and spatial dimensions\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and Shift\n    X_scaled = X_norm * gamma.reshape(1, channels, 1, 1) + beta.reshape(1, channels, 1, 1)\n    \n    # Return as a list\n    return np.round(X_scaled, decimals=4).tolist()\n\n# Example to test the function\nif __name__ == \"__main__\":\n    batch_size, channels, height, width = 4, 3, 2, 2\n    X = np.random.randn(batch_size, channels, height, width)\n    gamma = np.random.randn(channels)\n    beta = np.random.randn(channels)\n    epsilon = 1e-5\n    \n    output = batch_normalization(X, gamma, beta, epsilon)\n    print(output)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n\n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is evaluated.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative of the polynomial term at point x,\n           rounded to 4 decimal places.\n    \"\"\"\n    # Applying the power rule: d/dx(c * x^n) = c * n * x^(n-1)\n    derivative = c * n * (x ** (n - 1))\n    return round(derivative, 4)\n\n# Example check function to verify the correctness of the generated function.\ndef check_derivative():\n    # Test cases\n    test_cases = [\n        (5.0, 1.0, 2.0, 10.0),  # c=5, x=1, n=2, expected derivative=10.0\n        (3.0, 2.0, 3.0, 36.0),  # c=3, x=2, n=3, expected derivative=36.0\n        (1.0, 0.0, 5.0, 0.0),   # c=1, x=0, n=5, expected derivative=0.0\n        (2.5, 2.5, 1.5, 9.375)  # c=2.5, x=2.5, n=1.5, expected derivative=9.375\n    ]\n    \n    # Checking each test case\n    for c, x, n, expected in test_cases:\n        result = poly_term_derivative(c, x, n)\n        assert result == expected, f\"Test failed: poly_term_derivative({c}, {x}, {n}) = {result}, expected {expected}\"\n    print(\"All tests passed!\")\n\n# Running the check function to verify the correctness\ncheck_derivative()\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    :param vectors: A list of 2D vectors.\n    :param tol: Tolerance to determine linear independence.\n    :return: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    # Convert the list of vectors into a numpy array\n    matrix = np.array(vectors, dtype=float)\n    n_vectors, dim = matrix.shape\n    basis = []\n\n    for i in range(n_vectors):\n        # Subtract the projection of the vector onto the existing basis\n        projection = np.sum([np.dot(matrix[i], basis_j) * basis_j for basis_j in basis], axis=0)\n        new_vector = matrix[i] - projection\n        norm = np.linalg.norm(new_vector)\n        \n        # If the vector is not almost zero, add it to the basis after normalization\n        if norm > tol:\n            new_vector /= norm\n            basis.append(new_vector.tolist())\n\n    return [np.around(np.array(b), 4).tolist() for b in basis]\n\n# Example usage:\nvectors = [[3, 2], [4, 7], [2, -1]]\ntol = 1e-5\nbasis = orthonormal_basis(vectors, tol)\nprint(basis)\n```\n\nIn this code block, I implemented the function `orthonormal_basis` that takes a list of 2D vectors. It then computes an orthonormal basis for these vectors using the Gram-Schmidt process. The vectors are processed one by one; for each vector, we subtract its projection onto the existing basis vectors to ensure orthogonality. Only vectors with a norm (length) above the given tolerance are added, ensuring non-zero basis vectors up to the tolerance level. The final orthonormal basis vectors are rounded to four decimal places as per requirements."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (np.ndarray): A numpy array of at least 3 elements representing the first vector.\n    b (np.ndarray): A numpy array of at least 3 elements representing the second vector.\n    \n    Returns:\n    list: The cross product of the two vectors rounded to 4 decimal places.\n    \"\"\"\n    cross = np.cross(a, b)\n    cross_rounded = np.round(cross, 4)\n    return cross_rounded.tolist()\n\n# Example usage\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nresult = cross_product(a, b)\nprint(result)  # Output should be [-3, 6, -3] since the exact result is [-3, 6, -3]\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): The constant vector.\n    \n    Returns:\n    list: A list of solutions if unique solution exists, else -1.\n    \"\"\"\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        return -1\n    \n    # Calculate the main determinant\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is zero, there is no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Prepare the solution vector\n    x = []\n    \n    # Solve for each x_i\n    for i in range(A.shape[1]):\n        # Create the matrix Ai by replacing ith column of A with b\n        Ai = A.copy()\n        Ai[:, i] = b\n        # Calculate the determinant of Ai\n        det_Ai = np.linalg.det(Ai)\n        # Using Cramer's rule to find x_i: det(Ai) / det(A)\n        x_i = det_Ai / det_A\n        # Append x_i to the solution vector\n        x.append(round(x_i, 4))\n    \n    return np.array(x).tolist()\n\n# Example usage\nA = np.array([[3, 2], [1, -1]])\nb = np.array([1, -2])\nprint(cramers_rule(A, b))  # Should output: [0.2, 0.4]\n\nA = np.array([[2, -1], [4, -2]])\nb = np.array([1, 2])\nprint(cramers_rule(A, b))  # Should output: -1\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two probability distributions.\n    \n    Args:\n    p: A list of floats representing the first probability distribution.\n    q: A list of floats representing the second probability distribution.\n    \n    Returns:\n    A float representing the Bhattacharyya distance rounded to 4 decimal places.\n    If inputs are of different lengths or empty, returns 0.0.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Ensure the distributions are numpy arrays for element-wise operations\n    p = np.array(p)\n    q = np.array(q)\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(p * q))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    return round(bd, 4)\n\n# Example checks\nprint(bhattacharyya_distance([0.1, 0.3, 0.6], [0.2, 0.5, 0.3]))  # Example with valid inputs\nprint(bhattacharyya_distance([], [0.2, 0.5, 0.3]))               # Example with an empty distribution\nprint(bhattacharyya_distance([0.1, 0.3, 0.6], [0.2, 0.5]))       # Example with distributions of different lengths\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] or int:\n    # Check if the lengths of both vectors are the same\n    if len(a) != len(b):\n        return -1\n    \n    # Perform element-wise addition\n    result = [x + y for x, y in zip(a, b)]\n    return result\n\n# Check function to test the vector_sum function with provided data points\ndef check_function():\n    assert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9], \"Test case 1 failed\"\n    assert vector_sum([1.5, 2.5, 3.5], [2.5, 3.5, 4.5]) == [4.0, 6.0, 8.0], \"Test case 2 failed\"\n    assert vector_sum([1, 2], [4, 5, 6]) == -1, \"Test case 3 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the results\ncheck_function()\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n    theta: np.ndarray, a 2D array representing the parameters of the policy.\n    episodes: list of episodes, where each episode is a list of tuples containing (state, action, reward) elements.\n    \n    Returns:\n    np.ndarray, the average gradient of the log-policy multiplied by the return.\n    \"\"\"\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    \n    for episode in episodes:\n        # Calculate returns for each time step in the episode\n        returns = []\n        G = 0\n        for t in range(len(episode)-1, -1, -1):\n            G = episode[t][2] + G\n            returns.append(G)\n        returns = returns[::-1]\n        \n        # Compute the softmax policy\n        policy = softmax(theta)\n        \n        for t, (state, action, _) in enumerate(episode):\n            # Policy gradient for each action\n            for a in range(num_actions):\n                if a == action:\n                    policy_grad = policy[state, a] - policy[state, a] ** 2\n                else:\n                    policy_grad = -policy[state, a] * policy[state, action]\n                gradient[state, a] += returns[t] * policy_grad\n    \n    # Average over all episodes\n    gradient /= len(episodes)\n    \n    # Round each value to the nearest 4th decimal and convert to list\n    gradient = np.round(gradient, decimals=4).tolist()\n    \n    return gradient\n\n# Example usage:\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [\n    [(0, 0, 1.0), (1, 1, 1.0)],\n    [(0, 1, 1.0), (1, 0, 1.0)]\n]\ngradient = compute_policy_gradient(theta, episodes)\nprint(gradient)\n```\n\nThis script defines the `compute_policy_gradient` function that calculates the average policy gradient using the REINFORCE algorithm given the policy parameters (`theta`) and a list of episodes. Each episode is represented as a list of (state, action, reward) tuples. The policy is computed via the softmax function, and the gradient for each state and action is updated based on the return of that action. Finally, the average gradient is rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer\n    compared to a dense layer. The function takes the number of experts, number\n    of active experts (sparsity), and the input/output dimensions to compute the\n    floating-point operations (FLOPs) and determine the savings percentage.\n    \n    Args:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active (selected) experts.\n    d_in (int): Input dimensions.\n    d_out (int): Output dimensions.\n    \n    Returns:\n    float: The computational cost savings percentage rounded to the nearest 1th decimal.\n    \"\"\"\n    # FLOPs calculation for a dense layer\n    flops_dense = 2 * d_in * d_out  # 2x for multiplication and addition\n    \n    # FLOPs calculation for the MoE layer\n    flops_moe = 2 * d_in * d_out * k_active / n_experts  # Adjust for active experts\n    \n    # Calculate savings\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    \n    # Round to the nearest 1th decimal\n    return round(savings, 1)\n\n# Example usage\nn_experts = 128\nk_active = 4\nd_in = 1024\nd_out = 1024\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Savings Percentage: {savings_percentage}%\")\n```\n\nIn this script, `compute_efficiency` calculates the floating-point operations (FLOPs) difference between a dense layer and a Mixture-of-Experts (MoE) layer and then determines the computational cost savings expressed as a percentage. The function accepts parameters for the number of experts, the sparsity (number of active experts), and the input/output dimensions. The result is rounded to the nearest 1th decimal as required."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n) -> list:\n    \"\"\"\n    Computes the Noisy Top-K Gating Function.\n    \n    Parameters:\n    X (np.ndarray): The input matrix.\n    W_g (np.ndarray): The gating weight matrix.\n    W_noise (np.ndarray): The noise weight matrix.\n    N (np.ndarray): Pre-sampled noise array.\n    k (int): Sparsity constraint, the top-k elements are selected.\n    \n    Returns:\n    list: The final gating probabilities matrix as a python list.\n    \"\"\"\n    # Compute logits\n    logits = X @ W_g + N * (X @ W_noise)\n    # Softmax for probabilities\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    # Add noise to probabilities\n    noisy_probs = probs + N\n    # Zero out all but the top-k gate probabilities\n    topk_indices = np.argpartition(-noisy_probs, k, axis=1)[:, :k]\n    topk_indices = np.sort(topk_indices, axis=1)\n    topk_probs = np.zeros_like(probs)\n    for i, indices in enumerate(topk_indices):\n        topk_probs[i, indices] = noisy_probs[i, indices]\n    # Normalize the top-k probabilities\n    topk_probs /= np.sum(topk_probs, axis=1, keepdims=True)\n    # Ensure rounding and convert to list\n    topk_probs = np.around(topk_probs, decimals=4).tolist()\n    return topk_probs\n\n# Example usage\nX = np.random.randn(2, 5)  # Example input matrix\nW_g = np.random.randn(5, 10)  # Example gating weight matrix\nW_noise = np.random.randn(5, 10)  # Example noise weight matrix\nN = np.random.randn(2, 10)  # Example pre-sampled noise array\nk = 3  # Example sparsity constraint\n\nresult = noisy_topk_gating(X, W_g, W_noise, N, k)\nprint(result)\n```\nThis function takes the input matrix `X`, the gating weight matrix `W_g`, the noise weight matrix `W_noise`, a pre-sampled noise array `N`, and a sparsity constraint `k` to compute the noisy top-k gating probabilities. The probabilities are normalized and rounded to the nearest 4th decimal as a final step. Note that the example usage provided will generate random input arrays; you should replace these with actual data for your use case."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x: Input tensor of shape (batch_size, input_dim).\n    We: Experts weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg: Gating weight matrix of shape (input_dim, n_experts).\n    n_experts: Number of experts in the layer.\n    top_k: Number of top experts to use for each input.\n    \n    Returns:\n    np.ndarray: MoE output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    batch_size, input_dim = x.shape\n    \n    # Compute gating scores\n    gating_scores = np.dot(x, Wg)  # Shape: (batch_size, n_experts)\n    gating_probs = np.apply_along_axis(lambda z: np.exp(z) / np.sum(np.exp(z)), 1, gating_scores)\n    \n    # Compute expert inputs\n    expert_inputs = np.dot(x, We.transpose(1, 0, 2))  # Shape: (batch_size, n_experts, output_dim)\n    \n    # Get top-k experts\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    \n    # Extract top-k expert outputs for each sample\n    top_k_expert_outputs = np.take_along_axis(expert_inputs, top_k_indices[:, :, None], axis=1)\n    top_k_gating_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    \n    # Normalize gating probabilities for top-k experts\n    top_k_gating_probs = np.apply_along_axis(lambda z: np.exp(z) / np.sum(np.exp(z)), 1, top_k_gating_probs)\n    \n    # Weighted sum of top-k experts' outputs\n    final_output = np.sum(top_k_expert_outputs * top_k_gating_probs[:, :, None], axis=1)\n    \n    # Round and convert to list\n    return np.round(final_output, 4).tolist()\n\n# Example usage\ninput_dim, output_dim = 10, 5\nn_experts = 3\ntop_k = 2\nbatch_size = 2\n\nx = np.random.rand(batch_size, input_dim)  # Input tensor\nWe = np.random.rand(n_experts, input_dim, output_dim)  # Expert weight matrices\nWg = np.random.rand(input_dim, n_experts)  # Gating weight matrix\n\nresult = moe(x, We, Wg, n_experts, top_k)\nprint(result)\n```\nThis code snippet includes the `moe` function which implements the Mixture-of-Experts concept using the specified parameters. It uses softmax gating to assign routing probabilities to each expert and uses top-k routing to select a subset of experts to process each input. The output from the selected experts is then weighted by these probabilities and combined to produce the final output for each input in the batch."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    :param X: Input tensor of shape (B, C, H, W)\n    :param gamma: Scale parameter of shape (1, C, 1, 1)\n    :param beta: Shift parameter of shape (1, C, 1, 1)\n    :param num_groups: Number of groups for normalization\n    :param epsilon: Small constant for numerical stability\n    :return: Normalized tensor of shape (B, C, H, W)\n    \"\"\"\n    B, C, H, W = X.shape\n    # Check if the number of channels can be divided by the number of groups\n    if C % num_groups != 0:\n        raise ValueError(\"Number of channels must be divisible by the number of groups.\")\n    # Reshape the tensor to group the channels together\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    # Calculate the mean and variance over each group's channels\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    # Normalize the input tensor\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    # Reshape the tensor back to its original shape\n    X_norm = X_norm.reshape(B, C, H, W)\n    # Apply scale and shift\n    X_out = gamma * X_norm + beta\n    return np.round(X_out, 4).tolist()\n\n# Example usage\nB, C, H, W = 2, 8, 3, 3  # Batch, Channels, Height, Width\nnum_groups = 2  # Example number of groups\nX = np.random.randn(B, C, H, W)\ngamma = np.random.randn(1, C, 1, 1)  # Scale parameters\nbeta = np.random.randn(1, C, 1, 1)  # Shift parameters\n\n# Call the group_normalization function\nnormalized_output = group_normalization(X, gamma, beta, num_groups)\nprint(normalized_output)\n```\n\nThis script defines a function `group_normalization` that normalizes a 4D input tensor across groups of channels, applies learned scale and shift parameters, and returns the result as a list. It includes an example of how to use the function with random data."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    x = start_x\n    for _ in range(max_iters):\n        diff = -df(x) * learning_rate\n        if abs(diff) < tolerance:\n            break\n        x += diff\n    \n    return round(x, 4)\n\n# Example usage\nstart_point = np.random.uniform(-10, 10)\ntreasure_location = find_treasure(start_point)\nprint(f\"The location of Captain Redbeard's treasure is at x = {treasure_location}.\")\n```\n\nThis Python function implements gradient descent to find the minimum value of the given function $f(x) = x^4 - 3x^3 + 2$. The gradient is calculated as $df(x) = 4x^3 - 9x^2$. The initial position is randomized between -10 and 10, and the function iterates until the change is very small (less than the defined tolerance) or the maximum number of iterations is reached. The result is rounded to the nearest 4th decimal, as requested."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function implements a normalization-free transformation inspired by the Tanh function.\n    This function is intended to replace layer normalization in Transformer architectures.\n    \n    :param x: Input array of data points.\n    :param alpha: Scaling factor for the output of the tanh function.\n    :param gamma: Scaling factor for the input.\n    :param beta: Bias term added to the scaled input.\n    :return: A list of transformed values, rounded to 4 decimal places.\n    \"\"\"\n    # Apply scaling and bias to input\n    scaled_x = gamma * x + beta\n    \n    # Apply tanh and scale by alpha\n    dyt_vals = alpha * np.tanh(scaled_x)\n    \n    # Round the results and convert to list\n    return np.round(dyt_vals, 4).tolist()\n\n# Example usage\nx = np.array([0.5, 1.0, -1.0, -0.5])\nalpha = 1.5\ngamma = 0.7\nbeta = 0.2\nprint(dynamic_tanh(x, alpha, gamma, beta))\n```\nThis script defines a `dynamic_tanh` function that implements a version of the activation function, which is inspired by the Tanh function but with additional parameters to scale and shift the input, allowing it to replace normalization layers in neural network architectures such as Transformers. The function applies a tanh transformation to the scaled and biased input and then scales the result by a factor `alpha`. The output is a list of floating-point numbers rounded to the nearest 4th decimal place."}
