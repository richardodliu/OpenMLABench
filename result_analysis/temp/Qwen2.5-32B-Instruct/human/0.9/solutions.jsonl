{"task_id": 1, "completion_id": 0, "solution": "import numpy as np\ndef matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    Returns a list representing the resulting vector if the operation is valid.\n    Returns -1 if the matrix and vector dimensions are incompatible.\n    \"\"\"\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        if len(row) != len(b):\n            return -1\n        result.append(sum((x * y for (x, y) in zip(row, b))))\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transposes a given matrix.\n    \n    Parameters:\n    a (list of list of int|float): The matrix to be transposed.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    if not a:\n        return []\n    num_rows = len(a)\n    num_cols = len(a[0])\n    transposed_matrix = [[0 for _ in range(num_rows)] for _ in range(num_cols)]\n    for i in range(num_rows):\n        for j in range(num_cols):\n            transposed_matrix[j][i] = a[i][j]\n    return transposed_matrix"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape a given matrix into a specified shape using numpy. If the reshaping is not possible, return an empty list.\n    \n    Parameters:\n    a (list[list[int|float]]): The matrix to reshape.\n    new_shape (tuple[int, int]): The new shape of the matrix.\n    \n    Returns:\n    list[list[int|float]]: The reshaped matrix as a Python list, or an empty list if reshaping is not possible.\n    \"\"\"\n    arr = np.array(a)\n    if arr.size == new_shape[0] * new_shape[1]:\n        reshaped_arr = arr.reshape(new_shape)\n        return reshaped_arr.tolist()\n    else:\n        return []\nnew_shape = (4, 1)"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists representing the matrix with numerical values.\n    :param mode: A string, either 'row' or 'column', defining the axis for calculating the mean.\n    :return: A list of means.\n    \"\"\"\n    if not matrix or not matrix[0]:\n        raise ValueError('Matrix cannot be empty')\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        return [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    else:\n        raise ValueError(\"Mode should either be 'row' or 'column'\")\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given 2D matrix by the scalar value and returns the resulting matrix.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix to be multiplied.\n    :param scalar: An integer or float representing the scalar value to multiply the matrix by.\n    :return: A 2D list representing the matrix after scalar multiplication.\n    \"\"\"\n    return [[elem * scalar for elem in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix given as a list of lists.\n    :return: A list of eigenvalues sorted in descending order.\n    \"\"\"\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    return sorted(eigenvalues, reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_matrix = np.array(A)\n        T_matrix = np.array(T)\n        S_matrix = np.array(S)\n        if T_matrix.shape[0] != T_matrix.shape[1] or S_matrix.shape[0] != S_matrix.shape[1]:\n            return -1\n        T_det = np.linalg.det(T_matrix)\n        S_det = np.linalg.det(S_matrix)\n        if T_det == 0 or S_det == 0:\n            return -1\n        T_inv = np.linalg.inv(T_matrix)\n        transformed_matrix = np.round(T_inv @ A_matrix @ S_matrix, 4)\n        return transformed_matrix.tolist()\n    except Exception as e:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 0], [0, 1]]"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    - matrix: A list of two lists, each containing two float elements.\n    \n    Returns:\n    - The inverse matrix as a list of two lists, each containing two float elements.\n    - Returns None if the matrix is not invertible.\n    \"\"\"\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inv_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inv_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b. Returns -1 if the matrices cannot be multiplied.\n    \n    Parameters:\n    a (list of list of int/float): The first matrix.\n    b (list of list of int/float): The second matrix.\n    \n    Returns:\n    list of list of int/float or int: The product matrix or -1 if multiplication is not possible.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    product = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                product[i][j] += a[i][k] * b[k][j]\n    return product"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of lists of float): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of lists of float: A covariance matrix.\n    \"\"\"\n    data = np.array(vectors)\n    cov_matrix = np.cov(data)\n    return cov_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Uses the Jacobi method to solve a system of linear equations given by Ax = b.\n    \n    Parameters:\n    A (np.ndarray): The coefficient matrix of the system.\n    b (np.ndarray): The target vector.\n    n (int): The number of iterations to perform.\n    \n    Returns:\n    list: The approximate solution, rounded to four decimal places.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Input matrix must be a square matrix.')\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        x = x_new\n        x = np.round(x, 4)\n    return x.tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([24, -10, -10, -10])\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Calculates the singular values of a 2x2 matrix A using an iterative method inspired by SVD,\n    without using numpy's direct svd function.\n    Returns the singular values rounded to 4 decimal places.\n    \"\"\"\n    A_T = A.T\n    ATA = np.dot(A_T, A)\n    trace = np.trace(ATA)\n    det = np.linalg.det(ATA)\n    eigenvalues = sorted((trace + np.sqrt(trace ** 2 - 4 * det)) / 2, (trace - np.sqrt(trace ** 2 - 4 * det)) / 2, reverse=True)\n    singular_values = [np.sqrt(val) for val in eigenvalues]\n    rounded_singular_values = [round(val, 4) for val in singular_values]\n    return tuple(rounded_singular_values)"}
{"task_id": 13, "completion_id": 0, "solution": "from typing import List, Union\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion method.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix as a float.\n    \"\"\"\n\n    def determinant_3x3(submatrix: List[List[Union[int, float]]]) -> float:\n        \"\"\"\n        Helper function to calculate the determinant of a 3x3 matrix.\n        \n        :param submatrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the submatrix as a float.\n        \"\"\"\n        return submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) - submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) + submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input matrix must be a 4x4 list of lists')\n    det = 0\n    for i in range(4):\n        minor = [row[:i] + row[i + 1:] for row in matrix[1:]]\n        det += (-1) ** i * matrix[0][i] * determinant_3x3(minor)\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    X is a list of lists, where each sublist represents one data point's features.\n    y is a list of floats representing the target values.\n    The function returns the coefficients of the linear regression model.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.column_stack((np.ones(X_np.shape[0]), X_np))\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    theta_rounded = np.round(theta, decimals=4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    - X: np.ndarray, the feature matrix with a column of ones for the intercept.\n    - y: np.ndarray, the target variable.\n    - alpha: float, learning rate.\n    - iterations: int, number of iterations.\n    \n    Returns:\n    - list, the coefficients of the linear regression model rounded to four decimal places.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / m\n        theta -= alpha * gradient\n    return np.around(theta, decimals=4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array as input, and returns two 2D lists:\n    one scaled by standardization and one by min-max normalization.\n    \"\"\"\n    std_scaled = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_max_scaled = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_scaled = np.nan_to_num(min_max_scaled)\n    std_scaled_list = np.round(std_scaled, 4).tolist()\n    min_max_scaled_list = np.round(min_max_scaled, 4).tolist()\n    return (std_scaled_list, min_max_scaled_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        new_centroids = np.array([points[closest_centroid == k].mean(axis=0) for k in range(centroids.shape[0])])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = [(round(x, 4), round(y, 4)) for (x, y) in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(6.0, 2.0), (1.0, 2.0), (3.0, 4.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n    \n    :param X: Numpy array of input features.\n    :param y: Numpy array of target variable.\n    :param k: Number of folds for cross-validation.\n    :param shuffle: Whether to shuffle the data before splitting.\n    :param random_seed: Seed for random number generator for reproducibility.\n    :return: List of tuples, each containing train and test indices for k folds.\n    \"\"\"\n    if random_seed:\n        np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start_idx = 0\n    splits = []\n    for i in range(k):\n        fold_end = start_idx + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start_idx:fold_end]\n        train_indices = np.concatenate((indices[:start_idx], indices[fold_end:]))\n        start_idx = fold_end\n        splits.append((train_indices, test_indices))\n    return splits\nX = np.random.rand(100, 5)\ny = np.random.randint(0, 2, size=(100,))\nk = 5\nrandom_seed = 42\nsplits = k_fold_cross_validation(X, y, k=k, shuffle=True, random_seed=random_seed)"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data and return the top k principal components.\n    \n    Parameters:\n    data (np.ndarray): The input dataset as a 2D numpy array where each row is a data sample and each column is a feature.\n    k (int): The number of principal components to return.\n    \n    Returns:\n    list[list[float]]: A list of lists representing the top k principal components.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    return [list(principal_components[:, i]) for i in range(k)]"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Construct a decision tree from given examples and attributes.\n    \n    :param examples: A list of dictionaries representing the examples, each with attribute-value pairs.\n    :param attributes: A list of attribute names to consider for splitting.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    unique_classes = set((example[target_attr] for example in examples))\n    if len(unique_classes) == 1:\n        return unique_classes.pop()\n    if not attributes:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    best_attr = max(attributes, key=lambda attr: info_gain(examples, attr, target_attr))\n    tree = {best_attr: {}}\n    for value in set((example[best_attr] for example in examples)):\n        subtree_examples = [example for example in examples if example[best_attr] == value]\n        if subtree_examples:\n            subattributes = [attr for attr in attributes if attr != best_attr]\n            tree[best_attr][value] = learn_decision_tree(subtree_examples, subattributes, target_attr)\n        else:\n            tree[best_attr][value] = Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    return tree\ndef entropy(s: list[str]) -> float:\n    \"\"\"\n    Calculate the entropy of a list of values.\n    \n    :param s: A list of values.\n    :return: The entropy of the list.\n    \"\"\"\n    counter = Counter(s)\n    probs = [count / len(s) for count in counter.values()]\n    return -sum((p * math.log(p, 2) for p in probs if p > 0))\ndef info_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n    \"\"\"\n    Calculate the information gain from splitting by a given attribute.\n    \n    :param examples: A list of examples.\n    :param attribute: The attribute to split by.\n    :param target_attr: The target attribute to classify.\n    :return: The information gain of the split.\n    \"\"\"\n    entropy_s = entropy([example[target_attr] for example in examples])\n    entropy_splits = sum((len([example for example in examples if example[attribute] == value]) / len(examples) * entropy([example[target_attr] for example in examples if example[attribute] == value]) for value in set((example[attribute] for example in examples))))\n    return entropy_s - entropy_splits"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_linear(x, y):\n    return np.dot(x, y.T)\ndef kernel_rbf(x, y, sigma):\n    return np.exp(-np.sum((x[:, np.newaxis] - y) ** 2, axis=2) / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    b = 0\n    for t in range(1, iterations + 1):\n        eta_t = 1 / (lambda_val * t)\n        if kernel == 'linear':\n            k = kernel_linear(data, data)\n        elif kernel == 'rbf':\n            k = kernel_rbf(data, data, sigma)\n        else:\n            raise ValueError(\"Unsupported kernel type. Use 'linear' or 'rbf'.\")\n        pred = np.sum(alpha[:, np.newaxis] * labels[:, np.newaxis] * k, axis=0) + b\n        pred_sign = np.sign(pred)\n        incorrect = pred_sign != labels\n        if kernel == 'linear':\n            grad = (labels[incorrect] * data[incorrect][:, np.newaxis]).sum(axis=0)\n            step = eta_t * (grad / t)\n        elif kernel == 'rbf':\n            step = eta_t * (k[:, incorrect] @ labels[incorrect]) / t\n        alpha += step * labels / t\n        alpha[alpha < 0] = 0\n        b += eta_t * labels[pred * labels < 1].mean()\n    alpha = np.round(alpha, 4).tolist()\n    b = np.round(b, 4)\n    return (alpha, b)\ndata = np.random.rand(100, 5)\nlabels = 2 * (np.random.rand(100) > 0.5) - 1"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a given list of scores.\n    \n    The softmax function transforms each score in a list to a value between 0 and 1,\n    and normalizes the scores so that their sum is 1. This function is often used in\n    machine learning for converting a vector of arbitrary real values to a probability distribution.\n    \n    Args:\n    scores (list of float): The input scores.\n    \n    Returns:\n    list of float: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    features_array = np.array(features)\n    weights_array = np.array(weights)\n    net_input = features_array.dot(weights_array) + bias\n    predicted_probabilities = sigmoid(net_input)\n    mse = ((predicted_probabilities - np.array(labels)) ** 2).mean()\n    return (predicted_probabilities.round(4).tolist(), round(mse, 4))\nfeatures = [[0, 0], [0, 1], [1, 0], [1, 1]]\nweights = [0.7, 0.3]\nbias = 0.5"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef calculate_mse_loss(predictions: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Calculate the mean squared error loss.\"\"\"\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_losses = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        mse_loss = calculate_mse_loss(predictions, labels)\n        mse_losses.append(round(mse_loss, 4))\n        error = predictions - labels\n        derivative = sigmoid_derivative(linear_output)\n        gradient = error * derivative\n        weights_gradient = np.dot(features.T, gradient)\n        bias_gradient = np.sum(gradient)\n        weights = weights - learning_rate * weights_gradient\n        bias = bias - learning_rate * bias_gradient\n    mse_losses = [round(loss, 4) for loss in mse_losses]\n    return (weights.tolist(), round(bias, 4), mse_losses)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.1, 0.2])\ninitial_bias = 0.3\nlearning_rate = 0.5\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C.\n    \n    Parameters:\n    B (list of lists of int): The basis vectors of B as rows in a matrix.\n    C (list of lists of int): The basis vectors of C as rows in a matrix.\n    \n    Returns:\n    list of lists of float: The transformation matrix from B to C, rounded to 4 decimal places.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    B_inv = np.linalg.inv(B_array)\n    P = np.dot(C_array, B_inv)\n    P_rounded = np.round(P, decimals=4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the Singular Value Decomposition (SVD) for a 2x2 matrix A.\n    Returns U, S, V such that A \u2248 U*S*V.T.\n    \n    :param A: A 2x2 numpy array\n    :return: Tuple of U, S, V where U and V are orthogonal matrices and S is a diagonal matrix of singular values.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    S = np.diag(np.sqrt(eigenvalues))\n    V = eigenvectors\n    U = np.zeros_like(V)\n    for i in range(S.shape[0]):\n        if S[i, i] > 1e-08:\n            U[:, i] = np.dot(A, V[:, i]) / S[i, i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.T.tolist())\nA = np.array([[3, 2], [2, 6]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two numpy arrays, X and y, maintaining their correspondence.\n    \n    :param X: A 2D numpy array where rows are samples.\n    :param y: A 1D/2D numpy array where elements correspond to the labels of the samples in X.\n    :param seed: An optional seed for the random number generator to ensure reproducibility.\n    :return: Two numpy arrays (X_shuffled, y_shuffled) with the same data but in a random order.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    n_samples = X.shape[0]\n    if y.shape[0] != n_samples:\n        raise ValueError('X and y must have the same number of samples')\n    indices = np.random.permutation(n_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\ny = np.array([0, 1, 0, 1])"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Iterates over numpy arrays X and y in batches of specified size.\n    \n    Parameters:\n    - X: numpy array of input data\n    - y: optional numpy array of target labels\n    - batch_size: size of each batch to yield\n    \n    Yields:\n    - If y is provided, yields (batch_X, batch_y) pairs as lists.\n    - If y is not provided, yields batch_X as lists.\n    \"\"\"\n    if y is not None:\n        assert X.shape[0] == y.shape[0], 'X and y must have the same number of samples'\n        if y.ndim == 1:\n            y = y[:, np.newaxis]\n        data = np.hstack((X, y))\n        for i in range(0, data.shape[0], batch_size):\n            batch = data[i:i + batch_size]\n            (batch_X, batch_y) = np.hsplit(batch, [-1])\n            yield (batch_X.tolist(), batch_y.flatten().tolist())\n    else:\n        for i in range(0, X.shape[0], batch_size):\n            yield X[i:i + batch_size].tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset X based on whether the feature at index feature_i is greater than or equal to the threshold.\n\n    Parameters:\n    - X: numpy.ndarray, the dataset to be divided, consisting of samples and features.\n    - feature_i: int, the index of the feature to divide the dataset with.\n    - threshold: float, the threshold value to compare the feature against.\n\n    Returns:\n    - Two lists containing the subset of the dataset where the feature is greater than or equal to the threshold,\n      and the subset where the feature is less than the threshold, respectively.\n    \"\"\"\n    above_threshold = X[:, feature_i] >= threshold\n    above = X[above_threshold].tolist()\n    below = X[~above_threshold].tolist()\n    return (above, below)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features\n    with degree less than or equal to the specified degree. For example, if an input sample is\n    two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    - X: numpy array of shape [n_samples, n_features]\n    - degree: integer, the degree of the polynomial features\n    \n    Returns:\n    - numpy array containing the new features after expanding with polynomial features\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    new_features = []\n    for deg in range(degree + 1):\n        for indices in combinations_with_replacement(range(n_features), deg):\n            if indices:\n                new_feature = np.prod(X[:, indices], axis=1)\n                new_features.append(new_feature)\n    new_features.insert(0, np.ones(n_samples))\n    return np.column_stack(new_features).tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of n_subsets random subsets of the dataset.\n    \n    Parameters:\n    X (numpy.ndarray): The feature matrix of the dataset.\n    y (numpy.ndarray): The labels of the dataset.\n    n_subsets (int): The number of subsets to generate.\n    replacements (bool): Whether sampling should be with or without replacements.\n    seed (int): The seed for the random number generator.\n    \n    Returns:\n    list: A list of tuples, each representing a random subset of the dataset. \n    \"\"\"\n    rng = np.random.default_rng(seed)\n    all_indices = np.arange(X.shape[0])\n    subsets = []\n    for _ in range(n_subsets):\n        if replacements:\n            indices = rng.choice(all_indices, size=all_indices.shape, replace=True)\n        else:\n            indices = rng.permutation(all_indices)\n        X_subset = X[indices].tolist()\n        y_subset = y[indices].tolist()\n        subsets.append((X_subset, y_subset))\n    return subsets\nn_subsets = 3"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers to a 2D numpy array with one-hot encoding.\n    \n    Parameters:\n    x (numpy.array): A 1D numpy array of integers.\n    n_col (int, optional): The number of columns for the one-hot encoding. If None, it will be determined by the max value in x.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded matrix.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([1, 2, 3, 2, 0])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x: A 1D numpy array representing the diagonal elements.\n    \n    Returns:\n    - A 2D numpy array representing the diagonal matrix converted from the 1D array.\n    \"\"\"\n    x = np.array(x)\n    diag_matrix = np.diag(x)\n    return diag_matrix.tolist()\nx = np.array([1, 2, 3])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (np.array): 1D array containing the true labels.\n    y_pred (np.array): 1D array containing the predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The lengths of y_true and y_pred must match.')\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 2, 0, 2, 1])\ny_pred = np.array([0, 2, 2, 0, 2, 1])"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array, dataset.\n    Y (np.ndarray, optional): 2D numpy array, another dataset to calculate the correlation with X.\n    \n    Returns:\n    list: The correlation matrix as a list of lists, with values rounded to 4 decimal places.\n    \"\"\"\n    X = np.array(X)\n    if Y is not None:\n        Y = np.array(Y)\n        mean_X = np.mean(X, axis=0)\n        mean_Y = np.mean(Y, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        std_Y = np.std(Y, axis=0, ddof=1)\n        X = (X - mean_X) / std_X\n        Y = (Y - mean_Y) / std_Y\n        correlation_matrix = np.dot(X.T, Y) / (X.shape[0] - 1)\n    else:\n        mean_X = np.mean(X, axis=0)\n        std_X = np.std(X, axis=0, ddof=1)\n        X = (X - mean_X) / std_X\n        correlation_matrix = np.dot(X.T, X) / (X.shape[0] - 1)\n    return np.round(correlation_matrix, 4).tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the AdaBoost classifier fit method.\n    \n    Parameters:\n    X (numpy.ndarray): The training data, shape (n_samples, n_features).\n    y (numpy.ndarray): The target labels, shape (n_samples,).\n    n_clf (int): The number of classifiers to use.\n    \n    Returns:\n    list: A list of dictionaries, each containing the classifier parameters.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.full(n_samples, 1 / n_samples)\n    clfs = []\n    for _ in range(n_clf):\n        best = {'clf': None, 'error': 1, 'alpha': -1}\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.ones(n_samples)\n                pred[X[:, feature] < threshold] = -1\n                error = np.sum(w[y != pred])\n                if error < best['error']:\n                    best['clf'] = {'feature': feature, 'threshold': threshold}\n                    best['error'] = error\n        clf = best['clf']\n        pred = np.ones(n_samples)\n        pred[X[:, clf['feature']] < clf['threshold']] = -1\n        best['alpha'] = 0.5 * np.log((1 - best['error']) / best['error'])\n        w *= np.exp(-best['alpha'] * y * pred)\n        w /= np.sum(w)\n        clfs.append({'feature': clf['feature'], 'threshold': clf['threshold'], 'alpha': np.round(best['alpha'], 4)})\n    return clfs\nn_clf = 2"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Compute the log-softmax of a given list of scores.\n    \n    Parameters:\n    scores (list): A list of numerical scores.\n    \n    Returns:\n    list: The log-softmax of the scores, rounded to the nearest 4th decimal.\n    \"\"\"\n    scores_np = np.array(scores)\n    log_softmax_values = scores_np - np.max(scores_np)\n    log_softmax_values = log_softmax_values - np.log(np.sum(np.exp(log_softmax_values)))\n    log_softmax_values = np.around(log_softmax_values, decimals=4)\n    return log_softmax_values.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport math\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDClassifier\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.input_shape = input_shape\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            grad_W = self.layer_input.T.dot(accum_grad)\n            self.W = self.optimizer_W.update(self.W, grad_W)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)\nX_train = scaler.fit_transform(X_train)\noutput = dense_layer.forward_pass(X_train[0:5], training=True)\naccum_grad = np.ones_like(output)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution between an input matrix and a convolutional kernel.\n    \n    :param input_matrix: 2D numpy array representing the input matrix.\n    :param kernel: 2D numpy array representing the convolutional kernel.\n    :param padding: Integer value for padding the input matrix.\n    :param stride: Integer value for stride size during convolution.\n    :return: 2D numpy array representing the resultant feature map.\n    \"\"\"\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (in_height, in_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    out_height = (in_height - kernel_height + 2 * padding) // stride + 1\n    out_width = (in_width - kernel_width + 2 * padding) // stride + 1\n    output = np.zeros((out_height, out_width))\n    for y in range(0, out_height):\n        for x in range(0, out_width):\n            output[y, x] = np.sum(padded_input[y * stride:y * stride + kernel_height, x * stride:x * stride + kernel_width] * kernel)\n    output = np.round(output, 4)\n    return output.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 0\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A single input value.\n    \n    Returns:\n    float: The input value if it's greater than 0, otherwise 0.\n    \"\"\"\n    return max(0.0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Function to calculate Ridge Regression loss.\n    \n    Parameters:\n    X (np.ndarray): 2D array, the feature matrix.\n    w (np.ndarray): 1D array, the coefficients.\n    y_true (np.ndarray): 1D array, the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The calculated Ridge loss.\n    \"\"\"\n    predictions = X @ w\n    mse = np.mean((predictions - y_true) ** 2)\n    regularization = 0.5 * alpha * w.T @ w\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value to the Leaky ReLU function.\n    - alpha (float): The slope of the function for z < 0. Default is 0.01.\n    \n    Returns:\n    - float: The output of the Leaky ReLU function, which is z if z >= 0, and alpha*z if z < 0.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Args:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n    \n    Returns:\n    float: The linear kernel value which is the dot product of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision, given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: Precision value.\n    \n    Raises:\n    ValueError: If the input arrays are not of the same length.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must be the same length.')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    else:\n        return true_positives / predicted_positives"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Implements a flexible gradient descent function that can perform SGD, Batch GD, and Mini-batch GD.\n    \n    Parameters:\n    X: Feature matrix with shape (n_samples, n_features) including a column of ones for the intercept.\n    y: Target vector with shape (n_samples,).\n    weights: Initial weights array of shape (n_features,).\n    learning_rate: The step size at each iteration while moving toward a minimum of a loss function.\n    n_iterations: Number of iterations of the algorithm.\n    batch_size: Number of samples to work through before updating the internal model parameters.\n    method: A string indicating which variant of gradient descent to use: 'batch', 'stochastic', or 'mini-batch'.\n    \n    Returns:\n    weights: The final weights array after completing the specified number of iterations.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    if method == 'batch':\n        batch_size = n_samples\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index + 1]\n                yi = y[random_index:random_index + 1]\n                gradient = 2 * xi.T.dot(xi.dot(weights) - yi)\n                weights = weights - learning_rate * gradient\n        elif method == 'mini-batch':\n            indices = np.random.permutation(n_samples)\n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            for i in range(0, n_samples, batch_size):\n                X_i = X_shuffled[i:i + batch_size]\n                y_i = y_shuffled[i:i + batch_size]\n                gradient = 2 * X_i.T.dot(X_i.dot(weights) - y_i)\n                weights = weights - learning_rate * gradient\n        else:\n            predictions = X.dot(weights)\n            error = predictions - y\n            gradient = 2 * X.T.dot(error) / n_samples\n            weights = weights - learning_rate * gradient\n    return np.round(weights, 4).tolist()\nX = np.c_[np.ones((100, 1)), np.random.rand(100, 2)]\ny = np.dot(X, np.array([1, 2, 3])) + np.random.randn(100)\nweights = np.zeros(3)\nlearning_rate = 0.01\nn_iterations = 1000\nbatch_size = 32"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix)\n    (num_rows, num_cols) = matrix.shape\n    r = 0\n    c = 0\n    while r < num_rows and c < num_cols:\n        swap_row = r\n        for i in range(r + 1, num_rows):\n            if abs(matrix[i, c]) > abs(matrix[swap_row, c]):\n                swap_row = i\n        if matrix[swap_row, c] != 0:\n            matrix[[r, swap_row]] = matrix[[swap_row, r]]\n            pivot = matrix[r, c]\n            matrix[r, :] /= pivot\n            for i in range(num_rows):\n                if i != r:\n                    factor = matrix[i, c]\n                    matrix[i, :] -= factor * matrix[r, :]\n            r += 1\n            c += 1\n        else:\n            c += 1\n    return matrix.tolist()\nmatrix = [[2, 1, -1, 3], [-3, -1, 2, -1], [-2, 1, 2, -1]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        mt = m / (1 - beta1 ** t)\n        vt = v / (1 - beta2 ** t)\n        x = x - learning_rate * mt / (np.sqrt(vt) + epsilon)\n    return x.round(4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implementation of Lasso Regression using Gradient Descent.\n\n    Parameters:\n    -----------\n    X : np.array\n        Input features.\n    y : np.array\n        Target values.\n    alpha : float, optional\n        Regularization parameter. The default is 0.1.\n    learning_rate : float, optional\n        Learning rate for gradient descent. The default is 0.01.\n    max_iter : int, optional\n        Maximum number of iterations for gradient descent. The default is 1000.\n    tol : float, optional\n        Tolerance for stopping criteria. The default is 1e-4.\n\n    Returns:\n    --------\n    tuple\n        A tuple containing the weights and bias.\n    \"\"\"\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    prev_cost = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        loss = y_pred - y\n        gradient_w = 1 / m * np.dot(X.T, loss) + alpha * np.sign(w)\n        gradient_b = 1 / m * np.sum(loss)\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        cost = 1 / (2 * m) * np.sum(loss ** 2) + alpha / m * np.sum(np.abs(w))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (np.round(w, 4).tolist(), np.round(b, 4).tolist())"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    - source (str): The source string.\n    - target (str): The target string.\n    \n    Returns:\n    - int: The minimum number of edits required to convert the source into the target string.\n    \"\"\"\n    matrix = np.zeros((len(source) + 1, len(target) + 1), dtype=int)\n    for i in range(len(source) + 1):\n        matrix[i][0] = i\n    for j in range(len(target) + 1):\n        matrix[0][j] = j\n    for i in range(1, len(source) + 1):\n        for j in range(1, len(target) + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            substitution_cost = matrix[i - 1][j - 1] + cost\n            deletion_cost = matrix[i - 1][j] + 1\n            insertion_cost = matrix[i][j - 1] + 1\n            transposition_cost = float('inf')\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                transposition_cost = matrix[i - 2][j - 2] + cost\n            matrix[i][j] = min(substitution_cost, deletion_cost, insertion_cost, transposition_cost)\n    return matrix[len(source)][len(target)]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall of a binary classification model.\n    \n    Parameters:\n    y_true (list of int): List of true binary labels.\n    y_pred (list of int): List of predicted binary labels.\n    \n    Returns:\n    float: The recall value rounded to 3 decimal places. Returns 0.0 if TP + FN == 0.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_val = TP / (TP + FN)\n    return round(recall_val, 3)\ny_true = [0, 1, 0, 1, 0, 1]\ny_pred = [0, 0, 1, 1, 0, 1]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    Args:\n    X: Input sequence represented as 2D numpy array of shape (sequence_length, input_dim).\n    W_q: Query matrix of shape (input_dim, dim_k).\n    W_k: Key matrix of shape (input_dim, dim_k).\n    W_v: Value matrix of shape (input_dim, dim_v).\n    \n    Returns:\n    The self-attention output as a list after reshaping into (sequence_length, dim_v) and rounding to 4 decimal places.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T)\n    dim_k = W_k.shape[1]\n    scaled_scores = attention_scores / np.sqrt(dim_k)\n    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n    self_attn_output = np.dot(attention_weights, V)\n    return np.round(self_attn_output, 4).tolist()\nX = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0]])\nW_v = np.array([[0.3, 0.4, 0.5], [0.6, 0.7, 0.8], [0.9, 1.0, 1.1]])"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements the forward pass of an RNN cell with tanh activation.\n\n    Parameters:\n    input_sequence (list of list of float): List of input vectors for each time step.\n    initial_hidden_state (list of float): The initial hidden state.\n    Wx (list of list of float): Weight matrix for input-to-hidden connections.\n    Wh (list of list of float): Weight matrix for hidden-to-hidden connections.\n    b (list of float): Bias vector for the hidden state updates.\n    \n    Returns:\n    list of float: The final hidden state after processing the entire sequence, rounded to 4 decimals.\n    \"\"\"\n    input_sequence = np.array(input_sequence, dtype=np.float32)\n    initial_hidden_state = np.array(initial_hidden_state, dtype=np.float32)\n    Wx = np.array(Wx, dtype=np.float32)\n    Wh = np.array(Wh, dtype=np.float32)\n    b = np.array(b, dtype=np.float32)\n    h = initial_hidden_state\n    for x_t in input_sequence:\n        h = np.tanh(Wx @ x_t + Wh @ h + b)\n    return np.round(h, 4).tolist()\ninput_sequence = [[1, 0, 2], [0, 1, 3], [2, 1, 1]]\ninitial_hidden_state = [0, 0, 0]\nWx = [[0.1, 0.2, 0.3], [0.1, 0.2, 0.3], [0.1, 0.2, 0.3]]\nWh = [[0.4, 0.5, 0.6], [0.4, 0.5, 0.6], [0.4, 0.5, 0.6]]\nb = [0.1, 0.1, 0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translate a list of 2D points by a specified distance in the x and y directions.\n    \n    Parameters:\n    points (list of list): A list of [x, y] coordinates.\n    tx (float): The translation distance in the x direction.\n    ty (float): The translation distance in the y direction.\n    \n    Returns:\n    list: The list of translated [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_array = np.hstack((points_array, ones))\n    translated_points = np.dot(translation_matrix, points_array.T).T\n    return translated_points[:, :2].tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution.\n    sigma_p (float): Standard deviation of the first normal distribution.\n    mu_q (float): Mean of the second normal distribution.\n    sigma_q (float): Standard deviation of the second normal distribution.\n    \n    Returns:\n    float: KL divergence between the two distributions.\n    \"\"\"\n    term1 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term2 = np.log(sigma_q / sigma_p)\n    kl_div = term1 + term2 - 0.5\n    return kl_div\nmu_p = 1.0\nsigma_p = 1.0\nmu_q = 0.0\nsigma_q = 1.0"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Implements the Gauss-Seidel method for solving a system of linear equations.\n    \n    Parameters:\n    - A: np.array: A square matrix of coefficients.\n    - b: np.array: The right-hand side vector.\n    - n: int: Number of iterations.\n    - x_ini: np.array, optional: Initial guess for the solution vector.\n    \n    Returns:\n    - x: list: The approximated solution vector after n iterations.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square')\n    if x_ini is None:\n        x = np.zeros_like(b, dtype='float')\n    else:\n        x = x_ini.astype(float)\n    n_eq = A.shape[0]\n    for it_count in range(n):\n        for i in range(n_eq):\n            s = 0\n            for j in range(n_eq):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x[i] = (b[i] - s) / A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 20\nx = gauss_seidel(A, b, n)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian Elimination with partial pivoting\n    \n    Parameters:\n    A : np.ndarray\n        Coefficient matrix of the system\n    b : np.ndarray\n        Constant terms of the system\n        \n    Returns:\n    x : list\n        Solution vector x, rounded to 4 decimal places\n    \"\"\"\n    n = len(A)\n    aug_matrix = np.hstack((A, b.reshape(-1, 1)))\n\n    def forward_elimination():\n        for i in range(n):\n            max_row = i + np.argmax(np.abs(aug_matrix[i:, i]))\n            aug_matrix[[i, max_row]] = aug_matrix[[max_row, i]]\n            for j in range(i + 1, n):\n                factor = aug_matrix[j, i] / aug_matrix[i, i]\n                aug_matrix[j, i:] -= factor * aug_matrix[i, i:]\n    forward_elimination()\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (aug_matrix[i, -1] - np.dot(aug_matrix[i, :-1], x)) / aug_matrix[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[2, -1, 0], [2, 1, 4], [1, 1, -1]])\nb = np.array([-2, 6, 0])"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef tanh(x):\n    return np.tanh(x)\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n        for t in range(x.shape[1]):\n            xt = x[:, t].reshape(-1, 1)\n            combined = np.vstack((hidden_state, xt))\n            ft = sigmoid(np.dot(self.Wf, combined) + self.bf)\n            it = sigmoid(np.dot(self.Wi, combined) + self.bi)\n            cct = tanh(np.dot(self.Wc, combined) + self.bc)\n            ot = sigmoid(np.dot(self.Wo, combined) + self.bo)\n            cell_state = ft * cell_state + it * cct\n            hidden_state = ot * tanh(cell_state)\n            hidden_states.append(hidden_state.round(4).tolist())\n        final_hidden_state = np.array(hidden_states[-1], dtype=np.float32).round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\ninitial_hidden_state = np.zeros((2, 1), dtype=np.float32)\ninitial_cell_state = np.zeros((2, 1), dtype=np.float32)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty.')\n    num_docs = len(corpus)\n    query = set(query)\n    tf_scores = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        if doc_word_count == 0:\n            tf_scores.append({})\n        else:\n            tf = {}\n            for word in doc:\n                if word in query:\n                    tf[word] = tf.get(word, 0) + 1 / doc_word_count\n            tf_scores.append(tf)\n    idf_scores = {}\n    for q_word in query:\n        df = sum((q_word in doc for doc in corpus))\n        idf_scores[q_word] = math.log((1 + num_docs) / (1 + df))\n    tf_idf_scores = []\n    for doc_tf in tf_scores:\n        tf_idf = {}\n        for q_word in query:\n            tf_idf[q_word] = doc_tf.get(q_word, 0) * idf_scores[q_word]\n        tf_idf_scores.append(list(tf_idf.values()))\n    return np.round(np.array(tf_idf_scores, dtype=float), 4).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    bb = beta ** 2\n    fscore = (1 + bb) * (precision * recall) / (bb * precision + recall) if precision + recall > 0 else 0\n    return round(fscore, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        return x * (1 - x)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        h = np.zeros((self.hidden_size, 1))\n        last_inputs = []\n        last_hiddens = []\n        outputs = []\n        for t in range(len(x)):\n            h = np.tanh(np.dot(self.W_xh, x[t]) + np.dot(self.W_hh, h) + self.b_h)\n            y = self.sigmoid(np.dot(self.W_hy, h) + self.b_y)\n            last_inputs.append(x[t])\n            last_hiddens.append(h)\n            outputs.append(y)\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN with respect to the sequence of inputs and expected output.\n        \"\"\"\n        dh_next = np.zeros_like(last_hiddens[0])\n        (dW_xh, dW_hh, dW_hy) = (np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy))\n        (db_h, db_y) = (np.zeros_like(self.b_h), np.zeros_like(self.b_y))\n        for t in reversed(range(len(input_sequence))):\n            h = last_hiddens[t]\n            y = outputs[t]\n            x = last_inputs[t]\n            dy = y - expected_output[t]\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dhrec = (1 - h ** 2) * dh\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n            dW_xh += np.dot(dhrec, x.reshape(1, -1))\n            dW_hh += np.dot(dhrec, last_hiddens[t - 1].T) if t > 0 else np.zeros_like(self.W_hh)\n            db_h += dhrec\n            dh_next = np.dot(self.W_hh.T, dhrec)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x (rounded to 8 decimal places and converted to a list)\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros(len(b))\n    x = np.array(x0)\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, decimals=8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    total_samples = len(y)\n    if total_samples == 0:\n        return 0.0\n    (classes, counts) = np.unique(y, return_counts=True)\n    class_frequencies = counts / total_samples\n    gini = 1.0 - np.sum(class_frequencies ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        return ([], [], [])\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col, item) in enumerate(row):\n            if item != 0:\n                values.append(item)\n                col_indices.append(col)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, col_indices, row_pointers[:-1])"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v_np = np.array(v)\n    L_np = np.array(L)\n    unit_L = L_np / np.linalg.norm(L_np)\n    proj = np.dot(v_np, unit_L) * unit_L\n    proj_list = [round(x, 3) for x in proj.tolist()]\n    return proj_list"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\ndense_matrix = [[1, 0, 0], [0, 2, 0], [3, 0, 4]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the column space (image) of a given matrix A.\n    \n    Args:\n    - A (numpy.ndarray): The input matrix.\n    \n    Returns:\n    - numpy.ndarray: The basis vectors (from the original matrix) that span the column space of A.\n    \"\"\"\n    A = A.astype('float')\n    (h, w) = A.shape\n    lead = 0\n    row_count = h\n    for r in range(row_count):\n        if lead >= w:\n            break\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if i == row_count:\n                i = r\n                lead += 1\n                if lead == w:\n                    break\n        A[[i, r]] = A[[r, i]]\n        lv = A[r, lead]\n        A[r] = A[r] / lv\n        for i in range(row_count):\n            if i != r:\n                lv = A[i, lead]\n                A[i] = A[i] - lv * A[r]\n        lead += 1\n    independent_columns = [col for col in range(w) if np.any(A[:, col] == 1)]\n    basis_vectors = A[:, independent_columns]\n    return np.round(basis_vectors, 8).tolist()\nA = np.array([[1, -2, 3, -4, 5], [2, -3, 1, 7, -1], [3, -4, 2, 8, 1]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) between the true and predicted values.\n    \n    Parameters:\n    y_true (np.array): Array of true values.\n    y_pred (np.array): Array of predicted values.\n    \n    Returns:\n    float: Rounded R-squared value.\n    \"\"\"\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented as a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): 2D matrix representing the image.\n    \n    Returns:\n    float: Average brightness of the image rounded to two decimal places, or -1 for invalid input.\n    \"\"\"\n    if not img or not all((len(row) == len(img[0]) for row in img)):\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate and return the Root Mean Square Error (RMSE) between the two numpy arrays\n    representing true values and predicted values. The RMSE is rounded to three decimal places.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have mismatched shapes, are empty, or not arrays.\n    TypeError: If input types are not numpy arrays.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_diffs = (y_true - y_pred) ** 2\n    rmse_value = np.sqrt(np.mean(squared_diffs))\n    return round(rmse_value, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index for given true labels and predicted labels.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 0.0\n    else:\n        jaccard = intersection / union\n        return round(float(jaccard), 3)\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.ndarray): 1D array of true binary labels.\n    y_pred (np.ndarray): 1D array of predicted binary labels.\n    \n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    numerator = 2 * np.sum(y_true * y_pred)\n    denominator = np.sum(y_true) + np.sum(y_pred)\n    if denominator == 0:\n        return float('nan')\n    dice = numerator / denominator\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_binary_hv(seed, dim):\n    \"\"\"Create a binary hypervector using the given seed.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\ndef bind_hv(hv1, hv2):\n    \"\"\"Element-wise multiplication of two hypervectors to bind them.\"\"\"\n    return hv1 * hv2\ndef bundle_hvs(hvs):\n    \"\"\"Bundle a list of hypervectors through element-wise sum and normalization.\"\"\"\n    bundled = np.sum(hvs, axis=0)\n    return np.sign(bundled)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a dataset row.\"\"\"\n    feature_hvs = []\n    for (feature, value) in row.items():\n        feature_name_hv = create_binary_hv(hash(feature), dim)\n        feature_value_hv = create_binary_hv(random_seeds[feature], dim)\n        bound_hv = bind_hv(feature_name_hv, feature_value_hv)\n        feature_hvs.append(bound_hv)\n    composite_hv = bundle_hvs(feature_hvs)\n    return composite_hv.tolist()\ndim = 10\nrow = {'age': 30, 'income': 50000, 'height': 175}\nrandom_seeds = {'age': 3, 'income': 22, 'height': 101}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    :param data: list of lists where each inner list is [y_true, y_pred]\n    :return: A 2x2 confusion matrix represented as a list of lists.\n    \"\"\"\n    states = {'TP': (1, 1), 'TN': (0, 0), 'FP': (0, 1), 'FN': (1, 0)}\n    counts = Counter((tuple(item) for sublist in data for item in [tuple(sublist)]))\n    TP = counts[states['TP']]\n    TN = counts[states['TN']]\n    FP = counts[states['FP']]\n    FN = counts[states['FN']]\n    matrix = [[TN, FP], [FN, TP]]\n    return matrix\ndata = [[1, 0], [0, 0], [1, 1], [1, 1], [1, 1], [1, 0], [0, 1]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: numpy array representing the first vector.\n    - v2: numpy array representing the second vector.\n    \n    Returns:\n    - The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Input vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot be of zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError('The actual and predicted lists must have the same length.')\n    if not all((x in [0, 1] for x in actual)) or not all((x in [0, 1] for x in predicted)):\n        raise ValueError('All elements in the actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = np.zeros((2, 2), dtype=int)\n    for (true, pred) in zip(actual, predicted):\n        confusion_matrix[true, pred] += 1\n    true_pos = confusion_matrix[1, 1]\n    true_neg = confusion_matrix[0, 0]\n    false_pos = confusion_matrix[0, 1]\n    false_neg = confusion_matrix[1, 0]\n    accuracy = round(float((true_pos + true_neg) / len(actual)), 3)\n    precision = round(float(true_pos / (true_pos + false_pos)), 3) if true_pos + false_pos > 0 else 0\n    recall = round(float(true_pos / (true_pos + false_neg)), 3) if true_pos + false_neg > 0 else 0\n    f1_score = round(2 * (precision * recall / (precision + recall)), 3) if precision + recall > 0 else 0\n    specificity = round(float(true_neg / (true_neg + false_pos)), 3) if true_neg + false_pos > 0 else 0\n    negative_predictive_value = round(float(true_neg / (true_neg + false_neg)), 3) if true_neg + false_neg > 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 0, 1, 0, 0, 1]\npredicted = [1, 0, 0, 0, 1, 1, 0, 1]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculate various descriptive statistics for the provided data.\n    \n    Parameters:\n    data (list or numpy.ndarray): A list or array of numerical data.\n    \n    Returns:\n    dict: A dictionary of descriptive statistics for the data.\n    \"\"\"\n    data = np.array(data) if not isinstance(data, np.ndarray) else data\n    statistics = {}\n    statistics['mean'] = round(np.mean(data), 4)\n    statistics['median'] = round(np.median(data), 4)\n    statistics['mode'] = round(float(stats.mode(data)[0]), 4) if len(data) > 1 else float(stats.mode(data)[0])\n    statistics['variance'] = round(np.var(data, ddof=1), 4)\n    statistics['standard_deviation'] = round(np.std(data, ddof=1), 4)\n    statistics['25th_percentile'] = round(np.percentile(data, 25), 4)\n    statistics['50th_percentile'] = round(np.percentile(data, 50), 4)\n    statistics['75th_percentile'] = round(np.percentile(data, 75), 4)\n    statistics['interquartile_range'] = round(np.percentile(data, 75) - np.percentile(data, 25), 4)\n    return statistics"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value of the normal distribution at x, rounded to 5 decimal places.\n    \"\"\"\n    power = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = 1 / denominator * math.exp(power)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    \n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events (float rounded to 5 decimal places)\n    \"\"\"\n    factorial_k = math.factorial(k)\n    e_power = math.exp(-lam)\n    lam_power_k = lam ** k\n    probability = lam_power_k * e_power / factorial_k\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input image must be a 2D numpy array.')\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\nimg = np.array([[20, 50, 30], [40, 70, 60], [80, 100, 90]])\ncontrast = calculate_contrast(img)"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)\nvec1 = np.array([1, 2, 3])\nvec2 = np.array([4, 5, 6])"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    transformed_data = []\n    for x in data:\n        transformed_features = [x ** i for i in range(1, degree + 1)]\n        transformed_data.append(transformed_features)\n    return np.round(transformed_data, 8).tolist()"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate the positional encoding for a Transformer model.\n    \n    :param position: Length of the sequence (int)\n    :param d_model: Model dimensionality (int)\n    :return: If position or d_model is invalid, returns -1. Otherwise, returns a list of positional encodings.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n    for pos in range(position):\n        pe[pos, 0::2] = np.sin(pos * div_term)\n        pe[pos, 1::2] = np.cos(pos * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())\nparameter = np.array([0.5])\ngrad = np.array([0.1])\nm = np.array([0.0])\nv = np.array([0.0])\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [{'attn': {'c_attn': np.random.rand(10, 30), 'c_proj': np.random.rand(30, 10)}, 'ln_1': {'g': np.ones(10), 'b': np.zeros(10)}, 'mlp': {'c_fc': np.random.rand(10, 20), 'c_proj': np.random.rand(20, 10)}, 'ln_2': {'g': np.ones(10), 'b': np.zeros(10)}}] * 2, 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\ndef attention(q, k, v):\n    scores = np.matmul(q, k.T) / np.sqrt(k.shape[-1])\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    return np.matmul(weights, v)\ndef feed_forward(x, c_fc, c_proj):\n    return np.matmul(np.maximum(0, np.matmul(x, c_fc)), c_proj)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_token_ids = encoder.encode(prompt)\n    output = []\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][input_token_ids]\n        pos_embeddings = params['wpe'][np.arange(len(input_token_ids))]\n        x = token_embeddings + pos_embeddings\n        for block in params['blocks']:\n            attn_in = x\n            qkv = np.matmul(x, block['attn']['c_attn'])\n            (q, k, v) = np.split(qkv, [10, 20], axis=-1)\n            attn_out = attention(q, k, v)\n            attn_out = layer_norm(attn_out + x, block['ln_1']['g'], block['ln_1']['b'])\n            x = attn_out\n            mlp_in = attn_out\n            mlp_out = feed_forward(mlp_in, block['mlp']['c_fc'], block['mlp']['c_proj'])\n            x = layer_norm(mlp_out + attn_out, block['ln_2']['g'], block['ln_2']['b'])\n        x = layer_norm(x, params['ln_f']['g'], params['ln_f']['b'])\n        last_token = x[-1]\n        new_token_id = np.argmax(last_token)\n        output.append(new_token_id)\n        input_token_ids = input_token_ids[1:] + [new_token_id]\n    return encoder.decode(output)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    crystals = np.array(crystal_values).reshape(n, dimension)\n    weights = np.random.rand(n, dimension)\n    weighted_crystals = np.tanh(np.dot(crystals, weights.T))\n    attention_scores = np.dot(weighted_crystals, weighted_crystals.T)\n    attention_weights = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_weights, weighted_crystals)\n    final_patterns = np.round(weighted_patterns.flatten(), 4)\n    return final_patterns.tolist()\nn = 4\ndimension = 1"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate the BM25 scores for documents in the corpus based on the query.\n\n    :param corpus: List of documents, where each document is a list of terms.\n    :param query: A list of terms representing the query.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    corpus = [Counter(doc) for doc in corpus]\n    query = Counter(query)\n    avgdl = np.mean([sum(doc.values()) for doc in corpus])\n    N = len(corpus)\n    doc_len = np.array([sum(doc.values()) for doc in corpus])\n    idf = {}\n    for term in query:\n        df = sum((1 for doc in corpus if doc.get(term, 0) > 0))\n        idf[term] = np.log((N - df + 0.5) / (df + 0.5))\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = doc[term]\n                score += idf[term] * (tf * (k1 + 1) / (tf + k1 * (1 - b + b * (doc_len / avgdl))))\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['information', 'retrieval', 'system'], ['information', 'retrieval', 'system', 'design'], ['design', 'information', 'retrieval'], ['machine', 'learning', 'and', 'information', 'retrieval']]\nquery = ['information', 'retrieval']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    y_true_set = set(y_true)\n    y_pred_set = set(y_pred)\n    tp = len(y_true_set.intersection(y_pred_set))\n    fp = len(y_pred_set - y_true_set)\n    fn = len(y_true_set - y_pred_set)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 2, 3, 4, 5]\ny_pred = [2, 4, 6, 8, 10]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    This function takes a list of daily power consumption data and predicts the power consumption for day 15,\n    after removing the daily fluctuation and fitting a linear regression model on the detrended data.\n    A 5% safety margin is added to the prediction, and the final result is an integer.\n    \"\"\"\n    fluctuation = np.array([10 * math.sin(2 * PI * i / 10) for i in range(1, len(consumption_data) + 1)])\n    detrended_data = np.array(consumption_data) - fluctuation\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_prediction = model.predict(day_15)[0]\n    fluctuation_15 = 10 * math.sin(2 * PI * 15 / 10)\n    final_prediction = base_prediction + fluctuation_15\n    final_prediction = math.ceil(final_prediction * 1.05)\n    return final_prediction\nconsumption_data = [120, 125, 130, 135, 138, 140, 143, 145, 150, 155]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError('Both y_true and y_pred should be of type numpy.ndarray.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred should have the same shape.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the query, key, and value matrices.\n    :param X: Input tensor of shape (batch_size, seq_len, d_model).\n    :param W_q: Weight matrix for query of shape (d_model, d_k).\n    :param W_k: Weight matrix for key of shape (d_model, d_k).\n    :param W_v: Weight matrix for value of shape (d_model, d_v).\n    :return: A tuple of query, key, and value matrices each of shape (batch_size, seq_len, d_k or d_v).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes self-attention using the query, key, and value matrices.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: A tensor of shape (batch_size, seq_len, d_v).\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention = np.dot(np.softmax(scores, axis=-1), V)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism.\n    :param X: Input tensor of shape (batch_size, seq_len, d_model).\n    :param W_q: Weight matrix for query of shape (n_heads, d_model, d_k).\n    :param W_k: Weight matrix for key of shape (n_heads, d_model, d_k).\n    :param W_v: Weight matrix for value of shape (n_heads, d_model, d_v).\n    :param n_heads: The number of attention heads.\n    :return: A python list of the multi-head attention output rounded to the nearest 4th decimal.\n    \"\"\"\n    (batch_size, seq_len, _) = X.shape\n    head_dim = W_q.shape[-1]\n    d_v = W_v.shape[-1]\n    Q_heads = []\n    K_heads = []\n    V_heads = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        Q_heads.append(Q)\n        K_heads.append(K)\n        V_heads.append(V)\n    attention_heads = []\n    for i in range(n_heads):\n        attention_head = self_attention(Q_heads[i], K_heads[i], V_heads[i])\n        attention_heads.append(attention_head)\n    attention = np.concatenate(attention_heads, axis=-1).reshape(batch_size, seq_len, n_heads, d_v)\n    attention = np.mean(attention, axis=2)\n    attention_rounded = np.round(attention, decimals=4).tolist()\n    return attention_rounded\nX = np.random.rand(1, 5, 128)\nW_q = np.random.rand(8, 128, 16)\nW_k = np.random.rand(8, 128, 16)\nW_v = np.random.rand(8, 128, 16)\nn_heads = 8"}
{"task_id": 95, "completion_id": 0, "solution": "from typing import List\nimport numpy as np\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x)\n    y_array = np.array(y)\n    n11 = np.sum(x_array * y_array)\n    n00 = np.sum((1 - x_array) * (1 - y_array))\n    n10 = np.sum(x_array * (1 - y_array))\n    n01 = np.sum((1 - x_array) * y_array)\n    numerator = n11 * n00 - n10 * n01\n    denominator = (n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)\n    if denominator == 0:\n        return 0.0\n    phi = numerator / np.sqrt(denominator)\n    return round(phi, 4)\nx = [0, 0, 0, 1, 1, 1, 1, 1]\ny = [0, 1, 1, 0, 0, 1, 1, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to the 4th decimal place\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Softsign of the input, rounded to the 4th decimal place\n    \"\"\"\n    result = x / (1.0 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n    \n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    surrogate_objective = np.minimum(rhos * A, clipped_rhos * A)\n    kl_divergence = np.mean(np.log(pi_theta_old / pi_theta_ref))\n    grpo_value = np.mean(surrogate_objective) - beta * kl_divergence\n    return round(grpo_value, 6)\nrhos = [1.2, 0.8, 1.5, 0.9, 1.05]\nA = [0.3, 0.1, -0.8, 0.5, 0.1]\npi_theta_old = [0.7, 0.3, 0.8, 0.4, 0.5]\npi_theta_ref = [0.6, 0.2, 0.7, 0.3, 0.45]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    logits = np.dot(X, weights) + bias\n    probabilities = sigmoid(logits)\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (m, n) = X.shape\n    K = len(np.unique(y))\n    X = np.insert(X, 0, 1, axis=1)\n    B = np.zeros((n + 1, K))\n\n    def softmax(S):\n        exp_scores = np.exp(S - np.max(S, axis=1, keepdims=True))\n        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n    def to_onehot(y):\n        return np.eye(K)[y]\n    Y_onehot = to_onehot(y)\n    losses = []\n    for _ in range(iterations):\n        S = X @ B\n        predictions = softmax(S)\n        gradient = X.T @ (predictions - Y_onehot) / m\n        B -= learning_rate * gradient\n        entropy = Y_onehot * np.log(predictions + 1e-10)\n        loss = -np.sum(entropy) / m\n        losses.append(loss.round(4))\n    B = B.round(4)\n    return (B.tolist(), [loss for loss in np.round(losses, 4)])"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    intercept = np.ones((X.shape[0], 1))\n    X = np.concatenate((intercept, X), axis=1)\n    coefficients = np.zeros(X.shape[1])\n    loss_history = []\n    for i in range(iterations):\n        z = np.dot(X, coefficients)\n        predictions = 1 / (1 + np.exp(-z))\n        loss = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n        loss_history.append(round(loss, 4))\n        gradient = np.dot(X.T, predictions - y) / y.size\n        coefficients -= learning_rate * gradient\n    return (coefficients.tolist(), np.array(loss_history).tolist())"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q (np.ndarray): Query matrix.\n        K (np.ndarray): Key matrix.\n        V (np.ndarray): Value matrix.\n        mask (np.ndarray): Mask matrix, used to mask out some elements.\n        \n    Returns:\n        np.ndarray: Output of the attention mechanism after applying the mask.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T)\n    attention_scores = np.where(mask, attention_scores, -np.inf)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "from typing import List\nfrom collections import Counter\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The more evenly distributed the colors, the higher the measure of disorder.\n    \n    Args:\n    apples (List[int]): A list of integers representing the colors of apples.\n    \n    Returns:\n    float: A float value representing the disorder in the basket of apples.\n    \"\"\"\n    frequency = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in frequency.values()]\n    entropy = -sum((p * math.log(p, 2) for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies layer normalization on a 3D array X with scaling and shifting parameters.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, seq_length, feature_num)\n    - gamma: np.ndarray of scaling factors, should match feature_num\n    - beta: np.ndarray of shifting factors, should match feature_num\n    - epsilon: float, a small constant for numerical stability\n    \n    Returns:\n    - np.ndarray: Layer-normalized X, rounded to 5 decimal places and converted to list\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma * X_norm + beta\n    X_scaled_shifted_rounded = np.round(X_scaled_shifted, 5)\n    return X_scaled_shifted_rounded.tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef get_precision_recall(reference, candidate):\n    ref_counter = Counter(reference)\n    cand_counter = Counter(candidate)\n    match_counter = ref_counter & cand_counter\n    matches = sum(match_counter.values())\n    precision = matches / len(candidate) if len(candidate) > 0 else 0\n    recall = matches / len(reference) if len(reference) > 0 else 0\n    return (precision, recall)\ndef fragmentation_cost(reference, candidate, beta):\n    ref_words = set(reference)\n    cand_words = set(candidate)\n    common_words = ref_words.intersection(cand_words)\n    frag_cost = 0\n    for word in common_words:\n        ref_positions = [i for (i, x) in enumerate(reference) if x == word]\n        cand_positions = [i for (i, x) in enumerate(candidate) if x == word]\n        n_frags = abs(len(ref_positions) - len(cand_positions)) + sum((abs(a - b) for (a, b) in zip(ref_positions, cand_positions)))\n        frag_cost += n_frags\n    return frag_cost / (beta * len(reference))\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference = reference.split()\n    candidate = candidate.split()\n    (precision, recall) = get_precision_recall(reference, candidate)\n    f_mean = (1 + alpha ** 2) * precision * recall / (alpha ** 2 * precision + recall) if alpha ** 2 * precision + recall > 0 else 0\n    frag_cost = fragmentation_cost(reference, candidate, beta)\n    penalty = np.exp(-gamma * frag_cost) if frag_cost > 0 else 1\n    meteor = f_mean * penalty\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The joint occurrence count of two events.\n    total_counts_x (int): The total individual occurrence count of event x.\n    total_counts_y (int): The total individual occurrence count of event y.\n    total_samples (int): The total number of samples.\n    \n    Returns:\n    float: PMI rounded to 3 decimal places.\n    \"\"\"\n    if total_counts_x == 0 or total_counts_y == 0 or joint_counts == 0:\n        return 0.0\n    pmi = np.log2(joint_counts / total_samples / (total_counts_x / total_samples * (total_counts_y / total_samples)))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    The normalized values are rounded to 4 decimal places.\n    \n    :param x: List of integers.\n    :return: List of normalized floats.\n    \"\"\"\n    if not x or max(x) == min(x):\n        return [0.0 for _ in x]\n    minimum = min(x)\n    maximum = max(x)\n    normalized = [round((val - minimum) / (maximum - minimum), 4) for val in x]\n    return normalized"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Args:\n    x (np.ndarray): Input array of shape (batch_size, input_dim).\n    w1 (np.ndarray): Weight matrix of the first layer of shape (input_dim, hidden_dim).\n    w2 (np.ndarray): Weight matrix of the second layer of shape (hidden_dim, input_dim).\n    \n    Returns:\n    np.ndarray: Output array of the residual block after a final ReLU application, \n                rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    hidden = np.maximum(0, np.dot(x, w1))\n    out = np.dot(hidden, w2) + x\n    output = np.maximum(0, out)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels) representing feature maps.\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) where each element is the mean of the \n                values in the corresponding feature map from the input array.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> list:\n    \"\"\"\n    Performs Batch Normalization on a 4D input array X with scale (gamma) and shift (beta) parameters.\n    The input X is expected to be in BCHW format.\n    \n    Parameters:\n    X:          Input array in BCHW format (batch, channels, height, width)\n    gamma:      Scale parameter (per channel)\n    beta:       Shift parameter (per channel)\n    epsilon:    Small constant to avoid division by zero\n    \n    Returns:\n    A list representation of the normalized array rounded to 4 decimal places.\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = X_norm * gamma.reshape(1, channels, 1, 1) + beta.reshape(1, channels, 1, 1)\n    return np.round(X_scaled, decimals=4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculates the derivative of a polynomial term c * x^n at a given point x.\n\n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is evaluated.\n    n (float): The exponent of the polynomial term.\n\n    Returns:\n    float: The value of the derivative of the polynomial term at point x,\n           rounded to 4 decimal places.\n    \"\"\"\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    :param vectors: A list of 2D vectors.\n    :param tol: Tolerance to determine linear independence.\n    :return: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    matrix = np.array(vectors, dtype=float)\n    (n_vectors, dim) = matrix.shape\n    basis = []\n    for i in range(n_vectors):\n        projection = np.sum([np.dot(matrix[i], basis_j) * basis_j for basis_j in basis], axis=0)\n        new_vector = matrix[i] - projection\n        norm = np.linalg.norm(new_vector)\n        if norm > tol:\n            new_vector /= norm\n            basis.append(new_vector.tolist())\n    return [np.around(np.array(b), 4).tolist() for b in basis]\nvectors = [[3, 2], [4, 7], [2, -1]]\ntol = 1e-05\nbasis = orthonormal_basis(vectors, tol)"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    Parameters:\n    a (np.ndarray): A numpy array of at least 3 elements representing the first vector.\n    b (np.ndarray): A numpy array of at least 3 elements representing the second vector.\n    \n    Returns:\n    list: The cross product of the two vectors rounded to 4 decimal places.\n    \"\"\"\n    cross = np.cross(a, b)\n    cross_rounded = np.round(cross, 4)\n    return cross_rounded.tolist()\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): The constant vector.\n    \n    Returns:\n    list: A list of solutions if unique solution exists, else -1.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(A.shape[1]):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x_i = det_Ai / det_A\n        x.append(round(x_i, 4))\n    return np.array(x).tolist()\nA = np.array([[2, -1], [4, -2]])\nb = np.array([1, 2])"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two probability distributions.\n    \n    Args:\n    p: A list of floats representing the first probability distribution.\n    q: A list of floats representing the second probability distribution.\n    \n    Returns:\n    A float representing the Bhattacharyya distance rounded to 4 decimal places.\n    If inputs are of different lengths or empty, returns 0.0.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] or int:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Compute the policy gradient using the REINFORCE algorithm.\n    \n    Parameters:\n    theta: np.ndarray, a 2D array representing the parameters of the policy.\n    episodes: list of episodes, where each episode is a list of tuples containing (state, action, reward) elements.\n    \n    Returns:\n    np.ndarray, the average gradient of the log-policy multiplied by the return.\n    \"\"\"\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = []\n        G = 0\n        for t in range(len(episode) - 1, -1, -1):\n            G = episode[t][2] + G\n            returns.append(G)\n        returns = returns[::-1]\n        policy = softmax(theta)\n        for (t, (state, action, _)) in enumerate(episode):\n            for a in range(num_actions):\n                if a == action:\n                    policy_grad = policy[state, a] - policy[state, a] ** 2\n                else:\n                    policy_grad = -policy[state, a] * policy[state, action]\n                gradient[state, a] += returns[t] * policy_grad\n    gradient /= len(episodes)\n    gradient = np.round(gradient, decimals=4).tolist()\n    return gradient\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [[(0, 0, 1.0), (1, 1, 1.0)], [(0, 1, 1.0), (1, 0, 1.0)]]\ngradient = compute_policy_gradient(theta, episodes)"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of a Mixture-of-Experts (MoE) layer\n    compared to a dense layer. The function takes the number of experts, number\n    of active experts (sparsity), and the input/output dimensions to compute the\n    floating-point operations (FLOPs) and determine the savings percentage.\n    \n    Args:\n    n_experts (int): Total number of experts in the MoE layer.\n    k_active (int): Number of active (selected) experts.\n    d_in (int): Input dimensions.\n    d_out (int): Output dimensions.\n    \n    Returns:\n    float: The computational cost savings percentage rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * d_in * d_out * k_active / n_experts\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nn_experts = 128\nk_active = 4\nd_in = 1024\nd_out = 1024"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Computes the Noisy Top-K Gating Function.\n    \n    Parameters:\n    X (np.ndarray): The input matrix.\n    W_g (np.ndarray): The gating weight matrix.\n    W_noise (np.ndarray): The noise weight matrix.\n    N (np.ndarray): Pre-sampled noise array.\n    k (int): Sparsity constraint, the top-k elements are selected.\n    \n    Returns:\n    list: The final gating probabilities matrix as a python list.\n    \"\"\"\n    logits = X @ W_g + N * (X @ W_noise)\n    probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n    noisy_probs = probs + N\n    topk_indices = np.argpartition(-noisy_probs, k, axis=1)[:, :k]\n    topk_indices = np.sort(topk_indices, axis=1)\n    topk_probs = np.zeros_like(probs)\n    for (i, indices) in enumerate(topk_indices):\n        topk_probs[i, indices] = noisy_probs[i, indices]\n    topk_probs /= np.sum(topk_probs, axis=1, keepdims=True)\n    topk_probs = np.around(topk_probs, decimals=4).tolist()\n    return topk_probs\nX = np.random.randn(2, 5)\nW_g = np.random.randn(5, 10)\nW_noise = np.random.randn(5, 10)\nN = np.random.randn(2, 10)\nk = 3"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x: Input tensor of shape (batch_size, input_dim).\n    We: Experts weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg: Gating weight matrix of shape (input_dim, n_experts).\n    n_experts: Number of experts in the layer.\n    top_k: Number of top experts to use for each input.\n    \n    Returns:\n    np.ndarray: MoE output tensor of shape (batch_size, output_dim).\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.apply_along_axis(lambda z: np.exp(z) / np.sum(np.exp(z)), 1, gating_scores)\n    expert_inputs = np.dot(x, We.transpose(1, 0, 2))\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_expert_outputs = np.take_along_axis(expert_inputs, top_k_indices[:, :, None], axis=1)\n    top_k_gating_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_gating_probs = np.apply_along_axis(lambda z: np.exp(z) / np.sum(np.exp(z)), 1, top_k_gating_probs)\n    final_output = np.sum(top_k_expert_outputs * top_k_gating_probs[:, :, None], axis=1)\n    return np.round(final_output, 4).tolist()\nn_experts = 3\ntop_k = 2\nbatch_size = 2\nx = np.random.rand(batch_size, input_dim)\nWg = np.random.rand(input_dim, n_experts)"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    :param X: Input tensor of shape (B, C, H, W)\n    :param gamma: Scale parameter of shape (1, C, 1, 1)\n    :param beta: Shift parameter of shape (1, C, 1, 1)\n    :param num_groups: Number of groups for normalization\n    :param epsilon: Small constant for numerical stability\n    :return: Normalized tensor of shape (B, C, H, W)\n    \"\"\"\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by the number of groups.')\n    X = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_out = gamma * X_norm + beta\n    return np.round(X_out, 4).tolist()\nnum_groups = 2\nX = np.random.randn(B, C, H, W)\ngamma = np.random.randn(1, C, 1, 1)\nbeta = np.random.randn(1, C, 1, 1)"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        diff = -df(x) * learning_rate\n        if abs(diff) < tolerance:\n            break\n        x += diff\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Dynamic Tanh (DyT) function implements a normalization-free transformation inspired by the Tanh function.\n    This function is intended to replace layer normalization in Transformer architectures.\n    \n    :param x: Input array of data points.\n    :param alpha: Scaling factor for the output of the tanh function.\n    :param gamma: Scaling factor for the input.\n    :param beta: Bias term added to the scaled input.\n    :return: A list of transformed values, rounded to 4 decimal places.\n    \"\"\"\n    scaled_x = gamma * x + beta\n    dyt_vals = alpha * np.tanh(scaled_x)\n    return np.round(dyt_vals, 4).tolist()\nx = np.array([0.5, 1.0, -1.0, -0.5])\nalpha = 1.5\ngamma = 0.7\nbeta = 0.2"}
