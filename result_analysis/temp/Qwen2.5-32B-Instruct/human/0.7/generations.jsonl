{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float or -1: The resulting vector as a list or -1 if the operation is invalid.\n    \"\"\"\n    # Check if the number of columns in 'a' matches the length of 'b'\n    if not a or len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product\n    result = []\n    for row in a:\n        row_result = sum(x*y for x, y in zip(row, b))\n        result.append(row_result)\n    \n    return result\n\n# Example usage\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\nvector = [1, 2, 3]\n\nprint(matrix_dot_vector(matrix, vector))  # Expected: [14, 32, 50]\nprint(matrix_dot_vector(matrix, [1, 2]))  # Expected: -1\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transpose the given matrix.\n    \n    :param a: List of lists where each inner list represents a row in the matrix.\n    :return: The transposed matrix as a list of lists.\n    \"\"\"\n    # Using list comprehension to transpose the matrix\n    return [list(row) for row in zip(*a)]\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    transposed_matrix = transpose_matrix(matrix)\n    print(\"Original matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nTransposed matrix:\")\n    for row in transposed_matrix:\n        print(row)\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape the given matrix 'a' into the specified shape 'new_shape'.\n    If the reshape operation is not possible, return an empty list.\n    \n    :param a: List of lists representing a 2D matrix.\n    :param new_shape: A tuple representing the new shape (rows, columns).\n    :return: Reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    try:\n        # Convert the list of lists to a numpy array\n        np_array = np.array(a)\n        # Attempt to reshape the array\n        reshaped_array = np_array.reshape(new_shape)\n        # Convert the reshaped numpy array back to a list of lists\n        return reshaped_array.tolist()\n    except ValueError:\n        # If reshaping fails, return an empty list\n        return []\n\n# Check function to verify the correctness of the code\ndef check_reshape_matrix():\n    assert reshape_matrix([[1, 2], [3, 4]], (1, 4)) == [[1, 2, 3, 4]], \"Test case 1 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (4, 1)) == [[1], [2], [3], [4]], \"Test case 2 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (3, 3)) == [], \"Test case 3 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function\ncheck_reshape_matrix()\n```\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshape the given matrix 'a' into the specified shape 'new_shape'.\n    If the reshape operation is not possible, return an empty list.\n    \n    :param a: List of lists representing a 2D matrix.\n    :param new_shape: A tuple representing the new shape (rows, columns).\n    :return: Reshaped matrix as a list of lists, or an empty list if reshape is not possible.\n    \"\"\"\n    try:\n        # Convert the list of lists to a numpy array\n        np_array = np.array(a)\n        # Attempt to reshape the array\n        reshaped_array = np_array.reshape(new_shape)\n        # Convert the reshaped numpy array back to a list of lists\n        return reshaped_array.tolist()\n    except ValueError:\n        # If reshaping fails, return an empty list\n        return []\n\n# Check function to verify the correctness of the code\ndef check_reshape_matrix():\n    assert reshape_matrix([[1, 2], [3, 4]], (1, 4)) == [[1, 2, 3, 4]], \"Test case 1 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (4, 1)) == [[1], [2], [3], [4]], \"Test case 2 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (3, 3)) == [], \"Test case 3 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function\ncheck_reshape_matrix()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix by row or by column.\n    \n    :param matrix: A list of lists containing numerical values.\n    :param mode: A string indicating whether to calculate the mean by 'row' or 'column'.\n    :return: A list containing the means, either for each row or for each column, as specified by the mode.\n    \"\"\"\n    if mode.lower() not in ['row', 'column']:\n        raise ValueError(\"Mode should be either 'row' or 'column'\")\n    \n    if not all(len(row) == len(matrix[0]) for row in matrix):\n        raise ValueError(\"All rows in the matrix must have the same length\")\n    \n    if mode.lower() == 'row':\n        return [np.mean(row) for row in matrix]\n    else:\n        # Transpose the matrix to calculate column means as row means\n        transposed_matrix = np.array(matrix).T.tolist()\n        return [np.mean(col) for col in transposed_matrix]\n\n# Example usage:\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(\"Row means:\", calculate_matrix_mean(matrix, 'row'))\nprint(\"Column means:\", calculate_matrix_mean(matrix, 'column'))\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the scalar and returns the resulting matrix.\n    \n    :param matrix: A list of lists where each inner list represents a row of the matrix.\n    :param scalar: A single number (int or float) to multiply each element of the matrix by.\n    :return: A new matrix (list of lists) with each element multiplied by the scalar.\n    \"\"\"\n    # Multiply each element by the scalar and return the new matrix\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage and check function\ndef check_solution():\n    matrix1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    scalar1 = 2\n    expected1 = [[2, 4, 6], [8, 10, 12], [14, 16, 18]]\n    assert scalar_multiply(matrix1, scalar1) == expected1, \"Test case 1 failed\"\n    \n    matrix2 = [[0, 1], [2, 3]]\n    scalar2 = 3.5\n    expected2 = [[0, 3.5], [7, 10.5]]\n    assert scalar_multiply(matrix2, scalar2) == expected2, \"Test case 2 failed\"\n    \n    print(\"All test cases passed!\")\n\n# Run the check function to verify the correctness of the code\ncheck_solution()\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix.\n    \n    Args:\n    matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list of float: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    # Convert input matrix to numpy array\n    np_matrix = np.array(matrix)\n    # Calculate eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    # Sort the eigenvalues from highest to lowest\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    # Convert numpy array to list and return\n    return sorted_eigenvalues.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    example_matrix = [[4, 2], [1, 3]]\n    print(calculate_eigenvalues(example_matrix))\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Performs the matrix transformation T^{-1}AS on matrix A using numpy.\n    T and S are expected to be invertible matrices.\n    Returns -1 if T or S are not invertible.\n    \"\"\"\n    try:\n        # Convert lists to numpy arrays for easy manipulation\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n\n        # Check if T and S are invertible\n        if np.linalg.matrix_rank(T_np) != T_np.shape[0] or np.linalg.matrix_rank(S_np) != S_np.shape[0]:\n            return -1\n\n        # Calculate the inverse of T\n        T_inv = np.linalg.inv(T_np)\n\n        # Perform the transformation T^{-1}AS\n        transformed_A = T_inv @ A_np @ S_np\n\n        # Round to the nearest 4th decimal and convert to list\n        transformed_A_rounded = np.round(transformed_A, 4).tolist()\n\n        return transformed_A_rounded\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return -1\n\n# Example usage\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 5]]\nresult = transform_matrix(A, T, S)\nprint(result)\n```\n\nRemember to test the function with a variety of matrices, including cases where the matrices are not invertible, to ensure that the function behaves as expected."}
{"task_id": 8, "completion_id": 0, "completion": "```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]] or None:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list of list of float): A 2x2 matrix.\n    \n    Returns:\n    list of list of float or None: The inverse of the matrix if it is invertible, None otherwise.\n    \"\"\"\n    # Extracting coefficients from the matrix\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    \n    # Calculating the determinant\n    determinant = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None  # The matrix is not invertible\n    \n    # Calculate the inverse matrix\n    inverse_matrix = [[d / determinant, -b / determinant],\n                      [-c / determinant, a / determinant]]\n    \n    return inverse_matrix\n\n# Function to test the correctness of the inverse_2x2 function\ndef check_inverse(matrix, expected):\n    result = inverse_2x2(matrix)\n    if result == expected:\n        print(f\"Test passed: {matrix} -> {result}\")\n    else:\n        print(f\"Test failed: {matrix} -> {result} (expected: {expected})\")\n\n# Provided data points to verify the correctness\ncheck_inverse([[4, 7], [2, 6]], [[0.6, -0.7], [-0.2, 0.4]])\ncheck_inverse([[1, 2], [3, 4]], [[-2.0, 1.0], [1.5, -0.5]])\ncheck_inverse([[1, 1], [1, 1]], None)  # Not invertible\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices if their dimensions allow matrix multiplication.\n    Otherwise, returns -1.\n    \n    Args:\n    a: A 2D list representing the first matrix.\n    b: A 2D list representing the second matrix.\n    \n    Returns:\n    A 2D list representing the product of the two matrices, or -1 if the multiplication is not possible.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    \n    # Check if the matrices can be multiplied\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix_a = [[1, 2], [3, 4]]\n    matrix_b = [[2, 0], [1, 2]]\n    print(matrixmul(matrix_a, matrix_b))\n    # Expected output: [[4, 4], [10, 8]]\n    \n    matrix_c = [[1, 2, 3], [4, 5, 6]]\n    matrix_d = [[7, 8], [9, 10]]\n    print(matrixmul(matrix_c, matrix_d))\n    # Expected output: -1\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors: A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    A list of lists representing the covariance matrix.\n    \"\"\"\n    # Convert the list of lists into a numpy array for easy manipulation\n    data = np.array(vectors)\n    # Calculate the covariance matrix using numpy's cov function\n    cov_matrix = np.cov(data, bias=True)\n    # Convert the numpy array back to a list of lists and return\n    return cov_matrix.tolist()\n\n# Test cases\ndef check_covariance_matrix():\n    # Test case 1: Two features with three observations each\n    vectors1 = [[1, 2, 3], [2, 4, 6]]\n    expected1 = [[1.0, 2.0], [2.0, 4.0]]\n    assert calculate_covariance_matrix(vectors1) == expected1, f\"Test 1 Failed: Expected {expected1}, got {calculate_covariance_matrix(vectors1)}\"\n    \n    # Test case 2: Three features with four observations each\n    vectors2 = [[1, 2, 3, 4], [4, 3, 2, 1], [2, 2, 2, 2]]\n    expected2 = [[1.25, -1.25, 0.0], [-1.25, 1.25, 0.0], [0.0, 0.0, 0.0]]\n    assert calculate_covariance_matrix(vectors2) == expected2, f\"Test 2 Failed: Expected {expected2}, got {calculate_covariance_matrix(vectors2)}\"\n    \n    print(\"All tests passed.\")\n\ncheck_covariance_matrix()\n```\n\nThis script defines a function `calculate_covariance_matrix` that calculates the covariance matrix for a given set of vectors. It uses the `numpy` library for the covariance calculation due to its reliability and efficiency. The script also includes a `check_covariance_matrix` function that tests the covariance function with two sets of vectors to ensure its correctness."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Constant terms vector.\n    n (int): Number of iterations.\n    \n    Returns:\n    list: The approximate solution vector x, rounded to four decimal places.\n    \"\"\"\n    # Ensure the matrix is square\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Coefficient matrix must be square.\")\n    \n    # Ensure matrix A and vector b are of the same size\n    if A.shape[0] != b.shape[0]:\n        raise ValueError(\"Size mismatch between coefficient matrix and constants vector.\")\n    \n    # Initialize the solution vector\n    x = np.zeros_like(b, dtype=np.float64)\n    \n    # Iterate for n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i, :], x) - A[i, i] * x[i]\n            x_new[i] = (b[i] - s) / A[i, i]\n        # Round the values to 4 decimal places\n        x_new = np.round(x_new, decimals=4)\n        x = x_new\n    \n    return x.tolist()\n\n# Example use case\nif __name__ == \"__main__\":\n    A = np.array([[4, -1, 0, 0],\n                  [-1, 4, -1, 0],\n                  [0, -1, 4, -1],\n                  [0, 0, -1, 3]])\n    b = np.array([12, 18, 22, 14])\n    n = 100  # Number of iterations\n    solution = solve_jacobi(A, b, n)\n    print(\"Approximate solution:\", solution)\n```\n\nThis code snippet provides a clear and concise implementation of the Jacobi method for solving linear equations. It includes input validation, iteration, rounding of intermediate results, and conversion of the final result to a Python list as required."}
{"task_id": 12, "completion_id": 0, "completion": "Sure, here is a self-contained Python script that solves the specified task. The script approximates the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method and avoids using the `numpy` `svd` function directly to provide a learning experience.\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    This function approximates the singular values of a 2x2 matrix A using the Jacobian method.\n    It avoids using the direct SVD function from numpy to provide an educational insight.\n    \n    Parameters:\n    - A (np.ndarray): A 2x2 matrix\n    \n    Returns:\n    - tuple: A tuple containing the two singular values of matrix A, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure the matrix is 2x2\n    if A.shape != (2, 2):\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Compute the covariance matrix AA^T and A^TA\n    cov_mat = A @ A.T\n    cov_mat_T = A.T @ A\n    \n    # Function to compute the eigenvalues of a matrix\n    def eigenvalues(mat):\n        return np.linalg.eigvalsh(mat)\n    \n    # Compute the eigenvalues of both covariance matrices\n    # They should be the same and represent the squares of the singular values\n    eigvals = eigenvalues(cov_mat)\n    eigvals_T = eigenvalues(cov_mat_T)\n    \n    # Handle potential numerical inaccuracies by taking the average\n    avg_eigvals = (eigvals + eigvals_T) / 2\n    \n    # Compute the singular values by taking the square root of the eigenvalues\n    singular_values = np.sqrt(avg_eigvals)\n    \n    # Round to 4 decimal places\n    sv_rounded = tuple(round(sv, 4) for sv in singular_values)\n    \n    return sv_rounded\n\n# Example matrix\nA = np.array([[2, 4], [1, 3]])\n\n# Compute singular values\nsingular_values = svd_2x2_singular_values(A)\n\n# Print the result\nprint(\"Singular values:\", singular_values)\n```\n\nThis script defines a function `svd_2x2_singular_values` that computes the singular values of a 2x2 matrix using the Jacobian method in a simplified manner. It does not use the direct SVD computation from numpy, instead, it uses the property that singular values are the square roots of the eigenvalues of the matrix \\(AA^T\\) or \\(A^TA\\)."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    - matrix: A list of lists representing a 4x4 matrix where each element is either an int or a float.\n    \n    Returns:\n    - The determinant of the matrix as a float.\n    \"\"\"\n    if len(matrix) != 4 or any(len(row) != 4 for row in matrix):\n        raise ValueError(\"Input must be a 4x4 matrix\")\n\n    def determinant_3x3(mat: list[list[int|float]]) -> float:\n        \"\"\"\n        Helper function to calculate the determinant of a 3x3 matrix.\n        \n        Args:\n        - mat: A list of lists representing a 3x3 matrix.\n        \n        Returns:\n        - The determinant of the matrix as a float.\n        \"\"\"\n        return (mat[0][0] * (mat[1][1] * mat[2][2] - mat[1][2] * mat[2][1]) -\n                mat[0][1] * (mat[1][0] * mat[2][2] - mat[1][2] * mat[2][0]) +\n                mat[0][2] * (mat[1][0] * mat[2][1] - mat[1][1] * mat[2][0]))\n\n    det = 0\n    for j in range(4):\n        # Construct the minor matrix by removing the first row and the j-th column\n        minor = [row[:j] + row[j+1:] for row in matrix[1:]]\n        # Calculate the determinant of the minor using the helper function\n        det += ((-1) ** (0 + j)) * matrix[0][j] * determinant_3x3(minor)\n    \n    return det\n\n# Test the function with a 4x4 matrix\ntest_matrix = [[2, 0, 1, -1],\n               [3, 0, 2, -2],\n               [1, 1, 0, 0],\n               [-1, 0, 0, 1]]\n\nprint(determinant_4x4(test_matrix))  # Expected output: 1.0\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation.\n    \n    Parameters:\n    X (list of list of float): The feature matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    # Convert list to numpy arrays for easier manipulation\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the bias term\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    \n    # Calculate the normal equation\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    \n    # Round the coefficients to the nearest 4th decimal\n    theta_rounded = np.round(theta, decimals=4)\n    \n    # Convert back to list for output\n    return theta_rounded.tolist()\n\n# Example usage\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 6]\nprint(linear_regression_normal_equation(X, y))\n```\nThis script defines a function `linear_regression_normal_equation` that calculates the coefficients for a linear regression model using the normal equation. The function accepts lists of features and targets, converts them into numpy arrays for matrix operations, and then returns the rounded coefficients as a list."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    :param X: np.ndarray, feature matrix with a column of ones for the intercept.\n    :param y: np.ndarray, target vector.\n    :param alpha: float, learning rate.\n    :param iterations: int, number of iterations for gradient descent.\n    :return: list, coefficients of the linear regression model.\n    \"\"\"\n    # Initialize coefficients\n    m = X.shape[0]\n    n = X.shape[1]\n    theta = np.zeros(n)\n    \n    # Gradient descent\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / m\n        theta = theta - alpha * gradient\n    \n    # Round coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Return coefficients as a list\n    return theta_rounded.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)  # For reproducibility\n    X = np.random.rand(100, 1)\n    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)  # Adding intercept column\n    y = 3*X[:, 1] + 2 + np.random.randn(100, 1).flatten()  # True model: y = 3x + 2\n    \n    alpha = 0.01  # Learning rate\n    iterations = 1000  # Number of iterations\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a 2D NumPy array using standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D NumPy array with samples as rows and features as columns.\n    \n    Returns:\n    tuple: A tuple containing two 2D lists, the first standardized and the second min-max normalized.\n    \"\"\"\n    # Standardization: (x - mean) / std\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Min-max normalization: (x - min) / (max - min)\n    min_vals = np.min(data, axis=0)\n    max_vals = np.max(data, axis=0)\n    normalized_data = (data - min_vals) / (max_vals - min_vals)\n    \n    # Rounding to the nearest 4th decimal and converting to list\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    \n    return standardized_data, normalized_data\n\n# Check function with provided data points\ndata = np.array([\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n])\n\nstandardized, normalized = feature_scaling(data)\nprint(\"Standardized:\\n\", standardized)\nprint(\"Normalized:\\n\", normalized)\n```\nThis script defines a function `feature_scaling` that takes a NumPy array as input and returns two lists: the first list contains the data standardized, and the second list contains the data normalized using min-max normalization. The results are rounded to the nearest 4th decimal as required."}
{"task_id": 17, "completion_id": 0, "completion": "Below is a self-contained Python script implementing the k-Means clustering algorithm using the given specifications:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a set of points.\n    \n    Parameters:\n    points: A list of points (each point is a tuple of coordinates)\n    k: Number of clusters\n    initial_centroids: Initial centroids for the clusters\n    max_iterations: Maximum number of iterations to perform\n    \n    Returns:\n    A list of the final centroids (each centroid rounded to the nearest fourth decimal).\n    \"\"\"\n    # Convert the points and centroids to numpy arrays for easier manipulation\n    points_array = np.array(points)\n    centroids = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Step 1: Assign each point to the closest centroid\n        distances = np.sqrt(((points_array - centroids[:, np.newaxis])**2).sum(axis=2))\n        cluster_labels = np.argmin(distances, axis=0)\n        \n        # Step 2: Update centroids based on the mean of the points in each cluster\n        for i in range(k):\n            if len(points_array[cluster_labels == i]) > 0:\n                centroids[i] = np.mean(points_array[cluster_labels == i], axis=0)\n    \n    # Return the centroids rounded to the nearest fourth decimal\n    return [tuple(np.round(centroid, 4)) for centroid in centroids]\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis script defines the function `k_means_clustering` which takes a set of points, the number of clusters, initial centroids, and the maximum number of iterations to perform. The function then computes the k-Means clustering and returns the final centroids rounded to the nearest fourth decimal. The script also includes an example usage of the function."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Divides the dataset into k folds for cross-validation and returns a list of train-test indices for each fold.\n    \n    Parameters:\n    X (numpy.ndarray): The input features dataset.\n    y (numpy.ndarray): The target labels dataset.\n    k (int): Number of folds for cross-validation.\n    shuffle (bool): Whether to shuffle the dataset before splitting.\n    random_seed (int): A seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    list: A list of tuples containing train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    \n    if shuffle:\n        np.random.shuffle(indices)\n    \n    fold_size = n_samples // k\n    extra = n_samples % k\n    \n    start = 0\n    splits = []\n    for i in range(k):\n        end = start + fold_size + (1 if i < extra else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        splits.append((train_indices, test_indices))\n        start = end\n    \n    return splits\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]])\ny = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n\n# Call the function\nsplits = k_fold_cross_validation(X, y, k=4, shuffle=False, random_seed=42)\n\n# Print the splits\nfor i, (train_indices, test_indices) in enumerate(splits):\n    print(f\"Fold {i+1}:\")\n    print(f\"  Train Indices: {train_indices}\")\n    print(f\"  Test Indices: {test_indices}\")\n    print(f\"  Train Data: {X[train_indices]}\")\n    print(f\"  Test Data: {X[test_indices]}\")\n```\n\nThis provided script defines a function `k_fold_cross_validation` which receives a dataset's features and labels, divides them into k folds, and returns a list of train-test indices for each fold, including the option to shuffle the dataset before splitting. The example usage demonstrates how to call this function and print the resulting train and test splits."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    cov_matrix = np.cov(standardized_data.T)\n    \n    # Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n    \n    # Sort the eigenvalues in descending order and get the corresponding indices\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    \n    # Select the top k eigenvectors\n    top_k_eigenvectors = eigenvectors[:, sorted_indices[:k]]\n    \n    # Return the principal components (top_k_eigenvectors) as a list of lists\n    return [list(map(lambda x: round(x, 4), row)) for row in top_k_eigenvectors.T]\n\n# Example usage:\n# Generating random data\ndata = np.random.rand(100, 5)\n# Perform PCA to get the top 2 principal components\nprincipal_components = pca(data, 2)\nprint(principal_components)\n```\n\nThis Python script defines a function `pca` that performs Principal Component Analysis (PCA) on a given 2D NumPy array. The function first standardizes the dataset, then computes the covariance matrix. Following that, it finds the eigenvalues and eigenvectors of the covariance matrix. Finally, it sorts the eigenvectors based on the corresponding eigenvalues in descending order and returns the top k eigenvectors as the principal components, ensuring all values are rounded to four decimal places."}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree from a list of examples and a list of attributes.\n    \n    :param examples: List of examples, each example is a dictionary of attribute-value pairs.\n    :param attributes: List of attribute names to consider for splitting.\n    :param target_attr: The name of the target attribute for classification.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    def entropy(examples, target_attr):\n        \"\"\"Calculate the entropy of the target attribute in the dataset.\"\"\"\n        cnt = Counter([example[target_attr] for example in examples])\n        total = len(examples)\n        return -sum((v / total) * math.log2(v / total) for v in cnt.values())\n\n    def information_gain(examples, attribute, target_attr):\n        \"\"\"Calculate the information gain for a given attribute.\"\"\"\n        overall_entropy = entropy(examples, target_attr)\n        partitions = {}\n        for example in examples:\n            key = example[attribute]\n            if key not in partitions:\n                partitions[key] = []\n            partitions[key].append(example)\n        weighted_entropy = sum(len(partition) / len(examples) * entropy(partition, target_attr) for partition in partitions.values())\n        return overall_entropy - weighted_entropy\n\n    def split(examples, attribute):\n        \"\"\"Split examples by the attribute values.\"\"\"\n        partitions = {}\n        for example in examples:\n            key = example[attribute]\n            if key not in partitions:\n                partitions[key] = []\n            partitions[key].append(example)\n        return partitions\n\n    def mode(examples, target_attr):\n        \"\"\"Get the most common value of the target attribute in the examples.\"\"\"\n        cnt = Counter([example[target_attr] for example in examples])\n        return cnt.most_common(1)[0][0]\n\n    if not examples or not attributes:\n        return None\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if len(attributes) == 1:\n        return mode(examples, target_attr)\n\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    partitions = split(examples, best_attribute)\n    for key, partition in partitions.items():\n        subtree = learn_decision_tree(partition, [attr for attr in attributes if attr != best_attribute], target_attr)\n        tree[best_attribute][key] = subtree\n\n    return tree\n\n# Example usage:\nexamples = [\n    {\"age\": \"adolescent\", \"job\": \"no\", \"credit_rating\": \"fair\", \"buys_computer\": \"no\"},\n    {\"age\": \"adolescent\", \"job\": \"no\", \"credit_rating\": \"excellent\", \"buys_computer\": \"no\"},\n    {\"age\": \"middle_aged\", \"job\": \"no\", \"credit_rating\": \"fair\", \"buys_computer\": \"yes\"},\n    {\"age\": \"elderly\", \"job\": \"yes\", \"credit_rating\": \"fair\", \"buys_computer\": \"yes\"},\n    {\"age\": \"elderly\", \"job\": \"yes\", \"credit_rating\": \"excellent\", \"buys_computer\": \"no\"},\n    {\"age\": \"elderly\", \"job\": \"no\", \"credit_rating\": \"excellent\", \"buys_computer\": \"yes\"},\n    {\"age\": \"middle_aged\", \"job\": \"yes\", \"credit_rating\": \"excellent\", \"buys_computer\": \"yes\"},\n    {\"age\": \"adolescent\", \"job\": \"no\", \"credit_rating\": \"fair\", \"buys_computer\": \"no\"},\n    {\"age\": \"elderly\", \"job\": \"no\", \"credit_rating\": \"fair\", \"buys_computer\": \"yes\"},\n]\nattributes = [\"age\", \"job\", \"credit_rating\"]\ntree = learn_decision_tree(examples, attributes, \"buys_computer\")\nprint(tree)\n```\nThis script implements the decision tree learning algorithm using recursive binary splitting based on entropy and information gain. It takes a list of examples and a list of attribute names to build a decision tree for classification. The example usage demonstrates how to use the function with a sample dataset."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_linear(x1, x2):\n    return np.dot(x1, x2.T)\n\ndef kernel_rbf(x1, x2, sigma=1.0):\n    return np.exp(-np.sum((x1 - x2) ** 2, axis=1) / (2 * (sigma ** 2)))\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    \n    kernel_func = kernel_linear if kernel == 'linear' else lambda x, y: kernel_rbf(x, y, sigma)\n    \n    for t in range(1, iterations + 1):\n        lambda_t = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            if kernel == 'linear':\n                prediction = np.sum(alpha * labels * kernel_linear(data, data[i])) + bias\n            else:\n                prediction = np.sum(alpha * labels * kernel_func(data, data[i])) + bias\n            if labels[i] * prediction < 1:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i] + lambda_t * labels[i]\n            else:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i]\n            bias += lambda_t * (labels[i] - prediction)\n    \n    bias /= iterations * n_samples\n    alpha = np.round(alpha, 4).tolist()\n    bias = np.round(bias, 4)\n    return alpha, bias\n\n# Example usage:\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\nlabels = np.array([1, 1, -1, -1, -1])\nalpha, bias = pegasos_kernel_svm(data, labels, kernel='rbf', lambda_val=0.1, iterations=1000, sigma=1.0)\nprint(\"Alpha coefficients:\", alpha)\nprint(\"Bias:\", bias)\n```\nThis Python script implements the Pegasos Kernel SVM algorithm for both linear and RBF kernels. It takes in a dataset and labels along with parameters like the type of kernel, regularization parameter, number of iterations, and sigma for the RBF kernel. The function iteratively updates the alpha coefficients and bias for the model, rounding the final results to the nearest 4th decimal and returning them as a list and a float respectively."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    - z (float): Input value to the sigmoid function.\n    \n    Returns:\n    - float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Test the function with a few data points\nprint(sigmoid(0))  # Expected output: 0.5\nprint(sigmoid(1))  # Expected output: 0.7311\nprint(sigmoid(-1)) # Expected output: 0.2689\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax activation for a given list of scores.\n    \n    Args:\n    - scores (list of float): The input scores.\n    \n    Returns:\n    - list of float: The softmax values rounded to four decimal places.\n    \"\"\"\n    # First, calculate the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Calculate the sum of the exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax values by dividing each exponential score by the sum of all\n    softmax_values = [round(score / sum_exp_scores, 4) for score in exp_scores]\n    \n    return softmax_values\n\n# Check function to verify the correctness of the softmax function\ndef check_softmax():\n    test_scores = [1.0, 2.0, 3.0]\n    expected_output = [0.0900, 0.2447, 0.6653]\n    assert softmax(test_scores) == expected_output, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Run the check function\ncheck_softmax()\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulate a single neuron with sigmoid activation for binary classification.\n    \n    :param features: List of feature vectors for each example.\n    :param labels: True binary labels for each example.\n    :param weights: Weights for each feature.\n    :param bias: Bias for the neuron.\n    :return: A tuple of predicted probabilities and mean squared error.\n    \"\"\"\n    predicted_probabilities = []\n    mse = 0.0\n    \n    for feature_vector in features:\n        # Calculate the weighted sum of inputs for each example\n        weighted_sum = sum(w * f for w, f in zip(weights, feature_vector)) + bias\n        # Apply sigmoid activation function\n        prob = sigmoid(weighted_sum)\n        predicted_probabilities.append(prob)\n    \n    # Calculate mean squared error\n    for actual, predicted in zip(labels, predicted_probabilities):\n        mse += (actual - predicted) ** 2\n    mse /= len(labels)\n    \n    # Round results to 4 decimal places\n    predicted_probabilities = np.round(predicted_probabilities, 4).tolist()\n    mse = np.round(mse, 4)\n    \n    return predicted_probabilities, mse\n\n# Example usage and check function\ndef check_model():\n    features = [[0, 0], [0, 1], [1, 0], [1, 1]]\n    labels = [0, 0, 0, 1]\n    weights = [0, 0]\n    bias = 0\n    predictions, mse = single_neuron_model(features, labels, weights, bias)\n    print(\"Predictions:\", predictions)\n    print(\"MSE:\", mse)\n\n# Run the check function\ncheck_model()\n```"}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid of x.\"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the mean squared error between true and predicted values.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n\n    for _ in range(epochs):\n        # Forward pass\n        z = np.dot(features, weights) + bias\n        predictions = sigmoid(z)\n\n        # Compute and store MSE\n        mse = mean_squared_error(labels, predictions)\n        mse_values.append(round(mse, 4))\n\n        # Backward pass\n        error = predictions - labels\n        derivative = sigmoid_derivative(z)\n        gradient_weights = np.dot(features.T, (error * derivative))\n        gradient_bias = np.sum((error * derivative))\n\n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, 0.5])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\n\nprint(\"Updated Weights:\", weights)\nprint(\"Updated Bias:\", bias)\nprint(\"MSE Values:\", mse_values)\n```\nThis script defines the `train_neuron` function simulating a single neuron's training over multiple epochs with backpropagation, using sigmoid activation and MSE loss. The function returns the updated weights, bias, and MSE values for each epoch, all rounded to the fourth decimal place."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0.0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a * b + c\ne = d.relu()\ne.backward()\n\nprint(f\"a.grad = {a.grad}, b.grad = {b.grad}, c.grad = {c.grad}, d.grad = {d.grad}\")\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C for R^3.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in B.\n    C (list of list of int): Basis vectors in C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from B to C, rounded to 4 decimal places.\n    \"\"\"\n    # Convert the list of basis vectors into numpy arrays\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    \n    # Compute the inverse of B_matrix\n    B_inv = np.linalg.inv(B_matrix)\n    \n    # The transformation matrix P is obtained by multiplying C by the inverse of B\n    P = np.dot(C_matrix, B_inv)\n    \n    # Round the transformation matrix to 4 decimal places and convert to list\n    P_rounded = np.round(P, 4).tolist()\n    \n    return P_rounded\n\n# Example usage\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n# Check function to verify the correctness\ndef check_solution():\n    P = transform_basis(B, C)\n    print(\"Transformation Matrix P from B to C:\")\n    print(np.array(P))\n\ncheck_solution()\n```\n\nThis script provides a function `transform_basis` to compute the transformation matrix from one basis to another in R^3. It includes example usage and a check function to display the transformation matrix."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    Returns U, S, and V such that A \u2248 U*S*V.T.\n    \"\"\"\n    # Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Eigendecomposition of A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(ATA)\n    \n    # Sort eigenvalues and corresponding eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Construct S from the square roots of the eigenvalues\n    S = np.diag(np.sqrt(eigenvalues))\n    \n    # Construct V from the eigenvectors of A^T A\n    V = eigenvectors\n    \n    # Compute U for each non-zero singular value\n    U = np.zeros_like(A)\n    for i in range(A.shape[1]):\n        if eigenvalues[i] > 1e-10:  # considering floating point precision\n            U[:, i] = np.dot(A, V[:, i]) / np.sqrt(eigenvalues[i])\n    \n    # Rounding the results to the nearest 4th decimal\n    U = np.round(U, decimals=4)\n    S = np.round(S, decimals=4)\n    V = np.round(V, decimals=4)\n    \n    # Convert numpy arrays to lists\n    U = U.tolist()\n    S = S.tolist()\n    V = V.tolist()\n    \n    return U, S, V\n\n# Example usage\nA = np.array([[1, 2], [3, 4]], dtype=float)\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nPlease note that the example provided is for a 2x2 matrix, and the SVD decomposition is computed using the method based on the eigenvalues and eigenvectors of $A^T A$. The output matrices U, S, and V are rounded to the 4th decimal place and returned as Python lists for ease of inspection and further processing."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the elements of two numpy arrays, X and y, maintaining the correspondence between them.\n    \n    Parameters:\n    X (numpy.ndarray): The first array to be shuffled.\n    y (numpy.ndarray): The second array to be shuffled.\n    seed (int, optional): An optional seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing the shuffled versions of X and y as python lists.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    \n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    np.random.shuffle(indices)\n    \n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    \n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    X_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\n    print(\"Shuffled X:\\n\", X_shuffled)\n    print(\"Shuffled y:\\n\", y_shuffled)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A function to create batch iterators for datasets.\n    \n    Parameters:\n    - X (numpy.ndarray): The input dataset.\n    - y (numpy.ndarray, optional): The target dataset. Defaults to None.\n    - batch_size (int): The size of each batch. Defaults to 64.\n    \n    Yields:\n    - A list or a tuple of lists representing a batch of the dataset.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        end = i + batch_size\n        if y is not None:\n            yield X[i:end].tolist(), y[i:end].tolist()\n        else:\n            yield X[i:end].tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([0, 1, 0, 1])\n\n# Testing the function without y\nprint(\"Batches of X:\")\nfor batch in batch_iterator(X, batch_size=2):\n    print(batch)\n\n# Testing the function with y\nprint(\"\\nBatches of (X, y):\")\nfor batch in batch_iterator(X, y, batch_size=2):\n    print(batch)\n```\n\nThis script defines a function `batch_iterator` that takes in a dataset `X`, an optional target dataset `y`, and a `batch_size` to yield batches of the dataset(s). The examples at the bottom demonstrate how to use the function both with and without the target dataset `y`."}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Splits the dataset based on whether a feature's value is >= a threshold.\n    \n    :param X: numpy array, the dataset to be split.\n    :param feature_i: int, the index of the feature to split on.\n    :param threshold: float, the threshold to split the dataset.\n    :return: split into two lists based on the condition.\n    \"\"\"\n    # Apply the split condition to obtain the boolean mask\n    split_mask = X[:, feature_i] >= threshold\n    \n    # Split the dataset using the mask\n    subset1 = X[split_mask].tolist()\n    subset2 = X[~split_mask].tolist()\n    \n    return subset1, subset2\n\n# Check function\ndef check_divide_on_feature():\n    # Create a dataset\n    np.random.seed(0)\n    X = np.random.rand(5, 3)\n    \n    # Define the feature index and threshold\n    feature_i = 1\n    threshold = 0.5\n    \n    # Perform the split\n    subset1, subset2 = divide_on_feature(X, feature_i, threshold)\n    \n    # Expected results based on the random seed\n    expected_subset1 = [[0.5488135, 0.71518937, 0.60276338], [0.54488318, 0.4236548, 0.64589411]]\n    expected_subset2 = [[0.43758721, 0.05821103, 0.39988991], [0.79172504, 0.52889492, 0.56804456], [0.92559664, 0.07103606, 0.0871293 ]]\n\n    # Check if the output matches the expected results\n    assert subset1 == expected_subset1, f\"Subset 1 does not match expected output: {subset1} != {expected_subset1}\"\n    assert subset2 == expected_subset2, f\"Subset 2 does not match expected output: {subset2} != {expected_subset2}\"\n    \n    print(\"All checks passed.\")\n\n# Run the check function\ncheck_divide_on_feature()\n```\n\nThis code defines a function `divide_on_feature` that splits a dataset into two subsets based on a feature's value being greater than or equal to a provided threshold. The function uses numpy for array manipulation and returns the subsets as native Python lists. The check function `check_divide_on_feature` ensures that the implementation is correct by comparing the output to expected results."}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features\n    with degree less than or equal to the specified degree. For example, if an input sample is\n    two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    - X : numpy array of shape (n_samples, n_features)\n    - degree : int, degree of the polynomial features\n    \n    Returns:\n    - numpy array of shape (n_samples, new_n_features)\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    \n    n_samples, n_features = X.shape\n    new_features = []\n    \n    # Generate combinations of features and their degrees\n    for deg in range(degree + 1):\n        for indices in combinations_with_replacement(range(n_features), deg):\n            if len(indices) == 0:\n                new_features.append(np.ones(n_samples))\n            else:\n                feature = np.prod([X[:, idx] ** (deg + 1 if idx == indices[0] else 1) for idx in indices], axis=0)\n                new_features.append(feature)\n    \n    # Stack all the features along the second axis\n    return np.column_stack(new_features).tolist()\n\n# Example usage\nX = np.array([[2, 3], [3, 5]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis solution creates a function `polynomial_features` that generates all polynomial combinations of features up to the specified degree. It leverages `combinations_with_replacement` to find all combinations of the features within the specified degree and then calculates the product of these combinations to generate the polynomial features. The resulting features are stacked and returned as a list, conforming to the output constraints specified."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset (X, y).\n\n    Parameters:\n    X (numpy.ndarray): The feature data as a 2D numpy array.\n    y (numpy.ndarray): The target data as a D numpy array.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Random seed for reproducibility.\n\n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    \n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Check function to verify the correctness\ndef check_function():\n    X = np.array([[10, 20], [30, 40], [50, 60], [70, 80]])\n    y = np.array([1, 2, 3, 4])\n    n_subsets = 3\n    result = get_random_subsets(X, y, n_subsets, replacements=True)\n    print(\"Result with replacement:\")\n    for subset in result:\n        print(subset)\n    \n    result = get_random_subsets(X, y, n_subsets, replacements=False)\n    print(\"\\nResult without replacement:\")\n    for subset in result:\n        print(subset)\n\n# Run check function\ncheck_function()\n```\n\nThis script defines the function `get_random_subsets` that generates random subsets of a given dataset. The check function demonstrates the functionality of `get_random_subsets` with example data."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. \n                           If not provided, it will be determined from the input array.\n    \n    Returns:\n    list: One-hot encoded array, reshaped and returned as a list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Example usage and check function\ndef check_to_categorical():\n    test_x = np.array([0, 1, 3, 2])\n    expected_output = [\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0],\n        [0.0, 0.0, 1.0, 0.0]\n    ]\n    assert to_categorical(test_x) == expected_output, \"Test failed\"\n    print(\"Test passed\")\n\n# Run the check function to verify the solution\ncheck_to_categorical()\n```\n\nThis script defines the function `to_categorical` as requested, and includes a test case and a check function to ensure the implementation is correct."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    :param x: 1D numpy array\n    :return: 2D numpy array representing the diagonal matrix of x, converted to a list\n    \"\"\"\n    # Ensure that x is a numpy array\n    x = np.array(x)\n    # Create a diagonal matrix from x\n    diagonal_matrix = np.diag(x)\n    # Convert the numpy array to a list for output\n    return diagonal_matrix.tolist()\n\n# Example usage:\nvector = np.array([1, 2, 3])\ndiagonal_matrix = make_diagonal(vector)\nprint(diagonal_matrix)\n```\n\nThis code snippet first converts the input into a numpy array to ensure compatibility, then uses `np.diag` to create a diagonal matrix from the 1D array. Finally, it converts the resulting 2D numpy array into a list for output."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n\n    Parameters:\n    y_true (numpy.ndarray): 1D array of true labels.\n    y_pred (numpy.ndarray): 1D array of predicted labels.\n\n    Returns:\n    float: The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    # Check if the input arrays are 1D\n    if y_true.ndim != 1 or y_pred.ndim != 1:\n        raise ValueError(\"Input arrays must be 1D.\")\n    # Check if the input arrays are of the same length\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"Input arrays must be of the same length.\")\n    \n    # Calculate the accuracy score\n    accuracy = np.mean(y_true == y_pred)\n    \n    # Return the accuracy score rounded to 4 decimal places\n    return round(accuracy, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([0, 1, 2, 2, 2])\n    y_pred = np.array([0, 0, 2, 2, 1])\n    print(accuracy_score(y_true, y_pred))  # Output should be 0.6\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for the given dataset(s).\n    \n    Parameters:\n    X (2D numpy array): The first dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, the correlation matrix of X with itself is calculated.\n    \n    Returns:\n    2D numpy array: The correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Calculate the correlation matrix\n    correlation_matrix = np.corrcoef(X, Y)\n    # Round to 4 decimal places and convert to list\n    return np.round(correlation_matrix, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    Y = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n    \n    # Calculate correlation matrix of X with itself\n    print(\"Correlation Matrix of X with itself:\")\n    print(calculate_correlation_matrix(X))\n    \n    # Calculate correlation matrix of X with Y\n    print(\"\\nCorrelation Matrix of X with Y:\")\n    print(calculate_correlation_matrix(X, Y))\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n\n    Parameters:\n    X (numpy.ndarray): 2D array of shape (n_samples, n_features) representing the dataset.\n    y (numpy.ndarray): 1D array of shape (n_samples,) representing the labels.\n    n_clf (int): Number of classifiers.\n\n    Returns:\n    list: A list of dictionaries, each containing parameters for each classifier.\n    \"\"\"\n    n_samples, n_features = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n\n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_classifier = None\n\n        for feature_idx in range(n_features):\n            feature_values = X[:, feature_idx]\n            thresholds = np.unique(feature_values)\n\n            for threshold in thresholds:\n                pred = np.where(X[:, feature_idx] > threshold, 1, -1)\n                error = np.sum(sample_weights * (pred != y))\n                if error > 0.5:\n                    error = 1 - error\n                    pred = -pred\n                if error < best_error:\n                    best_error = error\n                    best_classifier = {\n                        'feature_idx': feature_idx,\n                        'threshold': threshold,\n                        'prediction': pred,\n                    }\n        \n        alpha = 0.5 * np.log((1 - best_error) / best_error)\n        classifiers.append({'classifier': best_classifier, 'alpha': round(float(alpha), 4)})\n        pred = best_classifier['prediction']\n        sample_weights *= np.exp(-alpha * y * pred)\n        sample_weights /= np.sum(sample_weights)\n\n    return classifiers\n\n# Example usage\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, -1, 1, -1])\nn_clf = 2\nprint(adaboost_fit(X, y, n_clf))\n```\n\nThis code provides a self-contained implementation of the AdaBoost fit method for a classifier. It iteratively selects the best feature and threshold to partition the data, calculates the error, updates the sample weights, and stores the classifier parameters. The output is a list of classifiers with their parameters, each rounded to the nearest 4th decimal."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax is calculated as the logarithm of the softmax function,\n    which transforms the input vector into a probability distribution.\n    This function helps in numerical stability by avoiding underflow or overflow.\n    \n    :param scores: A list of scores.\n    :return: A list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    # Compute softmax\n    max_score = np.max(scores)\n    exp_scores = np.exp(scores - max_score)  # subtract max(score) for numerical stability\n    softmax = exp_scores / np.sum(exp_scores)\n    # Compute log-softmax\n    log_softmax = np.log(softmax)\n    # Return the result rounded to 4 decimal places\n    return np.round(log_softmax, 4).tolist()\n\n# Sample input\nscores = [1.0, 2.0, 3.0]\nprint(log_softmax(scores))\n```\n\nThis code snippet defines a function `log_softmax` that takes a list of scores and returns the log-softmax values for those scores. The implementation takes care of numerical stability by subtracting the maximum score before the exponentiation step in the softmax computation. The result is then rounded to the 4th decimal place as per the task's requirement."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_optimizer = None\n        self.w0_optimizer = None\n\n    def initialize(self, optimizer):\n        input_shape = self.input_shape[0]\n        lim = 1 / math.sqrt(input_shape)\n        self.W = np.random.uniform(-lim, lim, (input_shape, self.n_units))\n        self.w0 = np.zeros((self.n_units,))\n        self.W_optimizer = copy.copy(optimizer)\n        self.w0_optimizer = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X @ self.W + self.w0\n\n    def backward_pass(self, accum_grad):\n        w0_grad = np.sum(accum_grad, axis=0)\n        W_grad = self.layer_input.T @ accum_grad\n        self.w0 = self.w0_optimizer.update(self.w0, w0_grad)\n        self.W = self.W_optimizer.update(self.W, W_grad)\n        return accum_grad @ self.W.T\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Define a simple optimizer for testing\nclass SimpleSGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def update(self, w, grad):\n        return w - self.learning_rate * grad\n\n# Test the Dense layer\nif __name__ == \"__main__\":\n    # Create a Dense layer\n    dense_layer = Dense(n_units=128, input_shape=(64,))\n    # Initialize the layer\n    dense_layer.initialize(SimpleSGD())\n\n    # Test the forward and backward passes\n    X = np.random.rand(10, 64)  # A batch of 10 samples\n    output = dense_layer.forward_pass(X)\n    print(\"Output shape:\", output.shape)\n    print(\"Output sample (rounded to 4 decimals):\", output[0].round(4).tolist())\n\n    grad_out = np.random.rand(10, 128)  # Random gradient from the next layer\n    grad_in = dense_layer.backward_pass(grad_out)\n    print(\"Gradient shape:\", grad_in.shape)\n    print(\"Gradient sample (rounded to 4 decimals):\", grad_in[0].round(4).tolist())\n```\nThis script provides an implementation of a Dense layer in Python, including initialization of weights and biases, computing forward and backward passes, and handling of gradients for parameter updates. It also includes a simple SGD optimizer class for testing purposes."}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution on the given input_matrix with a kernel,\n    applying a specified padding and stride.\n    \n    Parameters:\n    - input_matrix: np.ndarray, the input 2D matrix.\n    - kernel: np.ndarray, the convolutional kernel.\n    - padding: int, the number of rows and columns of zeros to be added to the input_matrix's borders.\n    - stride: int, the stride length of the kernel.\n    \n    Returns:\n    - output: list, the result of the convolution operation as a list.\n    \"\"\"\n    # Add padding to the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Dimensions\n    input_height, input_width = padded_input.shape\n    kernel_height, kernel_width = kernel.shape\n    \n    # Output dimensions\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    \n    # Initialize the output matrix\n    output_matrix = np.zeros((output_height, output_width))\n    \n    # Perform the convolution operation\n    for i in range(0, input_height - kernel_height + 1, stride):\n        for j in range(0, input_width - kernel_width + 1, stride):\n            patch = padded_input[i:i+kernel_height, j:j+kernel_width]\n            output_matrix[i // stride, j // stride] = np.sum(patch * kernel)\n    \n    # Round to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, 4)\n    \n    # Return as a list\n    return output_matrix.tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1\n\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```\nThis script defines the `simple_conv2d` function which performs a simple 2D convolution operation on the input matrix with the specified kernel, padding, and stride. It returns the result as a list, with values rounded to the nearest 4th decimal as required."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A float number to apply the ReLU function to.\n    \n    Returns:\n    float: The result after applying the ReLU function.\n    \"\"\"\n    return max(0.0, z)\n\n# Test the relu function with some data points\nif __name__ == \"__main__\":\n    test_values = [-10.0, -1.0, 0.0, 1.0, 10.0]\n    expected_results = [0.0, 0.0, 0.0, 1.0, 10.0]\n    for value, expected in zip(test_values, expected_results):\n        result = relu(value)\n        assert result == expected, f\"relu({value}) = {result}, expected {expected}\"\n    print(\"All tests passed!\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss which is the sum of squared error term and\n    the squared L2 norm of the coefficient vector, weighted by a regularization parameter.\n    \n    :param X: A 2D numpy array of shape (n_samples, n_features) that represents the feature matrix.\n    :param w: A 1D numpy array of shape (n_features,) that represents the model coefficients.\n    :param y_true: A 1D numpy array of shape (n_samples,) that represents the true labels.\n    :param alpha: A float representing the regularization strength.\n    :return: The Ridge loss as a float, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Predicted labels\n    y_pred = X @ w\n    \n    # Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # L2 norm squared of the coefficients\n    l2_norm_squared = w.T @ w\n    \n    # Ridge loss\n    ridge_loss = mse + alpha * l2_norm_squared\n    \n    return round(ridge_loss, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)  # For reproducibility\n    X = np.random.randn(100, 10)  # 100 samples, 10 features\n    w_true = np.random.randn(10)  # True coefficients\n    noise = np.random.randn(100)  # Noise\n    y_true = X @ w_true + noise  # True labels\n    \n    # Estimated coefficients (can be initialized randomly or otherwise)\n    w = np.random.randn(10)\n    alpha = 1.0  # Regularization parameter\n    \n    # Compute Ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Args:\n        z (float): The input value to the function.\n        alpha (float): The slope for the value when z < 0. Default is 0.01.\n        \n    Returns:\n        float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Example usage:\n# Uncomment the lines below to test the function.\n# print(leaky_relu(-10))  # Should print a value close to -0.1 for the default alpha\n# print(leaky_relu(10))   # Should print 10, as z > 0\n# print(leaky_relu(-10, 0.1))  # Should print -1, as we're using a custom alpha\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    The linear kernel is defined as the dot product of the two vectors.\n    Both input vectors must be of the same length.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel value.\n    \"\"\"\n    # Ensure that inputs are numpy arrays\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    \n    # Calculate the dot product\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\n\n# Example usage\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\n\n# Compute the linear kernel\nresult = kernel_function(x1, x2)\nprint(f\"The linear kernel value between {x1} and {x2} is {result}\")\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculates the precision, which is the ratio of true positive predictions to the total\n    number of positive predictions (true positives and false positives).\n    \n    :param y_true: numpy array of true binary labels\n    :param y_pred: numpy array of predicted binary labels\n    :return: Precision score as a float\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives and false positives\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if true_positives + false_positives == 0:\n        return 0 # To avoid division by zero\n    precision_value = true_positives / (true_positives + false_positives)\n    \n    return precision_value\n\n# Example check function to verify the correctness of the generated function\ndef check_precision():\n    y_true = np.array([1, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 0, 0, 0, 0, 1])\n    expected_precision = 2 / 3\n    calculated_precision = precision(y_true, y_pred)\n    assert np.isclose(calculated_precision, expected_precision), f\"Expected {expected_precision}, got {calculated_precision}\"\n    print(f\"Precision metric is working correctly: {calculated_precision}\")\n\n# Run the check function\ncheck_precision()\n```\n\nThis script defines the `precision` function that calculates the precision metric by taking two numpy arrays of true and predicted binary labels as input. It also includes a check function to verify that the `precision` function works correctly with an example data set and expected precision value."}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent (Stochastic, Batch, or Mini-batch) to find optimal weights for a linear regression model.\n    \n    :param X: numpy array of shape (m, n) containing the training data\n    :param y: numpy array of shape (m,) containing the target values\n    :param weights: numpy array of shape (n,) containing initial weights\n    :param learning_rate: float, the learning rate for the gradient descent\n    :param n_iterations: int, number of iterations to perform\n    :param batch_size: int, size of the batch to be used in mini-batch and stochastic gradient descent\n    :param method: str, specifies the type of gradient descent ('batch', 'stochastic', 'mini-batch')\n    :return: list, the optimized weights rounded to the 4th decimal place\n    \"\"\"\n    m = len(y)  # Number of samples\n    n = X.shape[1]  # Number of features\n    \n    for _ in range(n_iterations):\n        if method == 'batch':\n            predictions = np.dot(X, weights)\n            errors = predictions - y\n            gradients = (1 / m) * np.dot(X.T, errors)\n            weights -= learning_rate * gradients\n        elif method == 'stochastic':\n            for i in range(m):\n                prediction = np.dot(X[i], weights)\n                error = prediction - y[i]\n                gradient = -(2 / m) * X[i] * error\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, m, batch_size):\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n                predictions = np.dot(X_batch, weights)\n                errors = predictions - y_batch\n                gradients = (1 / batch_size) * np.dot(X_batch.T, errors)\n                weights -= learning_rate * gradients\n        else:\n            raise ValueError(\"Method must be one of 'batch', 'stochastic', or 'mini-batch'.\")\n    \n    optimized_weights = np.round(weights, 4)\n    return optimized_weights.tolist()\n\n# Example usage\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.001\nn_iterations = 1000\n\n# Perform gradient descent\noptimized_weights = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=2, method='mini-batch')\nprint(optimized_weights)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    This function takes a matrix and returns its Reduced Row Echelon Form (RREF).\n    :param matrix: A 2D numpy array representing the matrix to be converted to RREF.\n    :return: A 2D list representing the RREF of the input matrix.\n    \"\"\"\n    # Convert the input matrix to a numpy array for easier manipulation\n    matrix = np.array(matrix)\n    rows, cols = matrix.shape\n    r = 0  # Current row being processed\n    for c in range(cols):  # For each column\n        # Find the row with the largest absolute value in the current column and swap it with the current row\n        max_row = np.argmax(np.abs(matrix[r:rows, c])) + r\n        if matrix[max_row, c] == 0:\n            continue  # If the column is zero, move to the next column\n        matrix[[r, max_row]] = matrix[[max_row, r]]  # Swap rows to put pivot at the top of the column\n        pivot = matrix[r, c]\n        matrix[r] /= pivot  # Make pivot 1\n        for i in range(rows):\n            if i != r:\n                factor = matrix[i, c]\n                matrix[i] -= factor * matrix[r]  # Eliminate other entries in the column\n        r += 1\n        if r == rows:\n            break  # Move to the next row\n    return matrix.tolist()  # Convert numpy array back to list before returning\n\n# Example usage:\nmatrix = np.array([[1, 2, -1, -4], [-2, -3, 4, 4], [3, 5, -5, -7]])\nrref_matrix = rref(matrix)\nprint(rref_matrix)\n```\n\nThis script provides a function `rref` that takes a matrix as an input and returns its Reduced Row Echelon Form (RREF). The function utilizes numpy for efficient matrix operations and handles the special cases as described in the task, such as the diagonal entries being zero and rows of all zeros."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm implementation.\n    \n    Parameters:\n    f : function\n        The objective function to be optimized.\n    grad : function\n        Computes the gradient of the objective function.\n    x0 : array-like\n        Initial parameter values.\n    learning_rate : float, optional\n        The step size for the optimization (default is 0.001).\n    beta1 : float, optional\n        Exponential decay rate for the first moment estimates (default is 0.9).\n    beta2 : float, optional\n        Exponential decay rate for the second moment estimates (default is 0.999).\n    epsilon : float, optional\n        A small constant for numerical stability (default is 1e-8).\n    num_iterations : int, optional\n        Number of iterations to run the optimizer (default is 1000).\n    \n    Returns:\n    list\n        Optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    \n    for t in range(1, num_iterations + 1):\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return np.round(x0, 4).tolist()\n\n# Example usage\ndef quadratic_function(x):\n    return x[0]**2 + x[1]**2\n\ndef grad_quadratic(x):\n    return np.array([2*x[0], 2*x[1]])\n\nx0 = np.array([1.5, -0.5])\noptimized_params = adam_optimizer(quadratic_function, grad_quadratic, x0)\nprint(\"Optimized parameters:\", optimized_params)\n```\n\nThis script defines an `adam_optimizer` function that optimizes a given function `f` with its gradient `grad` using the Adam algorithm. It uses default parameters typical for Adam optimization and runs for a specified number of iterations. The example provided at the end uses a simple quadratic function to demonstrate the usage of the optimizer."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = np.inf\n\n    for _ in range(max_iter):\n        # Predicted values\n        y_pred = np.dot(X, weights) + bias\n        # Compute the loss\n        cost = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n\n        # Compute the gradients\n        d_weights = -(1 / n_samples) * np.dot(X.T, (y - y_pred)) + alpha * np.sign(weights)\n        d_bias = -(1 / n_samples) * np.sum(y - y_pred)\n        \n        # Update weights and bias\n        weights -= learning_rate * d_weights\n        bias -= learning_rate * d_bias\n\n    # Round the weights and bias to 4 decimal places\n    weights = np.round(weights, 4)\n    bias = np.round(bias, 4)\n    return weights.tolist(), bias.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([1, 2, 3])\nweights, bias = l1_regularization_gradient_descent(X, y)\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias)\n```\n\nThis script defines a function `l1_regularization_gradient_descent` that implements Lasso Regression using gradient descent. The function takes as input the feature matrix `X`, the target vector `y`, the regularization parameter `alpha`, the learning rate, the maximum number of iterations, and a tolerance value for convergence. It returns the weights and bias after training, with values rounded to four decimal places."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    m, n = len(source), len(target)\n    # Create a 2D array to store the edit distances\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    # Initialize the base cases\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n        \n    # Fill the dp array\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            # Calculate the minimum cost of operations\n            dp[i][j] = min(dp[i-1][j] + 1,                # deletion\n                           dp[i][j-1] + 1,                # insertion\n                           dp[i-1][j-1] + cost)           # substitution\n            # Handle transposition\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                dp[i][j] = min(dp[i][j], dp[i-2][j-2] + 1)\n    \n    return dp[m][n]\n\n# Example usage\nprint(OSA(\"caper\", \"acer\"))  # Output: 2\n```\n\nThis Python script defines the `OSA` function which calculates the minimum number of edits (insertions, deletions, substitutions, and transpositions) needed to transform one string into another, as per the optimal string alignment distance metric. The example usage demonstrates calculating the distance between the strings `caper` and `acer`."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    Parameters:\n    - y_true: List[int], true labels.\n    - y_pred: List[int], predicted labels.\n    \n    Returns:\n    - float, recall value rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall avoiding division by zero\n    if tp + fn == 0:\n        return 0.0\n    else:\n        recall_value = tp / (tp + fn)\n        return round(recall_value, 3)\n\n# Example usage:\ny_true = [1, 1, 0, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]\nprint(recall(y_true, y_pred))  # Expected output: 0.75\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Compute the self-attention of the input sequence.\n    \n    Args:\n    X: numpy array (seq_len, d_model) representing the input sequence.\n    W_q: numpy array (d_model, d_k) representing the query weight matrix.\n    W_k: numpy array (d_model, d_k) representing the key weight matrix.\n    W_v: numpy array (d_model, d_v) representing the value weight matrix.\n    \n    Returns:\n    numpy array (seq_len, d_v) representing the self-attention output.\n    \"\"\"\n    # Compute the query, key, and value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(W_k.shape[1])\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    \n    # Compute the weighted sum of the values\n    output = np.dot(attention_weights, V)\n    \n    # Round to the nearest 4th decimal and convert to list\n    return np.round(output, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a sample input sequence and weight matrices\n    X = np.array([[1, 0, 1, 0], [0, 2, 0, 2], [1, 1, 1, 1]])\n    W_q = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n    W_k = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n    W_v = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0], [0, 0, 1]])\n    \n    # Call the self_attention function\n    output = self_attention(X, W_q, W_k, W_v)\n    \n    # Print the output\n    print(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell and returns the final hidden state after processing the input sequence.\n    \n    Parameters:\n    - input_sequence: A list of input vectors.\n    - initial_hidden_state: The initial hidden state.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to 4 decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state\n    hidden_state = initial_hidden_state\n    \n    # Iterate over the input sequence\n    for x in input_sequence:\n        # Compute the new hidden state\n        hidden_state = np.tanh(np.dot(x, Wx) + np.dot(hidden_state, Wh) + b)\n    \n    # Round the final hidden state to 4 decimal places and convert back to list\n    return np.round(hidden_state, decimals=4).tolist()\n\n# Example usage\ninput_sequence = [[0.5, 0.3], [0.2, 0.8]]\ninitial_hidden_state = [0.1, 0.2]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, -0.1]\n\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_hidden_state)\n```\n\nThis code defines a function `rnn_forward` that implements a simple RNN cell and processes a given sequence of input vectors to produce the final hidden state. The function uses the `tanh` activation function for the hidden state updates and rounds the final hidden state to the nearest 4th decimal place before returning the result."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n    \n    Parameters:\n    points (list of lists): A list of [x, y] coordinates.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: A list of [x, y] coordinates after applying the translation.\n    \"\"\"\n    # Convert the list of points to a NumPy array\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Append a column of ones to the points for matrix multiplication\n    ones = np.ones((points_array.shape[0], 1))\n    points_homogeneous = np.hstack((points_array, ones))\n    \n    # Perform the translation\n    translated_points = np.dot(points_homogeneous, translation_matrix.T)\n    \n    # Remove the last column which is not needed\n    translated_points = translated_points[:, :-1].tolist()\n    \n    return translated_points\n\n# Check function to verify the solution\ndef check_solution():\n    input_points = [[1, 2], [3, 4], [5, 6]]\n    tx = 2\n    ty = 3\n    output_points = translate_object(input_points, tx, ty)\n    desired_output = [[3, 5], [5, 7], [7, 9]]\n    assert output_points == desired_output, f\"Expected output does not match. Expected: {desired_output}, Got: {output_points}\"\n\n# Running the check function\ncheck_solution()\n```"}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    term3 = -0.5\n    kl_div = term1 + term2 + term3\n    \n    return kl_div\n\n# Example usage:\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\nprint(f\"KL divergence: {kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve the system of linear equations Ax = b using the Gauss-Seidel method.\n    \n    Parameters:\n    A (np.array): A square matrix of coefficients.\n    b (np.array): The right-hand side vector.\n    n (int): The number of iterations.\n    x_ini (np.array, optional): Initial guess for the solution vector. Defaults to None.\n\n    Returns:\n    list: The approximated solution vector x as a list.\n    \"\"\"\n    # Ensure A, b are numpy arrays\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the size of the matrix A\n    size = A.shape[0]\n    \n    # Initialize x as zeros if x_ini is not provided\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform Gauss-Seidel iteration\n    for _ in range(n):\n        for i in range(size):\n            # Compute the new value of x[i] using the most recent values available\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = [[4, 1, 0], [1, 4, 1], [0, 1, 4]]\nb = [1, 1, 1]\nn = 10\nsolution = gauss_seidel(A, b, n)\nprint(solution)\n```\n\nThis script defines a function `gauss_seidel` that implements the Gauss-Seidel method to solve a system of linear equations. The solution vector is updated iteratively using the latest values, and the results are rounded to the nearest 4th decimal place as per the requirement."}
{"task_id": 58, "completion_id": 0, "completion": "Here is a self-contained Python script fulfilling the provided task:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (numpy.ndarray): Coefficient matrix.\n    b (numpy.ndarray): Constant vector.\n    \n    Returns:\n    list: Solution vector (x) rounded to the nearest 4th decimal place.\n    \"\"\"\n    n = len(A)\n    # Augment A with b\n    Ab = np.hstack([A, b.reshape(-1,1)])\n\n    # Gaussian elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row\n        pivot = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, pivot]] = Ab[[pivot, i]]  # Swap rows\n\n        pivot_value = Ab[i, i]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / pivot_value\n            Ab[j, i:] -= factor * Ab[i, i:]\n\n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n\n    # Rounding the result to 4 decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nprint(\"Solution vector:\", gaussian_elimination(A, b))\n```\n\nThis script defines the function `gaussian_elimination` which uses Gaussian Elimination with partial pivoting to solve the system of linear equations. It includes an example to demonstrate how the function works. The solution vector returned is a list of numbers rounded to the 4th decimal place as requested."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        h = initial_hidden_state\n        c = initial_cell_state\n        hidden_states = []\n\n        for t in range(T):\n            xt = x[t].reshape(-1, 1)\n            concat = np.vstack((h, xt))\n\n            ft = self.sigmoid(self.Wf @ concat + self.bf)\n            it = self.sigmoid(self.Wi @ concat + self.bi)\n            ct_hat = self.tanh(self.Wc @ concat + self.bc)\n            ot = self.sigmoid(self.Wo @ concat + self.bo)\n\n            c = ft * c + it * ct_hat\n            h = ot * self.tanh(c)\n\n            hidden_states.append(h.flatten())\n\n        return np.around(np.array(hidden_states), 4).tolist(), np.around(h.flatten(), 4).tolist(), np.around(c.flatten(), 4).tolist()\n```\n\nThis code defines an LSTM network with the specified tasks and constraints. The `forward` method processes the given sequence of inputs `x`, starting from the provided initial hidden state and cell state. It returns the hidden states at each time step, as well as the final hidden state and cell state, all rounded to the nearest 4th decimal and converted to Python lists using the `tolist` method."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom math import log\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus is empty.\")\n    \n    # Calculate Term Frequency (TF)\n    tf_scores = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        doc_tf = {}\n        for word in doc:\n            if word in doc_tf:\n                doc_tf[word] += 1\n            else:\n                doc_tf[word] = 1\n        tf_scores.append(doc_tf)\n    \n    # Calculate Document Frequency (DF)\n    df_scores = {}\n    for doc in corpus:\n        for word in set(doc):\n            if word in df_scores:\n                df_scores[word] += 1\n            else:\n                df_scores[word] = 1\n    \n    # Calculate Inverse Document Frequency (IDF)\n    idf_scores = {}\n    num_docs = len(corpus)\n    for word in df_scores:\n        idf_scores[word] = log(num_docs / (df_scores[word] + 1))  # Adding 1 for smoothing\n    \n    # Calculate TF-IDF for each query word in each document\n    tf_idf_scores = []\n    for doc_tf in tf_scores:\n        doc_query_tfidf = []\n        for q_word in query:\n            if q_word in doc_tf and q_word in idf_scores:\n                tf = doc_tf[q_word] / len(doc_tf)\n                idf = idf_scores[q_word]\n                tf_idf = tf * idf\n                doc_query_tfidf.append(round(tf_idf, 4))\n            else:\n                doc_query_tfidf.append(0.0)  # Word not found in document or corpus\n        tf_idf_scores.append(doc_query_tfidf)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage\ncorpus = [\n    [\"the\", \"cat\", \"in\", \"the\", \"hat\"],\n    [\"a\", \"dog\", \"is\", \"a\", \"man's\", \"best\", \"friend\"],\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n]\nquery = [\"the\", \"dog\", \"cat\", \"quick\", \"friend\"]\n\n# Compute and print TF-IDF scores\ntf_idf_scores = compute_tf_idf(corpus, query)\nprint(tf_idf_scores)\n```\n\nThis script defines and implements the `compute_tf_idf` function for calculating the TF-IDF scores of query terms over a set of documents. It handles edge cases such as an empty corpus or query terms not present in the corpus. The function outputs the scores rounded to four decimal places, as required."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate the true positives, false positives, and false negatives\n    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n    false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n    \n    # Precision and Recall calculation\n    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n    \n    # F-Score calculation\n    if precision == 0 and recall == 0:\n        f_beta_score = 0\n    else:\n        f_beta_score = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta_score, 3)\n\n# Example check function for the f_score function\ndef check_f_score(beta):\n    y_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 1, 1, 0, 0, 1])\n    print(f\"F-Score with beta={beta}: {f_score(y_true, y_pred, beta)}\")\n\n# Test the function with beta=1 (F1-Score) and beta=2\ncheck_f_score(1)\ncheck_f_score(2)\n```\n\nThis script provides a function `f_score` that calculates the F-Score given true labels, predicted labels, and a beta value. It also includes a `check_f_score` function to test the `f_score` function with different beta values."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size)*0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n        self.W_hy = np.random.randn(output_size, hidden_size)*0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(x)\n        hidden_states = np.zeros((self.hidden_size, T+1))\n        outputs = np.zeros((self.W_hy.shape[0], T))\n        \n        for t in range(T):\n            hidden_states[:, t+1] = np.tanh(self.W_xh @ x[t] + self.W_hh @ hidden_states[:, t] + self.b_h).ravel()\n            outputs[:, t] = self.W_hy @ hidden_states[:, t+1] + self.b_y.ravel()\n        \n        return outputs, hidden_states\n\n    def backward(self, x, y, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \"\"\"\n        T = len(x)\n        outputs, hidden_states = self.forward(x)\n        dW_xh, dW_hh, dW_hy = np.zeros_like(self.W_xh), np.zeros_like(self.W_hh), np.zeros_like(self.W_hy)\n        db_h, db_y = np.zeros_like(self.b_h), np.zeros_like(self.b_y)\n        \n        for t in reversed(range(T)):\n            dy = outputs[:, t] - y[t]\n            dW_hy += dy[:, None] @ hidden_states[:, t+1][:, None].T\n            db_y += dy\n            \n            dhidden = self.W_hy.T @ dy\n            \n            dhraw = (1 - hidden_states[:, t+1]**2) * dhidden\n            \n            db_h += dhraw\n            dW_xh += dhraw[:, None] @ x[t][:, None].T\n            dW_hh += dhraw[:, None] @ hidden_states[:, t][:, None].T\n            \n            for t_ in reversed(range(t)):\n                dhraw = self.W_hh.T @ dhraw * (1 - hidden_states[:, t_+1]**2)\n                dW_xh += dhraw[:, None] @ x[t_][:, None].T\n                dW_hh += dhraw[:, None] @ hidden_states[:, t_][:, None].T\n                db_h += dhraw\n        \n        # Update the weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        \n        # Return the loss\n        loss = 0.5 * np.sum((y - outputs)**2)\n        return loss\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the training sequence\n    x_train = np.array([[0,1], [1,0], [0,1], [1,0]])\n    y_train = np.array([[1,0], [0,1], [1,0], [0,1]])\n    \n    # Initialize the RNN\n    rnn = SimpleRNN(input_size=2, hidden_size=2, output_size=2)\n    \n    # Perform forward and backward pass\n    print(f\"Initial loss: {rnn.backward(x_train, y_train, learning_rate=0.1):.4f}\")\n    for epoch in range(1000):\n        loss = rnn.backward(x_train, y_train, learning_rate=0.1)\n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n    \n    # Test the trained RNN\n    outputs, _ = rnn.forward(x_train)\n    print(\"Predictions:\")\n    print(outputs)\n```\nThis Python code implements a simple RNN with the forward and backward passes. The forward pass computes the hidden states and outputs for a given sequence of inputs. The backward pass computes the gradients of the loss function with respect to the weights and biases, and updates the weights and biases using gradient descent. The example usage section shows how to train the RNN on a simple sequence prediction task and print the initial and final loss, as well as the predictions after training."}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    \n    r = b - np.dot(A, x0)  # Initial residual\n    p = r.copy()  # Initial search direction\n    rsold = np.dot(r, r)\n    \n    x = x0.copy()  # Start with the initial guess\n    \n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + (rsnew / rsold) * p\n        rsold = rsnew\n    \n    return np.round(x, 8).tolist()\n\n# Example usage:\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 50\nx0 = np.array([0, 0])\nx = conjugate_gradient(A, b, n, x0)\nprint(x)\n```\n\nThis script defines a function `conjugate_gradient` that implements the Conjugate Gradient method for solving a symmetric, positive-definite system of linear equations. The solution vector `x` is returned rounded to 8 decimal places and in list form."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Convert the list to a numpy array for efficient computation\n    y_array = np.array(y)\n    # Find the unique class labels and their counts\n    classes, counts = np.unique(y_array, return_counts=True)\n    # Calculate the total number of instances\n    total = np.sum(counts)\n    # Calculate the probability of each class\n    probabilities = counts / total\n    # Calculate Gini Impurity\n    gini = 1 - np.sum(probabilities ** 2)\n    # Round the impurity to three decimal places\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    sample_y = [1, 2, 3, 1, 2, 2, 3, 3, 3, 3, 1, 1, 2]\n    # Calculate the Gini Impurity\n    print(\"Gini Impurity of the sample dataset:\", gini_impurity(sample_y))\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    \n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    col_indices = []\n    row_pointers = [0]  # The first row starts from index 0\n    \n    for row in dense_matrix:\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n        row_pointers.append(len(values))  # End of the row is the current length of values\n    \n    return (values, col_indices, row_pointers)\n\n# Check function to verify the solution\ndef check_solution():\n    mat1 = [\n        [1, 0, 0, 0],\n        [0, 2, 0, 3],\n        [0, 0, 0, 4],\n        [5, 0, 0, 0]\n    ]\n    expected1 = ([1, 2, 3, 4, 5], [0, 1, 3, 3, 0], [0, 1, 3, 4, 5])\n    assert compressed_row_sparse_matrix(mat1) == expected1, \"Test case 1 failed\"\n    \n    mat2 = [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]\n    ]\n    expected2 = ([], [], [0, 0, 0, 0])\n    assert compressed_row_sparse_matrix(mat2) == expected2, \"Test case 2 failed\"\n    \n    print(\"All test cases passed.\")\n\n# Run the check function\ncheck_solution()\n```\n\nThis Python script includes a function `compressed_row_sparse_matrix` that converts a given dense matrix into the Compressed Row Sparse (CSR) format, along with a check function to validate its correctness using predefined test cases."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    projection = (np.dot(v, L) / np.dot(L, L)) * L\n    return [round(x, 3) for x in projection]\n\n# Check function to verify the correctness of the generated function\ndef check_orthogonal_projection():\n    assert orthogonal_projection([1, 2], [1, 0]) == [1.0, 0.0], \"Test case 1 failed\"\n    assert orthogonal_projection([3, 4], [1, 1]) == [3.5, 3.5], \"Test case 2 failed\"\n    assert orthogonal_projection([2, 3, 4], [1, 0, 0]) == [2.0, 0.0, 0.0], \"Test case 3 failed\"\n    assert orthogonal_projection([1, 2, 3], [1, 2, 1]) == [1.667, 3.333, 1.667], \"Test case 4 failed\"\n    print(\"All test cases passed\")\n\n# Run the check function to verify the solution\ncheck_orthogonal_projection()\n```\n\nThis script contains a function `orthogonal_projection` that calculates the projection of a vector `v` onto a line defined by vector `L` and a check function `check_orthogonal_projection` that verifies the correctness of the solution against several test cases."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row_indices, column_pointer)\n    \"\"\"\n    rows = len(dense_matrix)\n    cols = len(dense_matrix[0]) if rows > 0 else 0\n\n    # Initialize the column pointer list with a zero.\n    column_pointer = [0]\n    \n    # Initialize the values list and row_indices list.\n    values = []\n    row_indices = []\n\n    # Iterate over the columns of the matrix in column-major order.\n    for col in range(cols):\n        for row in range(rows):\n            # If the element is non-zero, add it to the values list and its row index to the row_indices list.\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        # Append the number of non-zero elements found so far to the column_pointer list.\n        column_pointer.append(len(values))\n\n    return values, row_indices, column_pointer\n\n# Example usage\ndense_matrix = [\n    [1, 0, 0, 0],\n    [0, 2, 0, 0],\n    [0, 0, 3, 0],\n    [0, 0, 0, 0],\n    [4, 0, 0, 5]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\nThis script defines the function `compressed_col_sparse_matrix` and demonstrates its usage with an example matrix. The output consists of the non-zero values, their corresponding row indices, and the column pointer array that describes the CSC representation of the input matrix."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Finds the basis vectors for the column space (image) of matrix A.\n    \n    Parameters:\n    A (list of lists): The input matrix represented as a list of lists.\n    \n    Returns:\n    list of lists: The basis vectors of the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    # Convert input list to numpy matrix\n    A_np = np.array(A, dtype=float)\n    \n    # Get the row echelon form of A\n    A_rref, _ = np.linalg.qr(A_np.T)\n    A_rref = A_rref.T\n    \n    # Identify pivot columns by finding columns with leading 1s\n    pivot_columns = []\n    for row in A_rref:\n        for i, val in enumerate(row):\n            if np.abs(val) > 1e-8:  # consider as pivot if > 1e-8 to avoid numerical issues\n                pivot_columns.append(i)\n                break\n    \n    # Extract the corresponding columns from the original matrix\n    basis_vectors = A_np[:, pivot_columns]\n    \n    # Round the result to 8 decimal places and convert to list\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    \n    return basis_vectors_rounded\n\n# Example usage\nA = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(matrix_image(A))\n```\n\nThis code snippet uses QR decomposition as a method to find the reduced row echelon form (rref) of the matrix, which is a common approach to find the independent columns (basis vectors) of the original matrix. The `np.linalg.qr` function is used to achieve this. Note that the method of identifying pivot columns assumes that the QR decomposition provides a similar form to the row echelon form, where the leading non-zero elements (pivots) signify the independent columns. The result is then rounded and returned as a list of lists."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a regression model.\n    \n    Parameters:\n    y_true (np.ndarray): An array of true values.\n    y_pred (np.ndarray): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    # Ensure numpy arrays for operations\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    return round(r2, 3)\n\n# Example usage\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(r_squared(y_true, y_pred))\n```\n\nThis script defines the function `r_squared` to calculate the coefficient of determination, R-squared, for regression analysis based on provided true and predicted values. It includes an example usage to demonstrate the function's application."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    if not img or any(len(row) != len(img[0]) for row in img):\n        # Check if the image is empty or if rows have inconsistent lengths\n        return -1\n    \n    total_brightness = 0\n    pixel_count = 0\n    \n    for row in img:\n        for pixel in row:\n            if not (0 <= pixel <= 255):\n                # Check if pixel values are outside the valid range\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    \n    if pixel_count == 0:\n        # Prevent division by zero if there are no pixels\n        return -1\n    \n    average_brightness = total_brightness / pixel_count\n    return round(average_brightness, 2)\n\n# Example usage\nimage = [\n    [130, 120, 150],\n    [160, 180, 170],\n    [190, 200, 210]\n]\n\nprint(calculate_brightness(image))  # Expected output: 166.67\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate and return the Root Mean Square Error (RMSE) between the arrays y_true and y_pred.\n    The RMSE is rounded to three decimal places.\n    Parameters:\n    - y_true: array-like, true target values.\n    - y_pred: array-like, predicted target values.\n    \n    Returns:\n    - RMSE value as a float.\n    \"\"\"\n    # Check if inputs are valid numpy arrays\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError(\"Inputs must be numpy arrays.\")\n    \n    # Check if input arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check if input arrays have the same length\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same length.\")\n    \n    # Calculate RMSE\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    \n    # Return RMSE rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n    print(rmse(y_true, y_pred))  # Expected output: 0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    - y_true: np.array, the true binary labels.\n    - y_pred: np.array, the predicted binary labels.\n    \n    Returns:\n    - float, the Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays if they are not already\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Avoid division by zero error\n    if union == 0:\n        return 0.0\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    return round(jaccard, 3)\n\n# Example usage:\ny_true = np.array([1, 1, 0, 0, 1, 0, 1, 0])\ny_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])\nprint(jaccard_index(y_true, y_pred))  # Output should be 0.4\n```\n\nThis is a self-contained Python script that defines a function `jaccard_index(y_true, y_pred)` to calculate the Jaccard Index for binary classification tasks. The script includes an example usage demonstrating how to use the function with two numpy arrays representing true and predicted labels."}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    # Calculate the intersection of true and predicted labels\n    intersection = np.sum(y_true * y_pred)\n    \n    # If no true positives and no predicted positives, Dice Score is 1\n    if (np.sum(y_true) == 0) and (np.sum(y_pred) == 0):\n        return round(1.0, 3)\n    \n    # Calculate Dice Score\n    dice = (2.0 * intersection) / (np.sum(y_true) + np.sum(y_pred))\n    \n    # Handle division by zero if there are no true or predicted positives\n    if np.isnan(dice):\n        return 0.0\n    \n    return round(dice, 3)\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    # Case 1: Perfect overlap\n    print(dice_score([1, 1, 1, 0, 0], [1, 1, 1, 0, 0]))  # Expected output: 1.0\n    # Case 2: No overlap\n    print(dice_score([1, 0, 0], [0, 1, 1]))  # Expected output: 0.0\n    # Case 3: Partial overlap\n    print(dice_score([1, 1, 0, 0], [1, 0, 1, 0]))  # Expected output: 0.5\n    # Case 4: No true positives and no predicted positives\n    print(dice_score([0, 0, 0], [0, 0, 0]))  # Expected output: 1.0\n    # Case 5: No true positives, some predicted positives\n    print(dice_score([0, 0, 0], [1, 1, 1]))  # Expected output: 0.0\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    def gen_hv(seed, dim):\n        \"\"\"Generate a random hypervector with a given seed.\"\"\"\n        np.random.seed(seed)\n        return np.random.choice([-1, 1], size=dim)\n    \n    def bind_hvs(hv1, hv2):\n        \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n        return hv1 * hv2\n    \n    def bundle_hvs(hvs):\n        \"\"\"Bundle a list of hypervectors using element-wise summation and normalization.\"\"\"\n        bundled_hv = np.sum(hvs, axis=0)\n        return np.sign(bundled_hv)\n    \n    # Generate hypervectors for each feature and its value, then bind them.\n    feature_hvs = []\n    for feature, value in row.items():\n        feature_seed = random_seeds.get(feature)\n        if feature_seed is None:\n            raise ValueError(f\"No seed provided for feature: {feature}\")\n        feature_name_hv = gen_hv(feature_seed, dim)\n        value_hv = gen_hv(hash((feature_seed, value)), dim)\n        bound_hv = bind_hvs(feature_name_hv, value_hv)\n        feature_hvs.append(bound_hv)\n    \n    # Bundle all feature hypervectors to create a composite row hypervector.\n    composite_hv = bundle_hvs(feature_hvs)\n    \n    return composite_hv.tolist()\n\n# Example usage\nrow = {'temperature': 30, 'humidity': 50, 'wind': 10}\ndim = 10000\nrandom_seeds = {'temperature': 123, 'humidity': 234, 'wind': 345}\nhv = create_row_hv(row, dim, random_seeds)\nprint(hv)\n```\n\nThis script implements the `create_row_hv` function as specified in the task. It uses helper functions to generate, bind, and bundle hypervectors. The `gen_hv` function generates a random hypervector with a given seed to ensure reproducibility. The `bind_hvs` function binds two hypervectors, and the `bundle_hvs` function bundles a list of hypervectors to create a composite hypervector. The example usage demonstrates how to generate a composite hypervector for a given dataset row."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification.\n    \n    Parameters:\n    data (list of lists): A list of [y_true, y_pred] pairs, where y_true is the true label and y_pred is the predicted label.\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix where the rows are [true negatives, false positives] and [false negatives, true positives].\n    \"\"\"\n    # Initialize a counter for the four states: TN, FP, FN, TP\n    # We use a dictionary to map the tuple (y_true, y_pred) to an index in the confusion matrix\n    state_counter = Counter()\n    for y_true, y_pred in data:\n        # Convert true and predicted values into indices\n        # 0 for negative, 1 for positive\n        index_true = int(y_true)\n        index_pred = int(y_pred)\n        # Map the pair to an index in the confusion matrix: TN, FP, FN, TP\n        state_counter[(index_true, index_pred)] += 1\n    \n    # Extract counts from the counter, default to 0 if the state was not encountered\n    tn = state_counter[(0, 0)]  # True Negative\n    fp = state_counter[(0, 1)]  # False Positive\n    fn = state_counter[(1, 0)]  # False Negative\n    tp = state_counter[(1, 1)]  # True Positive\n    \n    # Construct the confusion matrix\n    matrix = [[tn, fp],\n              [fn, tp]]\n    \n    return matrix\n\n# Example usage:\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [1, 1], [0, 0]]\nprint(confusion_matrix(data))\n```\n\nThis solution would output the confusion matrix based on the provided `data` list of true and predicted pairs. The Counter object is used to aggregate the counts for each combination of true and predicted labels. The confusion matrix is then constructed with the counts for true negatives, false positives, false negatives, and true positives."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1, v2: Numpy arrays representing the input vectors.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both input vectors must have the same shape.\")\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return round(dot_product / (norm_v1 * norm_v2), 3)\n\n# Example usage:\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\nprint(cosine_similarity(vector1, vector2))\n```\n\nThis script defines a function `cosine_similarity` that calculates the cosine similarity between two vectors, following the constraints and requirements specified in the task. It also includes a simple example of how to use the function."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, f1_score\nimport numpy as np\n\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification problem.\n    \n    Parameters:\n    actual (list[int]): The actual class labels.\n    predicted (list[int]): The predicted class labels.\n    \n    Returns:\n    tuple: A tuple containing confusion matrix, accuracy, f1_score, specificity, and negative_predictive_value.\n    \"\"\"\n    # Check if lengths of actual and predicted lists are equal\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    \n    # Calculate the confusion matrix\n    conf_matrix = confusion_matrix(actual, predicted)\n    \n    # Extract TP, TN, FP, FN from the confusion matrix\n    TN, FP, FN, TP = conf_matrix.ravel()\n    \n    # Calculate accuracy\n    accuracy = round((TP + TN) / (TP + TN + FP + FN), 3)\n    \n    # Calculate F1 score\n    f1 = round(f1_score(actual, predicted), 3)\n    \n    # Calculate specificity\n    specificity = round(TN / (TN + FP), 3)\n    \n    # Calculate negative predictive value\n    NPV = round(TN / (TN + FN), 3)\n    \n    return conf_matrix, accuracy, f1, specificity, NPV\n\n# Example usage\nactual = [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 0, 1, 1, 0, 1]\nconf_matrix, accuracy, f1, specificity, NPV = performance_metrics(actual, predicted)\nprint(f\"Confusion Matrix:\\n{conf_matrix}\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"Specificity: {specificity}\")\nprint(f\"Negative Predictive Value: {NPV}\")\n```\n\nThis implementation of the `performance_metrics` function calculates the required performance metrics for a binary classification problem, using the actual and predicted class labels. It returns a tuple containing the confusion matrix and four performance metrics, each rounded to three decimal places."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Convert data to numpy array for computation\n    data_array = np.array(data)\n    \n    # Calculate various statistics\n    mean = np.mean(data_array)\n    median = np.median(data_array)\n    mode = stats.mode(data_array)[0][0]  # Mode from scipy.stats to handle multimodal data\n    variance = np.var(data_array, ddof=1)  # Sample variance\n    standard_deviation = np.std(data_array, ddof=1)  # Sample standard deviation\n    percentiles = np.percentile(data_array, [25, 50, 75])\n    iqr = stats.iqr(data_array)  # Interquartile range\n    \n    # Prepare the output dictionary\n    output = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles[0], 4),\n        '50th_percentile': round(percentiles[1], 4),\n        '75th_percentile': round(percentiles[2], 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    \n    return output\n\n# Example usage\ndata = [1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10]\nprint(descriptive_statistics(data))\n```\n\nThis script defines a function `descriptive_statistics` that takes a list or NumPy array of numerical values and calculates various statistical metrics, returning a dictionary with the specified keys. The use of the `scipy.stats.mode` function ensures proper handling of multimodal datasets. Other values are rounded to four decimal places as required."}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage:\n# Calculate the probability of getting exactly 3 heads in 5 coin flips with a fair coin (p=0.5)\nprint(binomial_probability(5, 3, 0.5))\n```\n\nThe provided code defines a function `binomial_probability(n, k, p)` that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials with a success probability `p` for each trial. The function uses the binomial distribution formula and rounds the result to five decimal places as per the requirement. The example usage calculates the probability of getting exactly 3 heads in 5 coin flips with a fair coin."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\n\n# Test the function\nif __name__ == \"__main__\":\n    # Test cases\n    print(normal_pdf(0, 0, 1))  # Should print the PDF of the standard normal distribution at 0\n    print(normal_pdf(1, 0, 1))  # Should print the PDF of the standard normal distribution at 1\n    print(normal_pdf(-1, 0, 1)) # Should print the PDF of the standard normal distribution at -1\n    print(normal_pdf(0, 1, 2))  # Should print the PDF of a normal distribution with mean 1 and std dev 2 at 0\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate (mean) of occurrences lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events (float rounded to 5 decimal places)\n    \"\"\"\n    numerator = math.pow(lam, k) * math.exp(-lam)\n    denominator = math.factorial(k)\n    probability = numerator / denominator\n    return round(probability, 5)\n\n# Example usage:\n# Calculate the probability of observing exactly 3 events when the average rate of occurrences is 2\nprint(poisson_probability(3, 2))\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    # Ensure the input is a numpy array\n    img = np.asarray(img)\n    # Calculate the maximum pixel value\n    max_val = np.max(img)\n    # Calculate the minimum pixel value\n    min_val = np.min(img)\n    # Calculate contrast as the difference between max and min pixel values\n    contrast = max_val - min_val\n    return contrast\n\n# Example usage\nif __name__ == \"__main__\":\n    # Creating a sample 2D grayscale image\n    sample_img = np.array([[0, 10, 20], [30, 255, 200], [100, 150, 255]])\n    # Calculating the contrast of the sample image\n    contrast = calculate_contrast(sample_img)\n    print(f\"The contrast of the image is: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        int or float: The dot product of the two vectors.\n    \"\"\"\n    # Ensure the vectors are numpy arrays\n    vec1 = np.array(vec1)\n    vec2 = np.array(vec2)\n    \n    # Calculate the dot product using numpy's dot function\n    dot_product = np.dot(vec1, vec2)\n    \n    return dot_product\n\n# Example usage:\n# Create two 1D numpy arrays\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\n\n# Calculate the dot product\nresult = calculate_dot_product(vector1, vector2)\nprint(\"Dot Product:\", result)\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n    \n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    # Checking if degree is less than 0\n    if degree < 0:\n        return []\n    \n    # Convert the input list to a numpy array\n    data_np = np.array(data)\n    \n    # Generate polynomial features\n    poly_features = np.vander(data_np, N=degree+1, increasing=True)\n    \n    # Convert back to list of lists and round to 8 decimal places\n    poly_features_list = [list(map(lambda x: round(x, 8), row)) for row in poly_features]\n    \n    return poly_features_list\n\n# Example usage:\ndata = [1.0, 2.0, 3.0]\ndegree = 3\nprint(phi_transform(data, degree))\n```\n\nThis solution employs `numpy`'s `vander` function to generate the polynomial features, which creates a Vandermonde matrix, where the columns are powers of the input vector. The `increasing=True` argument ensures that the powers increase from left to right, aligning with the polynomial expansion from lower to higher degrees. The result is then rounded to 8 decimal places to comply with the output constraints."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a given sequence length and model dimension.\n    \n    Args:\n    position: The length of the sequence for which to compute positional encodings.\n    d_model: The dimensionality of the model, must be even.\n    \n    Returns:\n    A list of positional encodings if position and d_model are valid, -1 otherwise.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Ensure d_model is even\n    if d_model % 2 != 0:\n        raise ValueError(\"d_model must be even.\")\n    \n    # Initialize the positional encoding array\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Calculate pe\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            angle_rads = pos / np.power(10000, (2 * (i // 2)) / np.float16(d_model))\n            pe[pos, i] = np.sin(angle_rads)\n            pe[pos, i + 1] = np.cos(angle_rads)\n    \n    return pe.tolist()\n\n# Example usage\nprint(pos_encoding(3, 4))\n```\n\nPlease note that the function raises a `ValueError` if `d_model` is not even, as the positional encoding formula provided above relies on pairs of sine and cosine functions. This is part of the original implementation details of the Transformer model."}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        # Overfitting condition\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        # Underfitting condition\n        return -1\n    else:\n        # Good fit condition\n        return 0\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    print(model_fit_quality(0.9, 0.6))  # Overfitting, should return 1\n    print(model_fit_quality(0.65, 0.6)) # Underfitting, should return -1\n    print(model_fit_quality(0.8, 0.78)) # Good fit, should return 0\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second raw moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Correct bias in first moment\n    m_hat = m / (1 - beta1 ** t)\n    \n    # Correct bias in second moment\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round and convert to list if necessary\n    if isinstance(parameter, np.ndarray):\n        parameter = np.round(parameter, 5).tolist()\n        m = np.round(m, 5).tolist()\n        v = np.round(v, 5).tolist()\n    else:\n        parameter = round(parameter, 5)\n        m = round(m, 5)\n        v = round(v, 5)\n    \n    return parameter, m, v\n\n# Example usage\nparameter = np.array([1.0, -2.0])\ngrad = np.array([0.1, -0.1])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1\n\nparameter, m, v = adam_optimizer(parameter, grad, m, v, t)\nprint(\"Updated parameter:\", parameter)\nprint(\"Updated first moment (m):\", m)\nprint(\"Updated second moment (v):\", v)\n```\n\nThis script defines the `adam_optimizer` function which implements the Adam optimizer update step according to the description provided. It handles both scalar and array inputs, applies bias correction, and rounds the results to 5 decimal places as required. The example usage demonstrates how the function can be called with a set of parameters and gradients, updating the parameters and moments accordingly."}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],  # Placeholder for transformer blocks\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit.\"\"\"\n    cdf = 0.5 * (1.0 + np.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))))\n    return x * cdf\n\ndef layer_norm(x, g, b, eps: float = 1e-5):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    std = np.std(x, axis=-1, keepdims=True)\n    return g * (x - mean) / (std + eps) + b\n\ndef attention(q, k, v, mask):\n    scores = np.matmul(q, np.transpose(k, (0, 2, 1))) / np.sqrt(k.shape[-1])\n    scores = scores - 1e9 * mask\n    scores = np.exp(scores) / (np.sum(np.exp(scores), axis=-1, keepdims=True) + 1e-5)\n    output = np.matmul(scores, v)\n    return output\n\ndef feed_forward(x, w1, w2, b1, b2):\n    return gelu(np.matmul(x, w1) + b1) @ w2 + b2\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    n_ctx = hparams[\"n_ctx\"]\n    \n    # Token Embeddings\n    token_embeddings = params[\"wte\"][token_ids]\n    \n    # Positional Embeddings\n    pos_embeddings = params[\"wpe\"][np.arange(len(token_ids))]\n    \n    # Combine\n    x = token_embeddings + pos_embeddings\n\n    # Dummy transformer blocks (replace with actual implementation)\n    for _ in range(n_tokens_to_generate):\n        x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        q = k = v = x  # Simplified for illustration\n        mask = np.triu(np.ones((len(x), len(x))), k=1).astype(bool)\n        attention_out = attention(q, k, v, mask)\n        x = x + attention_out\n        x = x + feed_forward(x, np.random.rand(10, 10), np.random.rand(10, 10), np.zeros(10), np.zeros(10))\n    \n    # Predicting next token\n    logits = x[-1] @ params[\"wte\"].T\n    next_token_id = np.argmax(logits, axis=-1)\n    token_ids.append(next_token_id)\n\n    return encoder.decode(token_ids)\n\n# Example usage\nprint(gen_text(\"hello world\"))\n```\nThis script is a simplified version that doesn't include all the complexities of the actual GPT-2 architecture, such as the actual transformer blocks and a more detailed attention mechanism. The attention mechanism and feed-forward network are also vastly simplified for the purpose of illustration. In a real-world scenario, the `load_encoder_hparams_and_params` function would load pre-trained weights and hyperparameters, and the transformer blocks would be implemented with proper attention heads and layer normalization."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    \n    # Convert the list of crystal values into a numpy array\n    crystals = np.array(crystal_values).reshape((n, dimension))\n    \n    # Initialize a list to store the final weighted patterns\n    final_patterns = []\n    \n    # For each crystal, calculate its relationship with every other crystal\n    for i in range(n):\n        # Initialize an array to hold the attention scores\n        attention_scores = []\n        for j in range(n):\n            # Calculate the dot product as a simple form of interaction\n            score = np.dot(crystals[i], crystals[j])\n            attention_scores.append(score)\n        \n        # Apply softmax to get the attention weights\n        attention_weights = softmax(attention_scores)\n        \n        # Derive the final weighted pattern for each crystal\n        weighted_pattern = np.dot(attention_weights, crystals)\n        \n        # Append the final weighted pattern to the list\n        final_patterns.append(weighted_pattern)\n    \n    # Flatten the list and round each value to the 4th decimal place\n    result = [round(value, 4) for pattern in final_patterns for value in pattern]\n    \n    return result\n\n# Example usage:\nn = 4\ncrystal_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\ndimension = 3\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script follows the task instructions, implementing a simplified self-attention mechanism for the given sequence of crystal values, and returns the final weighted patterns for each crystal rounded to the fourth decimal place."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate the BM25 scores for each document in the corpus given a query.\n    \n    :param corpus: A list of documents, where each document is a list of terms.\n    :param query: A list of query terms.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: A list of BM25 scores for each document in the corpus.\n    \"\"\"\n    # Calculate IDF values\n    N = len(corpus)\n    query_term_set = set(query)\n    idf = {term: np.log((N - sum(term in doc for doc in corpus) + 0.5) / (sum(term in doc for doc in corpus) + 0.5))\n           for term in query_term_set}\n    \n    # Calculate average document length\n    avgdl = sum(len(doc) for doc in corpus) / N\n    \n    # Calculate BM25 scores\n    bm25_scores = []\n    for doc in corpus:\n        doc_length = len(doc)\n        tf = Counter(doc)\n        score = sum(idf[term] * (tf[term] * (k1 + 1)) / (tf[term] + k1 * (1 - b + b * doc_length / avgdl))\n                    for term in query_term_set if term in tf)\n        bm25_scores.append(round(score, 3))\n    \n    return bm25_scores\n\n# Example usage\ncorpus = [\n    [\"information\", \"retrieval\", \"system\"],\n    [\"information\", \"retrieval\", \"information\", \"retrieval\"],\n    [\"system\", \"information\", \"database\"]\n]\nquery = [\"information\", \"retrieval\"]\n\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines the `calculate_bm25_scores` function to compute the BM25 scores for a given corpus and query. The function takes into account parameters for term frequency saturation (`k1`) and document length normalization (`b`). It first calculates the Inverse Document Frequency (IDF) for each term in the query, then computes the average document length, and finally calculates the BM25 score for each document in the corpus based on these values."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0  # True Positives\n    fp = 0  # False Positives\n    fn = 0  # False Negatives\n\n    # Calculate tp, fp, fn\n    for t, p in zip(y_true, y_pred):\n        if t == 1 and p == 1:\n            tp += 1\n        elif t == 0 and p == 1:\n            fp += 1\n        elif t == 1 and p == 0:\n            fn += 1\n\n    # Precision\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    # Recall\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    # F1 Score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return round(f1_score, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]\nprint(calculate_f1_score(y_true, y_pred))  # Output should be the F1 score\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Forecast the power consumption for day 15 after detrending the daily fluctuation,\n    predict the base consumption using linear regression, and add back the fluctuation\n    with a 5% safety margin.\n    :param consumption_data: List of daily power consumption data for the first 10 days.\n    :return: Integer value representing the forecasted consumption for day 15 with safety margin.\n    \"\"\"\n    # Detrend data by removing daily fluctuation\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * i / 10) for i in range(len(consumption_data))]\n    \n    # Prepare data for linear regression\n    X = [[i] for i in range(1, 11)]  # Days 1 to 10\n    Y = detrended_data  # Detrended data\n    \n    # Perform linear regression\n    model = LinearRegression()\n    model.fit(X, Y)\n    \n    # Predict day 15's base consumption\n    base_day_15 = model.predict([[15]])[0]\n    \n    # Add the day 15 fluctuation back\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_day_15 = base_day_15 + day_15_fluctuation\n    \n    # Round, then add a 5% safety margin (rounded up)\n    final_prediction = math.ceil(predicted_day_15 * 1.05)\n    \n    return int(final_prediction)\n\n# Example usage and verifying the correctness\ndef check():\n    example_data = [100, 110, 115, 120, 125, 130, 135, 140, 145, 150]\n    expected_output = 160  # This is a hypothetical expected value for verification purpose\n    actual_output = power_grid_forecast(example_data)\n    assert actual_output == expected_output, f\"Expected {expected_output}, but got {actual_output}\"\n    print(\"Test passed!\")\n\ncheck()\n```"}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"Both y_true and y_pred must be of type numpy.ndarray\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\n\n# Example usage:\n# true_values = np.array([3, -0.5, 2, 7])\n# predicted_values = np.array([2.5, 0.0, 2, 8])\n# print(mae(true_values, predicted_values))\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"Compute the query, key, and value matrices for the self-attention mechanism.\"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"Calculate the self-attention output of query, key, and value matrices.\"\"\"\n    d_k = K.shape[-1]\n    attention_scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"Implement the multi-head attention mechanism.\"\"\"\n    batch_size, seq_len, d_model = X.shape\n    d_k = d_model // n_heads\n    \n    # Split the last dimension into n_heads and d_k\n    X = X.reshape(batch_size, seq_len, n_heads, d_k)\n    \n    # Compute Q, K, V for each head\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    Q = Q.reshape(batch_size, seq_len, n_heads, d_k)\n    K = K.reshape(batch_size, seq_len, n_heads, d_k)\n    V = V.reshape(batch_size, seq_len, n_heads, d_k)\n    \n    # Compute self-attention for each head\n    heads = []\n    for i in range(n_heads):\n        head = self_attention(Q[:, :, i, :], K[:, :, i, :], V[:, :, i, :])\n        heads.append(head)\n    output = np.concatenate(heads, axis=-1)\n    \n    # Round to the nearest 4th decimal and convert to list\n    output = np.round(output, decimals=4).tolist()\n    \n    return output\n\n# Example usage\nX = np.random.randn(1, 4, 20)  # batch_size=1, seq_len=4, d_model=20\nW_q = np.random.randn(20, 20)  # d_model=d_k=d_v=20\nW_k = np.random.randn(20, 20)\nW_v = np.random.randn(20, 20)\nn_heads = 4\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(output)\n```\n\nThis script defines a multi-head attention mechanism as requested. It includes the `compute_qkv` function to compute the query, key, and value matrices, the `self_attention` function to compute the self-attention output, and the `multi_head_attention` function that integrates these and handles the multi-head aspect of the attention mechanism. The output of the multi-head attention is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Counting occurrences of 0s and 1s in both lists\n    x0 = sum(1 for i in x if i == 0)\n    x1 = sum(1 for i in x if i == 1)\n    y0 = sum(1 for i in y if i == 0)\n    y1 = sum(1 for i in y if i == 1)\n    \n    # Creating a contingency table\n    table = np.array([[x0 - sum(1 for i, j in zip(x, y) if i == 0 and j == 0), sum(1 for i, j in zip(x, y) if i == 0 and j == 1)],\n                      [sum(1 for i, j in zip(x, y) if i == 1 and j == 0), x1 - sum(1 for i, j in zip(x, y) if i == 1 and j == 1)]])\n\n    # Chi-squared test\n    chi2, _, _, _ = chi2_contingency(table)\n    n = len(x)\n    phi = np.sqrt(chi2 / n)\n    \n    return round(phi, 4)\n\n# Example usage\nx = [0, 1, 1, 1, 0, 0, 0, 1]\ny = [1, 0, 0, 1, 1, 0, 0, 1]\nprint(phi_corr(x, y))\n```\nThis solution first constructs a contingency table from the two input lists `x` and `y`, which are assumed to contain only 0s and 1s. Then, it uses the `chi2_contingency` function from `scipy.stats` to perform a chi-squared test and calculate the Phi coefficient from the chi-squared statistic. The returned Phi coefficient is rounded to 4 decimal places as requested."}
{"task_id": 96, "completion_id": 0, "completion": "Here is the Python script that implements the Hard Sigmoid activation function:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Function to test the hard_sigmoid implementation\ndef check_hard_sigmoid():\n    test_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\n    expected_outputs = [0.0, 0.0, 0.3, 0.5, 0.7, 1.0, 1.0]\n    for x, expected in zip(test_values, expected_outputs):\n        assert hard_sigmoid(x) == expected, f\"Value {x} did not return the expected output {expected}.\"\n    print(\"All tests passed!\")\n\n# Running the check function\ncheck_hard_sigmoid()\n```\n\nThis script includes a simple test function `check_hard_sigmoid` to verify the correctness of the `hard_sigmoid` function implementation with a few test cases."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n        \n# Test the function with some values\ndef check_elu():\n    print(f\"ELU(2.0): {elu(2.0):.4f}\")\n    print(f\"ELU(-2.0): {elu(-2.0):.4f}\")\n    print(f\"ELU(0.0): {elu(0.0):.4f}\")\n    print(f\"ELU(-1.0, 0.5): {elu(-1.0, 0.5):.4f}\")\n    print(f\"ELU(1.0, 0.5): {elu(1.0, 0.5):.4f}\")\n\ncheck_elu()\n```\n\nThis code snippet implements the ELU activation function as described in the task. It also includes a simple testing function to demonstrate the function's behavior with various inputs."}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Testing the PReLU function with different inputs and alpha values\n    print(prelu(-1.0))  # Using default alpha value\n    print(prelu(-1.5, alpha=0.1))  # Using a custom alpha value\n    print(prelu(2.0))  # Positive input, alpha value does not affect the output\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n    \n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Test cases to verify the correctness of the function\nif __name__ == \"__main__\":\n    print(softplus(10000))  # Should return 10000 as it is a large positive number\n    print(softplus(-10000)) # Should return 0 as it is a large negative number\n    print(softplus(0))      # Should return 0.6931\n    print(softplus(1))      # Should return 1.3133\n    print(softplus(-1))     # Should return 0.3133\n```\n\nIn the provided solution, the `softplus` function handles edge cases where `x` is a very large positive or negative number to prevent numerical issues and returns the softplus value rounded to the 4th decimal place for other cases. The test cases included at the end of the script are used to verify the correctness of the implementation."}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Test the function with sample data\nif __name__ == \"__main__\":\n    test_values = [-10.0, -1.5, 0.0, 1.5, 10.0]\n    for val in test_values:\n        print(f\"Softsign({val}) = {softsign(val)}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence(pi_theta_old, pi_theta_ref):\n    \"\"\"\n    Compute the KL divergence between the old policy and the reference policy.\n\n    Args:\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n\n    Returns:\n        The KL divergence value.\n    \"\"\"\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    kl = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    return kl\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Clip likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    objective = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # KL divergence penalty\n    kl_penalty = beta * kl_divergence(pi_theta_old, pi_theta_ref)\n    \n    # GRPO objective\n    grpo_obj = objective - kl_penalty\n    \n    return round(grpo_obj, 6)\n\n# Example usage\nrhos = [1.2, 0.8, 1.5, 0.9]\nA = [2.0, 1.5, 0.5, 1.0]\npi_theta_old = [0.7, 0.6, 0.8, 0.9]\npi_theta_ref = [0.6, 0.5, 0.7, 0.8]\n\nprint(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\nThis code defines the `grpo_objective` function and the necessary `kl_divergence` function to compute the KL divergence between the old policy and the reference policy, which is used as a penalty term in the GRPO objective. The likelihood ratios are clipped to ensure stability, and the objective value is returned rounded to the nearest 6th decimal place."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return x * (1 / (1 + math.exp(-x)))\n\n# Test the function with some sample inputs\ntest_inputs = [-2.0, -1.0, 0.0, 1.0, 2.0]\nresults = [round(swish(x), 4) for x in test_inputs]\nprint(results)\n```\n\nThis code will calculate the swish activation function for the test inputs and round the results to 4 decimal places, as required."}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Test the function to ensure it works as expected\ndef check_function():\n    test_values = [-1.0, 0.0, 1.0]\n    expected_results = [0.1681, 0.0000, 1.0507]\n    for val, expected in zip(test_values, expected_results):\n        result = round(selu(val), 4)\n        assert result == expected, f\"Test failed for input {val}. Expected {expected}, got {result}\"\n    print(\"All tests passed!\")\n\ncheck_function()\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights\n    linear_combination = np.dot(X, weights) + bias\n    # Apply the sigmoid function\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    # Convert probabilities to binary predictions (0 or 1)\n    predictions = (probabilities >= 0.5).astype(int)\n    # Return the predictions as a list\n    return predictions.tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    weights = np.array([0.5, 0.5])\n    bias = 0.2\n\n    # Make predictions\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)  # Output based on the given X, weights, and bias\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = len(np.unique(y))\n    B = np.zeros((n_features, n_classes))\n    losses = []\n\n    for _ in range(iterations):\n        # Compute softmax scores\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores)\n        softmax_scores = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        \n        # Compute cross-entropy loss\n        log_likelihood = -np.log(softmax_scores[range(n_samples), y])\n        loss = np.sum(log_likelihood) / n_samples\n        losses.append(round(float(loss), 4))\n        \n        # Compute gradient\n        softmax_scores[range(n_samples), y] -= 1\n        gradient = np.dot(X.T, softmax_scores) / n_samples\n        \n        # Update weights\n        B -= learning_rate * gradient\n\n    return B.round(4).tolist(), [round(loss, 4) for loss in losses]\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# y = np.array([0, 1, 0])\n# learning_rate = 0.01\n# iterations = 1000\n# B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n# print(\"Coefficients:\", B)\n# print(\"Losses:\", losses)\n```\nThis Python script defines a function `train_softmaxreg` which implements gradient descent for training a Softmax regression model. It also calculates and returns the optimized coefficients and the loss over iterations, as requested."}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid function that maps any real value into another value between 0 and 1.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Insert a column of 1s at the beginning of X for the intercept term\n    X = np.insert(X, 0, 1, axis=1)\n    n, m = X.shape  # n samples, m features (including intercept)\n    weights = np.zeros(m)  # Initialize weights\n    \n    losses = []  # Collect losses for each iteration\n\n    for _ in range(iterations):\n        # Compute predictions\n        z = X.dot(weights)\n        predictions = sigmoid(z)\n        \n        # Compute loss (binary cross entropy)\n        loss = (-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)).mean()\n        losses.append(round(loss, 4))\n        \n        # Compute gradients\n        gradients = X.T.dot(predictions - y) / n\n        # Update weights\n        weights -= learning_rate * gradients\n\n    # Round the weights and convert to list\n    weights = np.round(weights, 4).tolist()\n    return weights, losses\n\n# Example usage\nX = np.array([[0.5, 0.7], [0.1, 0.2], [0.3, 0.4]])\ny = np.array([0, 1, 1])\nlearning_rate = 0.01\niterations = 1000\nweights, losses = train_logreg(X, y, learning_rate, iterations)\nprint(\"Weights:\", weights)\nprint(\"Losses:\", losses)\n```\n\nThis script defines a function `train_logreg` that trains a logistic regression model using gradient descent. It takes as input the feature matrix `X`, the target vector `y`, the learning rate, and the number of iterations. The function returns a tuple containing the optimized weights and the list of losses over iterations, both rounded to the nearest 4th decimal."}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    d_k = K.shape[-1]  # Size of the last dimension of K\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)  # Attention score matrix\n    scores += mask  # Apply mask to scores\n    p_attn = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), 0, scores)  # Softmax\n    output = np.matmul(p_attn, V)  # Compute weighted sum of values\n    return output.tolist()  # Return as a Python list\n\n# Example usage\nX = np.array([[1, 0], [0, 1]])  # Example input\nW_q = np.array([[1, 0], [0, 1]])  # Example weight matrices\nW_k = np.array([[1, 0], [0, 1]])\nW_v = np.array([[1, 0], [0, 1]])\nmask = np.array([[0, -np.inf], [0, 0]])  # Example attention mask\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```\n```python\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom typing import List\nfrom collections import Counter\nimport math\n\ndef disorder(apples: List[int]) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The disorder is calculated using the Shannon Entropy formula, which measures\n    the uncertainty or randomness in the distribution of colors.\n    \"\"\"\n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the Shanon Entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # Round the result to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Check function to verify the correctness of the generated function\ndef check_disorder():\n    assert disorder([0,0,0,0]) == 0.0, \"Test case 1 failed.\"\n    assert disorder([1,1,0,0]) > 0.0, \"Test case 2 failed.\"\n    assert disorder([0,1,2,3]) > disorder([1,1,0,0]), \"Test case 3 failed.\"\n    assert disorder([0,0,1,1,2,2,3,3]) > disorder([0,0,0,0,0,1,2,3]), \"Test case 4 failed.\"\n    print(\"All test cases passed.\")\n\n# Run the check function\ncheck_disorder()\n```\n\nThis Python script defines a function `disorder` that measures the disorder (or entropy) in a basket of apples based on their colors, using the Shannon Entropy formula for the calculation. The results are rounded to the nearest 4th decimal place as requested. The script also includes a `check_disorder` function to verify the correctness of the `disorder` function against the specified criteria."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform layer normalization on the input tensor X.\n    \n    Args:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim).\n    gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    epsilon (float): A small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_norm = X_norm * gamma + beta\n    \n    # Return the result rounded to 5 decimal places\n    return np.round(X_norm, decimals=5).tolist()\n\n# Example usage:\nbatch_size, seq_length, feature_dim = 2, 3, 4\nX = np.random.rand(batch_size, seq_length, feature_dim)\ngamma = np.random.rand(feature_dim)\nbeta = np.random.rand(feature_dim)\n\nnormalized_X = layer_normalization(X, gamma, beta)\nprint(np.array(normalized_X))\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef ngrams(sequence, n):\n    \"\"\"Return a Counter of n-grams from the given sequence.\"\"\"\n    return Counter(zip(*[sequence[i:] for i in range(n)]))\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Compute the METEOR score for a given reference and candidate translation.\n    :param reference: list of words for the reference translation\n    :param candidate: list of words for the candidate translation\n    :param alpha: weight for the penalty component\n    :param beta: parameter for the fragmentation penalty\n    :param gamma: weight for the F-mean component\n    :return: METEOR score rounded to 3 decimal places\n    \"\"\"\n    ref_ngrams = ngrams(reference, 1)\n    cand_ngrams = ngrams(candidate, 1)\n    \n    matches = sum((ref_ngrams & cand_ngrams).values())\n    precision = matches / len(cand_ngrams) if cand_ngrams else 0\n    recall = matches / len(ref_ngrams) if ref_ngrams else 0\n    \n    # Calculate F-mean\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    # Calculate fragmentation penalty\n    ref_len = len(reference)\n    cand_len = len(candidate)\n    fragment_penalty = 0.0 if ref_len == cand_len else max(ref_len, cand_len) / float(min(ref_len, cand_len))\n    \n    # Apply fragmentation penalty\n    fragment_penalty = np.exp(-gamma * (fragment_penalty - 1))\n    \n    # Calculate final score\n    score = ((1 - alpha) * f_mean) + (alpha * fragment_penalty)\n    \n    return round(score, 3)\n\n# Example usage\nreference = \"the cat is on the mat\".split()\ncandidate = \"the cat is on the mat\".split()\nprint(meteor_score(reference, candidate))\n```\n\nThis code snippet defines a function `meteor_score` that calculates the METEOR score between a reference translation and a candidate translation. It uses unigram matches to calculate precision and recall, which are then used to compute the F-mean. The fragmentation penalty is applied based on the differences in the lengths of the reference and candidate translations. Finally, the F-mean and the penalty are combined to produce the METEOR score, which is returned rounded to three decimal places."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts (int): The joint occurrence count of event x and event y.\n    - total_counts_x (int): The total occurrence count of event x.\n    - total_counts_y (int): The total occurrence count of event y.\n    - total_samples (int): The total number of samples.\n    \n    Returns:\n    - float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Avoid division by zero or log(0) by checking if counts are zero\n    if joint_counts == 0 or total_counts_x == 0 or total_counts_y == 0:\n        return 0.0  # By convention, PMI is set to 0 in these cases to avoid undefined values\n    \n    # Calculate probabilities\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    p_xy = joint_counts / total_samples\n    \n    # Calculate PMI\n    pmi_value = np.log2(p_xy / (p_x * p_y))\n    \n    # Return the rounded PMI value\n    return round(pmi_value, 3)\n\n# Example usage:\n# Here, we pretend that we have 100 joint occurrences of events x and y, 500 occurrences of x,\n# 1000 occurrences of y, and 10000 total samples.\npmi = compute_pmi(100, 500, 1000, 10000)\nprint(pmi)  # Expected output: 0.694\n```\n\nThis script defines a function `compute_pmi` that calculates the Pointwise Mutual Information (PMI) using the provided parameters. The function handles potential cases that could lead to undefined values and returns the PMI value rounded to three decimal points, as required."}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    Args:\n    x (list[int]): A list of integers to be normalized.\n    \n    Returns:\n    list[float]: A list of normalized floats rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    \n    min_value = min(x)\n    max_value = max(x)\n    if min_value == max_value:\n        return [1.0] * len(x)\n    \n    normalized_x = [(i - min_value) / (max_value - min_value) for i in x]\n    return [round(value, 4) for value in normalized_x]\n\n# Check function to verify the correctness of the generated function\ndef check_min_max():\n    assert min_max([1, 2, 3, 4, 5]) == [0.0, 0.25, 0.5, 0.75, 1.0], \"Test case 1 failed\"\n    assert min_max([10, 20, 30]) == [0.0, 0.5, 1.0], \"Test case 2 failed\"\n    assert min_max([5, 5, 5, 5]) == [1.0, 1.0, 1.0, 1.0], \"Test case 3 failed\"\n    assert min_max([]) == [], \"Test case 4 (empty list) failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function\ncheck_min_max()\n```"}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implement a simple residual block.\n    \n    Args:\n    x (np.ndarray): The input array.\n    w1 (np.ndarray): The weight matrix for the first layer.\n    w2 (np.ndarray): The weight matrix for the second layer.\n    \n    Returns:\n    np.ndarray: The output array after passing through the residual block.\n    \"\"\"\n    # Ensure x can be matrix-multiplied by w1\n    assert x.shape[-1] == w1.shape[0], \"Input size does not match with the first weight matrix\"\n    assert w1.shape[1] == w2.shape[0], \"Shape mismatch between the two weight matrices\"\n    \n    # First layer\n    y = np.matmul(x, w1)\n    y = np.maximum(y, 0)  # ReLU activation\n    \n    # Second layer\n    y = np.matmul(y, w2)\n    y = np.maximum(y, 0)  # ReLU activation\n    \n    # Shortcut connection\n    y += x\n    \n    # Final ReLU activation\n    y = np.maximum(y, 0)\n    \n    # Rounding to 4 decimal places\n    y = np.round(y, 4)\n    \n    return y.tolist()\n\n# Example usage and check function\ndef check_residual_block():\n    w1 = np.array([[1, -1], [-1, 1]])\n    w2 = np.array([[2, 0], [0, 2]])\n    x = np.array([[-2, 2], [1, -1]])\n    \n    expected_output = [[0, 4], [2, 0]]\n    np.testing.assert_array_equal(residual_block(x, w1, w2), expected_output)\n    print(\"Test passed!\")\n\n# Run the check function to verify the implementation\ncheck_residual_block()\n```\n\nThis code defines a simple residual block function using NumPy, adhering to the description provided in the task. The function takes an input array `x` and two weight matrices `w1` and `w2`, and applies a series of linear operations and ReLU activations, including a shortcut connection, to produce the output. The check function `check_residual_block` is used to verify the correctness of the implementation with a predefined example."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels), representing feature maps.\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,), where each element is the average of \n                all values in the corresponding feature map.\n    \"\"\"\n    # Compute the mean over the height and width axes (axis 0 and 1)\n    return np.mean(x, axis=(0, 1))\n\n# Example usage:\n# Create a random 3D numpy array of shape (4, 4, 2) representing 2 feature maps of size 4x4\nfeature_maps = np.random.rand(4, 4, 2)\n\n# Apply Global Average Pooling\npooled_output = global_avg_pool(feature_maps)\n\n# Output should be a 1D array of size equal to the number of channels\nprint(\"Pooled Output Shape:\", pooled_output.shape)\nprint(\"Pooled Output:\", pooled_output)\n\n# Check function to verify the output\ndef check_global_avg_pool():\n    test_data = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    expected_output = np.array([3, 4, 6, 8])\n    actual_output = global_avg_pool(test_data)\n    assert np.allclose(actual_output, expected_output), \"The function output does not match the expected output.\"\n    print(\"Check passed.\")\n\n# Run the check function\ncheck_global_avg_pool()\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Applies batch normalization to a 4D numpy array in BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input array in BCHW format.\n    gamma (np.ndarray): Scale parameter array.\n    beta (np.ndarray): Shift parameter array.\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized array, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Batch, Channel, Height, Width\n    batch_size, channels, height, width = X.shape\n    \n    # Calculate mean and variance per channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    X_norm = gamma[:, np.newaxis, np.newaxis] * X_norm + beta[:, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(X_norm, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    np.random.seed(0)\n    X = np.random.rand(2, 3, 4, 4)  # Example input in BCHW format\n    gamma = np.random.rand(3)  # Scale parameters for each channel\n    beta = np.random.rand(3)   # Shift parameters for each channel\n    \n    # Perform batch normalization\n    normalized_output = batch_normalization(X, gamma, beta)\n    \n    # Output: normalized_output\n    print(normalized_output)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term of the form c*x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    # Apply the power rule for derivatives: d(c*x^n)/dx = c*n*x^(n-1)\n    derivative_value = c * n * (x ** (n - 1))\n    # Return the rounded value\n    return round(derivative_value, 4)\n\n# Example usage and verification with provided data points\ndef check():\n    # Test cases\n    test_cases = [\n        (2.5, 1.0, 3.0, 7.5),\n        (3.0, 2.0, 2.0, 12.0),\n        (1.0, 0.0, 1.0, 0.0),\n        (4.5, -1.0, 2.0, 0.0)\n    ]\n    \n    for c, x, n, expected in test_cases:\n        result = poly_term_derivative(c, x, n)\n        assert result == expected, f\"Expected {expected}, got {result} for inputs c={c}, x={x}, n={n}\"\n    print(\"All test cases passed successfully.\")\n\n# Run the check function to verify the correctness of the poly_term_derivative function.\ncheck()\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Implement the Gram-Schmidt process to compute an orthonormal basis from a list of 2D vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of 2D vectors.\n    tol (float): A tolerance value to determine linear independence.\n    \n    Returns:\n    list of list of float: A list of orthonormal vectors.\n    \"\"\"\n    if not vectors:\n        return []\n\n    # Convert list of vectors to a numpy array\n    V = np.array(vectors).T\n    m, n = V.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    for j in range(n):\n        v = V[:, j]\n        for i in range(j):\n            R[i, j] = np.dot(Q[:, i].T, V[:, j])\n            v = v - R[i, j] * Q[:, i]\n        norm_v = np.linalg.norm(v)\n        if norm_v < tol:  # Check for linear independence\n            continue\n        R[j, j] = norm_v\n        Q[:, j] = v / norm_v\n    \n    # Filter out zero rows and return the orthonormal basis\n    Q_clean = Q[:, np.linalg.norm(Q, axis=0) > tol]\n    return [list(map(lambda x: round(x, 4), Q_clean[:, i].tolist())) for i in range(Q_clean.shape[1])]\n\n# Check function with provided data points\ndef check():\n    vectors = [\n        [1, 1],\n        [2, 2],\n        [1, 0],\n        [0, 1]\n    ]\n    tol = 1e-10\n    result = orthonormal_basis(vectors, tol)\n    expected = [\n        [0.7071, 0.7071],\n        [0.7071, -0.7071]\n    ]\n    assert len(result) == len(expected), \"The length of the result does not match the expected.\"\n    for i in range(len(result)):\n        # Allow for two possible orientations of the basis vector\n        assert np.allclose(np.abs(result[i]), np.abs(expected[i]), atol=tol), f\"Result {result[i]} does not match expected {expected[i]}.\"\n\ncheck()\n```"}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3-dimensional vectors.\n    \n    :param a: List or numpy array, first vector\n    :param b: List or numpy array, second vector\n    :return: List representing the cross product vector\n    \"\"\"\n    # Convert inputs to numpy arrays for vector operations\n    vec_a = np.array(a)\n    vec_b = np.array(b)\n    \n    # Compute the cross product using np.cross function\n    cross_vec = np.cross(vec_a, vec_b)\n    \n    # Round the results to 4 decimal places and convert to list\n    result = np.round(cross_vec, 4).tolist()\n    \n    return result\n\n# Example usage:\n# Define two 3D vectors\nvector1 = [1, 2, 3]\nvector2 = [4, 5, 6]\n\n# Compute the cross product\ncross_product_result = cross_product(vector1, vector2)\n\nprint(cross_product_result)\n```\n\nThis script fulfills the requirement by defining a function `cross_product` that takes two 3D vectors as input, computes their cross product using NumPy's `np.cross` function, rounds the result to 4 decimal places, and returns it as a list."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (list of lists): A square coefficient matrix.\n    b (list): A constant vector.\n    \n    Returns:\n    list: Solution vector x, rounded to 4 decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    # Convert A and b to numpy arrays for easier manipulation\n    A_np = np.array(A, dtype=float)\n    b_np = np.array(b, dtype=float)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A_np)\n    \n    # If the determinant is zero, return -1\n    if det_A == 0:\n        return -1\n    \n    n = len(b_np)\n    x = []\n    \n    # For each variable, replace the corresponding column in A with b and calculate the determinant\n    for i in range(n):\n        Ai = A_np.copy()\n        Ai[:, i] = b_np\n        det_Ai = np.linalg.det(Ai)\n        x.append(round(det_Ai / det_A, 4))\n    \n    # Return the solution vector x as a list\n    return x\n\n# Example usage\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]\nsolution = cramers_rule(A, b)\nprint(solution)  # Expected output: [1.0, -2.0, -2.0]\n```"}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions p and q.\n    \n    Parameters:\n    p (list of float): The first probability distribution.\n    q (list of float): The second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance, rounded to 4 decimal places. Returns 0.0 if inputs are invalid.\n    \"\"\"\n    # Check if inputs are valid\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    coefficient = np.sum(np.sqrt(np.multiply(p, q)))\n    \n    # Calculate the Bhattacharyya distance\n    distance = -np.log(coefficient)\n    \n    return round(distance, 4)\n\n# Check function to verify the correctness of the generated function\ndef check_bhatt_distance():\n    p = [0.1, 0.2, 0.3, 0.4]\n    q = [0.05, 0.15, 0.35, 0.45]\n    expected = 0.0044  # This value needs to be manually calculated or known for the specific distributions\n    assert bhattacharyya_distance(p, q) == expected, f\"Expected {expected} but got {bhattacharyya_distance(p, q)}\"\n    print(\"Test passed successfully.\")\n\n# Run check function\ncheck_bhatt_distance()\n```\n\nNote: For the `check_bhatt_distance` function, the `expected` value must be calculated manually or obtained from a trusted source for the specific distributions provided. The provided value `0.0044` is illustrative and may not be the actual Bhattacharyya distance for the given distributions."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Args:\n    a: A list of integers or floats representing the first vector.\n    b: A list of integers or floats representing the second vector.\n    \n    Returns:\n    A list containing the element-wise sum of the two vectors if the lengths match.\n    Otherwise, returns -1.\n    \"\"\"\n    # Check if vectors have the same dimensions\n    if len(a) != len(b):\n        return -1\n    # Compute the element-wise sum\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result\n\n# Function to check the correctness of the vector_sum function\ndef check_vector_sum():\n    assert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9], \"Test case 1 failed\"\n    assert vector_sum([1.5, 2.5], [3.5, 4.5]) == [5.0, 7.0], \"Test case 2 failed\"\n    assert vector_sum([1, 2], [1, 2, 3]) == -1, \"Test case 3 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the correctness of the solution\ncheck_vector_sum()\n```"}
{"task_id": 122, "completion_id": 0, "completion": "Below is the requested Python script that implements the REINFORCE algorithm for the policy gradient estimator, as per the given task description.\n\n```python\nimport numpy as np\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    \n    def softmax(x):\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n    \n    # Initialize gradient\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        returns = 0\n        for s, a, r in reversed(episode):\n            returns += r  # Calculate return from the end to the beginning\n            \n            policy = softmax(theta[s, :])  # Softmax over the state's row in theta\n            log_policy = np.log(policy)  # Log of the policy\n            \n            # Compute the gradient of the log-policy\n            gradient[s, :] += policy * returns\n            gradient[s, a] -= returns  # Subtract return from the action taken\n            \n    gradient = gradient / len(episodes)  # Average gradient over all episodes\n    \n    # Round to the nearest 4th decimal\n    gradient = np.round(gradient, decimals=4)\n    \n    # Return the gradient as a list\n    return gradient.tolist()\n\n# Example usage\ntheta = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nepisodes = [\n    [(0, 1, 1.0), (1, 2, -1.0), (2, 0, 2.0)],\n    [(0, 2, 0.5), (1, 1, 0.5), (2, 2, -1.0)]\n]\n\npolicy_gradient = compute_policy_gradient(theta, episodes)\nprint(policy_gradient)\n```\n\nThis script defines the `compute_policy_gradient` function that takes a parameter matrix `theta` and a list of episodes. It computes the policy gradient using the REINFORCE algorithm by iterating through each episode in reverse to calculate the returns and then computing the gradient of the log-policy for each state-action pair. The policy is computed using the softmax function, and the gradients are averaged and rounded to the nearest 4th decimal before being returned as a list."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    :param n_experts: Total number of experts in the MoE layer.\n    :param k_active: Number of active experts.\n    :param d_in: Input dimension to the layer.\n    :param d_out: Output dimension from the layer.\n    :return: The percentage of FLOPs saved by using MoE over a dense layer.\n    \"\"\"\n    # Calculate FLOPs for a dense layer\n    flops_dense = 2 * d_in * d_out  # 2 because each multiply-accumulate operation involves 2 FLOPs\n    \n    # Calculate FLOPs for MoE layer\n    # Each expert has its own transformation, but only k_active experts are used\n    flops_expert = 2 * d_in * d_out\n    flops_moe = k_active * flops_expert + 2 * d_in * n_experts  # + routing FLOPs\n    \n    # Calculate savings\n    savings = flops_dense - flops_moe\n    savings_percentage = (savings / flops_dense) * 100\n    \n    # Return the result rounded to the nearest decimal\n    return round(savings_percentage, 1)\n\n# Example usage\nn_experts = 100  # Example number of experts\nk_active = 10    # Example number of active experts\nd_in = 512       # Example input dimension\nd_out = 512      # Example output dimension\n\nprint(compute_efficiency(n_experts, k_active, d_in, d_out))\n```\n\nIn this script, the `compute_efficiency` function takes the number of experts, the number of active experts, and the input and output dimensions to calculate the percentage of FLOPs saved by using a mixture-of-experts (MoE) layer over a dense layer. The formula for computing FLOPs takes into account the operations involved in both the dense layer and the MoE layer, including an estimate for routing operations in the MoE layer."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models.\n    \n    Parameters:\n    X (np.ndarray): Input matrix of shape (batch_size, input_features).\n    W_g (np.ndarray): Weight matrix for gating of shape (input_features, num_experts).\n    W_noise (np.ndarray): Weight matrix for noise of shape (input_features, num_experts).\n    N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts).\n    k (int): Sparsity constraint indicating the number of experts to activate.\n    \n    Returns:\n    np.ndarray: Final gating probabilities matrix of shape (batch_size, num_experts).\n    \"\"\"\n    # Compute the logits for gating\n    logits = np.dot(X, W_g)\n    # Compute the noise\n    noise = np.dot(X, W_noise) * N\n    # Combine logits and noise\n    noisy_logits = logits + noise\n    # Apply softmax to get probabilities\n    exp_logits = np.exp(noisy_logits)\n    probs = exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n    # Get top-k indices\n    topk_indices = np.argsort(-probs, axis=-1)[:, :k]\n    # Create a mask for top-k values\n    mask = np.zeros_like(probs)\n    np.put_along_axis(mask, topk_indices, 1, axis=-1)\n    # Apply mask and renormalize probabilities\n    masked_probs = probs * mask\n    renorm_probs = masked_probs / np.sum(masked_probs, axis=-1, keepdims=True)\n    # Return the rounded and reshaped result as a list\n    return np.round(renorm_probs, decimals=4).tolist()\n\n# Example usage\nX = np.array([[1, 2], [3, 4]])\nW_g = np.array([[0.1, 0.2], [0.3, 0.4]])\nW_noise = np.array([[0.05, 0.1], [0.15, 0.2]])\nN = np.array([[0.01, 0.02], [0.03, 0.04]])\nk = 1\n\nresult = noisy_topk_gating(X, W_g, W_noise, N, k)\nprint(result)\n```\n\nThis script defines the `noisy_topk_gating` function which implements the Noisy Top-K gating mechanism used in Mixture-of-Experts (MoE) models. It takes in the input matrix `X`, gating weight matrix `W_g`, noise weight matrix `W_noise`, pre-sampled noise matrix `N`, and a sparsity constraint `k`. The output is a list of gating probabilities, where each row corresponds to a sample from the input matrix `X`, and each column corresponds to the probability of activating a particular expert."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer with softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n    \n    Returns:\n    np.ndarray: Output tensor after applying the MoE layer.\n    \"\"\"\n    batch_size, input_dim = x.shape\n    output_dim = We.shape[2]\n    \n    # Compute gating scores\n    gating_scores = x @ Wg  # (batch_size, n_experts)\n    gating_probs = np.exp(gating_scores - gating_scores.max(axis=-1, keepdims=True))\n    gating_probs /= gating_probs.sum(axis=-1, keepdims=True)  # Softmax\n    \n    # Select top-k experts for each sample\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]  # (batch_size, top_k)\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)  # (batch_size, top_k)\n    \n    # Normalize probs of top-k experts\n    top_k_probs /= top_k_probs.sum(axis=-1, keepdims=True)\n    \n    # Compute expert outputs for all experts\n    expert_outputs = x[:, np.newaxis, :] @ We  # (batch_size, n_experts, output_dim)\n    \n    # Select top-k expert outputs\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, np.newaxis], axis=1)  # (batch_size, top_k, output_dim)\n    \n    # Weight expert outputs by their probabilities\n    weighted_outputs = top_k_expert_outputs * top_k_probs[:, :, np.newaxis]  # (batch_size, top_k, output_dim)\n    \n    # Aggregate weighted expert outputs\n    final_output = weighted_outputs.sum(axis=1)  # (batch_size, output_dim)\n    \n    return np.round(final_output, decimals=4).tolist()\n\n# Example usage:\nbatch_size, input_dim, output_dim, n_experts, top_k = 2, 3, 4, 5, 2\nx = np.random.rand(batch_size, input_dim)\nWe = np.random.rand(n_experts, input_dim, output_dim)\nWg = np.random.rand(input_dim, n_experts)\n\nresult = moe(x, We, Wg, n_experts, top_k)\nprint(result)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "Below is a self-contained Python script to perform Group Normalization on a 4D input tensor according to the provided specifications.\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n    \n    Parameters:\n    - X: np.ndarray of shape (B, C, H, W) representing the input tensor.\n    - gamma: np.ndarray of shape (C,) representing the scale parameter.\n    - beta: np.ndarray of shape (C,) representing the shift parameter.\n    - num_groups: int representing the number of groups to split the channels into.\n    - epsilon: float representing a small constant to avoid division by zero.\n    \n    Returns:\n    - np.ndarray of the normalized tensor, reshaped and rounded to 4th decimal.\n    \"\"\"\n    B, C, H, W = X.shape\n    \n    # Check if number of channels is divisible by the number of groups\n    assert C % num_groups == 0, \"Number of channels must be divisible by the number of groups\"\n    \n    # Calculate channels per group\n    channels_per_group = C // num_groups\n    \n    # Reshape to split into groups\n    X_grouped = X.reshape(B, num_groups, channels_per_group, H, W)\n    \n    # Calculate mean and variance within each group\n    mean = np.mean(X_grouped, axis=(2,3,4), keepdims=True)\n    var = np.var(X_grouped, axis=(2,3,4), keepdims=True)\n    \n    # Normalize\n    X_normed = (X_grouped - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back and apply gamma and beta\n    X_normed = X_normed.reshape(B, C, H, W)\n    X_normed = X_normed * gamma + beta\n    \n    # Round to 4th decimal and convert to list\n    X_normed = np.round(X_normed, decimals=4).tolist()\n    \n    return X_normed\n\n# Example usage\nX = np.random.rand(2, 6, 4, 4)\ngamma = np.random.rand(6)\nbeta = np.random.rand(6)\nnum_groups = 3\n\nnormalized_X = group_normalization(X, gamma, beta, num_groups)\nprint(normalized_X)\n```\n\nThis script defines a function `group_normalization` that performs group normalization on the input tensor `X`. It divides the channels into `num_groups` groups, normalizes each group independently, and then applies the learned scale and shift parameters. The result is rounded to the fourth decimal place and returned as a Python list."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Implements gradient descent to find the minimum point of a given function.\n    \n    Args:\n        start_x (float): The starting point for gradient descent.\n        learning_rate (float): The step size at each iteration.\n        tolerance (float): The threshold for stopping the iteration.\n        max_iters (int): Maximum number of iterations to perform.\n    \n    Returns:\n        float: The x value where the function reaches its minimum, rounded to the nearest 4th decimal.\n    \"\"\"\n    \n    def f(x):\n        \"\"\"The function to minimize: f(x) = x^4 - 3x^3 + 2.\"\"\"\n        return x**4 - 3*x**3 + 2\n    \n    def f_prime(x):\n        \"\"\"The derivative of the function: f'(x) = 4x^3 - 9x^2.\"\"\"\n        return 4*x**3 - 9*x**2\n    \n    x = start_x\n    for _ in range(max_iters):\n        gradient = f_prime(x)\n        # Update x with the negative gradient times the learning rate\n        x -= learning_rate * gradient\n        # If the gradient is small enough, we have converged\n        if abs(gradient) < tolerance:\n            break\n    \n    return round(x, 4)\n\n# Example usage\nstart_x = np.random.uniform(-10, 10)\nprint(f\"Starting point: {start_x}\")\nresult = find_treasure(start_x)\nprint(f\"Minimum point found: {result}\")\n```\n\nThis script implements a gradient descent algorithm to find the minimum point of the provided function \\( f(x) = x^4 - 3x^3 + 2 \\) starting from a random initial position within a specified range. The function to be minimized and its derivative are defined within the script, and the gradient descent is performed iteratively until the gradient is smaller than a specified tolerance or a maximum number of iterations is reached. The final result is rounded to the nearest fourth decimal place as required."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) function to the input array.\n    \n    Parameters:\n    x (np.ndarray): The input array of any shape.\n    alpha (float): A scaling factor for the input to DyT.\n    gamma (float): A scaling factor applied to the result of the DyT function.\n    beta (float): An additive factor applied to the result of the DyT function.\n    \n    Returns:\n    list[float]: The transformed array, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Apply the Dynamic Tanh transformation\n    dyt = np.tanh(alpha * x)\n    dyt = gamma * dyt + beta\n    \n    # Round to the nearest 4th decimal place and convert to list\n    dyt = np.round(dyt, 4).tolist()\n    \n    return dyt\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example input array, alpha, gamma, and beta values\n    example_x = np.array([0.5, -0.2, 1.0, -1.5])\n    alpha_val = 2.0\n    gamma_val = 1.5\n    beta_val = 0.5\n    \n    # Call the dynamic_tanh function with the example values\n    result = dynamic_tanh(example_x, alpha_val, gamma_val, beta_val)\n    \n    # Output the result\n    print(\"Dynamic Tanh result:\", result)\n```\n\nThis code snippet defines the function `dynamic_tanh`, which takes an input array `x` and three transformation parameters (`alpha`, `gamma`, `beta`). The function applies a transformed Tanh operation to the input array, scaling and shifting the result according to the specified parameters. The output is rounded to the nearest 4th decimal place and returned as a list of floats. An example of how to use the function is provided at the end of the script."}
