{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float: The resulting vector if the operation is valid, or -1 if the dimensions are incompatible.\n    \"\"\"\n    if not a or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix `a`.\n    \n    Parameters:\n    a (list of list of int|float): The input matrix to be transposed.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    if not a or not all((isinstance(row, list) for row in a)):\n        return []\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes a given 2D list (matrix) into a new shape specified by new_shape.\n    If the reshaping is not possible, returns an empty list.\n    \n    :param a: 2D list of integers or floats representing the matrix.\n    :param new_shape: A tuple of two integers specifying the new shape (rows, columns).\n    :return: A reshaped 2D list of the matrix or an empty list if reshaping is not possible.\n    \"\"\"\n    np_array = np.array(a)\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = np_array.reshape(new_shape)\n    return reshaped_array.tolist()"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column based on the given mode.\n    \n    Parameters:\n    - matrix (list of lists): The matrix to calculate means from.\n    - mode (str): 'row' or 'column' to specify the calculation mode.\n    \n    Returns:\n    - list of floats: The means calculated according to the specified mode.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    if mode.lower() == 'row':\n        means = np_matrix.mean(axis=1)\n    elif mode.lower() == 'column':\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    return list(means)\nmatrix = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the scalar value.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar value.\n    :return: A 2D list representing the matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    Args:\n    matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return [float(val) for val in sorted_eigenvalues]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_matrix = np.round(np.dot(np.dot(T_inv, A_np), S_inv), 4)\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 2]]\nS = [[2, 0], [0, 1]]"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list of list of float): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list of list of float: The inverse of the matrix, if it exists. Returns None if the matrix is not invertible.\n    \"\"\"\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError('Input matrix must be 2x2.')\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    if det == 0:\n        return None\n    inv_matrix = [[matrix[1][1] / det, -matrix[0][1] / det], [-matrix[1][0] / det, matrix[0][0] / det]]\n    return inv_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b. Returns -1 if the matrices cannot be multiplied.\n    \n    :param a: A list of lists representing the first matrix.\n    :param b: A list of lists representing the second matrix.\n    :return: The product of the two matrices or -1 if multiplication is not possible.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result\nmatrix_a = [[1, 2, 3], [4, 5, 6]]\nmatrix_b = [[7, 8], [9, 10], [11, 12]]\nresult = matrixmul(matrix_a, matrix_b)"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors (list of list of float): A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    matrix = np.array(vectors)\n    cov_matrix = np.cov(matrix)\n    return cov_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): The coefficient matrix.\n    b (np.ndarray): The constant terms vector.\n    n (int): The number of iterations.\n    \n    Returns:\n    list: The solution vector x as a list.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    x = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x_new = np.round(x_new, 4)\n        x = x_new\n    return x.tolist()\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    Returns the singular values rounded to the nearest 4th decimal.\n    \n    :param A: 2x2 numpy array\n    :return: tuple of two singular values\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    AAT = np.dot(A, A.T)\n\n    def eigenvalues_2x2(M):\n        (a, b) = (M[0, 0], M[1, 1])\n        c = M[0, 1]\n        delta = np.sqrt((a - b) ** 2 + 4 * c ** 2)\n        return ((a + b + delta) / 2, (a + b - delta) / 2)\n    eigvals_ATA = eigenvalues_2x2(ATA)\n    eigvals_AAT = eigenvalues_2x2(AAT)\n    singular_values = np.sqrt(np.maximum(eigvals_ATA, eigvals_AAT))\n    singular_values_rounded = tuple((round(val, 4) for val in singular_values))\n    return singular_values_rounded\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(sub_matrix: list[list[int | float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix.\n        \n        Args:\n        sub_matrix (list of list of int|float): A 3x3 matrix.\n        \n        Returns:\n        float: The determinant of the 3x3 matrix.\n        \"\"\"\n        return sub_matrix[0][0] * (sub_matrix[1][1] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][1]) - sub_matrix[0][1] * (sub_matrix[1][0] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][0]) + sub_matrix[0][2] * (sub_matrix[1][0] * sub_matrix[2][1] - sub_matrix[1][1] * sub_matrix[2][0])\n    det = 0\n    for col in range(4):\n        sub_matrix = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        det += (-1) ** col * matrix[0][col] * determinant_3x3(sub_matrix)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    X (list of list of float): The input features matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny = [2, 3, 4, 5]"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input features matrix with a column of ones for the intercept.\n    y (np.ndarray): The target vector.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to perform.\n    \n    Returns:\n    list: The coefficients of the linear regression model.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / m\n        theta -= alpha * gradient\n    return np.round(theta, decimals=4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data using both standardization and min-max normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    \n    Returns:\n    - Two 2D lists: the first one is the standardized data, the second one is the min-max normalized data.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    normalized_data = (data - min_values) / (max_values - min_values)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    return (standardized_data, normalized_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a list of points.\n    \n    Parameters:\n    - points: A list of points, each point is a tuple of coordinates.\n    - k: Number of clusters.\n    - initial_centroids: Initial centroids for the clusters.\n    - max_iterations: Maximum number of iterations to perform.\n    \n    Returns:\n    - A list of the final centroids, each rounded to the nearest fourth decimal.\n    \"\"\"\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[closest_centroid == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids_array == new_centroids):\n            break\n        centroids_array = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(6.0, 2.0), (1.0, 2.0), (7.0, 3.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation and returns the indices for each fold.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix of the dataset.\n    y (np.ndarray): The labels of the dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): The seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    list: A list of tuples containing the train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    indices_list = []\n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        indices_list.append((train_indices, test_indices))\n    return indices_list"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_list = [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]\n    return principal_components_list"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree from the given examples and attributes.\n    \n    :param examples: List of examples, each example is a dict of attribute-value pairs.\n    :param attributes: List of attribute names.\n    :param target_attr: The name of the target attribute.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n\n    def entropy(examples, target_attr):\n        \"\"\"Calculate the entropy of the target attribute.\"\"\"\n        counter = Counter([example[target_attr] for example in examples])\n        total = len(examples)\n        return -sum((count / total * math.log2(count / total) for count in counter.values()))\n\n    def information_gain(examples, attribute, target_attr):\n        \"\"\"Calculate the information gain of a split on the attribute.\"\"\"\n        total_entropy = entropy(examples, target_attr)\n        partitions = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in partitions:\n                partitions[value] = []\n            partitions[value].append(example)\n        weighted_entropy = sum((len(partition) / len(examples) * entropy(partition, target_attr) for partition in partitions.values()))\n        return total_entropy - weighted_entropy\n\n    def split(examples, attribute):\n        \"\"\"Split the examples based on the attribute.\"\"\"\n        partitions = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in partitions:\n                partitions[value] = []\n            partitions[value].append(example)\n        return partitions\n\n    def most_common_value(examples, target_attr):\n        \"\"\"Find the most common value of the target attribute in the examples.\"\"\"\n        counter = Counter([example[target_attr] for example in examples])\n        return counter.most_common(1)[0][0]\n    if not examples:\n        return {}\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_value(examples, target_attr)\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n    for (value, subset) in split(examples, best_attribute).items():\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'}, {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_matrix(X, kernel='linear', sigma=1.0):\n    n = X.shape[0]\n    K = np.zeros((n, n))\n    if kernel == 'linear':\n        K = X @ X.T\n    elif kernel == 'rbf':\n        for i in range(n):\n            for j in range(n):\n                diff = X[i] - X[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            if labels[i] * (np.sum(alpha * labels * K[:, i]) - alpha[i]) < 1:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i] + lambda_t * labels[i]\n            else:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i]\n    alpha = np.round(alpha, 4)\n    bias = 0\n    support_indices = np.where(alpha > 1e-05)[0]\n    for i in support_indices:\n        bias += labels[i] - np.sum(alpha * labels * K[:, i])\n    bias /= len(support_indices)\n    bias = np.round(bias, 4)\n    return (alpha.tolist(), bias)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax for a given list of scores.\n    The softmax function transforms each score in a list to a value between 0 and 1,\n    and the sum of all these transformed values equals 1.\n    \n    Args:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of lists where each sublist represents the feature vector of an example.\n    :param labels: A list of binary labels (0 or 1) for each example.\n    :param weights: A list of weights for each feature.\n    :param bias: A float representing the bias term.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for example in features:\n        linear_combination = sum((w * f for (w, f) in zip(weights, example))) + bias\n        prediction = sigmoid(linear_combination)\n        predictions.append(prediction)\n    mse = sum(((p - l) ** 2 for (p, l) in zip(predictions, labels))) / len(labels)\n    predictions = [round(pred, 4) for pred in predictions]\n    mse = round(mse, 4)\n    return (predictions, mse)\nfeatures = [[0.5, 0.2], [0.1, 0.8], [0.3, 0.4]]\nlabels = [1, 0, 1]\nweights = [0.7, 0.3]\nbias = 0.1"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mse_loss(y_true, y_pred):\n    return ((y_true - y_pred) ** 2).mean()\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        loss = mse_loss(labels, predictions)\n        mse_values.append(round(loss, 4))\n        error = predictions - labels\n        derivative = sigmoid_derivative(np.dot(features, weights) + bias)\n        gradient = np.dot(features.T, error * derivative) / len(features)\n        bias_gradient = np.mean(error * derivative)\n        weights -= learning_rate * gradient\n        bias -= learning_rate * bias_gradient\n    return (weights.tolist(), round(bias, 4), mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in basis B.\n    C (list of list of int): Basis vectors in basis C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from basis B to C.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    B_inv = np.linalg.inv(B_array)\n    P = C_array @ B_inv\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    Returns U, S, V such that A = U * S * V.T.\n    \"\"\"\n    ATA = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros_like(A)\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = A @ V[:, i] / singular_values[i]\n        else:\n            U[:, i] = V[:, i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the rows of two numpy arrays, X and y, maintaining their correspondence.\n    Useful for shuffling dataset samples and labels in unison for training machine learning models.\n    \n    Parameters:\n    - X: numpy array, feature dataset, 2D array with samples as rows\n    - y: numpy array, labels corresponding to the dataset, 1D array\n    - seed: int, optional random seed integer for reproducibility\n    \n    Returns:\n    - Shuffled X and y as python lists using numpy's tolist() method.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError('X and y must have the same number of rows (samples).')\n    np.random.seed(seed)\n    permutation = np.random.permutation(X.shape[0])\n    shuffled_X = X[permutation]\n    shuffled_y = y[permutation]\n    return (shuffled_X.tolist(), shuffled_y.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterator that yields batches of a specified size from a numpy array X.\n    If y is provided, it yields paired batches from both X and y.\n    \n    :param X: numpy array to be batched\n    :param y: optional numpy array to be batched alongside X\n    :param batch_size: size of each batch\n    :yield: batches of X, or paired batches of (X, y)\n    \"\"\"\n    assert type(X) == np.ndarray, 'X must be a numpy array'\n    assert y is None or type(y) == np.ndarray, 'y must be a numpy array or None'\n    assert len(X) == len(y) if y is not None else True, 'X and y must have the same length'\n    n_samples = len(X)\n    for i in range(0, n_samples, batch_size):\n        end_idx = min(i + batch_size, n_samples)\n        batch_X = X[i:end_idx].tolist()\n        if y is not None:\n            batch_y = y[i:end_idx].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X\nX = np.array([i for i in range(100)])\ny = np.array([i ** 2 for i in range(100)])"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n    \n    Parameters:\n    X (np.ndarray): The dataset to divide.\n    feature_i (int): The index of the feature to divide the dataset on.\n    threshold (float): The threshold to compare the feature values against.\n    \n    Returns:\n    tuple: Two lists representing the subsets of the dataset. The first list contains samples where the feature value is >= threshold, and the second list contains the rest.\n    \"\"\"\n    split_func = lambda sample: sample[feature_i] >= threshold\n    X_above = np.array([sample for sample in X if split_func(sample)]).tolist()\n    X_below = np.array([sample for sample in X if not split_func(sample)]).tolist()\n    return (X_above, X_below)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    X : numpy.ndarray\n        The input samples. Shape: [n_samples, n_features]\n    degree : int\n        The degree of the polynomial features.\n        \n    Returns:\n    numpy.ndarray\n        The matrix of features, where the columns are polynomial combinations of the features with degree less than or equal to the specified degree.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    combinations = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combinations = [item for sublist in combinations for item in sublist]\n    n_output_features = len(combinations)\n    X_new = np.empty((n_samples, n_output_features))\n    for (i, index_combination) in enumerate(combinations):\n        X_new[:, i] = np.prod(X[:, index_combination], axis=1)\n    return X_new.tolist()\nX = np.array([[2, 3], [3, 5]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature dataset.\n    y (np.ndarray): 1D numpy array representing the labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Seed for the random number generator.\n    \n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    indices = np.arange(len(X))\n    for _ in range(n_subsets):\n        if replacements:\n            chosen_indices = np.random.choice(indices, len(indices), replace=True)\n        else:\n            chosen_indices = np.random.choice(indices, len(indices), replace=False)\n        X_subset = X[chosen_indices]\n        y_subset = y[chosen_indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers into a 2D one-hot encoded numpy array.\n    \n    Parameters:\n    x (np.array): A 1D numpy array of integers.\n    n_col (int, optional): The number of columns for the one-hot encoded array.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([1, 0, 3])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): 1D numpy array\n    \n    Returns:\n    list: 2D list representing the diagonal matrix\n    \"\"\"\n    x = np.array(x)\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3, 4])"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (2D numpy array): The first dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, X is used.\n    \n    Returns:\n    2D numpy array: The correlation matrix as a 2D numpy array.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    X_std = np.std(X, axis=0, keepdims=True)\n    Y_std = np.std(Y, axis=0, keepdims=True)\n    correlation_matrix = cov_matrix / (X_std.T * Y_std)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n    \n    Parameters:\n    X : numpy.ndarray\n        The input samples of shape (n_samples, n_features).\n    y : numpy.ndarray\n        The target values of shape (n_samples,).\n    n_clf : int\n        The number of weak classifiers to use.\n        \n    Returns:\n    list of dict\n        A list of dictionaries, each containing the classifier parameters.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best = {'error': 1, 'polarity': 1, 'feature_idx': 0, 'threshold': 0, 'alpha': 0}\n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    prediction = np.ones(n_samples)\n                    prediction[X[:, feature_idx] * polarity < threshold * polarity] = -1\n                    weighted_error = np.sum(weights[y != prediction])\n                    if weighted_error < best['error']:\n                        best = {'error': weighted_error, 'polarity': polarity, 'feature_idx': feature_idx, 'threshold': threshold}\n        best['alpha'] = 0.5 * np.log((1 - best['error']) / best['error'])\n        prediction = np.ones(n_samples)\n        prediction[X[:, best['feature_idx']] * best['polarity'] < best['threshold'] * best['polarity']] = -1\n        weights *= np.exp(-best['alpha'] * y * prediction)\n        weights /= np.sum(weights)\n        classifiers.append(best)\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax function is the logarithm of the softmax function, which is used to\n    convert a vector of scores into probabilities. This function is often used for numerical\n    stability when computing the softmax of large numbers.\n    \n    Parameters:\n    scores (list): A list of numerical scores.\n    \n    Returns:\n    list: A list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    softmax_scores = np.exp(scores - np.max(scores)) / np.exp(scores - np.max(scores)).sum()\n    log_softmax_scores = np.log(softmax_scores)\n    return np.round(log_softmax_scores, 4).tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        w_grad = self.layer_input.T.dot(accum_grad)\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, w_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, np.sum(accum_grad, axis=0, keepdims=True))\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution on the input_matrix using the given kernel,\n    with specified padding and stride. The result is rounded to the nearest 4th\n    decimal and returned as a list.\n    \n    :param input_matrix: 2D numpy array representing the input image\n    :param kernel: 2D numpy array representing the convolution kernel\n    :param padding: Integer specifying the padding size\n    :param stride: Integer specifying the stride length\n    :return: List of the convolved matrix\n    \"\"\"\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    output_h = int((input_h + 2 * padding - kernel_h) / stride) + 1\n    output_w = int((input_w + 2 * padding - kernel_w) / stride) + 1\n    output_matrix = np.zeros((output_h, output_w))\n    for i in range(0, output_h):\n        for j in range(0, output_w):\n            row = i * stride\n            col = j * stride\n            output_matrix[i, j] = np.sum(padded_input[row:row + kernel_h, col:col + kernel_w] * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A float number to which the ReLU function is applied.\n    \n    Returns:\n    float: The result of applying the ReLU function to z.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss, which is the sum of the Mean Squared Error (MSE) and a regularization term.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    loss = mse + regularization\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the function.\n    alpha (float): The slope for the negative part of the function. Default is 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n    \n    Returns:\n    The linear kernel value as a float, which is the dot product of x1 and x2.\n    \"\"\"\n    if x1.shape != x2.shape:\n        raise ValueError('The input vectors must have the same dimensions.')\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred do not match.')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    precision_score = true_positives / predicted_positives\n    return precision_score\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using MSE loss.\n    \n    Parameters:\n    X (np.ndarray): Input features.\n    y (np.ndarray): Target values.\n    weights (np.ndarray): Initial weights.\n    learning_rate (float): Learning rate for the gradient descent.\n    n_iterations (int): Number of iterations to perform.\n    batch_size (int): Size of the batch for mini-batch and stochastic gradient descent.\n    method (str): Method to use for gradient descent. Options: 'batch', 'stochastic', 'mini-batch'.\n    \n    Returns:\n    list: Optimized weights as a python list.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index + 1]\n                yi = y[random_index:random_index + 1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient / batch_size\n        else:\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * X.T.dot(error)\n            weights -= learning_rate * gradient / n_samples\n    return np.round(weights, 4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.001\nn_iterations = 1000\nbatch_size = 2"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list of lists: The RREF of the given matrix.\n    \"\"\"\n    mat = np.array(matrix, dtype=float)\n    (rows, cols) = mat.shape\n    lead = 0\n    for r in range(rows):\n        if cols <= lead:\n            return mat.tolist()\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if rows == i:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        mat[[i, r]] = mat[[r, i]]\n        mat[r] = mat[r] / mat[r, lead]\n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        lead += 1\n    return mat.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        grad_x = grad(x0)\n        m = beta1 * m + (1 - beta1) * grad_x\n        v = beta2 * v + (1 - beta2) * grad_x ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0, 4).tolist()\ndef grad(x):\n    return np.array([2 * x[0], 2 * x[1]])\nx0 = np.array([1.0, 1.0])"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implement Lasso Regression using Gradient Descent with L1 regularization.\n    \n    Parameters:\n    X (np.array): The input features, shape (n_samples, n_features).\n    y (np.array): The target values, shape (n_samples,).\n    alpha (float): The regularization parameter.\n    learning_rate (float): The learning rate for gradient descent.\n    max_iter (int): The maximum number of iterations for gradient descent.\n    tol (float): The tolerance for stopping criteria.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        loss = y_pred - y\n        gradient_weights = 1 / n_samples * np.dot(X.T, loss) + alpha * np.sign(weights)\n        gradient_bias = 1 / n_samples * np.sum(loss)\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n        cost = 1 / (2 * n_samples) * np.sum(loss ** 2)\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (np.round(weights, 4).tolist(), np.round(bias, 4).tolist())"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of operations required to convert\n    one string into another, where operations include insertion, deletion,\n    substitution, and transposition of adjacent characters.\n    \"\"\"\n    (m, n) = (len(source), len(target))\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    :param y_true: A list of true binary labels (0 or 1).\n    :param y_pred: A list of predicted binary labels (0 or 1).\n    :return: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n    \n    Args:\n    X: Input sequence as a numpy array of shape (seq_len, d_model).\n    W_q: Query weight matrix as a numpy array of shape (d_model, d_k).\n    W_k: Key weight matrix as a numpy array of shape (d_model, d_k).\n    W_v: Value weight matrix as a numpy array of shape (d_model, d_v).\n    \n    Returns:\n    A numpy array representing the self-attention output, rounded to 4 decimal places.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    d_k = W_k.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(attention, V)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell that processes a sequence of input vectors and produces the final hidden state.\n    \n    Parameters:\n    - input_sequence: A list of input vectors.\n    - initial_hidden_state: The initial hidden state.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    final_hidden_state = np.round(h, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[0.1, 0.2], [0.3, 0.4]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, -0.1]\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by tx and ty in the x and y directions respectively.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of points.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_with_ones = np.hstack((points_array, ones))\n    translated_points = np.dot(translation_matrix, points_with_ones.T).T\n    return translated_points[:, :2].tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive.')\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a linear system Ax = b using the Gauss-Seidel iterative method.\n    \n    Parameters:\n    A (numpy.ndarray): A square matrix of coefficients.\n    b (numpy.ndarray): The right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (numpy.ndarray, optional): Initial guess for the solution vector. Defaults to a vector of zeros.\n    \n    Returns:\n    list: The approximated solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    size = A.shape[0]\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(size):\n            x[i] = b[i]\n            for j in range(size):\n                if j != i:\n                    x[i] -= A[i, j] * x[j]\n            x[i] /= A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]])\nb = np.array([12, 12, 12, 12])\nn = 100\nx = gauss_seidel(A, b, n)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of the system of equations.\n    b (np.array): Constant terms of the system of equations.\n    \n    Returns:\n    list: Solution vector x as a list, rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n        for t in range(T):\n            concat = np.vstack((hidden_state, x[t]))\n            forget_gate = self.sigmoid(self.Wf @ concat + self.bf)\n            input_gate = self.sigmoid(self.Wi @ concat + self.bi)\n            candidate_cell_state = self.tanh(self.Wc @ concat + self.bc)\n            output_gate = self.sigmoid(self.Wo @ concat + self.bo)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden_state)\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = hidden_state.round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_size = 3\nhidden_size = 2\nx = [np.array([[1], [2], [3]]), np.array([[4], [5], [6]])]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('Corpus cannot be empty.')\n    num_docs = len(corpus)\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term) / doc_word_count if doc_word_count > 0 else 0\n            df = sum((1 for d in corpus if term in d)) + 1\n            idf = math.log((num_docs + 1) / df) + 1\n            tf_idf = tf * idf\n            doc_tf_idf.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_tf_idf)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'lazy', 'dog', 'quickly', 'jumps', 'over', 'the', 'fox']]\nquery = ['quick', 'dog', 'fox']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision == 0 or recall == 0:\n        f_beta = 0\n    else:\n        f_beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hidden_states = np.zeros((self.hidden_size, T + 1))\n        outputs = np.zeros((self.hidden_size, T))\n        last_inputs = np.zeros((self.hidden_size, T))\n        for t in range(T):\n            hidden_states[:, t + 1] = np.tanh(self.W_xh @ input_sequence[t].reshape(-1, 1) + self.W_hh @ hidden_states[:, t].reshape(-1, 1) + self.b_h).reshape(-1)\n            outputs[:, t] = self.W_hy @ hidden_states[:, t + 1].reshape(-1, 1) + self.b_y\n            last_inputs[:, t] = input_sequence[t]\n        return (outputs, last_inputs, hidden_states)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[:, 0])\n        for t in reversed(range(T)):\n            do = outputs[:, t].reshape(-1, 1) - expected_output[t].reshape(-1, 1)\n            dh = self.W_hy.T @ do + dh_next\n            dhraw = (1 - hidden_states[:, t + 1].reshape(-1, 1) ** 2) * dh\n            dW_hy += do @ hidden_states[:, t + 1].reshape(1, -1)\n            db_y += do\n            dW_xh += dhraw @ last_inputs[:, t].reshape(1, -1)\n            dW_hh += dhraw @ hidden_states[:, t].reshape(1, -1)\n            db_h += dhraw\n            dh_next = self.W_hh.T @ dhraw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_idx, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, column_indices, row_pointers)"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    projection = (np.dot(v, L) / np.dot(L, L) * L).round(3)\n    return projection.tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[0, 3, 0, 0], [2, 0, 0, 1], [0, 0, 0, 0], [0, 0, 4, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space of a given matrix A.\n    The function returns the basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix and correspond to the independent columns.\n    \n    Parameters:\n    A (np.ndarray): The input matrix.\n    \n    Returns:\n    list: The basis vectors that span the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    A = np.array(A)\n    row_echelon = np.linalg.matrix_rank(A)\n    (_, inds) = np.unique(A[:, :row_echelon], return_index=True, axis=1)\n    pivot_columns = sorted(inds)\n    basis_vectors = A[:, pivot_columns]\n    return np.round(basis_vectors, 8).tolist()\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) for a regression model.\n    \n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    total_pixels = 0\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): The actual values.\n    y_pred (np.ndarray): The predicted values.\n    \n    Returns:\n    float: The RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input is not a numpy array or list.\n    \"\"\"\n    if not (isinstance(y_true, (np.ndarray, list)) and isinstance(y_pred, (np.ndarray, list))):\n        raise TypeError('Inputs must be numpy arrays or lists.')\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for two binary arrays.\n    \n    Parameters:\n    - y_true: A binary array representing true labels.\n    - y_pred: A binary array representing predicted labels.\n    \n    Returns:\n    - The Jaccard Index as a float, rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A binary numpy array of true labels.\n    - y_pred: A binary numpy array of predicted labels.\n    \n    Returns:\n    - A float representing the Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    num_true = np.sum(y_true)\n    num_pred = np.sum(y_pred)\n    if num_true == 0 and num_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (num_true + num_pred)\n    return round(dice, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_hypervector(seed, dim):\n    \"\"\"Create a hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.where(np.random.rand(dim) > 0.5, 1, -1)\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a given dataset row using HDC.\n    \n    Parameters:\n    row (dict): A dictionary representing a dataset row.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds.\n    \n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    bundled_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_hv = create_hypervector(random_seeds[feature], dim)\n        value_seed = hash((feature, value))\n        value_hv = create_hypervector(value_seed, dim)\n        bound_hv = np.multiply(feature_hv, value_hv)\n        bundled_hv = np.add(bundled_hv, bound_hv)\n    bundled_hv = np.where(bundled_hv > 0, 1, -1)\n    return bundled_hv.tolist()\ndim = 10\nrandom_seeds = {'feature1': 10, 'feature2': 20, 'feature3': 30}"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    :param data: A list of lists, where each inner list represents a pair [y_true, y_pred]\n    :return: A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    counts = Counter((tuple(pair) for pair in data))\n    matrix = [[0, 0], [0, 0]]\n    for ((y_true, y_pred), count) in counts.items():\n        matrix[y_true][y_pred] = count\n    return matrix"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (np.array): First input vector.\n    v2 (np.array): Second input vector.\n    \n    Returns:\n    float: Cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both vectors must have the same shape.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple:\n    if len(actual) != len(predicted):\n        raise ValueError('The length of actual and predicted lists must be the same.')\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError('All elements in actual and predicted lists must be either 0 or 1.')\n    confusion_matrix = [[0, 0], [0, 0]]\n    for (a, p) in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n    tp = confusion_matrix[1][1]\n    tn = confusion_matrix[0][0]\n    fp = confusion_matrix[0][1]\n    fn = confusion_matrix[1][0]\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn > 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): The dataset for which to calculate statistics.\n    \n    Returns:\n    dict: A dictionary containing various descriptive statistics.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    percentiles_25 = np.percentile(data, 25)\n    percentiles_50 = np.percentile(data, 50)\n    percentiles_75 = np.percentile(data, 75)\n    iqr = stats.iqr(data)\n    output = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentiles_25, 4), '50th_percentile': round(percentiles_50, 4), '75th_percentile': round(percentiles_75, 4), 'interquartile_range': round(iqr, 4)}\n    return output\ndata = np.random.randn(100)"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input image must be a 2D numpy array.')\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: The transformed features as a list of lists.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_features = np.ones((data_array.size, degree + 1))\n    for d in range(1, degree + 1):\n        transformed_features[:, d] = data_array ** d\n    transformed_features = np.round(transformed_features, 8)\n    return transformed_features.tolist()"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a given position and model dimensionality.\n    \n    Args:\n    position (int): The length of the sequence.\n    d_model (int): The dimensionality of the model.\n    \n    Returns:\n    list: The positional encoding as a list of np.float16 values or -1 for invalid inputs.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position = np.arange(0, position, dtype=np.float16).reshape(-1, 1)\n    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float16) * -(np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    return (updated_parameter, updated_m, updated_v)\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    if len(token_ids) > hparams['n_ctx']:\n        raise ValueError(f\"Prompt is too long. It must be shorter than {hparams['n_ctx']} tokens.\")\n    generated_tokens = token_ids\n    for _ in range(n_tokens_to_generate):\n        token_embeddings = params['wte'][generated_tokens[-hparams['n_ctx']:], :]\n        positional_embeddings = params['wpe'][:len(generated_tokens[-hparams['n_ctx']:])]\n        embeddings = token_embeddings + positional_embeddings\n        next_token_id = np.random.randint(0, len(encoder.encoder_dict))\n        generated_tokens.append(next_token_id)\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\nprompt = 'hello world'\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    crystals = np.array(crystal_values).reshape(-1, 1)\n    weights = np.random.rand(dimension, dimension)\n    projected_crystals = np.dot(crystals, weights)\n    attention_scores = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            attention_scores[i, j] = np.dot(projected_crystals[i], projected_crystals[j])\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=attention_scores)\n    weighted_patterns = np.dot(attention_weights, crystals)\n    result = [round(num, 4) for num in weighted_patterns.flatten()]\n    return result\nn = 5\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in the corpus given a query.\n    \n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    doc_lengths = np.array([len(doc) for doc in corpus])\n    avg_doc_length = np.mean(doc_lengths)\n    term_freqs = [Counter(doc) for doc in corpus]\n    idf = {}\n    for term in query:\n        df = sum((1 for doc in corpus if term in doc))\n        idf[term] = np.log((len(corpus) - df + 0.5) / (df + 0.5))\n    scores = []\n    for (i, doc) in enumerate(corpus):\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = term_freqs[i][term]\n                numerator = idf[term] * (k1 + 1) * tf\n                denominator = tf + k1 * (1 - b + b * doc_lengths[i] / avg_doc_length)\n                score += numerator / denominator\n        scores.append(score)\n    return [round(score, 3) for score in scores]\ncorpus = [['this', 'is', 'the', 'first', 'document'], ['this', 'document', 'is', 'the', 'second', 'document'], ['and', 'this', 'is', 'the', 'third', 'one'], ['is', 'this', 'the', 'fourth', 'one']]\nquery = ['document', 'first']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (yt, yp) in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            tp += 1\n        elif yt == 0 and yp == 1:\n            fp += 1\n        elif yt == 1 and yp == 0:\n            fn += 1\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import numpy as np\nimport math\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    X = np.array(range(1, len(consumption_data) + 1)).reshape((-1, 1))\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    base_consumption_day_15 = model.predict(np.array([[15]]))[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    final_prediction = math.ceil(predicted_consumption_day_15 * 1.05)\n    return final_prediction\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the query, key, and value matrices for the self-attention mechanism.\n    :param X: Input matrix.\n    :param W_q: Weight matrix for query.\n    :param W_k: Weight matrix for key.\n    :param W_v: Weight matrix for value.\n    :return: Tuple of (query, key, value) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention mechanism.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: Output of the self-attention mechanism.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention = np.dot(np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True), V)\n    return attention\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism.\n    :param X: Input matrix.\n    :param W_q: Weight matrix for query.\n    :param W_k: Weight matrix for key.\n    :param W_v: Weight matrix for value.\n    :param n_heads: Number of attention heads.\n    :return: Output of the multi-head attention mechanism as a list.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    head_dim = Q.shape[-1] // n_heads\n    Q_heads = np.array([Q[:, i * head_dim:(i + 1) * head_dim] for i in range(n_heads)])\n    K_heads = np.array([K[:, i * head_dim:(i + 1) * head_dim] for i in range(n_heads)])\n    V_heads = np.array([V[:, i * head_dim:(i + 1) * head_dim] for i in range(n_heads)])\n    attention_heads = [self_attention(Q_heads[i], K_heads[i], V_heads[i]) for i in range(n_heads)]\n    concatenated_heads = np.concatenate(attention_heads, axis=-1)\n    result = np.round(concatenated_heads, 4)\n    return result.tolist()\nX = np.random.rand(2, 10)\nW_q = np.random.rand(10, 20)\nW_k = np.random.rand(10, 20)\nW_v = np.random.rand(10, 20)\nn_heads = 2"}
{"task_id": 95, "completion_id": 0, "solution": "import numpy as np\nfrom scipy.stats import chi2_contingency\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x)\n    y_array = np.array(y)\n    contingency_table = np.array([[np.sum((x_array == 0) & (y_array == 0)), np.sum((x_array == 0) & (y_array == 1))], [np.sum((x_array == 1) & (y_array == 0)), np.sum((x_array == 1) & (y_array == 1))]])\n    (chi2, _, _, _) = chi2_contingency(contingency_table)\n    phi = np.sqrt(chi2 / np.sum(contingency_table))\n    return round(phi, 4)\nx = [0, 0, 1, 1, 0, 1, 0, 1]\ny = [0, 1, 0, 1, 0, 1, 1, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    grpo_objective_value = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_objective_value -= beta * kl_divergence\n    return round(grpo_objective_value, 6)\nrhos = [1.2, 0.9, 1.1, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.7, 0.6, 0.5, 0.4]\npi_theta_ref = [0.6, 0.5, 0.4, 0.3]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    n_classes = len(np.unique(y))\n    B = np.zeros((n_features, n_classes))\n    y_onehot = np.eye(n_classes)[y]\n    losses = []\n    for _ in range(iterations):\n        logits = X @ B\n        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n        softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        gradient = X.T @ (softmax - y_onehot) / n_samples\n        B -= learning_rate * gradient\n        log_likelihood = -np.log(softmax[range(n_samples), y])\n        loss = np.sum(log_likelihood) / n_samples\n        losses.append(round(float(loss), 4))\n    return (B.round(4).tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    losses = []\n    for i in range(iterations):\n        z = np.dot(X, weights) + bias\n        y_pred = sigmoid(z)\n        loss = -1 / m * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        dw = 1 / m * np.dot(X.T, y_pred - y)\n        db = 1 / m * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n    optimized_params = np.append(weights, bias).tolist()\n    return (optimized_params, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    attention_scores += mask * -1000000000.0\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nfrom math import log\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    This function uses the Shannon entropy to measure the disorder.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * log(p, 2) for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Applies Layer Normalization to the input tensor X.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim).\n    gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized tensor of the same shape as X.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized * gamma + beta\n    return np.round(X_normalized, decimals=5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"\n    Calculates the harmonic mean of precision and recall with a given beta.\n    \"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\ndef fragmentation_cost(reference, candidate):\n    \"\"\"\n    Calculates the fragmentation cost based on the number of unigrams in the candidate\n    that are in the reference but not in the correct order.\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    frag_cost = 0\n    for i in range(len(cand_words)):\n        if i < len(ref_words) and cand_words[i] == ref_words[i]:\n            continue\n        else:\n            frag_cost += abs(i - ref_words.index(cand_words[i])) if cand_words[i] in ref_words else len(ref_words)\n    return frag_cost\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculates the METEOR score for the given reference and candidate translations.\n    \"\"\"\n    ref_count = Counter(reference.split())\n    cand_count = Counter(candidate.split())\n    matches = sum((ref_count & cand_count).values())\n    precision = matches / len(candidate.split())\n    recall = matches / len(reference.split())\n    f_mean = harmonic_mean(precision, recall, beta)\n    frag_penalty = np.exp(-gamma * fragmentation_cost(reference, candidate))\n    score = f_mean * frag_penalty\n    if len(candidate.split()) > len(reference.split()):\n        score *= alpha\n    return round(score, 3)\nreference = 'the cat is on the mat'\ncandidate = 'the cat is on mat'"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The number of times both events x and y occur together.\n    total_counts_x (int): The total number of occurrences of event x.\n    total_counts_y (int): The total number of occurrences of event y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)\njoint_counts = 100\ntotal_counts_x = 200\ntotal_counts_y = 250\ntotal_samples = 10000"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    :param x: List of integers to be normalized.\n    :return: List of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0] * len(x)\n    normalized = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    return [round(n, 4) for n in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Parameters:\n    - x: np.ndarray, the input array of shape (batch_size, input_dim).\n    - w1: np.ndarray, the first weight matrix of shape (input_dim, hidden_dim).\n    - w2: np.ndarray, the second weight matrix of shape (hidden_dim, input_dim).\n    \n    Returns:\n    - np.ndarray, the output array after passing through the residual block.\n    \"\"\"\n    hidden = np.maximum(0, np.dot(x, w1))\n    out = np.dot(hidden, w2)\n    output = np.maximum(0, out + x)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D numpy array.\n    \n    Parameters:\n    - x: np.ndarray of shape (height, width, channels), the input feature maps.\n    \n    Returns:\n    - np.ndarray of shape (channels,), the global average pooled output.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n    \n    Parameters:\n    - X: np.ndarray, input array of shape (batch, channels, height, width)\n    - gamma: np.ndarray, scale parameter of shape (channels,)\n    - beta: np.ndarray, shift parameter of shape (channels,)\n    - epsilon: float, small constant to prevent division by zero\n    \n    Returns:\n    - np.ndarray: normalized, scaled, and shifted output array\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma[None, :, None, None] * X_norm + beta[None, :, None, None]\n    return np.round(X_scaled_shifted, decimals=4).tolist()\nX = np.random.rand(2, 3, 4, 4)\ngamma = np.random.rand(3)\nbeta = np.random.rand(3)"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is to be calculated.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    :param vectors: A list of 2D vectors (lists of floats).\n    :param tol: Tolerance value to determine linear independence.\n    :return: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    V = np.array(vectors)\n    num_vectors = V.shape[0]\n    orthonormal_basis = []\n    for i in range(num_vectors):\n        for basis in orthonormal_basis:\n            V[i] -= np.dot(V[i], basis) * basis\n        if np.linalg.norm(V[i]) > tol:\n            orthonormal_basis.append(V[i] / np.linalg.norm(V[i]))\n    orthonormal_basis = [np.round(vec, 4).tolist() for vec in orthonormal_basis]\n    return orthonormal_basis\nvectors = [[1, 2], [2, 4], [1, 0]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): The first 3D vector.\n    b (list/tuple): The second 3D vector.\n    \n    Returns:\n    list: The cross product of the two vectors, rounded to 4 decimal places.\n    \"\"\"\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    return np.round(result, 4).tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n    \n    Returns:\n    list: Solution vector x as a list, or -1 if no unique solution exists.\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b)\n    if A.shape[0] != A.shape[1]:\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(len(A)):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        xi = det_Ai / det_A\n        x.append(round(xi, 4))\n    return np.array(x).tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n    \n    Args:\n    p: list[float] - A list of floats representing the first probability distribution.\n    q: list[float] - A list of floats representing the second probability distribution.\n    \n    Returns:\n    float - The Bhattacharyya distance between the two distributions rounded to 4 decimal places.\n            Returns 0.0 if the inputs are not of the same length or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = np.sum(np.sqrt(np.multiply(p, q)))\n    bd = -np.log(bc)\n    return round(bd, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n    \n    Returns:\n    list[int|float] | int: A new vector representing the element-wise sum of 'a' and 'b'\n    if they have the same length, otherwise -1.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    for episode in episodes:\n        returns = 0\n        for (state, action, reward) in reversed(episode):\n            returns += reward\n        for (state, action, _) in episode:\n            policy = softmax(theta[state, :])\n            gradient[state, action] += returns * policy[action]\n            gradient[state, :] -= returns * policy[action] * policy\n    gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()\ntheta = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\nepisodes = [[(0, 1, 1), (1, 2, 1), (2, 0, 1)], [(0, 2, 1), (1, 1, 1), (2, 2, 1)]]"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts during computation.\n    d_in (int): The input dimension for each expert.\n    d_out (int): The output dimension for each expert.\n    \n    Returns:\n    float: The percentage of computational cost savings rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * k_active * d_in * d_out + 2 * n_experts * d_in\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nn_experts = 128\nk_active = 4\nd_in = 1024\nd_out = 1024"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int) -> list:\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Weight matrix for the noise.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): Number of experts to select.\n    \n    Returns:\n    list: The final gating probabilities matrix as a list.\n    \"\"\"\n    logits = X @ W_g\n    noisy_logits = logits + N * W_noise\n    exp_scores = np.exp(noisy_logits - np.max(noisy_logits, axis=-1, keepdims=True))\n    probs = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    topk_indices = np.argsort(probs, axis=-1)[:, -k:]\n    topk_probs = np.zeros_like(probs)\n    for (i, idx) in enumerate(topk_indices):\n        topk_probs[i, idx] = probs[i, idx]\n    topk_probs /= np.sum(topk_probs, axis=-1, keepdims=True)\n    return np.round(topk_probs, decimals=4).tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts layer with softmax gating and top-k routing.\n    \n    Parameters:\n    x: Input tensor of shape (batch_size, input_dim).\n    We: Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg: Gating weight matrix of shape (input_dim, n_experts).\n    n_experts: Number of experts.\n    top_k: Number of top experts to consider for each input token.\n    \n    Returns:\n    Output tensor of shape (batch_size, output_dim) after applying MoE.\n    \"\"\"\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for (j, idx) in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = np.dot(x[i], We[idx])\n    final_output = np.sum(expert_outputs * top_k_probs_normalized[..., np.newaxis], axis=1)\n    return np.round(final_output, 4).tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    assert C % G == 0, 'The number of channels is not divisible by the number of groups.'\n    X = X.reshape(B, G, -1)\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    X_normalized = X_normalized * gamma + beta\n    X_normalized = np.round(X_normalized, 4)\n    return X_normalized.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Find the x value that minimizes the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n    \n    Parameters:\n    - start_x: The starting x position for the search.\n    - learning_rate: The step size at each iteration while moving toward a minimum of the function.\n    - tolerance: The maximum acceptable error in the position of the minimum.\n    - max_iters: The maximum number of iterations to perform.\n    \n    Returns:\n    - The x value at which f(x) is minimized, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * gradient\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which is a normalization-free transformation\n    inspired by the Tanh function. This function takes an array of input values and applies\n    the DyT transformation using given parameters alpha, gamma, and beta.\n    \n    Args:\n    x (np.ndarray): Input array of values.\n    alpha (float): Scaling factor for the input values.\n    gamma (float): Scaling factor for the tanh output.\n    beta (float): Bias for the tanh output.\n    \n    Returns:\n    list[float]: List of transformed values, rounded to the nearest 4th decimal.\n    \"\"\"\n    transformed = gamma * np.tanh(alpha * x) + beta\n    rounded_results = np.round(transformed, 4)\n    return rounded_results.tolist()\nx = np.array([0.5, -0.2, 1.0, 0.0])\nalpha = 1.0\ngamma = 1.5\nbeta = 0.1"}
