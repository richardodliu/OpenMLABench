{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Parameters:\n    a (list of list of int/float): The matrix.\n    b (list of int/float): The vector.\n    \n    Returns:\n    list of int/float: The resulting vector if the operation is valid, or -1 if the dimensions are incompatible.\n    \"\"\"\n    # Check if the number of columns in 'a' matches the length of 'b'\n    if not a or len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Test cases to verify the correctness\nif __name__ == \"__main__\":\n    # Test case 1: Valid matrix-vector multiplication\n    matrix1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    vector1 = [1, 0, 1]\n    print(matrix_dot_vector(matrix1, vector1))  # Expected: [4, 10, 16]\n\n    # Test case 2: Incompatible dimensions\n    matrix2 = [[1, 2], [3, 4]]\n    vector2 = [1, 0, 1]\n    print(matrix_dot_vector(matrix2, vector2))  # Expected: -1\n\n    # Test case 3: Empty matrix\n    matrix3 = []\n    vector3 = [1, 2]\n    print(matrix_dot_vector(matrix3, vector3))  # Expected: -1\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Computes the transpose of a given matrix `a`.\n    \n    Parameters:\n    a (list of list of int|float): The input matrix to be transposed.\n    \n    Returns:\n    list of list of int|float: The transposed matrix.\n    \"\"\"\n    # Check if the input list is empty or not a matrix\n    if not a or not all(isinstance(row, list) for row in a):\n        return []\n    \n    # Compute the transpose\n    transposed = [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n    \n    return transposed\n\n# Function to check the correctness of the transpose_matrix function\ndef check_transpose():\n    matrix1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    expected1 = [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n    assert transpose_matrix(matrix1) == expected1, f\"Test failed for input {matrix1}\"\n    \n    matrix2 = [[1, 2], [3, 4], [5, 6]]\n    expected2 = [[1, 3, 5], [2, 4, 6]]\n    assert transpose_matrix(matrix2) == expected2, f\"Test failed for input {matrix2}\"\n    \n    print(\"All tests passed!\")\n\n# Run the check function to verify the correctness of the solution\ncheck_transpose()\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes a given 2D list (matrix) into a new shape specified by new_shape.\n    If the reshaping is not possible, returns an empty list.\n    \n    :param a: 2D list of integers or floats representing the matrix.\n    :param new_shape: A tuple of two integers specifying the new shape (rows, columns).\n    :return: A reshaped 2D list of the matrix or an empty list if reshaping is not possible.\n    \"\"\"\n    # Convert the list to a numpy array\n    np_array = np.array(a)\n    \n    # Check if the reshape is possible\n    if np_array.size != new_shape[0] * new_shape[1]:\n        return []\n    \n    # Reshape the array\n    reshaped_array = np_array.reshape(new_shape)\n    \n    # Convert the numpy array back to a list and return\n    return reshaped_array.tolist()\n\n# Check function to verify the correctness of the solution\ndef check_reshape_matrix():\n    assert reshape_matrix([[1, 2], [3, 4]], (1, 4)) == [[1, 2, 3, 4]], \"Test case 1 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (4, 1)) == [[1], [2], [3], [4]], \"Test case 2 failed\"\n    assert reshape_matrix([[1, 2, 3], [4, 5, 6]], (3, 2)) == [[1, 2], [3, 4], [5, 6]], \"Test case 3 failed\"\n    assert reshape_matrix([[1, 2], [3, 4]], (3, 3)) == [], \"Test case 4 failed\"\n    print(\"All test cases passed!\")\n\n# Run the check function to verify the solution\ncheck_reshape_matrix()\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column based on the given mode.\n    \n    Parameters:\n    - matrix (list of lists): The matrix to calculate means from.\n    - mode (str): 'row' or 'column' to specify the calculation mode.\n    \n    Returns:\n    - list of floats: The means calculated according to the specified mode.\n    \"\"\"\n    # Convert the matrix to a numpy array for easier manipulation\n    np_matrix = np.array(matrix)\n    \n    if mode.lower() == 'row':\n        # Calculate the mean of each row\n        means = np_matrix.mean(axis=1)\n    elif mode.lower() == 'column':\n        # Calculate the mean of each column\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Mode must be either 'row' or 'column'\")\n    \n    # Convert numpy array back to list for the output\n    return list(means)\n\n# Example usage\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(\"Row means:\", calculate_matrix_mean(matrix, 'row'))\nprint(\"Column means:\", calculate_matrix_mean(matrix, 'column'))\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the scalar value.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar value.\n    :return: A 2D list representing the matrix after scalar multiplication.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a matrix and a scalar\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 2\n\n    # Perform scalar multiplication\n    result = scalar_multiply(matrix, scalar)\n\n    # Print the result\n    for row in result:\n        print(row)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    Args:\n    matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[float]: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    np_matrix = np.array(matrix)\n    \n    # Calculate the eigenvalues using numpy's linalg.eigvals function\n    eigenvalues = np.linalg.eigvals(np_matrix)\n    \n    # Sort the eigenvalues from highest to lowest\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    \n    # Return the sorted eigenvalues as a list of floats\n    return [float(val) for val in sorted_eigenvalues]\n\n# Example usage\nif __name__ == \"__main__\":\n    example_matrix = [[4, 2], [1, 3]]\n    print(calculate_eigenvalues(example_matrix))\n```\n\nThis solution involves using the `numpy` library to perform the mathematical computation of eigenvalues for a given 2x2 matrix. The function `calculate_eigenvalues` takes a 2x2 matrix as input, converts it into a numpy array, and then uses `numpy.linalg.eigvals` to calculate the eigenvalues. The eigenvalues are then sorted from highest to lowest and returned as a list of floats."}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    try:\n        # Convert lists to numpy arrays\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        \n        # Check if T and S are invertible by trying to compute their inverses\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        \n        # Perform the operation T^{-1}AS\n        transformed_matrix = np.round(np.dot(np.dot(T_inv, A_np), S_inv), 4)\n        \n        # Convert back to list and return\n        return transformed_matrix.tolist()\n    except np.linalg.LinAlgError:\n        # If T or S are not invertible, return -1\n        return -1\n\n# Example usage\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 2]]\nS = [[2, 0], [0, 1]]\nprint(transform_matrix(A, T, S))\n```\n\nThis script defines a function `transform_matrix` that takes three matrices $A$, $T$, and $S$ as input, checks if $T$ and $S$ are invertible, and then performs the operation $T^{-1}AS$. The result is rounded to the nearest 4th decimal and returned as a list. If $T$ or $S$ are not invertible, the function returns -1."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list of list of float): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list of list of float: The inverse of the matrix, if it exists. Returns None if the matrix is not invertible.\n    \"\"\"\n    # Check if the input is a 2x2 matrix\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input matrix must be 2x2.\")\n    \n    # Calculate determinant\n    det = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n    \n    # Check if the matrix is invertible\n    if det == 0:\n        return None\n    \n    # Calculate the inverse\n    inv_matrix = [\n        [matrix[1][1] / det, -matrix[0][1] / det],\n        [-matrix[1][0] / det, matrix[0][0] / det]\n    ]\n    \n    return inv_matrix\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [[4, 7], [2, 6]]\n    inv_matrix = inverse_2x2(matrix)\n    if inv_matrix:\n        print(\"Inverse of the matrix is:\", inv_matrix)\n    else:\n        print(\"The matrix is not invertible.\")\n```"}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b. Returns -1 if the matrices cannot be multiplied.\n    \n    :param a: A list of lists representing the first matrix.\n    :param b: A list of lists representing the second matrix.\n    :return: The product of the two matrices or -1 if multiplication is not possible.\n    \"\"\"\n    # Get the dimensions of the matrices\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    \n    # Check if multiplication is possible\n    if cols_a != rows_b:\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    # Perform matrix multiplication\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):  # cols_a is equal to rows_b\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# Matrix A (2x3)\nmatrix_a = [\n    [1, 2, 3],\n    [4, 5, 6]\n]\n\n# Matrix B (3x2)\nmatrix_b = [\n    [7, 8],\n    [9, 10],\n    [11, 12]\n]\n\n# Matrix multiplication\nresult = matrixmul(matrix_a, matrix_b)\n\n# Check function to verify the correctness of the result\ndef check_result(result):\n    if result == -1:\n        print(\"Multiplication is not possible.\")\n    else:\n        print(\"Result of matrix multiplication:\")\n        for row in result:\n            print(row)\n\ncheck_result(result)\n```\n\nThis script defines a function `matrixmul` that multiplies two matrices if possible, and returns -1 if the matrices cannot be multiplied. It also includes an example usage and a check function to verify the correctness of the multiplication result."}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Args:\n    vectors (list of list of float): A list of lists, where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: The covariance matrix as a list of lists.\n    \"\"\"\n    # Convert the list of lists to a numpy array for easier manipulation\n    matrix = np.array(vectors)\n    \n    # Calculate the covariance matrix using numpy's cov function\n    cov_matrix = np.cov(matrix)\n    \n    # Convert the numpy array back to a list of lists and return\n    return cov_matrix.tolist()\n\n# Test cases to verify the correctness of the function\ndef test_calculate_covariance_matrix():\n    vectors = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    expected_cov_matrix = [\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 1.0, 1.0]\n    ]\n    calculated_cov_matrix = calculate_covariance_matrix(vectors)\n    \n    # Check if the calculated covariance matrix matches the expected one\n    assert np.allclose(calculated_cov_matrix, expected_cov_matrix), \"Test failed: The calculated covariance matrix does not match the expected one.\"\n    \n    print(\"All tests passed!\")\n\n# Run the test cases\ntest_calculate_covariance_matrix()\n```\n\nThis script defines a function `calculate_covariance_matrix` that takes a list of lists as input and returns the covariance matrix as a list of lists. It uses the `numpy` library to perform the calculation. Additionally, it includes a test function `test_calculate_covariance_matrix` with a predefined set of vectors and the expected covariance matrix to verify the correctness of the implementation."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): The coefficient matrix.\n    b (np.ndarray): The constant terms vector.\n    n (int): The number of iterations.\n    \n    Returns:\n    list: The solution vector x as a list.\n    \"\"\"\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Initialize the solution vector with zeros\n    x = np.zeros_like(b, dtype=np.float64)\n    \n    # Iterate for n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        \n        # Round to the nearest 4th decimal\n        x_new = np.round(x_new, 4)\n        x = x_new\n    \n    # Return the result as a list\n    return x.tolist()\n\n# Example usage\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]])\nb = np.array([4, 7, 3])\nn = 25\nsolution = solve_jacobi(A, b, n)\nprint(\"Solution vector x:\", solution)\n```\n\nThis script defines a function `solve_jacobi` that implements the Jacobi iterative method to solve a system of linear equations. It takes as input a square matrix `A`, a vector `b`, and the number of iterations `n`. The solution vector `x` is iteratively refined, with each intermediate solution rounded to four decimal places, and finally returned as a list."}
{"task_id": 12, "completion_id": 0, "completion": "To approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method without directly using the `numpy.linalg.svd` function, we can follow an iterative approach. The Jacobian method can be used to find the eigenvalues of a matrix, which are related to the singular values of the original matrix. Here's a Python script that performs this task:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    Returns the singular values rounded to the nearest 4th decimal.\n    \n    :param A: 2x2 numpy array\n    :return: tuple of two singular values\n    \"\"\"\n    # Compute A^T * A and A * A^T\n    ATA = np.dot(A.T, A)\n    AAT = np.dot(A, A.T)\n    \n    # Function to compute the eigenvalues of a 2x2 matrix\n    def eigenvalues_2x2(M):\n        a, b = M[0, 0], M[1, 1]\n        c = M[0, 1]\n        delta = np.sqrt((a - b)**2 + 4 * c**2)\n        return (a + b + delta) / 2, (a + b - delta) / 2\n    \n    # Compute the eigenvalues of ATA and AAT\n    eigvals_ATA = eigenvalues_2x2(ATA)\n    eigvals_AAT = eigenvalues_2x2(AAT)\n    \n    # The singular values are the square roots of the eigenvalues of ATA or AAT\n    singular_values = np.sqrt(np.maximum(eigvals_ATA, eigvals_AAT))\n    \n    # Round the singular values to the nearest 4th decimal\n    singular_values_rounded = tuple(round(val, 4) for val in singular_values)\n    \n    return singular_values_rounded\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(singular_values)\n```\n\nThis script defines a function `svd_2x2_singular_values` that calculates the singular values of a 2x2 matrix `A` by computing the eigenvalues of `A^T * A` or `A * A^T` and taking their square roots. The function returns the singular values rounded to the nearest 4th decimal. Note that this method is a simplified approximation and may not be as accurate or robust as the full SVD algorithm implemented in libraries like NumPy."}
{"task_id": 13, "completion_id": 0, "completion": "```python\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    Args:\n    matrix (list of list of int|float): A 4x4 matrix.\n    \n    Returns:\n    float: The determinant of the matrix.\n    \"\"\"\n    \n    def determinant_3x3(sub_matrix: list[list[int|float]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix.\n        \n        Args:\n        sub_matrix (list of list of int|float): A 3x3 matrix.\n        \n        Returns:\n        float: The determinant of the 3x3 matrix.\n        \"\"\"\n        return (sub_matrix[0][0] * (sub_matrix[1][1] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][1]) -\n                sub_matrix[0][1] * (sub_matrix[1][0] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][0]) +\n                sub_matrix[0][2] * (sub_matrix[1][0] * sub_matrix[2][1] - sub_matrix[1][1] * sub_matrix[2][0]))\n    \n    # Base case for 4x4 matrix\n    det = 0\n    for col in range(4):\n        # Creating a sub_matrix by removing the first row and the current column\n        sub_matrix = [row[:col] + row[col+1:] for row in matrix[1:]]\n        # Calculating the determinant using Laplace's Expansion\n        det += ((-1) ** col) * matrix[0][col] * determinant_3x3(sub_matrix)\n    \n    return det\n\n# Example usage\nmatrix = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix))  # Output should be 0\n```"}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Perform linear regression using the normal equation method.\n    \n    Parameters:\n    X (list of list of float): The input features matrix.\n    y (list of float): The target vector.\n    \n    Returns:\n    list of float: The coefficients of the linear regression model.\n    \"\"\"\n    # Convert the input lists to numpy arrays\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    \n    # Calculate the coefficients using the normal equation\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, 4)\n    \n    # Convert the numpy array back to a list and return\n    return theta_rounded.tolist()\n\n# Example usage\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\ny = [2, 3, 4, 5]\nprint(linear_regression_normal_equation(X, y))\n```\nThis code snippet defines a function `linear_regression_normal_equation` that performs linear regression using the normal equation method. It takes a matrix of features `X` and a vector `y` as inputs, and returns the coefficients of the linear regression model, rounded to four decimal places."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input features matrix with a column of ones for the intercept.\n    y (np.ndarray): The target vector.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to perform.\n    \n    Returns:\n    list: The coefficients of the linear regression model.\n    \"\"\"\n    m, n = X.shape\n    theta = np.zeros(n)\n    \n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        error = predictions - y\n        gradient = np.dot(X.T, error) / m\n        theta -= alpha * gradient\n    \n    return np.round(theta, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    X = np.array([[1, 2], [1, 4], [1, 6], [1, 8]])\n    y = np.array([3, 7, 11, 15])\n    alpha = 0.01\n    iterations = 1000\n    \n    # Add a column of ones to X for the intercept\n    X = np.hstack([np.ones((X.shape[0], 1)), X[:, 1:]])\n    \n    # Perform linear regression using gradient descent\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(coefficients)\n```"}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Scales the input data using both standardization and min-max normalization.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    \n    Returns:\n    - Two 2D lists: the first one is the standardized data, the second one is the min-max normalized data.\n    \"\"\"\n    # Standardization: (x - mean) / std\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    # Min-max normalization: (x - min) / (max - min)\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    normalized_data = (data - min_values) / (max_values - min_values)\n    \n    # Rounding and converting to list\n    standardized_data = np.round(standardized_data, 4).tolist()\n    normalized_data = np.round(normalized_data, 4).tolist()\n    \n    return standardized_data, normalized_data\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Generating some random data for demonstration\n    np.random.seed(0)  # For reproducibility\n    data = np.random.rand(5, 3) * 100  # 5 samples, 3 features, values between 0 and 100\n    \n    standardized_data, normalized_data = feature_scaling(data)\n    print(\"Standardized Data:\\n\", standardized_data)\n    print(\"Normalized Data:\\n\", normalized_data)\n```\n\nThis script defines a function `feature_scaling` that takes a 2D NumPy array as input and returns two 2D lists: one with the data standardized and the other with the data normalized using min-max normalization. The example usage demonstrates how to use this function with a generated dataset."}
{"task_id": 17, "completion_id": 0, "completion": "Here is the self-contained Python script that solves the given task:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    \"\"\"\n    Perform k-Means clustering on a list of points.\n    \n    Parameters:\n    - points: A list of points, each point is a tuple of coordinates.\n    - k: Number of clusters.\n    - initial_centroids: Initial centroids for the clusters.\n    - max_iterations: Maximum number of iterations to perform.\n    \n    Returns:\n    - A list of the final centroids, each rounded to the nearest fourth decimal.\n    \"\"\"\n    # Convert points and centroids to numpy arrays for easier manipulation\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the closest centroid\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis])**2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        \n        # Update centroids to be the mean of the points assigned to them\n        new_centroids = np.array([points_array[closest_centroid == i].mean(axis=0) for i in range(k)])\n        \n        # If centroids have not changed, we have converged\n        if np.all(centroids_array == new_centroids):\n            break\n        \n        centroids_array = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    \n    return final_centroids\n\n# Example usage:\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0), (8.0, 2.0), (10.0, 2.0), (9.0, 3.0)]\nk = 3\ninitial_centroids = [(6.0, 2.0), (1.0, 2.0), (7.0, 3.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis script defines the `k_means_clustering` function which takes a list of points, the number of clusters `k`, a list of initial centroids, and the maximum number of iterations to perform. It then performs the k-Means clustering algorithm, updating the centroids until convergence or until the maximum number of iterations is reached. The final centroids are rounded to the nearest fourth decimal and returned as a list of tuples."}
{"task_id": 18, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Splits the dataset into k folds for cross-validation and returns the indices for each fold.\n    \n    Parameters:\n    X (np.ndarray): The feature matrix of the dataset.\n    y (np.ndarray): The labels of the dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): The seed for the random number generator to ensure reproducibility.\n    \n    Returns:\n    list: A list of tuples containing the train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)  # Ensure reproducibility\n    indices = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = X.shape[0] // k\n    indices_list = []\n    \n    for i in range(k):\n        start = i * fold_size\n        end = (i + 1) * fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        indices_list.append((train_indices, test_indices))\n    \n    return indices_list\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.random.rand(100, 10)  # Example feature matrix\n    y = np.random.randint(0, 2, 100)  # Example labels\n    k = 5  # Number of folds\n    splits = k_fold_cross_validation(X, y, k, shuffle=True, random_seed=42)\n    for i, (train_indices, test_indices) in enumerate(splits):\n        print(f\"Fold {i + 1}: Train indices {train_indices}, Test indices {test_indices}\")\n```"}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis on a given dataset.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Find the eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the top k eigenvectors\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Convert to list of lists and round to 4 decimal places\n    principal_components_list = [list(map(lambda x: round(x, 4), pc)) for pc in principal_components.T]\n    \n    return principal_components_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random dataset\n    np.random.seed(0)\n    data = np.random.rand(100, 5)\n    \n    # Perform PCA\n    principal_components = pca(data, 2)\n    \n    # Print the principal components\n    print(principal_components)\n```"}
{"task_id": 20, "completion_id": 0, "completion": "```python\nimport math\nfrom collections import Counter\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Builds a decision tree from the given examples and attributes.\n    \n    :param examples: List of examples, each example is a dict of attribute-value pairs.\n    :param attributes: List of attribute names.\n    :param target_attr: The name of the target attribute.\n    :return: A nested dictionary representing the decision tree.\n    \"\"\"\n    def entropy(examples, target_attr):\n        \"\"\"Calculate the entropy of the target attribute.\"\"\"\n        counter = Counter([example[target_attr] for example in examples])\n        total = len(examples)\n        return -sum((count / total) * math.log2(count / total) for count in counter.values())\n\n    def information_gain(examples, attribute, target_attr):\n        \"\"\"Calculate the information gain of a split on the attribute.\"\"\"\n        total_entropy = entropy(examples, target_attr)\n        partitions = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in partitions:\n                partitions[value] = []\n            partitions[value].append(example)\n        weighted_entropy = sum(len(partition) / len(examples) * entropy(partition, target_attr) for partition in partitions.values())\n        return total_entropy - weighted_entropy\n\n    def split(examples, attribute):\n        \"\"\"Split the examples based on the attribute.\"\"\"\n        partitions = {}\n        for example in examples:\n            value = example[attribute]\n            if value not in partitions:\n                partitions[value] = []\n            partitions[value].append(example)\n        return partitions\n\n    def most_common_value(examples, target_attr):\n        \"\"\"Find the most common value of the target attribute in the examples.\"\"\"\n        counter = Counter([example[target_attr] for example in examples])\n        return counter.most_common(1)[0][0]\n\n    # Base cases\n    if not examples:\n        return {}\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return most_common_value(examples, target_attr)\n\n    # Choose the attribute with the highest information gain\n    best_attribute = max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n    tree = {best_attribute: {}}\n\n    # Create a subtree for each value of the best attribute\n    for value, subset in split(examples, best_attribute).items():\n        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n\n    return tree\n\n# Example usage\nexamples = [\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': 'true', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': 'false', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': 'true', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': 'false', 'play': 'yes'},\n    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': 'true', 'play': 'no'}\n]\n\nattributes = ['outlook', 'temperature', 'humidity', 'windy']\ntarget_attr = 'play'\n\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```"}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_matrix(X, kernel='linear', sigma=1.0):\n    n = X.shape[0]\n    K = np.zeros((n, n))\n    if kernel == 'linear':\n        K = X @ X.T\n    elif kernel == 'rbf':\n        for i in range(n):\n            for j in range(n):\n                diff = X[i] - X[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\n    return K\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alpha = np.zeros(n_samples)\n    K = kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            if labels[i] * (np.sum(alpha * labels * K[:, i]) - alpha[i]) < 1:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i] + lambda_t * labels[i]\n            else:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i]\n    alpha = np.round(alpha, 4)\n    bias = 0\n    # Estimate bias\n    support_indices = np.where(alpha > 1e-5)[0]\n    for i in support_indices:\n        bias += labels[i] - np.sum(alpha * labels * K[:, i])\n    bias /= len(support_indices)\n    bias = np.round(bias, 4)\n    return alpha.tolist(), bias\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# labels = np.array([1, 1, -1, -1])\n# alpha, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\n# print(\"Alpha coefficients:\", alpha)\n# print(\"Bias:\", bias)\n```\nThis script defines a function `pegasos_kernel_svm` that implements a deterministic version of the Pegasos algorithm to train a kernel SVM. It supports both linear and RBF kernels and returns the model's alpha coefficients and bias, rounded to the nearest 4th decimal."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-1.0, 0.0, 1.0]\n    for value in test_values:\n        print(f\"sigmoid({value}) = {sigmoid(value)}\")\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax for a given list of scores.\n    The softmax function transforms each score in a list to a value between 0 and 1,\n    and the sum of all these transformed values equals 1.\n    \n    Args:\n    scores (list of float): A list of scores.\n    \n    Returns:\n    list of float: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    # Compute the exponent for each score\n    exp_scores = [math.exp(score) for score in scores]\n    # Sum of all exponent scores\n    sum_exp_scores = sum(exp_scores)\n    # Compute the softmax values\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values\n\n# Check function to verify the correctness of the softmax function\ndef check_softmax():\n    test_scores = [1.0, 2.0, 3.0]\n    expected_output = [0.0900, 0.2447, 0.6653]\n    assert softmax(test_scores) == expected_output, \"Test failed!\"\n    print(\"Test passed!\")\n\n# Run the check function\ncheck_softmax()\n```"}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: A list of lists where each sublist represents the feature vector of an example.\n    :param labels: A list of binary labels (0 or 1) for each example.\n    :param weights: A list of weights for each feature.\n    :param bias: A float representing the bias term.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n    \n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    \n    predictions = []\n    for example in features:\n        # Compute the linear combination of features and weights, then apply the sigmoid function.\n        linear_combination = sum(w * f for w, f in zip(weights, example)) + bias\n        prediction = sigmoid(linear_combination)\n        predictions.append(prediction)\n    \n    # Calculate the mean squared error.\n    mse = sum((p - l) ** 2 for p, l in zip(predictions, labels)) / len(labels)\n    \n    # Round the predictions and mse to 4 decimal places.\n    predictions = [round(pred, 4) for pred in predictions]\n    mse = round(mse, 4)\n    \n    return predictions, mse\n\n# Example usage:\nfeatures = [[0.5, 0.2], [0.1, 0.8], [0.3, 0.4]]\nlabels = [1, 0, 1]\nweights = [0.7, 0.3]\nbias = 0.1\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predictions:\", predictions)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis code snippet defines a function `single_neuron_model` that simulates a single neuron with a sigmoid activation function for binary classification. It takes as input a list of feature vectors, their corresponding binary labels, weights for each feature, and a bias term. The function returns the predicted probabilities after applying the sigmoid activation function and the mean squared error between the predicted probabilities and the true labels, both rounded to four decimal places."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef mse_loss(y_true, y_pred):\n    return ((y_true - y_pred) ** 2).mean()\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        loss = mse_loss(labels, predictions)\n        mse_values.append(round(loss, 4))\n\n        error = predictions - labels\n        derivative = sigmoid_derivative(np.dot(features, weights) + bias)\n        gradient = np.dot(features.T, error * derivative) / len(features)\n        bias_gradient = np.mean(error * derivative)\n\n        weights -= learning_rate * gradient\n        bias -= learning_rate * bias_gradient\n\n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Updated Weights:\", weights)\nprint(\"Updated Bias:\", bias)\nprint(\"MSE Values:\", mse_values)\n```\nThis script defines a function `train_neuron` that simulates a single neuron with sigmoid activation and updates its weights and bias using backpropagation based on the MSE loss. The function returns the updated weights, bias, and a list of MSE values for each epoch, each rounded to the nearest 4th decimal."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage:\nx = Value(2.0)\ny = Value(-4.0)\nz = x * y\nw = z.relu()\nw.backward()\nprint(x.grad, y.grad)  # Output should be -4.0, 0.0 because of the ReLU operation\n```"}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in basis B.\n    C (list of list of int): Basis vectors in basis C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from basis B to C.\n    \"\"\"\n    # Convert list of lists to numpy arrays\n    B_array = np.array(B)\n    C_array = np.array(C)\n    \n    # Compute the inverse of basis B\n    B_inv = np.linalg.inv(B_array)\n    \n    # Compute the transformation matrix P\n    P = C_array @ B_inv\n    \n    # Round the results to the nearest 4th decimal\n    P_rounded = np.round(P, 4)\n    \n    # Convert numpy array back to list of lists and return\n    return P_rounded.tolist()\n\n# Example usage\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(transform_basis(B, C))\n```\n\nThis script defines a function `transform_basis` that computes the transformation matrix from one basis to another in R^3, as per the task requirements. The function uses numpy for matrix operations and ensures the output is rounded to the nearest 4th decimal and returned as a list of lists."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Singular Value Decomposition (SVD) of a 2x2 matrix A.\n    Returns U, S, V such that A = U * S * V.T.\n    \"\"\"\n    # Compute A^T A\n    ATA = A.T @ A\n    \n    # Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(ATA)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Construct the singular values and the matrix S\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    \n    # Use the eigenvectors of A^T A as the columns of V\n    V = eigenvectors\n    \n    # Compute U by normalizing A * v_i for each non-zero singular value \u03c3_i\n    U = np.zeros_like(A)\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = (A @ V[:, i]) / singular_values[i]\n        else:\n            U[:, i] = V[:, i]\n    \n    # Round the results to the nearest 4th decimal\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    \n    # Convert to list and return\n    return U.tolist(), S.tolist(), V.tolist()\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines a function `svd_2x2` that computes the Singular Value Decomposition of a 2x2 matrix using the method based on the eigendecomposition of $A^T A$. The function returns the matrices U, S, and V as lists, with all elements rounded to the nearest 4th decimal."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the rows of two numpy arrays, X and y, maintaining their correspondence.\n    Useful for shuffling dataset samples and labels in unison for training machine learning models.\n    \n    Parameters:\n    - X: numpy array, feature dataset, 2D array with samples as rows\n    - y: numpy array, labels corresponding to the dataset, 1D array\n    - seed: int, optional random seed integer for reproducibility\n    \n    Returns:\n    - Shuffled X and y as python lists using numpy's tolist() method.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"X and y must have the same number of rows (samples).\")\n    \n    np.random.seed(seed)\n    permutation = np.random.permutation(X.shape[0])\n    shuffled_X = X[permutation]\n    shuffled_y = y[permutation]\n    \n    return shuffled_X.tolist(), shuffled_y.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    seed = 42  # For reproducibility\n\n    shuffled_X, shuffled_y = shuffle_data(X, y, seed)\n    print(\"Shuffled X:\", shuffled_X)\n    print(\"Shuffled y:\", shuffled_y)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    An iterator that yields batches of a specified size from a numpy array X.\n    If y is provided, it yields paired batches from both X and y.\n    \n    :param X: numpy array to be batched\n    :param y: optional numpy array to be batched alongside X\n    :param batch_size: size of each batch\n    :yield: batches of X, or paired batches of (X, y)\n    \"\"\"\n    assert type(X) == np.ndarray, \"X must be a numpy array\"\n    assert y is None or type(y) == np.ndarray, \"y must be a numpy array or None\"\n    assert len(X) == len(y) if y is not None else True, \"X and y must have the same length\"\n    \n    n_samples = len(X)\n    for i in range(0, n_samples, batch_size):\n        end_idx = min(i + batch_size, n_samples)\n        batch_X = X[i:end_idx].tolist()\n        if y is not None:\n            batch_y = y[i:end_idx].tolist()\n            yield batch_X, batch_y\n        else:\n            yield batch_X\n\n# Example usage:\nX = np.array([i for i in range(100)])\ny = np.array([i**2 for i in range(100)])\n\n# Testing the batch_iterator function\nfor batch_X, batch_y in batch_iterator(X, y, batch_size=10):\n    print(f\"Batch X: {batch_X}\\nBatch y: {batch_y}\\n\")\n```"}
{"task_id": 31, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is greater than or equal to a given threshold.\n    \n    Parameters:\n    X (np.ndarray): The dataset to divide.\n    feature_i (int): The index of the feature to divide the dataset on.\n    threshold (float): The threshold to compare the feature values against.\n    \n    Returns:\n    tuple: Two lists representing the subsets of the dataset. The first list contains samples where the feature value is >= threshold, and the second list contains the rest.\n    \"\"\"\n    # Split the dataset into two subsets based on the feature threshold\n    split_func = lambda sample: sample[feature_i] >= threshold\n    X_above = np.array([sample for sample in X if split_func(sample)]).tolist()\n    X_below = np.array([sample for sample in X if not split_func(sample)]).tolist()\n    \n    return X_above, X_below\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a random dataset\n    np.random.seed(0)\n    X = np.random.rand(10, 3)  # 10 samples, 3 features\n    feature_index = 1  # Divide based on the second feature\n    threshold_value = 0.5  # Threshold to compare the feature value against\n    \n    # Divide the dataset\n    X_above, X_below = divide_on_feature(X, feature_index, threshold_value)\n    \n    # Output the results\n    print(\"Above threshold:\", X_above)\n    print(\"Below threshold:\", X_below)\n```"}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    X : numpy.ndarray\n        The input samples. Shape: [n_samples, n_features]\n    degree : int\n        The degree of the polynomial features.\n        \n    Returns:\n    numpy.ndarray\n        The matrix of features, where the columns are polynomial combinations of the features with degree less than or equal to the specified degree.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n\n    n_samples, n_features = X.shape\n    combinations = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combinations = [item for sublist in combinations for item in sublist]\n    n_output_features = len(combinations)\n    X_new = np.empty((n_samples, n_output_features))\n    \n    for i, index_combination in enumerate(combinations):\n        X_new[:, i] = np.prod(X[:, index_combination], axis=1)\n    \n    return X_new.tolist()\n\n# Example usage\nX = np.array([[2, 3], [3, 5]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis code snippet defines a function `polynomial_features` that generates polynomial and interaction features up to the specified degree from the input dataset `X`. It uses `combinations_with_replacement` from the itertools module to create all possible combinations of input features for each degree up to the specified degree. Then, it calculates the product of these combinations for each sample to generate the new feature matrix. The resulting numpy array is converted to a list using `tolist()` before returning it."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature dataset.\n    y (np.ndarray): 1D numpy array representing the labels.\n    n_subsets (int): Number of subsets to generate.\n    replacements (bool): Whether to generate subsets with replacements.\n    seed (int): Seed for the random number generator.\n    \n    Returns:\n    list: A list of n_subsets random subsets of the dataset, where each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    indices = np.arange(len(X))\n    for _ in range(n_subsets):\n        if replacements:\n            chosen_indices = np.random.choice(indices, len(indices), replace=True)\n        else:\n            chosen_indices = np.random.choice(indices, len(indices), replace=False)\n        X_subset = X[chosen_indices]\n        y_subset = y[chosen_indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 0, 1, 0])\n    n_subsets = 3\n    subsets = get_random_subsets(X, y, n_subsets, replacements=False)\n    for i, subset in enumerate(subsets):\n        print(f\"Subset {i+1}:\")\n        print(\"X:\", subset[0])\n        print(\"y:\", subset[1])\n```"}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Convert a 1D numpy array of integers into a 2D one-hot encoded numpy array.\n    \n    Parameters:\n    x (np.array): A 1D numpy array of integers.\n    n_col (int, optional): The number of columns for the one-hot encoded array.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Example usage\nx = np.array([1, 0, 3])\nprint(to_categorical(x))\n```\n\nThis code defines the `to_categorical` function which takes a 1D numpy array and optionally the number of columns for the one-hot encoding. If the number of columns is not provided, it is automatically determined as the maximum value in the array plus one. The function then uses numpy's `eye` to create an identity matrix and indexes it with the input array to perform one-hot encoding, finally returning the result as a list of lists."}
{"task_id": 35, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    x (np.array): 1D numpy array\n    \n    Returns:\n    list: 2D list representing the diagonal matrix\n    \"\"\"\n    # Ensure x is a numpy array\n    x = np.array(x)\n    # Create a diagonal matrix from x\n    diagonal_matrix = np.diag(x)\n    # Convert the numpy array to a python list\n    return diagonal_matrix.tolist()\n\n# Example usage\nx = np.array([1, 2, 3, 4])\nprint(make_diagonal(x))\n```\n\nThis code snippet will take a 1D numpy array, convert it into a diagonal matrix, and then return the result as a list of lists, which is a python list representation of a 2D numpy array."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure that the inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the accuracy score\n    accuracy = np.mean(y_true == y_pred)\n    \n    # Return the accuracy score rounded to 4 decimal places\n    return round(accuracy, 4)\n\n# Example usage and check function\ndef check_accuracy():\n    y_true = np.array([1, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 1, 0, 0, 1])\n    expected_accuracy = 0.8333\n    assert accuracy_score(y_true, y_pred) == expected_accuracy, \"The calculated accuracy does not match the expected value.\"\n    print(\"Accuracy score is correct:\", accuracy_score(y_true, y_pred))\n\n# Run the check function\ncheck_accuracy()\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (2D numpy array): The first dataset.\n    Y (2D numpy array, optional): The second dataset. If not provided, X is used.\n    \n    Returns:\n    2D numpy array: The correlation matrix as a 2D numpy array.\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Centering the data\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    \n    # Calculating the covariance matrix\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    \n    # Normalizing to get the correlation matrix\n    X_std = np.std(X, axis=0, keepdims=True)\n    Y_std = np.std(Y, axis=0, keepdims=True)\n    correlation_matrix = cov_matrix / (X_std.T * Y_std)\n    \n    # Rounding to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Return as a python list\n    return correlation_matrix.tolist()\n\n# Example usage\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\n\n# Test the function\nprint(calculate_correlation_matrix(X, Y))\nprint(calculate_correlation_matrix(X))\n```"}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implement the fit method for an AdaBoost classifier.\n    \n    Parameters:\n    X : numpy.ndarray\n        The input samples of shape (n_samples, n_features).\n    y : numpy.ndarray\n        The target values of shape (n_samples,).\n    n_clf : int\n        The number of weak classifiers to use.\n        \n    Returns:\n    list of dict\n        A list of dictionaries, each containing the classifier parameters.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Initialize weights\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    \n    for _ in range(n_clf):\n        best = {'error': 1, 'polarity': 1, 'feature_idx': 0, 'threshold': 0, 'alpha': 0}\n        \n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    prediction = np.ones(n_samples)\n                    prediction[X[:, feature_idx] * polarity < threshold * polarity] = -1\n                    weighted_error = np.sum(weights[y != prediction])\n                    \n                    if weighted_error < best['error']:\n                        best = {'error': weighted_error, 'polarity': polarity, 'feature_idx': feature_idx, \n                                'threshold': threshold}\n        \n        # Calculate alpha\n        best['alpha'] = 0.5 * np.log((1 - best['error']) / best['error'])\n        \n        # Update weights\n        prediction = np.ones(n_samples)\n        prediction[X[:, best['feature_idx']] * best['polarity'] < best['threshold'] * best['polarity']] = -1\n        weights *= np.exp(-best['alpha'] * y * prediction)\n        weights /= np.sum(weights)\n        \n        classifiers.append(best)\n    \n    return classifiers\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n    y = np.array([1, 1, 1, -1, -1, -1])\n    n_clf = 2\n    classifiers = adaboost_fit(X, y, n_clf)\n    for clf in classifiers:\n        for key, value in clf.items():\n            if isinstance(value, float):\n                clf[key] = round(value, 4)\n    print(classifiers)\n```\n\nThis script defines and implements the `adaboost_fit` function, which fits an AdaBoost classifier to a given dataset. It iteratively selects the best weak classifier based on weighted error, updates the sample weights, and stores the classifier parameters. The final classifiers are returned as a list of dictionaries, each containing the parameters of a weak classifier."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    The log-softmax function is the logarithm of the softmax function, which is used to\n    convert a vector of scores into probabilities. This function is often used for numerical\n    stability when computing the softmax of large numbers.\n    \n    Parameters:\n    scores (list): A list of numerical scores.\n    \n    Returns:\n    list: A list of log-softmax values, rounded to 4 decimal places.\n    \"\"\"\n    scores = np.array(scores)\n    # Compute softmax\n    softmax_scores = np.exp(scores - np.max(scores)) / np.exp(scores - np.max(scores)).sum()\n    # Compute log-softmax\n    log_softmax_scores = np.log(softmax_scores)\n    # Round to 4 decimal places and return as a list\n    return np.round(log_softmax_scores, 4).tolist()\n\n# Check function with provided data points\ndef check_log_softmax():\n    test_scores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n    expected_output = [-3.4519, -2.4519, -1.4519, -0.4519, -3.4519, -2.4519, -1.4519]\n    assert log_softmax(test_scores) == expected_output, \"The log_softmax function does not produce expected output.\"\n    print(\"The log_softmax function works correctly with the provided data points.\")\n\n# Run the check function\ncheck_log_softmax()\n```"}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        # Initialize weights and biases\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        # Initialize optimizers\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Calculate gradient with respect to layer input\n        w_grad = self.layer_input.T.dot(accum_grad)\n        # Update weights and biases if trainable\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, w_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, np.sum(accum_grad, axis=0, keepdims=True))\n        # Return gradient with respect to the input\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    class Optimizer:\n        def update(self, param, grad):\n            # Simple SGD update rule\n            return param - 0.01 * grad\n\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    dense_layer.initialize(Optimizer())\n\n    input_data = np.array([[0.1, 0.2], [0.3, 0.4]])\n    forward_output = dense_layer.forward_pass(input_data, training=True)\n    backward_output = dense_layer.backward_pass(forward_output)\n\n    print(\"Forward Pass Output:\\n\", np.round(forward_output, 4).tolist())\n    print(\"Backward Pass Output:\\n\", np.round(backward_output, 4).tolist())\n    print(\"Total Parameters:\", dense_layer.parameters())\n```"}
{"task_id": 41, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Perform a simple 2D convolution on the input_matrix using the given kernel,\n    with specified padding and stride. The result is rounded to the nearest 4th\n    decimal and returned as a list.\n    \n    :param input_matrix: 2D numpy array representing the input image\n    :param kernel: 2D numpy array representing the convolution kernel\n    :param padding: Integer specifying the padding size\n    :param stride: Integer specifying the stride length\n    :return: List of the convolved matrix\n    \"\"\"\n    # Padding the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Dimensions\n    input_h, input_w = input_matrix.shape\n    kernel_h, kernel_w = kernel.shape\n    \n    # Output dimensions\n    output_h = int((input_h + 2*padding - kernel_h) / stride) + 1\n    output_w = int((input_w + 2*padding - kernel_w) / stride) + 1\n    \n    # Initialize the output matrix\n    output_matrix = np.zeros((output_h, output_w))\n    \n    # Perform convolution\n    for i in range(0, output_h):\n        for j in range(0, output_w):\n            row = i * stride\n            col = j * stride\n            output_matrix[i, j] = np.sum(padded_input[row:row+kernel_h, col:col+kernel_w] * kernel)\n    \n    # Round to the nearest 4th decimal\n    output_matrix = np.round(output_matrix, 4)\n    \n    # Return as a list\n    return output_matrix.tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1\nprint(simple_conv2d(input_matrix, kernel, padding, stride))\n```\n\nThis script includes the function `simple_conv2d` which performs a simple 2D convolution on a given input matrix using a convolutional kernel, with specified padding and stride. The output is rounded to the nearest 4th decimal and returned as a list. An example usage is provided at the end of the script demonstrating how to call the function with a sample input matrix, kernel, padding, and stride."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): A float number to which the ReLU function is applied.\n    \n    Returns:\n    float: The result of applying the ReLU function to z.\n    \"\"\"\n    return max(0, z)\n\n# Test the function with given data points\ntest_values = [-10.0, -1.0, 0.0, 1.0, 10.0]\nfor value in test_values:\n    print(f\"ReLU({value}) = {relu(value)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Computes the Ridge Regression loss, which is the sum of the Mean Squared Error (MSE) and a regularization term.\n    \n    Parameters:\n    X (np.ndarray): 2D numpy array representing the feature matrix.\n    w (np.ndarray): 1D numpy array representing the coefficients.\n    y_true (np.ndarray): 1D numpy array representing the true labels.\n    alpha (float): Regularization parameter.\n    \n    Returns:\n    float: The Ridge loss, rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w  # Compute the predicted values\n    mse = np.mean((y_true - y_pred) ** 2)  # Compute the Mean Squared Error\n    regularization = alpha * np.sum(w ** 2)  # Compute the regularization term\n    loss = mse + regularization  # Compute the total loss\n    return round(loss, 4)  # Return the loss rounded to the nearest 4th decimal\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, 0.5])\n    y_true = np.array([1, 2, 3])\n    alpha = 0.1\n    \n    # Calculate and print the ridge loss\n    loss = ridge_loss(X, w, y_true, alpha)\n    print(f\"Ridge Loss: {loss}\")\n```\n\nThis script defines a function `ridge_loss` that calculates the Ridge Regression loss, combining the Mean Squared Error (MSE) and a regularization term. It also includes an example usage to demonstrate how to calculate the ridge loss with given data points."}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the function.\n    alpha (float): The slope for the negative part of the function. Default is 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Test cases to validate the solution\nif __name__ == \"__main__\":\n    # Test with a positive input\n    print(leaky_relu(5.0))  # Expected output: 5.0\n    # Test with a negative input\n    print(leaky_relu(-5.0))  # Expected output: -0.05\n    # Test with a negative input and a custom alpha\n    print(leaky_relu(-5.0, 0.1))  # Expected output: -0.5\n    # Test with zero input\n    print(leaky_relu(0.0))  # Expected output: 0.0\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    Args:\n    x1: A numpy array representing the first vector.\n    x2: A numpy array representing the second vector.\n    \n    Returns:\n    The linear kernel value as a float, which is the dot product of x1 and x2.\n    \"\"\"\n    if x1.shape != x2.shape:\n        raise ValueError(\"The input vectors must have the same dimensions.\")\n    \n    return np.dot(x1, x2)\n\n# Example usage\nif __name__ == \"__main__\":\n    x1 = np.array([1, 2, 3])\n    x2 = np.array([4, 5, 6])\n    print(\"Linear kernel value:\", kernel_function(x1, x2))\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision score.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"The shape of y_true and y_pred do not match.\")\n    \n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    \n    if predicted_positives == 0:\n        return 1.0  # If no positive predictions were made, precision is 1 by definition.\n    \n    precision_score = true_positives / predicted_positives\n    return precision_score\n\n# Check function with provided data points\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(precision(y_true, y_pred))  # Output should be 0.75 as there are 3 true positives and 1 false positive.\n```"}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using MSE loss.\n    \n    Parameters:\n    X (np.ndarray): Input features.\n    y (np.ndarray): Target values.\n    weights (np.ndarray): Initial weights.\n    learning_rate (float): Learning rate for the gradient descent.\n    n_iterations (int): Number of iterations to perform.\n    batch_size (int): Size of the batch for mini-batch and stochastic gradient descent.\n    method (str): Method to use for gradient descent. Options: 'batch', 'stochastic', 'mini-batch'.\n    \n    Returns:\n    list: Optimized weights as a python list.\n    \"\"\"\n    n_samples, n_features = X.shape\n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            # Stochastic Gradient Descent\n            for i in range(n_samples):\n                random_index = np.random.randint(n_samples)\n                xi = X[random_index:random_index+1]\n                yi = y[random_index:random_index+1]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i+batch_size]\n                yi = y[i:i+batch_size]\n                prediction = np.dot(xi, weights)\n                error = prediction - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient / batch_size\n        else:\n            # Batch Gradient Descent\n            prediction = np.dot(X, weights)\n            error = prediction - y\n            gradient = 2 * X.T.dot(error)\n            weights -= learning_rate * gradient / n_samples\n            \n    return np.round(weights, 4).tolist()\n\n# Example usage\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.0, 0.0])\nlearning_rate = 0.001\nn_iterations = 1000\nbatch_size = 2\n\n# Batch Gradient Descent\nweights_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, method='batch')\nprint(\"Batch Gradient Descent Weights:\", weights_batch)\n\n# Stochastic Gradient Descent\nweights_stochastic = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='stochastic')\nprint(\"Stochastic Gradient Descent Weights:\", weights_stochastic)\n\n# Mini-Batch Gradient Descent\nweights_mini_batch = gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=2, method='mini-batch')\nprint(\"Mini-Batch Gradient Descent Weights:\", weights_mini_batch)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Converts a given matrix to its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D list representing the matrix to be converted.\n    \n    Returns:\n    list of lists: The RREF of the given matrix.\n    \"\"\"\n    # Convert the input list of lists into a numpy array\n    mat = np.array(matrix, dtype=float)\n    rows, cols = mat.shape\n    lead = 0\n    \n    for r in range(rows):\n        if cols <= lead:\n            return mat.tolist()\n        \n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if rows == i:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        \n        mat[[i, r]] = mat[[r, i]]\n        mat[r] = mat[r] / mat[r, lead]\n        \n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        lead += 1\n    \n    return mat.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n]\n\nprint(rref(matrix))\n```\n\nThis script defines a function `rref` that takes a matrix (in the form of a list of lists) and returns its Reduced Row Echelon Form (RREF) using Gaussian elimination with partial pivoting. The function uses numpy for matrix operations and returns the result as a list of lists."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    \n    for _ in range(num_iterations):\n        t += 1\n        grad_x = grad(x0)\n        \n        m = beta1 * m + (1 - beta1) * grad_x\n        v = beta2 * v + (1 - beta2) * (grad_x ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return np.round(x0, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial guess\nx0 = np.array([1.0, 1.0])\n\n# Optimize\noptimized_params = adam_optimizer(f, grad, x0)\nprint(optimized_params)\n```\n\nThis script defines the `adam_optimizer` function that implements the Adam optimization algorithm. The example usage at the bottom demonstrates how to use the function to optimize a simple quadratic function. The output will be the optimized parameters, rounded to the nearest 4th decimal, as a Python list."}
{"task_id": 50, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    \"\"\"\n    Implement Lasso Regression using Gradient Descent with L1 regularization.\n    \n    Parameters:\n    X (np.array): The input features, shape (n_samples, n_features).\n    y (np.array): The target values, shape (n_samples,).\n    alpha (float): The regularization parameter.\n    learning_rate (float): The learning rate for gradient descent.\n    max_iter (int): The maximum number of iterations for gradient descent.\n    tol (float): The tolerance for stopping criteria.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.zeros(n_features)\n    bias = 0\n    prev_cost = np.inf\n    \n    for _ in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        loss = y_pred - y\n        \n        # Gradient of the loss function\n        gradient_weights = (1 / n_samples) * np.dot(X.T, loss) + alpha * np.sign(weights)\n        gradient_bias = (1 / n_samples) * np.sum(loss)\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n        \n        # Cost function (mean squared error)\n        cost = (1 / (2 * n_samples)) * np.sum(loss ** 2)\n        \n        # Stopping criteria\n        if np.abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    return np.round(weights, 4).tolist(), np.round(bias, 4).tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# y = np.array([1, 2, 3, 4])\n# weights, bias = l1_regularization_gradient_descent(X, y)\n# print(\"Weights:\", weights)\n# print(\"Bias:\", bias)\n```\n\nThis code will implement Lasso Regression using Gradient Descent with L1 regularization. The `l1_regularization_gradient_descent` function takes the input features `X`, target values `y`, regularization parameter `alpha`, learning rate, maximum number of iterations, and tolerance for stopping criteria. It returns the weights and bias as a tuple of lists, with each value rounded to the nearest 4th decimal."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of operations required to convert\n    one string into another, where operations include insertion, deletion,\n    substitution, and transposition of adjacent characters.\n    \"\"\"\n    m, n = len(source), len(target)\n    # Initialize the matrix with dimensions (m+1) x (n+1)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    \n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1,        # Deletion\n                           dp[i][j - 1] + 1,        # Insertion\n                           dp[i - 1][j - 1] + cost) # Substitution\n            if i > 1 and j > 1 and source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost) # Transposition\n\n    return dp[m][n]\n\n# Example usage\nsource_str = \"caper\"\ntarget_str = \"acer\"\nprint(OSA(source_str, target_str))  # Expected output: 2\n```\n\nThis solution uses dynamic programming to calculate the OSA distance between two strings. The matrix `dp` is used to store the minimum number of operations required to convert substrings of the source to substrings of the target, considering the four types of operations allowed."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n    \n    :param y_true: A list of true binary labels (0 or 1).\n    :param y_pred: A list of predicted binary labels (0 or 1).\n    :return: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate recall\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    \n    return round(recall_value, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(recall(y_true, y_pred))  # Output should be the recall value\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism.\n    \n    Args:\n    X: Input sequence as a numpy array of shape (seq_len, d_model).\n    W_q: Query weight matrix as a numpy array of shape (d_model, d_k).\n    W_k: Key weight matrix as a numpy array of shape (d_model, d_k).\n    W_v: Value weight matrix as a numpy array of shape (d_model, d_v).\n    \n    Returns:\n    A numpy array representing the self-attention output, rounded to 4 decimal places.\n    \"\"\"\n    # Compute the query, key, and value matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Compute the attention scores\n    d_k = W_k.shape[1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply the softmax function to the scores\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the value vectors\n    output = np.dot(attention, V)\n    \n    # Round the output to 4 decimal places\n    output = np.round(output, 4)\n    \n    # Return the output as a list\n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input sequence and weight matrices\n    X = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\n    W_q = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    W_k = np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]])\n    W_v = np.array([[1.3, 1.4, 1.5], [1.6, 1.7, 1.8], [1.9, 2.0, 2.1]])\n    \n    # Call the self_attention function\n    output = self_attention(X, W_q, W_k, W_v)\n    print(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell that processes a sequence of input vectors and produces the final hidden state.\n    \n    Parameters:\n    - input_sequence: A list of input vectors.\n    - initial_hidden_state: The initial hidden state.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to four decimal places.\n    \"\"\"\n    # Convert inputs to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state\n    h = initial_hidden_state\n    \n    # Process each input in the sequence\n    for x in input_sequence:\n        # Compute the new hidden state\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    \n    # Round the final hidden state to 4 decimal places and convert back to list\n    final_hidden_state = np.round(h, decimals=4).tolist()\n    \n    return final_hidden_state\n\n# Example usage\ninput_sequence = [[0.1, 0.2], [0.3, 0.4]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, -0.1]\n\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_hidden_state)\n```\n\nThis script defines a function `rnn_forward` that implements a simple RNN cell, processes a sequence of input vectors, and returns the final hidden state after processing the entire sequence, rounded to four decimal places."}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by tx and ty in the x and y directions respectively.\n    \n    :param points: List of [x, y] coordinates.\n    :param tx: Translation distance in the x direction.\n    :param ty: Translation distance in the y direction.\n    :return: Translated list of points.\n    \"\"\"\n    # Convert the list of points to a numpy array for easier manipulation\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Add a column of ones to the points array to make the matrix multiplication possible\n    ones = np.ones((points_array.shape[0], 1))\n    points_with_ones = np.hstack((points_array, ones))\n    \n    # Perform the translation by multiplying the points with the translation matrix\n    translated_points = np.dot(translation_matrix, points_with_ones.T).T\n    \n    # Remove the last column (ones) and return the result as a list\n    return translated_points[:, :2].tolist()\n\n# Example usage\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\nThis script defines the `translate_object` function that takes a list of 2D points and translation distances in the x and y directions, and returns a new list of points after applying the translation matrix. The example usage demonstrates translating a set of points by 2 units in the x direction and 3 units in the y direction."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    # Ensure the standard deviations are positive\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    # KL divergence formula for two normal distributions\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n    return kl_div\n\n# Example usage\nmu_p, sigma_p = 0, 1  # Mean and standard deviation of distribution P\nmu_q, sigma_q = 2, 5  # Mean and standard deviation of distribution Q\nprint(f\"KL Divergence: {kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solve a linear system Ax = b using the Gauss-Seidel iterative method.\n    \n    Parameters:\n    A (numpy.ndarray): A square matrix of coefficients.\n    b (numpy.ndarray): The right-hand side vector.\n    n (int): Number of iterations.\n    x_ini (numpy.ndarray, optional): Initial guess for the solution vector. Defaults to a vector of zeros.\n    \n    Returns:\n    list: The approximated solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure A is a numpy array\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    \n    # Get the size of A\n    size = A.shape[0]\n    \n    # Initialize x if not provided\n    if x_ini is None:\n        x = np.zeros(size)\n    else:\n        x = np.array(x_ini, dtype=float)\n    \n    # Perform Gauss-Seidel iterations\n    for _ in range(n):\n        for i in range(size):\n            # Calculate the new value for x[i]\n            x[i] = b[i]\n            for j in range(size):\n                if j != i:\n                    x[i] -= A[i, j] * x[j]\n            x[i] /= A[i, i]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[4, -1, 0, 0],\n              [-1, 4, -1, 0],\n              [0, -1, 4, -1],\n              [0, 0, -1, 3]])\nb = np.array([12, 12, 12, 12])\nn = 100  # Number of iterations\nx = gauss_seidel(A, b, n)\nprint(x)\n```\n\nThis script defines a function `gauss_seidel` that implements the Gauss-Seidel iterative method for solving a system of linear equations. The function takes a square matrix `A`, a right-hand side vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. The solution vector `x` is iteratively updated and returned as a list after rounding each element to four decimal places."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solve the linear system Ax = b using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix of the system of equations.\n    b (np.array): Constant terms of the system of equations.\n    \n    Returns:\n    list: Solution vector x as a list, rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    # Augment A with b\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    \n    # Gaussian Elimination with partial pivoting\n    for i in range(n):\n        # Find the pivot row and swap\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    \n    # Return the solution vector as a list, rounded to 4 decimal places\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nprint(gaussian_elimination(A, b))\n```\n\nThis script defines a function `gaussian_elimination` that solves a system of linear equations using Gaussian Elimination with partial pivoting. The function takes as input a coefficient matrix `A` and a constant terms vector `b`, and returns the solution vector `x` as a list, with each element rounded to the nearest 4th decimal place. The example provided demonstrates how to use the function with a sample system of equations."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden_state = initial_hidden_state\n\n        for t in range(T):\n            # Concatenate input and hidden state\n            concat = np.vstack((hidden_state, x[t]))\n\n            # Compute gates\n            forget_gate = self.sigmoid(self.Wf @ concat + self.bf)\n            input_gate = self.sigmoid(self.Wi @ concat + self.bi)\n            candidate_cell_state = self.tanh(self.Wc @ concat + self.bc)\n            output_gate = self.sigmoid(self.Wo @ concat + self.bo)\n\n            # Update cell state and hidden state\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell_state\n            hidden_state = output_gate * self.tanh(cell_state)\n\n            # Store hidden state\n            hidden_states.append(hidden_state)\n\n        # Convert to list and round to 4th decimal\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = hidden_state.round(4).tolist()\n        final_cell_state = cell_state.round(4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage\ninput_size = 3\nhidden_size = 2\nlstm = LSTM(input_size, hidden_size)\n\n# Input data\nx = [np.array([[1], [2], [3]]), np.array([[4], [5], [6]])]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))\n\n# Forward pass\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\nprint(\"Hidden States:\", hidden_states)\nprint(\"Final Hidden State:\", final_hidden_state)\nprint(\"Final Cell State:\", final_cell_state)\n```"}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty.\")\n    \n    num_docs = len(corpus)\n    tf_idf_scores = []\n    \n    for doc in corpus:\n        doc_word_count = len(doc)\n        doc_tf_idf = []\n        for term in query:\n            tf = doc.count(term) / doc_word_count if doc_word_count > 0 else 0\n            df = sum(1 for d in corpus if term in d) + 1\n            idf = math.log((num_docs + 1) / df) + 1\n            tf_idf = tf * idf\n            doc_tf_idf.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_tf_idf)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"a\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n    [\"the\", \"lazy\", \"dog\", \"quickly\", \"jumps\", \"over\", \"the\", \"fox\"]\n]\nquery = [\"quick\", \"dog\", \"fox\"]\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script implements the `compute_tf_idf` function as requested, including handling for edge cases such as empty documents and terms not present in the corpus. It also ensures that the output is rounded to four decimal places and returned as a Python list."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision == 0 or recall == 0:\n        f_beta = 0\n    else:\n        f_beta = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta, 3)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0\nprint(f_score(y_true, y_pred, beta))  # Output should be the F-Score\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hidden_states = np.zeros((self.hidden_size, T+1))\n        outputs = np.zeros((self.hidden_size, T))\n        last_inputs = np.zeros((self.hidden_size, T))\n        \n        for t in range(T):\n            hidden_states[:, t+1] = np.tanh(self.W_xh @ input_sequence[t].reshape(-1, 1) + self.W_hh @ hidden_states[:, t].reshape(-1, 1) + self.b_h).reshape(-1)\n            outputs[:, t] = self.W_hy @ hidden_states[:, t+1].reshape(-1, 1) + self.b_y\n            last_inputs[:, t] = input_sequence[t]\n        \n        return outputs, last_inputs, hidden_states\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[:, 0])\n        \n        for t in reversed(range(T)):\n            do = outputs[:, t].reshape(-1, 1) - expected_output[t].reshape(-1, 1)\n            dh = self.W_hy.T @ do + dh_next\n            dhraw = (1 - hidden_states[:, t+1].reshape(-1, 1) ** 2) * dh\n            dW_hy += do @ hidden_states[:, t+1].reshape(1, -1)\n            db_y += do\n            dW_xh += dhraw @ last_inputs[:, t].reshape(1, -1)\n            dW_hh += dhraw @ hidden_states[:, t].reshape(1, -1)\n            db_h += dhraw\n            dh_next = self.W_hh.T @ dhraw\n        \n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n# Example usage\nif __name__ == \"__main__\":\n    rnn = SimpleRNN(input_size=2, hidden_size=3, output_size=2)\n    input_sequence = [np.array([1, 2]), np.array([2, 3]), np.array([3, 4])]\n    expected_output = [np.array([0.5, 0.6]), np.array([0.6, 0.7]), np.array([0.7, 0.8])]\n    learning_rate = 0.01\n    \n    outputs, last_inputs, hidden_states = rnn.forward(input_sequence)\n    rnn.backward(input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate)\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    \n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + (rsnew / rsold) * p\n        rsold = rsnew\n    \n    return x.round(8).tolist()\n\n# Example usage\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\nsolution = conjugate_gradient(A, b, n)\nprint(solution)\n```\n\nThis Python script defines the `conjugate_gradient` function, which implements the Conjugate Gradient method for solving a linear system `Ax = b`. The function accepts a symmetric positive-definite matrix `A`, a right-hand side vector `b`, a maximum number of iterations `n`, an optional initial guess vector `x0`, and a tolerance level `tol` for the stopping criteria. The returned solution vector `x` is rounded to 8 decimal places and converted to a list. An example usage of the function is also provided."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each class label\n    classes, counts = np.unique(y, return_counts=True)\n    # Calculate the probabilities of each class\n    probabilities = counts / len(y)\n    # Calculate Gini Impurity\n    gini = 1 - np.sum(probabilities**2)\n    # Return Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage and check function\ndef check_gini_impurity():\n    example_y = [1, 1, 2, 2, 3, 3, 1, 2, 2, 1]\n    expected_gini = 0.667  # Expected Gini Impurity for the example list\n    calculated_gini = gini_impurity(example_y)\n    assert calculated_gini == expected_gini, f\"Expected {expected_gini}, but got {calculated_gini}\"\n    print(f\"Test passed! Gini Impurity is {calculated_gini}\")\n\n# Run the check function to verify the correctness of the gini_impurity function\ncheck_gini_impurity()\n```\n\nThis code snippet defines a function `gini_impurity` that calculates the Gini Impurity for a given list of class labels. It uses NumPy for efficient computation of unique class labels and their counts, then calculates the Gini Impurity based on these counts. The result is rounded to three decimal places as requested. A check function is also provided to verify the correctness of the implementation with a given example."}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # The first row's start is always at index 0\n\n    for row in dense_matrix:\n        non_zero_count = 0  # Count of non-zero elements in the current row\n        for col_idx, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        # The row pointer for the next row is the current one plus the number of non-zero elements in this row\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n\n    return values, column_indices, row_pointers\n\n# Example usage\nif __name__ == \"__main__\":\n    dense_matrix = [\n        [1, 0, 0, 2],\n        [0, 3, 0, 0],\n        [4, 0, 5, 0],\n        [0, 0, 0, 0],\n        [6, 0, 7, 8]\n    ]\n    csr_representation = compressed_row_sparse_matrix(dense_matrix)\n    print(\"Values:\", csr_representation[0])\n    print(\"Column Indices:\", csr_representation[1])\n    print(\"Row Pointers:\", csr_representation[2])\n```\nThis script defines a function `compressed_row_sparse_matrix` that takes a dense matrix as input and returns its CSR representation. The example usage demonstrates converting a given dense matrix into CSR format and prints the resulting values, column indices, and row pointers."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    # Calculate the projection using the formula: proj_L(v) = ((v . L) / (L . L)) * L\n    projection = ((np.dot(v, L) / np.dot(L, L)) * L).round(3)\n    return projection.tolist()\n\n# Example usage:\n# v = [3, 4, 5]\n# L = [1, 2, 3]\n# print(orthogonal_projection(v, L))  # Output should be the projection of v onto L\n```\n\nThis solution uses the numpy library for vector operations to calculate the orthogonal projection of vector `v` onto vector `L`. The projection formula is applied, and the result is rounded to three decimal places as requested."}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n\n    values = []\n    row_indices = []\n    col_ptr = [0]\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n\n    return values, row_indices, col_ptr\n\n# Example usage:\ndense_matrix = [\n    [0, 3, 0, 0],\n    [2, 0, 0, 1],\n    [0, 0, 0, 0],\n    [0, 0, 4, 0]\n]\n\nvalues, row_indices, col_ptr = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", col_ptr)\n```\n\nThis script defines a function `compressed_col_sparse_matrix` that takes a dense matrix as input and returns its Compressed Column Sparse (CSC) representation as described in the task. The example usage at the end demonstrates how to use this function with a sample matrix."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Calculate the column space of a given matrix A.\n    The function returns the basis vectors that span the column space of A.\n    These vectors are extracted from the original matrix and correspond to the independent columns.\n    \n    Parameters:\n    A (np.ndarray): The input matrix.\n    \n    Returns:\n    list: The basis vectors that span the column space of A, rounded to 8 decimal places.\n    \"\"\"\n    # Convert the input matrix to a numpy array if it's not already one\n    A = np.array(A)\n    # Get the row echelon form of the matrix\n    row_echelon = np.linalg.matrix_rank(A)\n    # Identify the pivot columns (independent columns)\n    _, inds = np.unique(A[:, :row_echelon], return_index=True, axis=1)\n    pivot_columns = sorted(inds)\n    # Extract the columns from the original matrix corresponding to the pivot columns\n    basis_vectors = A[:, pivot_columns]\n    # Round the basis vectors to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n\n# Example usage\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(matrix_image(A))\n```\n\nNote: The above implementation assumes that the `np.unique` with `return_index` can be used to identify the unique columns, which is a simplification. To accurately identify the pivot columns (independent columns) in the row echelon form, a more sophisticated approach would be required, such as performing Gaussian elimination manually. The provided solution is a simplified version and may not work correctly for all matrices."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared (coefficient of determination) for a regression model.\n    \n    Parameters:\n    y_true (np.array): An array of true values.\n    y_pred (np.array): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    \n    # Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    return round(r2, 3)\n\n# Example usage\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(r_squared(y_true, y_pred))\n```\n\nThis script defines a function `r_squared` that calculates the R-squared value for a given set of true and predicted values, adhering to the output constraints provided."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    # Check if the image is empty\n    if not img or not img[0]:\n        return -1\n    \n    total_brightness = 0\n    total_pixels = 0\n    \n    # Check for inconsistent row lengths and pixel values\n    row_length = len(img[0])\n    for row in img:\n        if len(row) != row_length:\n            return -1\n        for pixel in row:\n            if pixel < 0 or pixel > 255:\n                return -1\n            total_brightness += pixel\n            total_pixels += 1\n    \n    # Calculate average brightness\n    if total_pixels == 0:\n        return -1\n    average_brightness = total_brightness / total_pixels\n    \n    return round(average_brightness, 2)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a test image\n    test_img = [\n        [128, 128, 128],\n        [128, 128, 128],\n        [128, 128, 128]\n    ]\n    print(calculate_brightness(test_img))  # Expected output: 128.0\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): The actual values.\n    y_pred (np.ndarray): The predicted values.\n    \n    Returns:\n    float: The RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If the input arrays have mismatched shapes or are empty.\n    TypeError: If the input is not a numpy array or list.\n    \"\"\"\n    # Check for valid input types\n    if not (isinstance(y_true, (np.ndarray, list)) and isinstance(y_pred, (np.ndarray, list))):\n        raise TypeError(\"Inputs must be numpy arrays or lists.\")\n    \n    # Convert to numpy arrays for consistency\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Check for empty arrays\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check for matching shapes\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate RMSE\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    \n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(rmse(y_true, y_pred))  # Output: 0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for two binary arrays.\n    \n    Parameters:\n    - y_true: A binary array representing true labels.\n    - y_pred: A binary array representing predicted labels.\n    \n    Returns:\n    - The Jaccard Index as a float, rounded to three decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays if they are not already\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where both arrays are all zeros\n    if union == 0:\n        return 1.0  # Perfect overlap as both are empty\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    return round(jaccard, 3)\n\n# Example check function to verify the correctness\ndef check_jaccard_index():\n    print(f\"Jaccard Index (no overlap): {jaccard_index([0, 0, 0, 0], [1, 1, 1, 1])}\")\n    print(f\"Jaccard Index (perfect overlap): {jaccard_index([1, 1, 1, 1], [1, 1, 1, 1])}\")\n    print(f\"Jaccard Index (partial overlap): {jaccard_index([1, 0, 1, 0], [1, 1, 0, 0])}\")\n\n# Run the check function\ncheck_jaccard_index()\n```\n\nThis code snippet defines a function `jaccard_index` that calculates the Jaccard Index for two binary arrays, `y_true` and `y_pred`. It includes a check function to verify the correctness of the implementation with different scenarios."}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    - y_true: A binary numpy array of true labels.\n    - y_pred: A binary numpy array of predicted labels.\n    \n    Returns:\n    - A float representing the Dice Score rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection of true and predicted labels\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the number of elements in true and predicted labels\n    num_true = np.sum(y_true)\n    num_pred = np.sum(y_pred)\n    \n    # Handle edge case where there are no true or predicted positives\n    if num_true == 0 and num_pred == 0:\n        return 1.0  # Perfect overlap when both sets are empty\n    \n    # Calculate Dice Score\n    dice = (2.0 * intersection) / (num_true + num_pred)\n    \n    return round(dice, 3)\n\n# Check function with provided data points\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(dice_score(y_true, y_pred))  # Expected output: 0.8\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_hypervector(seed, dim):\n    \"\"\"Create a hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.where(np.random.rand(dim) > 0.5, 1, -1)\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"Generate a composite hypervector for a given dataset row using HDC.\n    \n    Parameters:\n    row (dict): A dictionary representing a dataset row.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds.\n    \n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n    bundled_hv = np.zeros(dim)\n    for feature, value in row.items():\n        feature_hv = create_hypervector(random_seeds[feature], dim)\n        value_seed = hash((feature, value))  # Create a unique seed for the value based on feature and value\n        value_hv = create_hypervector(value_seed, dim)\n        # Bind the feature and value hypervectors by element-wise multiplication\n        bound_hv = np.multiply(feature_hv, value_hv)\n        # Bundle the bound hypervector into the composite hypervector\n        bundled_hv = np.add(bundled_hv, bound_hv)\n    # Normalize the composite hypervector\n    bundled_hv = np.where(bundled_hv > 0, 1, -1)\n    return bundled_hv.tolist()\n\n# Example usage\nrow = {'feature1': 1, 'feature2': 2, 'feature3': 3}\ndim = 10\nrandom_seeds = {'feature1': 10, 'feature2': 20, 'feature3': 30}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\n\nThis script defines a function `create_row_hv` that takes a row of data, a dimension for the hypervectors, and a dictionary of seeds for reproducibility. It generates a composite hypervector for the row by binding the hypervectors of each feature and its value, and then bundling them together. The hypervectors are normalized to either -1 or 1 before being returned as a list."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List\n\ndef confusion_matrix(data: List[List[int]]) -> List[List[int]]:\n    \"\"\"\n    Generates a confusion matrix for binary classification.\n    \n    :param data: A list of lists, where each inner list represents a pair [y_true, y_pred]\n    :return: A 2x2 confusion matrix as a list of lists.\n    \"\"\"\n    # Count occurrences of each pair\n    counts = Counter(tuple(pair) for pair in data)\n    \n    # Initialize the confusion matrix with zeros\n    matrix = [[0, 0], [0, 0]]\n    \n    # Populate the confusion matrix\n    for (y_true, y_pred), count in counts.items():\n        matrix[y_true][y_pred] = count\n    \n    return matrix\n\n# Check function to verify the correctness of the confusion_matrix function\ndef check_confusion_matrix():\n    test_data = [\n        [0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1], [1, 1], [0, 1]\n    ]\n    expected_output = [[2, 2], [2, 2]]\n    assert confusion_matrix(test_data) == expected_output, \"Test case failed!\"\n    print(\"Test case passed!\")\n\n# Run the check function\ncheck_confusion_matrix()\n```\n\nThis code snippet defines a function `confusion_matrix(data)` that takes a list of binary classification results and returns a 2x2 confusion matrix. It uses the `Counter` from the `collections` module to count the occurrences of each pair `[y_true, y_pred]`. The confusion matrix is then populated based on these counts. A check function is provided to verify the correctness of the implementation with a given test case."}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (np.array): First input vector.\n    v2 (np.array): Second input vector.\n    \n    Returns:\n    float: Cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError(\"Both vectors must have the same shape.\")\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude.\")\n    \n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    \n    return round(similarity, 3)\n\n# Check function with provided data points\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))  # Expected output: 0.975\n```"}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple:\n    # Validate inputs\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n    \n    # Calculate confusion matrix\n    confusion_matrix = [[0, 0], [0, 0]]\n    for a, p in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n    \n    # Extract values from confusion matrix\n    tp = confusion_matrix[1][1]  # True Positives\n    tn = confusion_matrix[0][0]  # True Negatives\n    fp = confusion_matrix[0][1]  # False Positives\n    fn = confusion_matrix[1][0]  # False Negatives\n    \n    # Calculate metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if (tn + fp) > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if (tn + fn) > 0 else 0\n    \n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\n\n# Example usage\nactual = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\nprint(performance_metrics(actual, predicted))\n```\n\nThis script defines a function `performance_metrics` that calculates the confusion matrix, accuracy, F1 score, specificity, and negative predictive value for a binary classification model based on the actual and predicted class labels. The function validates the inputs to ensure they meet the specified constraints and then computes the desired metrics, rounding them to three decimal places as required."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): The dataset for which to calculate statistics.\n    \n    Returns:\n    dict: A dictionary containing various descriptive statistics.\n    \"\"\"\n    # Convert the data to a NumPy array if it's not already\n    data = np.array(data)\n    \n    # Calculate statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]  # Get the mode value\n    variance = np.var(data, ddof=1)  # ddof=1 for sample variance\n    standard_deviation = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    percentiles_25 = np.percentile(data, 25)\n    percentiles_50 = np.percentile(data, 50)\n    percentiles_75 = np.percentile(data, 75)\n    iqr = stats.iqr(data)\n    \n    # Prepare the output dictionary\n    output = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentiles_25, 4),\n        '50th_percentile': round(percentiles_50, 4),\n        '75th_percentile': round(percentiles_75, 4),\n        'interquartile_range': round(iqr, 4)\n    }\n    \n    return output\n\n# Example usage\ndata = np.random.randn(100)  # Generate a random dataset\nprint(descriptive_statistics(data))\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient (n choose k)\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability using the binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# Calculating the probability of getting exactly 3 heads in 5 coin flips with a fair coin (p=0.5)\nprint(binomial_probability(5, 3, 0.5))\n```\n\nThis code snippet defines a function `binomial_probability` that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials with a success probability of `p` for each trial. The function uses the `math.comb` function to calculate the binomial coefficient and applies the binomial distribution formula. The result is rounded to five decimal places as per the requirement."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\n\n# Example usage:\n# mean = 0\n# std_dev = 1\n# x = 1\n# print(normal_pdf(x, mean, std_dev))  # Output will be the PDF value at x=1 for a standard normal distribution\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    # Calculate the Poisson probability\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example check function to verify the correctness of the generated function\ndef check_poisson_probability():\n    print(f\"Probability of 2 events with mean rate 3: {poisson_probability(2, 3)} (Expected: 0.22404)\")\n    print(f\"Probability of 0 events with mean rate 1: {poisson_probability(0, 1)} (Expected: 0.36788)\")\n    print(f\"Probability of 5 events with mean rate 2: {poisson_probability(5, 2)} (Expected: 0.03609)\")\n\n# Call the check function to display example probabilities\ncheck_poisson_probability()\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError(\"Input image must be a 2D numpy array.\")\n    \n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    contrast = max_pixel_value - min_pixel_value\n    \n    return contrast\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Creating a sample 2D grayscale image\n    img = np.array([[10, 20, 30],\n                    [40, 50, 60],\n                    [70, 80, 255]])\n    \n    # Calculating the contrast\n    contrast = calculate_contrast(img)\n    print(f\"The contrast of the image is: {contrast}\")\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of vec1 and vec2.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\n\nresult = calculate_dot_product(vector1, vector2)\nprint(f\"The dot product is: {result}\")\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: The transformed features as a list of lists.\n    \"\"\"\n    if degree < 0:\n        return []\n\n    # Convert the input data into a numpy array for easier manipulation\n    data_array = np.array(data)\n    # Initialize the transformed features with ones to include the constant term\n    transformed_features = np.ones((data_array.size, degree + 1))\n\n    # Generate the polynomial features\n    for d in range(1, degree + 1):\n        transformed_features[:, d] = data_array ** d\n\n    # Round the features to 8 decimal places as required\n    transformed_features = np.round(transformed_features, 8)\n\n    # Convert back to a list of lists and return\n    return transformed_features.tolist()\n\n# Example check function to verify the correctness\ndef check_phi_transform():\n    example_data = [1.0, 2.0, 3.0]\n    degree = 3\n    expected_output = [\n        [1.0, 1.0, 1.0, 1.0],\n        [1.0, 2.0, 4.0, 8.0],\n        [1.0, 3.0, 9.0, 27.0]\n    ]\n    output = phi_transform(example_data, degree)\n    assert output == expected_output, f\"Expected {expected_output}, but got {output}\"\n    print(\"phi_transform is working correctly with the provided example.\")\n\n# Uncomment the following line to run the check function\n# check_phi_transform()\n```\n\nIn this code, the `phi_transform` function generates polynomial features up to the specified degree for a given list of numerical data. The `check_phi_transform` function provides an example to verify the correctness of the `phi_transform` function. To run the check, uncomment the last line of the script."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encoding for a given position and model dimensionality.\n    \n    Args:\n    position (int): The length of the sequence.\n    d_model (int): The dimensionality of the model.\n    \n    Returns:\n    list: The positional encoding as a list of np.float16 values or -1 for invalid inputs.\n    \"\"\"\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position = np.arange(0, position, dtype=np.float16).reshape(-1, 1)\n    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float16) * -(np.log(10000.0) / d_model))\n    \n    pe[:, 0::2] = np.sin(position * div_term)\n    pe[:, 1::2] = np.cos(position * div_term)\n    \n    return pe.tolist()\n\n# Example check function to verify the correctness\ndef check_pos_encoding():\n    result = pos_encoding(3, 4)\n    expected = [\n        [0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0]\n    ] # This is a placeholder. The actual expected result will depend on the implementation details.\n    assert isinstance(result, list), \"Output must be a list\"\n    assert all(isinstance(x, list) and all(isinstance(y, np.float16) for y in x) for x in result), \"List elements must be np.float16\"\n    print(\"Check passed\")\n\ncheck_pos_encoding()\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1  # Overfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1  # Underfitting\n    else:\n        return 0  # Good fit\n\n# Test cases to verify the correctness of the function\nassert model_fit_quality(0.9, 0.6) == 1, \"Test Case 1 Failed\"\nassert model_fit_quality(0.6, 0.6) == -1, \"Test Case 2 Failed\"\nassert model_fit_quality(0.8, 0.75) == 0, \"Test Case 3 Failed\"\nassert model_fit_quality(0.65, 0.6) == -1, \"Test Case 4 Failed\"\nassert model_fit_quality(0.9, 0.85) == 0, \"Test Case 5 Failed\"\n\nprint(\"All test cases passed!\")\n```\n\nThis code snippet defines a function `model_fit_quality` that takes two parameters: `training_accuracy` and `test_accuracy`. The function returns an integer indicating whether the model is overfitting, underfitting, or performing well based on the provided accuracy values. The function is followed by several test cases to ensure its correctness."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    # Correct bias in first moment\n    m_hat = m / (1 - beta1 ** t)\n    # Correct bias in second moment\n    v_hat = v / (1 - beta2 ** t)\n    # Update parameters\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Rounding and converting to list\n    updated_parameter = np.round(parameter, 5).tolist()\n    updated_m = np.round(m, 5).tolist()\n    updated_v = np.round(v, 5).tolist()\n    \n    return updated_parameter, updated_m, updated_v\n\n# Example usage\nparameter = np.array([1.0, 2.0, 3.0])\ngrad = np.array([0.1, 0.2, 0.3])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(updated_parameter)\nprint(updated_m)\nprint(updated_v)\n```"}
{"task_id": 88, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n\n    # Encode the prompt into token ids\n    token_ids = encoder.encode(prompt)\n    # Ensure we have enough context for positional embeddings\n    if len(token_ids) > hparams[\"n_ctx\"]:\n        raise ValueError(f\"Prompt is too long. It must be shorter than {hparams['n_ctx']} tokens.\")\n\n    # Initialize the generated tokens with the prompt tokens\n    generated_tokens = token_ids\n\n    # Generate tokens\n    for _ in range(n_tokens_to_generate):\n        # Get the embeddings for the current sequence\n        token_embeddings = params[\"wte\"][generated_tokens[-hparams[\"n_ctx\"]:], :]\n        positional_embeddings = params[\"wpe\"][:len(generated_tokens[-hparams[\"n_ctx\"]:])]\n\n        # Sum the embeddings\n        embeddings = token_embeddings + positional_embeddings\n\n        # Here we would normally pass through the transformer blocks, but for simplicity\n        # we'll just generate a random token id for demonstration purposes.\n        next_token_id = np.random.randint(0, len(encoder.encoder_dict))\n\n        # Append the next token id to the generated tokens\n        generated_tokens.append(next_token_id)\n\n    # Decode the generated token ids back into text\n    generated_text = encoder.decode(generated_tokens)\n    return generated_text\n\n# Example usage:\nprompt = \"hello world\"\ngenerated_text = gen_text(prompt, n_tokens_to_generate=5)\nprint(generated_text)\n```\n\n**Note**: The provided code is a simplified and illustrative version of a GPT-2-like model. It omits the complex transformer blocks and uses random token generation for demonstration purposes. In a real-world scenario, the model would pass through multiple transformer blocks to generate each token based on the learned weights and biases."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Subtracting the max value for numerical stability\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    \n    # Convert the crystal values to a numpy array for easier manipulation\n    crystals = np.array(crystal_values).reshape(-1, 1)\n    \n    # Initialize a weight matrix with random values for demonstration purposes\n    # In a real scenario, these weights would be learned through training\n    weights = np.random.rand(dimension, dimension)\n    \n    # Project each crystal value into the higher-dimensional space\n    projected_crystals = np.dot(crystals, weights)\n    \n    # Initialize the attention scores matrix\n    attention_scores = np.zeros((n, n))\n    \n    # Compute the attention scores between each pair of crystals\n    for i in range(n):\n        for j in range(n):\n            # Calculate the dot product between the projected crystals\n            attention_scores[i, j] = np.dot(projected_crystals[i], projected_crystals[j])\n    \n    # Apply softmax to the attention scores for each crystal to get the weights\n    attention_weights = np.apply_along_axis(softmax, axis=1, arr=attention_scores)\n    \n    # Calculate the weighted sum of the crystal values using the attention weights\n    weighted_patterns = np.dot(attention_weights, crystals)\n    \n    # Flatten the result and round to 4 decimal places\n    result = [round(num, 4) for num in weighted_patterns.flatten()]\n    \n    return result\n\n# Example usage\nn = 5\ncrystal_values = [1, 2, 3, 4, 5]\ndimension = 3\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script defines a function `pattern_weaver` that takes the number of crystals `n`, a list of their values `crystal_values`, and the dimension into which they are projected `dimension`. It then computes the attention weights between each crystal and uses these to derive a weighted pattern for each crystal, which is returned as a list of floats rounded to the 4th decimal place. The softmax function is implemented for computing the attention weights, and a random weight matrix is used to project the crystal values into a higher-dimensional space for demonstration purposes."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for documents in the corpus given a query.\n    \n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :param k1: Term frequency saturation parameter.\n    :param b: Document length normalization parameter.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    # Calculate document length and average document length\n    doc_lengths = np.array([len(doc) for doc in corpus])\n    avg_doc_length = np.mean(doc_lengths)\n    \n    # Calculate term frequency for each document\n    term_freqs = [Counter(doc) for doc in corpus]\n    \n    # Calculate inverse document frequency for each term in the query\n    idf = {}\n    for term in query:\n        df = sum(1 for doc in corpus if term in doc)\n        idf[term] = np.log((len(corpus) - df + 0.5) / (df + 0.5))\n    \n    # Calculate BM25 score for each document\n    scores = []\n    for i, doc in enumerate(corpus):\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = term_freqs[i][term]\n                numerator = idf[term] * (k1 + 1) * tf\n                denominator = tf + k1 * (1 - b + b * doc_lengths[i] / avg_doc_length)\n                score += numerator / denominator\n        scores.append(score)\n    \n    # Return scores rounded to three decimal places\n    return [round(score, 3) for score in scores]\n\n# Example usage\ncorpus = [\n    [\"this\", \"is\", \"the\", \"first\", \"document\"],\n    [\"this\", \"document\", \"is\", \"the\", \"second\", \"document\"],\n    [\"and\", \"this\", \"is\", \"the\", \"third\", \"one\"],\n    [\"is\", \"this\", \"the\", \"fourth\", \"one\"]\n]\nquery = [\"document\", \"first\"]\n\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines the `calculate_bm25_scores` function that calculates the BM25 scores for a given corpus and query, applying the specified parameters for term frequency saturation (`k1`) and document length normalization (`b`). The example usage demonstrates how to call the function with a sample corpus and query, and prints the resulting scores."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize true positive, false positive, and false negative counters\n    tp = 0\n    fp = 0\n    fn = 0\n    \n    # Calculate tp, fp, fn\n    for yt, yp in zip(y_true, y_pred):\n        if yt == 1 and yp == 1:\n            tp += 1\n        elif yt == 0 and yp == 1:\n            fp += 1\n        elif yt == 1 and yp == 0:\n            fn += 1\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1 score\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Return F1 score rounded to 3 decimal places\n    return round(f1, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]\nprint(calculate_f1_score(y_true, y_pred))\n```\n\nThis code snippet defines a function `calculate_f1_score` that calculates the F1 score for binary classification based on the provided true and predicted labels. It then demonstrates the function's usage with a simple example."}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point.\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    \n    # Step 2: Perform linear regression on the detrended data.\n    X = np.array(range(1, len(consumption_data) + 1)).reshape((-1, 1))\n    y = np.array(detrended_data)\n    model = LinearRegression()\n    model.fit(X, y)\n    \n    # Step 3: Predict day 15's base consumption.\n    base_consumption_day_15 = model.predict(np.array([[15]]))[0]\n    \n    # Step 4: Add the day 15 fluctuation back.\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    \n    # Step 5: Round, then add a 5% safety margin (rounded up).\n    final_prediction = math.ceil(predicted_consumption_day_15 * 1.05)\n    \n    # Step 6: Return the final integer.\n    return final_prediction\n\n# Example usage:\nconsumption_data = [100, 105, 110, 115, 120, 125, 130, 135, 140, 145]  # Example data\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script uses the `numpy` library for numerical operations and the `LinearRegression` model from `sklearn.linear_model` to perform linear regression. The `power_grid_forecast` function takes a list of daily consumption data, detrends the data by removing the daily fluctuation, fits a linear regression model, predicts the consumption for day 15, adds the fluctuation for that day, and applies a 5% safety margin before returning the final integer value."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError(\"The length of y_true and y_pred must be the same.\")\n    \n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    \n    return round(mean_absolute_error, 3)\n\n# Example usage\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(mae(y_true, y_pred))  # Output: 0.5\n```"}
{"task_id": 94, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the query, key, and value matrices for the self-attention mechanism.\n    :param X: Input matrix.\n    :param W_q: Weight matrix for query.\n    :param W_k: Weight matrix for key.\n    :param W_v: Weight matrix for value.\n    :return: Tuple of (query, key, value) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention mechanism.\n    :param Q: Query matrix.\n    :param K: Key matrix.\n    :param V: Value matrix.\n    :return: Output of the self-attention mechanism.\n    \"\"\"\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention = np.dot(np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True), V)\n    return attention\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism.\n    :param X: Input matrix.\n    :param W_q: Weight matrix for query.\n    :param W_k: Weight matrix for key.\n    :param W_v: Weight matrix for value.\n    :param n_heads: Number of attention heads.\n    :return: Output of the multi-head attention mechanism as a list.\n    \"\"\"\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    head_dim = Q.shape[-1] // n_heads\n    Q_heads = np.array([Q[:, i*head_dim:(i+1)*head_dim] for i in range(n_heads)])\n    K_heads = np.array([K[:, i*head_dim:(i+1)*head_dim] for i in range(n_heads)])\n    V_heads = np.array([V[:, i*head_dim:(i+1)*head_dim] for i in range(n_heads)])\n    \n    attention_heads = [self_attention(Q_heads[i], K_heads[i], V_heads[i]) for i in range(n_heads)]\n    concatenated_heads = np.concatenate(attention_heads, axis=-1)\n    result = np.round(concatenated_heads, 4)\n    return result.tolist()\n\n# Example usage:\nX = np.random.rand(2, 10)  # Example input matrix\nW_q = np.random.rand(10, 20)  # Example weight matrix for query\nW_k = np.random.rand(10, 20)  # Example weight matrix for key\nW_v = np.random.rand(10, 20)  # Example weight matrix for value\nn_heads = 2  # Number of attention heads\n\noutput = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(output)\n```\n\nThis Python script implements the multi-head attention mechanism as described in the task. It includes the computation of query, key, and value matrices, the self-attention mechanism, and the multi-head attention mechanism, with the final output rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    x_array = np.array(x)\n    y_array = np.array(y)\n    \n    # Create a 2x2 contingency table\n    contingency_table = np.array([\n        [np.sum((x_array == 0) & (y_array == 0)), np.sum((x_array == 0) & (y_array == 1))],\n        [np.sum((x_array == 1) & (y_array == 0)), np.sum((x_array == 1) & (y_array == 1))]\n    ])\n    \n    # Chi-square test\n    chi2, _, _, _ = chi2_contingency(contingency_table)\n    \n    # Phi coefficient calculation\n    phi = np.sqrt(chi2 / np.sum(contingency_table))\n    \n    return round(phi, 4)\n\n# Example usage\nx = [0, 0, 1, 1, 0, 1, 0, 1]\ny = [0, 1, 0, 1, 0, 1, 1, 0]\n\nprint(phi_corr(x, y))\n```\n\nThis script defines a function `phi_corr` that calculates the Phi coefficient for two binary variables represented as lists of integers. The function uses the `chi2_contingency` function from the `scipy.stats` module to perform a chi-square test on a 2x2 contingency table constructed from the input lists. The Phi coefficient is then derived from the chi-square statistic and the total number of observations. The result is rounded to 4 decimal places as required."}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Test cases to verify the correctness of the function\n    test_cases = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\n    expected_outputs = [0.0, 0.0, 0.3, 0.5, 0.7, 1.0, 1.0]\n\n    for i, test_case in enumerate(test_cases):\n        output = hard_sigmoid(test_case)\n        assert output == expected_outputs[i], f\"Test case {test_case} failed, expected {expected_outputs[i]}, got {output}\"\n        print(f\"Test case {test_case} passed, output: {output}\")\n```"}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n        \n# Function to test the elu function with some data points\ndef check_elu():\n    test_cases = [\n        (-1.0, 1.0, -0.6321),\n        (0.0, 1.0, 0.0000),\n        (1.0, 1.0, 1.0000),\n        (-2.0, 0.5, -0.3935),\n        (2.0, 0.5, 2.0000)\n    ]\n    for x, alpha, expected in test_cases:\n        result = round(elu(x, alpha), 4)\n        assert result == expected, f\"elu({x}, {alpha}) = {result}, expected {expected}\"\n    print(\"All test cases passed.\")\n\n# Run the check function\ncheck_elu()\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test the prelu function with a few examples\n    print(prelu(-2.0))  # Expected output: -0.5\n    print(prelu(-1.0))  # Expected output: -0.25\n    print(prelu(0.0))   # Expected output: 0.0\n    print(prelu(1.0))   # Expected output: 1.0\n    print(prelu(2.0))   # Expected output: 2.0\n```"}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    # Handling edge cases to prevent numerical overflow or underflow\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Test the function with some data points\nprint(softplus(1.0))  # Expected: 1.3133\nprint(softplus(100))  # Expected: 100.0\nprint(softplus(-100)) # Expected: 0.0\nprint(softplus(-20))  # Expected: 2.0612e-09\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-10.0, -1.5, 0.0, 2.3, 100.0]\n    for val in test_values:\n        print(f\"softsign({val}) = {softsign(val)}\")\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n\n    # Compute the clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n\n    # Compute the GRPO objective value\n    grpo_objective_value = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n\n    # Compute the KL divergence between the old and reference policies\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n\n    # Apply the KL divergence penalty\n    grpo_objective_value -= beta * kl_divergence\n\n    return round(grpo_objective_value, 6)\n\n# Example usage\nrhos = [1.2, 0.9, 1.1, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.7, 0.6, 0.5, 0.4]\npi_theta_ref = [0.6, 0.5, 0.4, 0.3]\nprint(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```"}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)\n\n# Test the swish function with some example values\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for val in test_values:\n        print(f\"Swish({val}) = {swish(val)}\")\n```\n\nThis code snippet defines the `swish` function, which calculates the Swish activation value for a given input `x`. The Swish function is defined as `x / (1 + exp(-x))`, and the result is rounded to the nearest 4th decimal place as per the task requirements. The `if __name__ == \"__main__\":` block is used to test the function with a set of example values, demonstrating its usage and verifying its correctness."}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Test the function with some values\ntest_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\nresults = [round(selu(x), 4) for x in test_values]\nprint(results)\n```\n\nThis code snippet defines the SELU activation function and tests it with a set of values, rounding the results to the nearest 4th decimal as required."}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Compute the linear combination of inputs and weights plus bias\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to get probabilities\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Convert probabilities to binary predictions\n    predictions = (probabilities > 0.5).astype(int)\n    \n    # Return predictions as a list\n    return predictions.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X_example = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    weights_example = np.array([0.5, -0.6])\n    bias_example = 0.1\n\n    # Predict\n    predictions = predict_logistic(X_example, weights_example, bias_example)\n    print(predictions)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    # Number of features\n    n_samples, n_features = X.shape\n    # Number of classes\n    n_classes = len(np.unique(y))\n    # Initialize weights\n    B = np.zeros((n_features, n_classes))\n    # One-hot encode y\n    y_onehot = np.eye(n_classes)[y]\n    # List to store losses\n    losses = []\n\n    for _ in range(iterations):\n        # Compute logits\n        logits = X @ B\n        # Apply softmax\n        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n        softmax = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        # Compute gradient\n        gradient = X.T @ (softmax - y_onehot) / n_samples\n        # Update weights\n        B -= learning_rate * gradient\n        # Compute cross entropy loss\n        log_likelihood = -np.log(softmax[range(n_samples), y])\n        loss = np.sum(log_likelihood) / n_samples\n        losses.append(round(float(loss), 4))\n\n    # Return weights and losses\n    return B.round(4).tolist(), losses\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)\n    X = np.random.randn(100, 2)\n    y = np.random.randint(0, 3, 100)\n    learning_rate = 0.1\n    iterations = 1000\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    print(\"Optimized Coefficients:\", B)\n    print(\"Losses:\", losses)\n```\n\nThis script defines the `train_softmaxreg` function that trains a Softmax regression model using gradient descent. It initializes the model parameters, iteratively updates them based on the computed gradients, and collects the loss values over the specified number of iterations. The function returns the optimized coefficients and the collected losses, both rounded to the nearest 4th decimal."}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    m, n = X.shape\n    # Initialize weights and bias\n    weights = np.zeros(n)\n    bias = 0\n    losses = []\n\n    for i in range(iterations):\n        # Linear combination of weights and X\n        z = np.dot(X, weights) + bias\n        # Prediction\n        y_pred = sigmoid(z)\n        # Compute loss\n        loss = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        losses.append(round(loss, 4))\n        \n        # Compute gradients\n        dw = (1/m) * np.dot(X.T, (y_pred - y))\n        db = (1/m) * np.sum(y_pred - y)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n\n    # Collect and return optimized parameters and loss history\n    optimized_params = np.append(weights, bias).tolist()\n    return (optimized_params, losses)\n\n# Example usage:\n# X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])\n# y = np.array([1, 0, 0, 0])\n# learning_rate = 0.1\n# iterations = 1000\n# params, loss_history = train_logreg(X, y, learning_rate, iterations)\n# print(\"Optimized Parameters:\", params)\n# print(\"Loss History:\", loss_history)\n```"}
{"task_id": 107, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    \n    # Apply mask to attention scores\n    attention_scores += mask * -1e9\n    \n    # Apply softmax to get attention weights\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    \n    # Compute the output using attention weights and value matrix\n    output = np.dot(attention_weights, V)\n    \n    return output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input and weight matrices\n    X = np.array([[1, 0], [0, 1]])\n    W_q = np.array([[1, 0], [0, 1]])\n    W_k = np.array([[1, 0], [0, 1]])\n    W_v = np.array([[1, 0], [0, 1]])\n    \n    # Compute Q, K, V matrices\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Define the mask (1 indicates positions to be masked)\n    mask = np.array([[0, 1], [0, 0]])\n    \n    # Compute masked self-attention\n    output = masked_attention(Q, K, V, mask)\n    \n    print(output)\n```"}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom math import log\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    This function uses the Shannon entropy to measure the disorder.\n    \"\"\"\n    # Count the frequency of each color in the basket of apples\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    \n    # Calculate the probability of each color\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate the Shannon entropy as a measure of disorder\n    entropy = -sum(p * log(p, 2) for p in probabilities if p > 0)\n    \n    # Return the entropy rounded to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    print(disorder([0,0,0,0]))  # Expected output: 0.0\n    print(disorder([1,1,0,0]))  # Expected output: 1.0\n    print(disorder([0,1,2,3]))  # Expected output: 2.0\n    print(disorder([0,0,1,1,2,2,3,3]))  # Expected output: 2.0\n    print(disorder([0,0,0,0,0,1,2,3]))  # Expected output: 1.4591\n```"}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Applies Layer Normalization to the input tensor X.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim).\n    gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    epsilon (float): Small constant for numerical stability.\n    \n    Returns:\n    np.ndarray: Normalized tensor of the same shape as X.\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input tensor\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_normalized = X_normalized * gamma + beta\n    \n    # Round the result to 5 decimal places and convert to list\n    return np.round(X_normalized, decimals=5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input tensor X of shape (batch_size, seq_length, feature_dim)\n    X = np.random.randn(2, 3, 4)  # Example input with 2 samples, 3 timesteps, and 4 features\n    # Define gamma and beta parameters of shape (feature_dim,)\n    gamma = np.random.randn(4)  # Scaling parameters\n    beta = np.random.randn(4)   # Shifting parameters\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef harmonic_mean(precision, recall, beta):\n    \"\"\"\n    Calculates the harmonic mean of precision and recall with a given beta.\n    \"\"\"\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\ndef fragmentation_cost(reference, candidate):\n    \"\"\"\n    Calculates the fragmentation cost based on the number of unigrams in the candidate\n    that are in the reference but not in the correct order.\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    frag_cost = 0\n    for i in range(len(cand_words)):\n        if i < len(ref_words) and cand_words[i] == ref_words[i]:\n            continue\n        else:\n            frag_cost += abs(i - ref_words.index(cand_words[i])) if cand_words[i] in ref_words else len(ref_words)\n    return frag_cost\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculates the METEOR score for the given reference and candidate translations.\n    \"\"\"\n    ref_count = Counter(reference.split())\n    cand_count = Counter(candidate.split())\n    \n    # Calculate the number of matches\n    matches = sum((ref_count & cand_count).values())\n    \n    # Calculate precision and recall\n    precision = matches / len(candidate.split())\n    recall = matches / len(reference.split())\n    \n    # Calculate the F-mean\n    f_mean = harmonic_mean(precision, recall, beta)\n    \n    # Calculate the fragmentation penalty\n    frag_penalty = np.exp(-gamma * fragmentation_cost(reference, candidate))\n    \n    # Calculate the final score\n    score = f_mean * frag_penalty\n    \n    # Apply length penalty\n    if len(candidate.split()) > len(reference.split()):\n        score *= alpha\n    \n    return round(score, 3)\n\n# Test the function\nreference = \"the cat is on the mat\"\ncandidate = \"the cat is on the mat\"\nprint(meteor_score(reference, candidate))  # Expected output: 1.0\n\nreference = \"the cat is on the mat\"\ncandidate = \"the cat is on mat\"\nprint(meteor_score(reference, candidate))  # Expected output: around 0.9\n```\n\nThis script defines a function `meteor_score` that computes the METEOR score for evaluating the quality of a machine translation candidate against a reference. The function takes into account the precision and recall of the translation, computes their F-mean, and applies a penalty for fragmentation of word order. The final score is rounded to 3 decimal places as per the task requirement."}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Calculate the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    joint_counts (int): The number of times both events x and y occur together.\n    total_counts_x (int): The total number of occurrences of event x.\n    total_counts_y (int): The total number of occurrences of event y.\n    total_samples (int): The total number of samples.\n\n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate expected joint probability\n    expected_joint_prob = (total_counts_x / total_samples) * (total_counts_y / total_samples)\n    \n    # Calculate actual joint probability\n    actual_joint_prob = joint_counts / total_samples\n    \n    # Calculate PMI\n    pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    \n    return round(pmi, 3)\n\n# Example usage\njoint_counts = 100\ntotal_counts_x = 200\ntotal_counts_y = 250\ntotal_samples = 10000\n\nprint(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\nThis script defines a function `compute_pmi` that calculates the Pointwise Mutual Information (PMI) based on the provided counts and total samples. The function takes into account the joint occurrence of two events and their individual occurrences to determine how much the joint occurrence differs from what would be expected by chance. The result is returned as a float value rounded to three decimal places."}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    \n    :param x: List of integers to be normalized.\n    :return: List of floats representing the normalized values, rounded to 4 decimal places.\n    \"\"\"\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [1.0] * len(x)  # Avoid division by zero if all elements are the same\n    \n    normalized = [(float(i) - min_val) / (max_val - min_val) for i in x]\n    return [round(n, 4) for n in normalized]\n\n# Example usage\nif __name__ == \"__main__\":\n    sample_data = [10, 20, 15, 30, 25]\n    normalized_data = min_max(sample_data)\n    print(normalized_data)\n```\n\nThis code snippet defines a function `min_max` that takes a list of integers as input and returns a new list of floats, where each float represents the original value normalized to a range of [0, 1]. The normalization is done by subtracting the minimum value of the list from each element and then dividing by the range (max value - min value) of the list. The result is rounded to 4 decimal places to meet the output constraints."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection.\n    \n    Parameters:\n    - x: np.ndarray, the input array of shape (batch_size, input_dim).\n    - w1: np.ndarray, the first weight matrix of shape (input_dim, hidden_dim).\n    - w2: np.ndarray, the second weight matrix of shape (hidden_dim, input_dim).\n    \n    Returns:\n    - np.ndarray, the output array after passing through the residual block.\n    \"\"\"\n    # Apply first weight layer and ReLU activation\n    hidden = np.maximum(0, np.dot(x, w1))\n    # Apply second weight layer\n    out = np.dot(hidden, w2)\n    # Add shortcut connection and final ReLU activation\n    output = np.maximum(0, out + x)\n    # Round to 4 decimal places and convert to list\n    return output.round(4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define input and weights\n    x = np.array([[1, 2], [3, 4]])\n    w1 = np.array([[0.5, 1], [1.5, 2]])\n    w2 = np.array([[0.5, 0.25], [1, 1.5]])\n    \n    # Call the residual block function\n    output = residual_block(x, w1, w2)\n    print(output)\n```\nThis script defines a simple residual block function using NumPy, which processes the input through two weight layers with ReLU activations, adds the original input via a shortcut connection, and applies a final ReLU activation. The output is rounded to 4 decimal places and converted to a list for return."}
{"task_id": 114, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Performs Global Average Pooling on a 3D numpy array.\n    \n    Parameters:\n    - x: np.ndarray of shape (height, width, channels), the input feature maps.\n    \n    Returns:\n    - np.ndarray of shape (channels,), the global average pooled output.\n    \"\"\"\n    # Compute the average along the height and width dimensions (axis 0 and 1)\n    return np.mean(x, axis=(0, 1))\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample 3D input array of shape (4, 4, 2)\n    np.random.seed(0)  # For reproducibility\n    sample_input = np.random.rand(4, 4, 2)\n    print(\"Sample Input Shape:\", sample_input.shape)\n    print(\"Sample Input:\\n\", sample_input)\n    \n    # Apply Global Average Pooling\n    pooled_output = global_avg_pool(sample_input)\n    print(\"Pooled Output Shape:\", pooled_output.shape)\n    print(\"Pooled Output:\\n\", pooled_output)\n```"}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n    \n    Parameters:\n    - X: np.ndarray, input array of shape (batch, channels, height, width)\n    - gamma: np.ndarray, scale parameter of shape (channels,)\n    - beta: np.ndarray, shift parameter of shape (channels,)\n    - epsilon: float, small constant to prevent division by zero\n    \n    Returns:\n    - np.ndarray: normalized, scaled, and shifted output array\n    \"\"\"\n    # Compute the mean across batch and spatial dimensions for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    # Compute the variance across batch and spatial dimensions for each channel\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    # Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    # Scale and shift\n    X_scaled_shifted = gamma[None, :, None, None] * X_norm + beta[None, :, None, None]\n    \n    return np.round(X_scaled_shifted, decimals=4).tolist()\n\n# Example usage:\nX = np.random.rand(2, 3, 4, 4)  # Batch of 2, 3 channels, 4x4 feature maps\ngamma = np.random.rand(3)       # Scale parameter for each channel\nbeta = np.random.rand(3)        # Shift parameter for each channel\noutput = batch_normalization(X, gamma, beta)\n\nprint(output)\n```\n\nIn this script, the `batch_normalization` function implements batch normalization for a 4D input array in the BCHW format. It first calculates the mean and variance for each channel across the batch and spatial dimensions. It then normalizes the input, applies scaling and shifting, and finally rounds the output to 4 decimal places and converts it to a list for returning. The example usage demonstrates how to call the function with a random input and parameters."}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Computes the derivative of a polynomial term of the form c * x^n at a given point x.\n    \n    Parameters:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which the derivative is to be calculated.\n    n (float): The exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    # Calculate the derivative using the power rule: n * c * x^(n-1)\n    derivative_value = n * c * (x ** (n - 1))\n    return round(derivative_value, 4)\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Test cases to verify the correctness of the function\n    print(poly_term_derivative(3.0, 2.0, 2.0))  # Expected output: 12.0\n    print(poly_term_derivative(5.0, 3.0, 3.0))  # Expected output: 135.0\n    print(poly_term_derivative(2.5, 4.0, 1.0))  # Expected output: 2.5\n    print(poly_term_derivative(1.0, 0.0, 5.0))  # Expected output: 0.0\n```"}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors.\n    \n    :param vectors: A list of 2D vectors (lists of floats).\n    :param tol: Tolerance value to determine linear independence.\n    :return: A list of orthonormal vectors that span the same subspace.\n    \"\"\"\n    # Convert list of vectors to a numpy array for easier manipulation\n    V = np.array(vectors)\n    num_vectors = V.shape[0]\n    \n    # Initialize an empty list to store orthonormal basis\n    orthonormal_basis = []\n    \n    for i in range(num_vectors):\n        # Subtract the projection of V[i] onto the current orthonormal basis\n        for basis in orthonormal_basis:\n            V[i] -= np.dot(V[i], basis) * basis\n        \n        # Check if the resulting vector is linearly independent\n        if np.linalg.norm(V[i]) > tol:\n            # Normalize the vector and add it to the orthonormal basis\n            orthonormal_basis.append(V[i] / np.linalg.norm(V[i]))\n    \n    # Convert each basis vector to a list and round to 4 decimal places\n    orthonormal_basis = [np.round(vec, 4).tolist() for vec in orthonormal_basis]\n    \n    return orthonormal_basis\n\n# Example usage\nvectors = [\n    [1, 2],\n    [2, 4],\n    [1, 0]\n]\n\nprint(orthonormal_basis(vectors))\n```\n\nThis script defines a function `orthonormal_basis` that computes an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process. The function checks for linear independence with a given tolerance and returns a list of orthonormal vectors that span the same subspace as the input vectors, with each vector rounded to 4 decimal places."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): The first 3D vector.\n    b (list/tuple): The second 3D vector.\n    \n    Returns:\n    list: The cross product of the two vectors, rounded to 4 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    a = np.array(a)\n    b = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a, b)\n    \n    # Round the result to 4 decimal places and convert to list\n    return np.round(result, 4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    vector1 = (1, 2, 3)\n    vector2 = (4, 5, 6)\n    print(cross_product(vector1, vector2))\n```\n\nThis script defines a function `cross_product` that takes two 3D vectors as input and returns their cross product, rounded to four decimal places. The function uses NumPy's `cross` function for the computation and ensures the inputs are NumPy arrays for compatibility. The example usage at the end demonstrates how to use this function with two sample vectors."}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solve a system of linear equations using Cramer's Rule.\n    \n    Parameters:\n    A (np.array): A square coefficient matrix.\n    b (np.array): A constant vector.\n    \n    Returns:\n    list: Solution vector x as a list, or -1 if no unique solution exists.\n    \"\"\"\n    # Convert inputs to numpy arrays for consistency\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Check if A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        return -1\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If determinant is 0, system has no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Loop through each column of A to replace with b and calculate determinant\n    for i in range(len(A)):\n        # Replace the ith column of A with b\n        Ai = A.copy()\n        Ai[:, i] = b\n        # Calculate the determinant of the new matrix\n        det_Ai = np.linalg.det(Ai)\n        # Calculate the ith variable of the solution\n        xi = det_Ai / det_A\n        # Append to the solution vector\n        x.append(round(xi, 4))\n    \n    # Return the solution vector as a list\n    return np.array(x).tolist()\n\n# Example usage\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]\nprint(cramers_rule(A, b))  # Output should be the solution vector x\n```\n\nThis script defines a function `cramers_rule` that solves a system of linear equations using Cramer's Rule. It checks if the input matrix is square and if the determinant is non-zero before proceeding to calculate the solution vector. The solution is returned as a list of numbers rounded to the nearest 4th decimal."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two discrete probability distributions.\n    \n    Args:\n    p: list[float] - A list of floats representing the first probability distribution.\n    q: list[float] - A list of floats representing the second probability distribution.\n    \n    Returns:\n    float - The Bhattacharyya distance between the two distributions rounded to 4 decimal places.\n            Returns 0.0 if the inputs are not of the same length or are empty.\n    \"\"\"\n    # Check if the inputs are valid\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = np.sum(np.sqrt(np.multiply(p, q)))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    return round(bd, 4)\n\n# Check function to verify the correctness of the bhattacharyya_distance function\ndef check_bhattacharyya_distance():\n    p1 = [0.1, 0.2, 0.3, 0.4]\n    q1 = [0.4, 0.3, 0.2, 0.1]\n    p2 = [0.25, 0.25, 0.25, 0.25]\n    q2 = [0.25, 0.25, 0.25, 0.25]\n    p3 = [0.1, 0.2]\n    q3 = [0.3, 0.7, 0.1]\n    \n    assert bhattacharyya_distance(p1, q1) == 0.5493, f\"Incorrect distance for p1 and q1: {bhattacharyya_distance(p1, q1)}\"\n    assert bhattacharyya_distance(p2, q2) == 1.3863, f\"Incorrect distance for p2 and q2: {bhattacharyya_distance(p2, q2)}\"\n    assert bhattacharyya_distance(p3, q3) == 0.0, f\"Incorrect distance for p3 and q3: {bhattacharyya_distance(p3, q3)}\"\n    \n    print(\"All checks passed!\")\n\n# Run the check function\ncheck_bhattacharyya_distance()\n```"}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list[int|float]): The first vector.\n    b (list[int|float]): The second vector.\n    \n    Returns:\n    list[int|float] | int: A new vector representing the element-wise sum of 'a' and 'b'\n    if they have the same length, otherwise -1.\n    \"\"\"\n    # Check if vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute element-wise sum\n    result = [a[i] + b[i] for i in range(len(a))]\n    return result\n\n# Check function with provided data points\nassert vector_sum([1, 2, 3], [4, 5, 6]) == [5, 7, 9], \"Test case 1 failed\"\nassert vector_sum([1, 2], [3, 4, 5]) == -1, \"Test case 2 failed\"\nassert vector_sum([10.5, 20.3], [30.2, 40.1]) == [40.7, 60.4], \"Test case 3 failed\"\nassert vector_sum([], []) == [], \"Test case 4 failed\"\n\nprint(\"All test cases passed!\")\n```\n\nThis code snippet defines a function `vector_sum` that takes two lists, `a` and `b`, as input and returns their element-wise sum if they are of the same length. If the lengths of the vectors are not the same, it returns -1. The function is tested with several assertions to verify its correctness."}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=1, keepdims=True)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    \n    for episode in episodes:\n        returns = 0\n        for state, action, reward in reversed(episode):\n            returns += reward\n        \n        for state, action, _ in episode:\n            policy = softmax(theta[state, :])\n            gradient[state, action] += returns * policy[action]\n            gradient[state, :] -= returns * policy[action] * policy\n    \n    gradient /= len(episodes)\n    return np.round(gradient, 4).tolist()\n\n# Example usage:\ntheta = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float64)\nepisodes = [\n    [(0, 1, 1), (1, 2, 1), (2, 0, 1)],\n    [(0, 2, 1), (1, 1, 1), (2, 2, 1)]\n]\n\nprint(compute_policy_gradient(theta, episodes))\n```\n\nThis code defines a function `compute_policy_gradient` that computes the policy gradient using the REINFORCE algorithm. It uses a helper function `softmax` to compute the softmax of the policy parameters for each state. The function iterates over each episode, calculates the return for each time step, and then computes the gradient of the log-policy multiplied by the return. The average gradient is returned, rounded to the nearest 4th decimal, and reshaped into a list for output."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts during computation.\n    d_in (int): The input dimension for each expert.\n    d_out (int): The output dimension for each expert.\n    \n    Returns:\n    float: The percentage of computational cost savings rounded to the nearest 1th decimal.\n    \"\"\"\n    # FLOPs for dense layer: 2 * d_in * d_out (for multiplication and addition)\n    flops_dense = 2 * d_in * d_out\n    \n    # FLOPs for MoE layer: 2 * k_active * d_in * d_out (for active experts) + 2 * n_experts * d_in (for gating)\n    flops_moe = 2 * k_active * d_in * d_out + 2 * n_experts * d_in\n    \n    # Calculate savings\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    \n    # Return savings percentage rounded to the nearest 1th decimal\n    return round(savings, 1)\n\n# Example usage\nn_experts = 128  # Total number of experts\nk_active = 4     # Number of active experts\nd_in = 1024      # Input dimension\nd_out = 1024     # Output dimension\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational cost savings: {savings_percentage}%\")\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost savings of using a Mixture-of-Experts (MoE) layer over a dense layer. The function takes the total number of experts, the number of active experts, and the input/output dimensions as parameters. It computes the floating-point operations (FLOPs) for both the MoE and dense layers and then calculates the percentage of computational cost savings, rounding the result to the nearest 1th decimal."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n) -> list:\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism.\n    \n    Parameters:\n    X (np.ndarray): Input matrix.\n    W_g (np.ndarray): Weight matrix for the gating network.\n    W_noise (np.ndarray): Weight matrix for the noise.\n    N (np.ndarray): Pre-sampled noise matrix.\n    k (int): Number of experts to select.\n    \n    Returns:\n    list: The final gating probabilities matrix as a list.\n    \"\"\"\n    # Compute logits for each expert\n    logits = X @ W_g\n    \n    # Add noise to the logits\n    noisy_logits = logits + N * W_noise\n    \n    # Apply softmax to get probabilities\n    exp_scores = np.exp(noisy_logits - np.max(noisy_logits, axis=-1, keepdims=True))\n    probs = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    \n    # Select top-k experts\n    topk_indices = np.argsort(probs, axis=-1)[:, -k:]\n    topk_probs = np.zeros_like(probs)\n    for i, idx in enumerate(topk_indices):\n        topk_probs[i, idx] = probs[i, idx]\n    \n    # Normalize the top-k probabilities\n    topk_probs /= np.sum(topk_probs, axis=-1, keepdims=True)\n    \n    # Round to the nearest 4th decimal and convert to list\n    return np.round(topk_probs, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input data\n    X = np.array([[1, 2], [3, 4]])\n    W_g = np.array([[0.1, 0.2], [0.3, 0.4]])\n    W_noise = np.array([0.01, 0.02])\n    N = np.array([0.05, -0.03])\n    k = 1\n    \n    # Call the function with the example data\n    result = noisy_topk_gating(X, W_g, W_noise, N, k)\n    print(result)\n```\n\nThis script defines the `noisy_topk_gating` function which implements the Noisy Top-K gating mechanism as described in the task. The function takes an input matrix `X`, weight matrices `W_g` and `W_noise`, pre-sampled noise `N`, and a sparsity constraint `k`. It computes the final gating probabilities matrix and returns it as a list after rounding to the nearest 4th decimal."}
{"task_id": 125, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts layer with softmax gating and top-k routing.\n    \n    Parameters:\n    x: Input tensor of shape (batch_size, input_dim).\n    We: Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg: Gating weight matrix of shape (input_dim, n_experts).\n    n_experts: Number of experts.\n    top_k: Number of top experts to consider for each input token.\n    \n    Returns:\n    Output tensor of shape (batch_size, output_dim) after applying MoE.\n    \"\"\"\n    # Compute the gating scores.\n    gating_scores = np.dot(x, Wg)\n    # Apply softmax to get the probabilities.\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=1, keepdims=True)\n    \n    # Get the top-k experts for each input token.\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    \n    # Normalize the top-k probabilities.\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    \n    # Apply the top-k experts to the input.\n    batch_size, input_dim = x.shape\n    output_dim = We.shape[2]\n    expert_outputs = np.zeros((batch_size, top_k, output_dim))\n    for i in range(batch_size):\n        for j, idx in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = np.dot(x[i], We[idx])\n    \n    # Aggregate the expert outputs weighted by the normalized gating probabilities.\n    final_output = np.sum(expert_outputs * top_k_probs_normalized[..., np.newaxis], axis=1)\n    \n    # Round the results to the nearest 4th decimal and return as a python list.\n    return np.round(final_output, 4).tolist()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Example parameters\n    batch_size = 2\n    input_dim = 3\n    output_dim = 2\n    n_experts = 4\n    top_k = 2\n    \n    # Randomly initialize the input, expert weights, and gating weights.\n    np.random.seed(0)\n    x = np.random.rand(batch_size, input_dim)\n    We = np.random.rand(n_experts, input_dim, output_dim)\n    Wg = np.random.rand(input_dim, n_experts)\n    \n    # Compute the MoE output.\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```"}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    G = num_groups\n    \n    # Check if the number of channels is divisible by the number of groups\n    assert C % G == 0, \"The number of channels is not divisible by the number of groups.\"\n    \n    # Reshape input into groups\n    X = X.reshape(B, G, -1)\n    \n    # Calculate mean and variance for each group\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    \n    # Normalize\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Reshape back to original shape\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    \n    # Apply scale and shift\n    X_normalized = X_normalized * gamma + beta\n    \n    # Round to the nearest 4th decimal\n    X_normalized = np.round(X_normalized, 4)\n    \n    # Return as a list\n    return X_normalized.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(42)  # For reproducibility\n    X = np.random.randn(2, 8, 4, 4)  # Example input tensor\n    gamma = np.random.randn(8)  # Scale parameters\n    beta = np.random.randn(8)  # Shift parameters\n    num_groups = 2  # Number of groups\n    \n    normalized_output = group_normalization(X, gamma, beta, num_groups)\n    print(normalized_output)\n```\n\nThis script provides a function `group_normalization` that performs group normalization on a given 4D input tensor `X` with shape (B, C, H, W) by splitting the channels into a specified number of groups, normalizing each group, and then applying learned scale and shift parameters. The function ensures that the number of channels is divisible by the number of groups and rounds the output to the nearest 4th decimal place before returning it as a list."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Find the x value that minimizes the function f(x) = x^4 - 3x^3 + 2 using gradient descent.\n    \n    Parameters:\n    - start_x: The starting x position for the search.\n    - learning_rate: The step size at each iteration while moving toward a minimum of the function.\n    - tolerance: The maximum acceptable error in the position of the minimum.\n    - max_iters: The maximum number of iterations to perform.\n    \n    Returns:\n    - The x value at which f(x) is minimized, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        # Compute the derivative of the function\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Update x using the gradient and learning rate\n        x_new = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(x_new - x) < tolerance:\n            break\n        \n        x = x_new\n    \n    return round(x, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    start_x = np.random.uniform(-10, 10)  # Random starting point\n    print(f\"Starting position: {start_x}\")\n    treasure_location = find_treasure(start_x)\n    print(f\"Treasure located at: {treasure_location}\")\n```\n\nThis solution uses gradient descent to find the minimum of the given function. The derivative of the function is used to determine the direction of the steepest descent at each step. The learning rate controls how large of a step is taken in that direction, and the process continues until the change in position is less than a specified tolerance or until a maximum number of iterations is reached."}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which is a normalization-free transformation\n    inspired by the Tanh function. This function takes an array of input values and applies\n    the DyT transformation using given parameters alpha, gamma, and beta.\n    \n    Args:\n    x (np.ndarray): Input array of values.\n    alpha (float): Scaling factor for the input values.\n    gamma (float): Scaling factor for the tanh output.\n    beta (float): Bias for the tanh output.\n    \n    Returns:\n    list[float]: List of transformed values, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Apply the Dynamic Tanh transformation\n    transformed = gamma * np.tanh(alpha * x) + beta\n    \n    # Round the results to the nearest 4th decimal\n    rounded_results = np.round(transformed, 4)\n    \n    # Return the results as a list\n    return rounded_results.tolist()\n\n# Example usage\nx = np.array([0.5, -0.2, 1.0, 0.0])\nalpha = 1.0\ngamma = 1.5\nbeta = 0.1\nprint(dynamic_tanh(x, alpha, gamma, beta))\n```\n\nThis script defines the `dynamic_tanh` function as per the task's requirements and includes an example usage that demonstrates how to apply the function to an array of input values with specified parameters. The output is rounded to the nearest 4th decimal and returned as a list."}
