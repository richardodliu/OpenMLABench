{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float] | int:\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix.\n    \n    Returns:\n    list of list of int/float: The transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshapes the given 2D list (matrix) into the specified new shape if possible.\n    Returns the reshaped matrix as a Python list of lists.\n    If the reshaping is not possible (because the total number of elements does not match),\n    returns an empty list.\n    \n    :param a: The original 2D list (matrix) to reshape.\n    :param new_shape: A tuple specifying the new shape (rows, columns).\n    :return: The reshaped matrix as a Python list of lists, or an empty list if reshaping is not possible.\n    \"\"\"\n    a_array = np.array(a)\n    try:\n        reshaped_array = np.reshape(a_array, new_shape)\n        return reshaped_array.tolist()\n    except:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    Args:\n    matrix: A list of lists of floats, where each inner list represents a row of the matrix.\n    mode: A string that specifies the calculation mode: 'row' for row-wise mean or 'column' for column-wise mean.\n    \n    Returns:\n    A list of means either for each row or each column, depending on the mode.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    if mode == 'row':\n        means = np_matrix.mean(axis=1)\n    elif mode == 'column':\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Invalid mode. Use 'row' or 'column'.\")\n    return list(means)"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element in the given matrix by the scalar value.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar value.\n    :return: A 2D list of the same size as the matrix, with each element multiplied by the scalar.\n    \"\"\"\n    return [[elem * scalar for elem in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculate and return the eigenvalues of a 2x2 matrix in a sorted list from highest to lowest.\n    \n    Args:\n    - matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    - list of float: A sorted list of eigenvalues from highest to lowest.\n    \"\"\"\n    np_matrix = np.array(matrix)\n    eigenvalues = np.linalg.eig(np_matrix)[0]\n    return sorted(eigenvalues, reverse=True)"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        np.linalg.inv(T_np)\n        np.linalg.inv(S_np)\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_matrix = T_inv @ A_np @ S_inv\n        return np.round(transformed_matrix, 4).tolist()\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[2, 3], [1, 2]]\nS = [[4, 3], [2, 1]]"}
{"task_id": 8, "completion_id": 0, "solution": "import numpy as np\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix if it is invertible.\n    \n    Args:\n    - matrix (list[list[float]]): a 2x2 matrix represented as a nested list\n    \n    Returns:\n    - list[list[float]]: the inverse of the given matrix, or None if the matrix is not invertible\n    \"\"\"\n    (a, b, c, d) = (matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1])\n    det = a * d - b * c\n    if det == 0:\n        return None\n    inverse_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices a and b.\n    \n    Args:\n    a (list of list of int | float): First matrix.\n    b (list of list of int | float): Second matrix.\n    \n    Returns:\n    list of list of int | float: Resulting matrix of the multiplication or -1 if the matrices cannot be multiplied.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: A covariance matrix.\n    \"\"\"\n    vectors_np = np.array(vectors)\n    covariance_matrix = np.cov(vectors_np, rowvar=False)\n    covariance_matrix_list = covariance_matrix.tolist()\n    return covariance_matrix_list"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A : np.ndarray\n        The coefficient matrix of the system.\n    b : np.ndarray\n        The constant matrix of the system.\n    n : int\n        The number of iterations for the Jacobi method.\n    \n    Returns:\n    list\n        The approximate solution of the system, rounded to the 4th decimal place.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be a square matrix.')\n    x = np.zeros_like(b)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix using the properties of SVD.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 numpy array.\n    \n    Returns:\n    tuple: A tuple containing the two singular values, rounded to the nearest 4th decimal.\n    \"\"\"\n    ATA = A.T @ A\n    eigenvalues = np.linalg.eigvals(ATA)\n    singular_values = np.sqrt(eigenvalues)\n    singular_values = np.round(singular_values, 4)\n    singular_values.sort()\n    singular_values = singular_values[::-1]\n    return tuple(singular_values)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "from typing import List, Union\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(submatrix: List[List[Union[int, float]]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        :param submatrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the 3x3 matrix.\n        \"\"\"\n        return submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) - submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) + submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n    det = 0\n    for column in range(4):\n        minor = [row[:column] + row[column + 1:] for row in matrix[1:]]\n        minor_det = determinant_3x3(minor)\n        det += (-1) ** (0 + column) * matrix[0][column] * minor_det\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation and returns model coefficients.\n    Coefficients are rounded up to 4 decimal places.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    theta_rounded = np.round(theta, decimals=4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input features including a column of ones for the intercept.\n    y (np.ndarray): The target values.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to run the gradient descent.\n    \n    Returns:\n    list: The list of coefficients for the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    coefficients = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        y_pred = np.dot(X, coefficients)\n        error = y_pred - y\n        gradient = np.dot(X.T, error) / len(y)\n        coefficients -= alpha * gradient\n    rounded_coefficients = np.round(coefficients, 4).tolist()\n    return rounded_coefficients"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array and performs both standardization and min-max normalization.\n    Returns two lists: the first is data standardized, the second is data normalized.\n    \"\"\"\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized = (data - min_val) / (max_val - min_val)\n    standardized_list = np.round(standardized, 4).tolist()\n    normalized_list = np.round(normalized, 4).tolist()\n    return (standardized_list, normalized_list)\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for iteration in range(max_iterations):\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_idx = np.argmin(distances, axis=0)\n        new_centroids = np.array([points[closest_centroid_idx == i].mean(axis=0) if np.any(closest_centroid_idx == i) else centroids[i] for i in range(k)])\n        if np.array_equal(centroids, new_centroids):\n            break\n        else:\n            centroids = new_centroids\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): Input data.\n    y (np.ndarray): Targets for the input data.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting into batches. Default is True.\n    random_seed (int): Seed for the random number generator. Default is None.\n\n    Returns:\n    list of tuples: A list containing tuples of train and test indices for each fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        permutation = rng.permutation(n_samples)\n        X = X[permutation]\n        y = y[permutation]\n    fold_size = n_samples // k\n    indices = np.arange(n_samples)\n    folds = []\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = test_start + fold_size\n        if i == k - 1:\n            test_end = n_samples\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the top k principal components.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    result = [list(map(lambda x: round(x, 4), principal_components[:, i])) for i in range(k)]\n    return result"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef entropy(examples, target_attr):\n    \"\"\"\n    Calculate the entropy of the examples based on the target attribute.\n    \"\"\"\n    examples_count = len(examples)\n    if examples_count == 0:\n        return 0\n    target_values = [example[target_attr] for example in examples]\n    target_counts = Counter(target_values)\n    entropy_value = 0\n    for count in target_counts.values():\n        probability = count / examples_count\n        entropy_value -= probability * math.log2(probability)\n    return entropy_value\ndef information_gain(examples, attribute, target_attr):\n    \"\"\"\n    Calculate the information gain of a specific attribute.\n    \"\"\"\n    total_entropy = entropy(examples, target_attr)\n    examples_count = len(examples)\n    attribute_values = set([example[attribute] for example in examples])\n    weighted_entropy = 0\n    for value in attribute_values:\n        subset = [example for example in examples if example[attribute] == value]\n        subset_entropy = entropy(subset, target_attr)\n        probability = len(subset) / examples_count\n        weighted_entropy += probability * subset_entropy\n    return total_entropy - weighted_entropy\ndef majority_value(examples, target_attr):\n    \"\"\"\n    Find the most common target value in the examples.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    target_counts = Counter(target_values)\n    return target_counts.most_common(1)[0][0]\ndef choose_best_attribute(examples, attributes, target_attr):\n    \"\"\"\n    Select the attribute with the maximum information gain.\n    \"\"\"\n    if not attributes:\n        return None\n    gains = {attribute: information_gain(examples, attribute, target_attr) for attribute in attributes}\n    best_attribute = max(gains, key=gains.get)\n    return best_attribute\ndef get_sub_examples(examples, attribute, value, target_attr):\n    \"\"\"\n    Filter examples based on the attribute and value.\n    \"\"\"\n    return [example for example in examples if example[attribute] == value]\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"\n    Implement the decision tree learning algorithm.\n    \"\"\"\n    if not examples:\n        return majority_value(examples, target_attr)\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attribute = choose_best_attribute(examples, attributes, target_attr)\n    if not best_attribute:\n        return majority_value(examples, target_attr)\n    tree = {best_attribute: {}}\n    attribute_values = set([example[best_attribute] for example in examples])\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    for value in attribute_values:\n        sub_examples = get_sub_examples(examples, best_attribute, value, target_attr)\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\nexamples = [{'Age': 'Young', 'Income': 'High', 'Student': 'No', 'Credit_Rating': 'Fair', 'Buys_Computer': 'No'}, {'Age': 'Young', 'Income': 'High', 'Student': 'No', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'No'}, {'Age': 'Middle_Aged', 'Income': 'High', 'Student': 'No', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Senior', 'Income': 'Medium', 'Student': 'No', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Senior', 'Income': 'Low', 'Student': 'Yes', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Senior', 'Income': 'Low', 'Student': 'Yes', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'No'}, {'Age': 'Middle_Aged', 'Income': 'Low', 'Student': 'Yes', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'Yes'}, {'Age': 'Young', 'Income': 'Medium', 'Student': 'No', 'Credit_Rating': 'Fair', 'Buys_Computer': 'No'}, {'Age': 'Young', 'Income': 'Low', 'Student': 'Yes', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Senior', 'Income': 'Medium', 'Student': 'Yes', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Young', 'Income': 'Medium', 'Student': 'Yes', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'Yes'}, {'Age': 'Middle_Aged', 'Income': 'Medium', 'Student': 'No', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'Yes'}, {'Age': 'Middle_Aged', 'Income': 'High', 'Student': 'Yes', 'Credit_Rating': 'Fair', 'Buys_Computer': 'Yes'}, {'Age': 'Senior', 'Income': 'Medium', 'Student': 'No', 'Credit_Rating': 'Excellent', 'Buys_Computer': 'Yes'}]\nattributes = ['Age', 'Income', 'Student', 'Credit_Rating']\ntarget_attr = 'Buys_Computer'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_linear(x1, x2):\n    return np.dot(x1, x2)\ndef kernel_rbf(x1, x2, sigma=1.0):\n    return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    kernel_func = kernel_linear if kernel == 'linear' else lambda x, y: kernel_rbf(x, y, sigma)\n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            sum_kernels = np.sum(alphas * labels * np.array([kernel_func(data[j], data[i]) for j in range(n_samples)]))\n            if labels[i] * (sum_kernels + b) < 1:\n                alphas[i] = (1 - lambda_t * lambda_val) * alphas[i] + lambda_t * labels[i]\n                b += lambda_t * (1 - labels[i] * (sum_kernels + b))\n            else:\n                alphas[i] = (1 - lambda_t * lambda_val) * alphas[i]\n    b = 0\n    for i in range(n_samples):\n        sum_kernels = np.sum(alphas * labels * np.array([kernel_func(data[j], data[i]) for j in range(n_samples)]))\n        b += labels[i] - sum_kernels\n    b /= n_samples\n    return (alphas.tolist(), round(b, 4))\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\nlabels = np.array([1, 1, -1, -1])"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    This function takes a list of scores and computes the softmax activation for each score.\n    The softmax function is defined as exp(x)/sum(exp(x)) for each element in the input list.\n    The result is a probability distribution over the scores.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_scores"}
{"task_id": 24, "completion_id": 0, "solution": "import numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    This function takes feature vectors, binary labels, and neuron's parameters to return\n    predicted probabilities and mean squared error.\n    \n    :param features: A list of lists, where each sub-list represents a feature vector.\n    :param labels: A list of binary labels (0 or 1).\n    :param weights: A list of weights corresponding to each feature.\n    :param bias: A float representing the bias.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n    features_np = np.array(features)\n    labels_np = np.array(labels)\n    z = np.dot(features_np, weights) + bias\n    predictions = 1 / (1 + np.exp(-z))\n    mse = np.mean((predictions - labels_np) ** 2)\n    predictions_rounded = [round(pred, 4) for pred in predictions.tolist()]\n    mse_rounded = round(mse, 4)\n    return (predictions_rounded, mse_rounded)\nfeatures = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\nlabels = [0, 1, 1]\nweights = [0.4, 0.5]\nbias = 0.6"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x):\n    return x * (1 - x)\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_losses = []\n    for epoch in range(epochs):\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        mse_loss = mse(labels, predictions)\n        mse_losses.append(round(mse_loss, 4))\n        error = labels - predictions\n        adjustments = error * sigmoid_derivative(predictions)\n        weights += learning_rate * np.dot(features.T, adjustments)\n        bias += learning_rate * np.sum(adjustments)\n    return (weights.tolist(), round(bias, 4), mse_losses)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([[0], [1], [1], [0]])\ninitial_weights = np.array([[0.5], [0.5]])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f'Value(data={self.data})'\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C.\n    \n    Parameters:\n    B (list of lists of int): Basis vectors in basis B.\n    C (list of lists of int): Basis vectors in basis C.\n    \n    Returns:\n    list of lists of float: The transformation matrix from B to C.\n    \"\"\"\n    B_array = np.array(B)\n    C_array = np.array(C)\n    C_inv = np.linalg.inv(C_array)\n    transform_matrix = np.dot(C_inv, B_array)\n    transform_matrix = np.round(transform_matrix, 4).tolist()\n    return transform_matrix\nB = [[1, 2, 0], [0, 1, 3], [2, 0, 1]]\nC = [[-1, 1, 0], [1, 0, 2], [0, 1, 1]]"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD (U, S, V) of a 2x2 matrix A using the eigenvalues and eigenvectors of A^T A.\n    \n    Args:\n    - A: A 2x2 numpy array.\n    \n    Returns:\n    - U, S, V: The SVD matrices such that A = U * S * V.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(ATA)\n    S = np.sqrt(eigenvalues)\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    V = eigenvectors.T\n    S_inv = np.diag(1 / np.where(S > 0, S, 1))\n    U = np.dot(A, np.dot(V.T, S_inv))\n    U = np.round(U, decimals=4).tolist()\n    S = np.round(S, decimals=4).tolist()\n    S_diagonal = np.zeros((2, 2))\n    np.fill_diagonal(S_diagonal, S)\n    S_diagonal = S_diagonal.tolist()\n    V = np.round(V, decimals=4).tolist()\n    return (U, S_diagonal, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two numpy arrays, X and y, preserving their correspondence.\n    \n    Parameters:\n    - X: numpy array, the feature matrix.\n    - y: numpy array, the label array.\n    - seed: int, optional, the seed for the random number generator for reproducibility.\n    \n    Returns:\n    - X_shuffled as a list, the shuffled feature matrix.\n    - y_shuffled as a list, the shuffled label array.\n    \"\"\"\n    if not isinstance(seed, int):\n        np.random.seed(seed)\n    assert X.shape[0] == y.shape[0], 'X and y must have the same number of rows.'\n    idx = np.arange(X.shape[0])\n    np.random.shuffle(idx)\n    X_shuffled = X[idx]\n    y_shuffled = y[idx]\n    return (X_shuffled.tolist(), y_shuffled.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    This function creates a batch iterator for a given dataset X and an optional target dataset y.\n    It yields batches of dataset(s) as specified by the batch_size.\n\n    Parameters:\n    X (numpy.ndarray): Input dataset.\n    y (numpy.ndarray, optional): Target dataset. Defaults to None.\n    batch_size (int): Size of the batch to be yielded.\n\n    Returns:\n    tuple[list, list] or list: A batch of X and y (if y is provided), or a batch of X only.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        start_idx = i\n        end_idx = min(i + batch_size, n_samples)\n        if y is not None:\n            yield (X[start_idx:end_idx].tolist(), y[start_idx:end_idx].tolist())\n        else:\n            yield X[start_idx:end_idx].tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is\n    greater than or equal to a given threshold.\n    \n    Parameters:\n    X (numpy.ndarray): The input dataset.\n    feature_i (int): Index of the feature to be used for comparison.\n    threshold (float): Value to compare the feature against.\n    \n    Returns:\n    list: Two subsets of the dataset. First subset contains samples with\n          feature values >= threshold and the second subset has values < threshold.\n    \"\"\"\n    X = np.array(X)\n    idx_ge = np.where(X[:, feature_i] >= threshold)\n    idx_lt = np.where(X[:, feature_i] < threshold)\n    subset_ge = X[idx_ge].tolist()\n    subset_lt = X[idx_lt].tolist()\n    return [subset_ge, subset_lt]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    :param X: A 2D numpy array of shape (n_samples, n_features).\n    :param degree: An integer representing the highest degree of interaction features.\n    :return: A 2D numpy array with new polynomial features, reshaped and converted to a python list.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    indices = list(combinations_with_replacement(range(n_features), degree))\n    new_features = np.empty((n_samples, 0))\n    for index in indices:\n        new_feature = np.prod(X[:, index], axis=1).reshape(-1, 1)\n        new_features = np.hstack((new_features, new_feature))\n    return new_features.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: A 1D numpy array of integer values.\n    - n_col: An optional integer. The number of columns in the one-hot encoded array.\n             If not provided, it will be determined based on the maximum value in x.\n    \n    Returns:\n    - A list of lists representing the one-hot encoded data.\n    \"\"\"\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([0, 1, 2, 2, 3])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix and return it as a Python list.\n    \n    Parameters:\n    x (np.array): A 1D numpy array to be converted into a diagonal matrix.\n    \n    Returns:\n    list: The diagonal matrix as a Python list.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()\nx = np.array([1, 2, 3])\ndiagonal_matrix = make_diagonal(x)"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score as a float rounded to the 4th decimal.\n    \"\"\"\n    correct_predictions = np.sum(y_true == y_pred)\n    total_predictions = len(y_true)\n    accuracy = correct_predictions / total_predictions\n    return round(accuracy, 4)\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 4])"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset. If Y is not provided,\n    calculate the correlation matrix of X with itself.\n    \n    :param X: 2D numpy array\n    :param Y: Optional, 2D numpy array\n    :return: 2D numpy array representing the correlation matrix, as a python list\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    X_std = np.std(X, axis=0)\n    Y_std = np.std(Y, axis=0)\n    X_std[X_std == 0] = 1\n    Y_std[Y_std == 0] = 1\n    corr_matrix = cov_matrix / np.outer(X_std, Y_std)\n    return np.round(corr_matrix, 4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fitting method for an AdaBoost classifier.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features) representing the dataset.\n    - y: 1D numpy array of shape (n_samples,) representing the labels.\n    - n_clf: An integer representing the number of classifiers.\n    \n    Returns:\n    - classifiers: A list of dictionaries, each representing a classifier with parameters.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = {}\n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                pred = np.where(X[:, feature] > threshold, 1, -1)\n                error = np.sum(weights[y != pred])\n                if error > 0.5:\n                    error = 1 - error\n                    pred = -pred\n                if error < min_error:\n                    min_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'prediction': np.sign(np.sum(weights * y * (pred == y)))}\n                    best_pred = pred\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n        weights *= np.exp(-alpha * y * best_pred)\n        weights /= np.sum(weights)\n        best_clf['alpha'] = round(alpha, 4)\n        classifiers.append(best_clf)\n    return classifiers\nX = np.array([[2, 4], [4, 6], [6, 8], [8, 10]])\ny = np.array([1, -1, 1, -1])\nn_clf = 2"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a list of scores.\n    \n    The log-softmax function is defined as:\n    log_softmax(i) = log(exp(score[i]) / \u03a3(exp(score[j])) for all j)\n    \n    Args:\n        scores (list): A list of scores to compute the log-softmax for.\n    \n    Returns:\n        list: A list containing the log-softmax values of the original scores, rounded to 4 decimal places.\n    \"\"\"\n    scores_np = np.array(scores)\n    adjusted_scores = scores_np - np.max(scores_np)\n    exp_scores = np.exp(adjusted_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    softmax = exp_scores / sum_exp_scores\n    log_softmax = np.log(softmax)\n    return log_softmax.round(4).tolist()\nscores = [3.0, 1.0, 0.2]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        if self.input_shape:\n            input_shape = self.input_shape\n        else:\n            raise Exception('Dense layer is not the first layer, therefore it must receive its input_shape')\n        limit = 1 / math.sqrt(input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.optimizer_W.update(self.W, accum_grad.T.dot(self.layer_input))\n        w0 = self.optimizer_w0.update(self.w0, accum_grad)\n        self.W = W\n        self.w0 = w0\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution on an input matrix with a specified kernel, padding, and stride.\n    \n    Parameters:\n        input_matrix (np.ndarray): The input 2D matrix on which the convolution is applied.\n        kernel (np.ndarray): The convolutional kernel to use.\n        padding (int): The number of rows and columns of zeros to be added to the edges of the input matrix.\n        stride (int): The stride value for the convolution operation.\n    \n    Returns:\n        np.ndarray: The result of the convolution as a 2D numpy array, rounded to the nearest 4th decimal.\n    \"\"\"\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n    (n_in, m_in) = input_matrix.shape\n    (n_kernel, m_kernel) = kernel.shape\n    n_out = (n_in - n_kernel) // stride + 1\n    m_out = (m_in - m_kernel) // stride + 1\n    output_matrix = np.zeros((n_out, m_out))\n    for i in range(0, n_out):\n        for j in range(0, m_out):\n            current_matrix = input_matrix[i * stride:i * stride + n_kernel, j * stride:j * stride + m_kernel]\n            output_matrix[i, j] = np.sum(current_matrix * kernel)\n    return np.round(output_matrix, 4).tolist()\ninput_matrix = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) function to the input.\n    \n    Parameters:\n    z (float): A single float number to which the ReLU function will be applied.\n    \n    Returns:\n    float: The result after applying the ReLU function. It returns z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given the feature matrix, coefficients,\n    true labels, and regularization parameter.\n    \n    :param X: 2D numpy array, feature matrix\n    :param w: 1D numpy array, coefficients\n    :param y_true: 1D numpy array, true labels\n    :param alpha: float, regularization parameter\n    :return: float, Ridge loss rounded to 4 decimal places\n    \"\"\"\n    y_pred = X.dot(w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    total_loss = mse + regularization\n    return round(total_loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value.\n    - alpha (float): The slope for the negative part of the function. Default is 0.01.\n    \n    Returns:\n    - float: The Leaky ReLU output given the input value.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n\n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n\n    Returns:\n    float: The computed linear kernel value.\n\n    Raises:\n    ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError('Input vectors must be of the same length.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision score.\n    \n    Precision is the ratio of true positive predictions to the total positive predictions (including false positives).\n    \n    Parameters:\n    - y_true: Array of true binary labels (numpy array)\n    - y_pred: Array of predicted binary labels (numpy array)\n    \n    Returns:\n    - Precision score (float)\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0\n    precision_score = TP / (TP + FP)\n    return precision_score\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 0, 1])"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent to minimize the MSE loss function.\n    \n    :param X: numpy array of shape (n_samples, n_features), the input features.\n    :param y: numpy array of shape (n_samples,), the target values.\n    :param weights: numpy array of shape (n_features,), the initial weights.\n    :param learning_rate: float, the step size at each iteration while moving toward a minimum.\n    :param n_iterations: int, the number of iterations to perform.\n    :param batch_size: int, the size of the batch for gradient descent. Default is 1.\n    :param method: str, one of \"batch\", \"stochastic\", or \"mini-batch\". Default is \"batch\".\n    :return: list, the optimized weights.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    updated_weights = weights.copy()\n    for iteration in range(n_iterations):\n        if method == 'batch':\n            gradients = np.dot(X.T, np.dot(X, updated_weights) - y) / n_samples\n            updated_weights -= learning_rate * gradients\n        elif method == 'stochastic':\n            for sample_index in range(n_samples):\n                gradient = X[sample_index] * (np.dot(X[sample_index], updated_weights) - y[sample_index])\n                updated_weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for batch_start in range(0, n_samples, batch_size):\n                batch_end = batch_start + batch_size\n                X_batch = X[batch_start:batch_end]\n                y_batch = y[batch_start:batch_end]\n                gradients = np.dot(X_batch.T, np.dot(X_batch, updated_weights) - y_batch) / batch_size\n                updated_weights -= learning_rate * gradients\n    return np.round(updated_weights, decimals=4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Args:\n    matrix (list of list of float): The matrix to convert into RREF.\n    \n    Returns:\n    list of list of float: The RREF of the given matrix.\n    \"\"\"\n    mat = np.array(matrix, dtype=np.float64)\n    (rows, cols) = mat.shape\n    lead = 0\n    for r in range(rows):\n        if lead >= cols:\n            break\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        mat[[i, r]] = mat[[r, i]]\n        lead_coeff = mat[r, lead]\n        mat[r] = mat[r] / lead_coeff\n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        lead += 1\n    return mat.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm implementation.\n    \n    Parameters:\n    f: The objective function to be optimized.\n    grad: A function that computes the gradient of f.\n    x0: Initial parameter values.\n    learning_rate: The step size, default is 0.001.\n    beta1: Exponential decay rate for the first moment estimates, default is 0.9.\n    beta2: Exponential decay rate for the second moment estimates, default is 0.999.\n    epsilon: A small constant for numerical stability, default is 1e-8.\n    num_iterations: Number of iterations to run the optimizer, default is 1000.\n    \n    Returns:\n    The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0)\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x, 4).tolist()\nx0 = [1.0, 2.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (m, n) = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    prev_cost = float('inf')\n    for iter in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        dw = -2 / m * (np.dot(X.T, y - y_pred) + alpha * np.sign(weights))\n        db = -2 / m * np.sum(y - y_pred)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        cost = 1 / (2 * m) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    weights = np.round(weights, decimals=4)\n    bias = np.round(bias, decimals=4)\n    return (weights.tolist(), bias.tolist())"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    - source: The source string to be transformed.\n    - target: The target string to transform into.\n    \n    Returns:\n    - The OSA distance between the source and target strings.\n    \"\"\"\n    if len(source) > len(target):\n        (source, target) = (target, source)\n    distances = np.zeros((2, len(target) + 1), dtype=int)\n    for t in range(len(target) + 1):\n        distances[0][t] = t\n    for s in range(1, len(source) + 1):\n        distances[s % 2][0] = s\n        for t in range(1, len(target) + 1):\n            substitution_cost = 0 if source[s - 1] == target[t - 1] else 1\n            distances[s % 2][t] = min(distances[(s - 1) % 2][t] + 1, distances[s % 2][t - 1] + 1, distances[(s - 1) % 2][t - 1] + substitution_cost)\n            if s > 1 and t > 1 and (source[s - 1] == target[t - 2]) and (source[s - 2] == target[t - 1]):\n                distances[s % 2][t] = min(distances[s % 2][t], distances[(s - 2) % 2][t - 2] + 1)\n    return distances[len(source) % 2][len(target)]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification problem.\n\n    Parameters:\n    y_true (list of int): The true binary labels.\n    y_pred (list of int): The predicted binary labels.\n\n    Returns:\n    float: The recall value rounded to three decimal places. Returns 0.0 if the denominator is zero.\n    \"\"\"\n    true_positives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 1])\n    false_negatives = sum([1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 0])\n    if true_positives + false_negatives == 0:\n        return 0.0\n    recall_value = true_positives / (true_positives + false_negatives)\n    return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism on a batch of input data.\n    \n    Parameters:\n    X (numpy.ndarray): Input tensor of shape (batch_size, seq_length, embedding_dim).\n    W_q (numpy.ndarray): Query weight matrix of shape (embedding_dim, embedding_dim).\n    W_k (numpy.ndarray): Key weight matrix of shape (embedding_dim, embedding_dim).\n    W_v (numpy.ndarray): Value weight matrix of shape (embedding_dim, embedding_dim).\n\n    Returns:\n    numpy.ndarray: Output tensor of shape (batch_size, seq_length, embedding_dim) after applying self-attention.\n    \"\"\"\n    (batch_size, seq_length, embedding_dim) = X.shape\n    Q = np.einsum('ijk,kl->ijl', X, W_q)\n    K = np.einsum('ijk,kl->ijl', X, W_k)\n    V = np.einsum('ijk,kl->ijl', X, W_v)\n    attention_scores = np.einsum('ijk,ilk->ijl', Q, K) / np.sqrt(embedding_dim)\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    attention_output = np.einsum('ijk,ilk->ijl', attention_weights, V)\n    rounded_output = np.round(attention_output, 4)\n    return rounded_output.tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to compute the final hidden state.\n    \n    Args:\n    input_sequence: A list of input vectors for each time step.\n    initial_hidden_state: The initial hidden state.\n    Wx: The weight matrix for input-to-hidden connections.\n    Wh: The weight matrix for hidden-to-hidden connections.\n    b: The bias vector.\n    \n    Returns:\n    A list of floats representing the final hidden state after processing the entire input sequence.\n    \"\"\"\n    H = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        H = np.tanh(np.dot(x, Wx) + np.dot(H, Wh) + b)\n    return np.round(H, decimals=4).tolist()\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.1, 0.05], [0.05, 0.1]]\nb = [0.1, -0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n    \n    :param points: List of [x, y] coordinates\n    :param tx: Translation distance in the x direction\n    :param ty: Translation distance in the y direction\n    :return: Translated list of [x, y] coordinates\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones_column = np.ones(points_array.shape[0]).reshape(-1, 1)\n    points_homogeneous = np.hstack((points_array, ones_column))\n    translated_points_homogeneous = np.dot(translation_matrix, points_homogeneous.T).T\n    translated_points = translated_points_homogeneous[:, :-1].tolist()\n    return translated_points\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1.5\nty = -2.0\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution (P).\n    sigma_p (float): Standard deviation of the first normal distribution (P).\n    mu_q (float): Mean of the second normal distribution (Q).\n    sigma_q (float): Standard deviation of the second normal distribution (Q).\n    \n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    assert sigma_p > 0, 'sigma_p must be positive'\n    assert sigma_q > 0, 'sigma_q must be positive'\n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Apply the Gauss-Seidel iteration method to solve a system of linear equations.\n    \n    Parameters:\n    A (numpy.ndarray): The coefficient matrix of the system.\n    b (numpy.ndarray): The right-hand side vector.\n    n (int): Number of iterations to perform.\n    x_ini (numpy.ndarray or None): Initial guess for the solution vector. If not provided, a zero vector is used.\n    \n    Returns:\n    list: The approximated solution vector after n iterations.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be square.'\n    assert A.shape[0] == b.shape[0], 'The number of rows in A must match the size of b.'\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.double)\n    else:\n        x = np.array(x_ini, dtype=np.double)\n    for it_count in range(n):\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - s1 - s2) / A[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[10.0, -1.0, 2.0, 0.0], [-1.0, 11.0, -1.0, 3.0], [2.0, -1.0, 10.0, -1.0], [0.0, 3.0, -1.0, 8.0]])\nb = np.array([6.0, 25.0, -11.0, 15.0])\nn = 10\nx_ini = np.zeros_like(b)"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n        A (np.array): Coefficient matrix of the system.\n        b (np.array): Constants vector.\n        \n    Returns:\n        list: Solution vector x rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        pivot_row = np.argmax(abs(Ab[i:, i])) + i\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j] -= factor * Ab[i]\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, decimals=4).tolist()\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=np.float64)\nb = np.array([8, -11, -3], dtype=np.float64)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden = initial_hidden_state\n        for t in range(T):\n            xt_h = np.concatenate((x[t].reshape(-1, 1), hidden), axis=0)\n            forget_gate = self.sigmoid(self.Wf @ xt_h + self.bf)\n            input_gate = self.sigmoid(self.Wi @ xt_h + self.bi)\n            candidate_cell = self.tanh(self.Wc @ xt_h + self.bc)\n            output_gate = self.sigmoid(self.Wo @ xt_h + self.bo)\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell\n            hidden = output_gate * self.tanh(cell_state)\n            hidden_states.append(hidden.flatten())\n        final_hidden_state = hidden.flatten()\n        final_cell_state = cell_state.flatten()\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = final_hidden_state.round(4).tolist()\n        final_cell_state = final_cell_state.round(4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_size = 3\nhidden_size = 5\nx = [np.random.randn(input_size) for _ in range(10)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty.')\n    num_docs = len(corpus)\n    tf_idfs = []\n    for doc in corpus:\n        doc_tf_idfs = []\n        for term in query:\n            term_freq = doc.count(term) / len(doc) if len(doc) > 0 else 0\n            doc_freq = sum((1 for d in corpus if term in d))\n            if doc_freq == 0:\n                doc_freq = 1\n            idf = math.log((num_docs + 1) / doc_freq)\n            tf_idf = term_freq * idf\n            doc_tf_idfs.append(round(tf_idf, 4))\n        tf_idfs.append(doc_tf_idfs)\n    return np.array(tf_idfs).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'lazy', 'dog'], ['the', 'quick', 'dog']]\nquery = ['the', 'quick', 'dog']"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FP = np.sum((y_pred == 1) & (y_true == 0))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def dsigmoid(self, x):\n        return x * (1 - x)\n\n    def tanh_prime(self, x):\n        return 1 - x ** 2\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hiddens = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        inputs = []\n        for x in input_sequence:\n            x = np.reshape(x, (len(x), 1))\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, hiddens[-1]) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            hiddens.append(h)\n            outputs.append(y)\n            inputs.append(x)\n        return (outputs, inputs, hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, inputs, hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN with BPTT.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        d_h_next = np.zeros_like(hiddens[0])\n        loss = 0\n        for t in reversed(range(len(input_sequence))):\n            y = outputs[t]\n            x = inputs[t]\n            h = hiddens[t + 1]\n            loss += 0.5 * np.sum((y - np.reshape(expected_output[t], (len(y), 1))) ** 2)\n            dy = y - np.reshape(expected_output[t], (len(y), 1))\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + d_h_next\n            dh_raw = (1 - h * h) * dh\n            db_h += dh_raw\n            dW_hh += np.dot(dh_raw, hiddens[t].T)\n            dW_xh += np.dot(dh_raw, x.T)\n            d_h_next = np.dot(self.W_hh.T, dh_raw)\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -1, 1, out=dparam)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss\ninput_size = 5\nhidden_size = 10\noutput_size = 3\ninput_sequence = [np.random.rand(input_size) for _ in range(5)]\nexpected_output = [np.random.rand(output_size) for _ in range(5)]\nloss = rnn.backward(input_sequence, expected_output, outputs, inputs, hiddens, learning_rate=0.01)"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + rsnew / rsold * p\n        rsold = rsnew\n    return np.round(x, decimals=8).tolist()\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n    \n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (values, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini_imp = 1 - np.sum(probabilities ** 2)\n    return round(gini_imp, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        row_pointers.append(len(values))\n    return (values, column_indices, row_pointers[:-1])"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    vector_v = np.array(v)\n    vector_L = np.array(L)\n    scalar_projection = np.dot(vector_v, vector_L) / np.linalg.norm(vector_L)\n    vector_projection = scalar_projection * vector_L / np.linalg.norm(vector_L)\n    return [round(x, 3) for x in vector_projection]"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        raise ValueError('Empty matrix')\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 2, 0], [3, 0, 0], [0, 0, 4], [5, 0, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Given a matrix A, returns the basis vectors of its column space.\n    The basis vectors are extracted from the original matrix A and correspond to its independent columns.\n    The returned matrix is rounded to 8 decimal places.\n    \n    Args:\n    A: 2D numpy array representing the matrix.\n    \n    Returns:\n    A list of lists representing the matrix of basis vectors, rounded to 8 decimal places.\n    \"\"\"\n    A_np = np.array(A)\n    row_echelon = np.linalg.qr(A_np.T)[0].T\n    independent_cols = []\n    for col in range(row_echelon.shape[1]):\n        if np.linalg.norm(row_echelon[:, col]) > 1e-08:\n            independent_cols.append(col)\n    basis_vectors = A_np[:, independent_cols]\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    return basis_vectors_rounded"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value (coefficient of determination) for regression analysis.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented by a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): The grayscale image as a 2D matrix.\n    \n    Returns:\n    float: The average brightness rounded to two decimal places, or -1 if the image is invalid.\n    \"\"\"\n    if not img or not all(img):\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual and predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true target values.\n    y_pred (np.ndarray): Array of predicted target values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have different shapes or are empty.\n    TypeError: If inputs are not of type np.ndarray or the elements cannot be converted to float.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Inputs y_true and y_pred must be of type np.ndarray.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays y_true and y_pred must have the same shape.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays cannot be empty.')\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index, a measure of similarity between two binary sets.\n    \n    Parameters:\n    - y_true: An iterable of true binary labels.\n    - y_pred: An iterable of predicted binary labels.\n    \n    Returns:\n    - The Jaccard Index rounded to three decimal places as a float.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    :param y_true: numpy array of true binary labels.\n    :param y_pred: numpy array of predicted binary labels.\n    :return: Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    elements_y_true = np.sum(y_true)\n    elements_y_pred = np.sum(y_pred)\n    if elements_y_true + elements_y_pred == 0:\n        return 1.0\n    dice = 2 * intersection / (elements_y_true + elements_y_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_hypervector(length, seed):\n    \"\"\"\n    Generates a random hypervector of a given dimension with a specific seed for reproducibility.\n    \"\"\"\n    np.random.seed(seed)\n    hv = np.random.choice([-1, 1], size=length)\n    return hv\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using HDC.\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n    \n    :param row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    :param dim: The dimensionality of the hypervectors.\n    :param random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    :return: A composite hypervector representing the entire row as a list.\n    \"\"\"\n    composite_hv = np.zeros(dim)\n    for (feature, value) in row.items():\n        feature_hv = create_hypervector(dim, random_seeds[feature])\n        value_hv = create_hypervector(dim, hash((feature, value)))\n        bound_hv = np.multiply(feature_hv, value_hv)\n        composite_hv = np.add(composite_hv, bound_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification.\n\n    Args:\n    data (list of lists): A list where each element is a list [y_true, y_pred] representing\n                          the true and predicted labels for a single observation.\n\n    Returns:\n    list of lists: A 2x2 confusion matrix as [[TP, FP], [FN, TN]].\n    \"\"\"\n    cm = Counter()\n    for (y_true, y_pred) in data:\n        if y_true == 1 and y_pred == 1:\n            cm['TP'] += 1\n        elif y_true == 0 and y_pred == 1:\n            cm['FP'] += 1\n        elif y_true == 1 and y_pred == 0:\n            cm['FN'] += 1\n        elif y_true == 0 and y_pred == 0:\n            cm['TN'] += 1\n    TP = cm.get('TP', 0)\n    FP = cm.get('FP', 0)\n    FN = cm.get('FN', 0)\n    TN = cm.get('TN', 0)\n    return [[TP, FP], [FN, TN]]\ndata = [[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors v1 and v2.\n    \n    Parameters:\n    v1 (numpy.ndarray): The first input vector.\n    v2 (numpy.ndarray): The second input vector.\n    \n    Returns:\n    float: The cosine similarity rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Vectors must have the same shape')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude')\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    return round(cos_sim, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    \"\"\"\n    Calculate various performance metrics for a binary classification problem.\n    \"\"\"\n    assert len(actual) == len(predicted), 'Both lists must have the same length.'\n    assert all((x in [0, 1] for x in actual)), 'All elements in actual must be either 0 or 1.'\n    assert all((x in [0, 1] for x in predicted)), 'All elements in predicted must be either 0 or 1.'\n    counts = Counter(zip(actual, predicted))\n    tp = counts[1, 1]\n    tn = counts[0, 0]\n    fp = counts[0, 1]\n    fn = counts[1, 0]\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0\n    specificity = tn / (tn + fp) if tn + fp > 0 else 0\n    specificity = round(specificity, 3)\n    negative_predictive_value = tn / (tn + fn) if tn + fn > 0 else 0\n    negative_predictive_value = round(negative_predictive_value, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates and returns various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): A list or array of numerical values.\n    \n    Returns:\n    dict: A dictionary containing computed statistics.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data, ddof=1).round(4)\n    standard_deviation = np.std(data, ddof=1).round(4)\n    percentile_25 = np.percentile(data, 25).round(4)\n    percentile_50 = np.percentile(data, 50).round(4)\n    percentile_75 = np.percentile(data, 75).round(4)\n    iqr = (percentile_75 - percentile_25).round(4)\n    statistics_dict = {'mean': mean, 'median': median, 'mode': mode, 'variance': variance, 'standard_deviation': standard_deviation, '25th_percentile': percentile_25, '50th_percentile': percentile_50, '75th_percentile': percentile_75, 'interquartile_range': iqr}\n    return statistics_dict\ndata = np.random.randn(100)"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-(x - mean) ** 2 / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)\nk = 2\nlam = 3.5"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length.')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_np = np.array(data).reshape(-1, 1)\n    expanded_data = np.hstack([data_np ** d for d in range(1, degree + 1)])\n    expanded_data_rounded = [np.round(row, 8).tolist() for row in expanded_data]\n    return expanded_data_rounded"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encodings for a given sequence length and model dimensionality.\n    \n    Args:\n        position (int): The length of the sequence.\n        d_model (int): The dimensionality of the model.\n    \n    Returns:\n        list or int: A list of positional encodings if valid input, otherwise -1.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    for pos in range(position):\n        pe[pos, 0::2] = np.sin(pos * div_term)\n        pe[pos, 1::2] = np.cos(pos * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist())\nparameter = np.array([0.5])\ngrad = np.array([0.2])\nm = np.array([0.0])\nv = np.array([0.0])\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    context = encoder.encode(prompt)\n    for _ in range(n_tokens_to_generate):\n        context.append(np.random.choice([0, 1, 2]))\n    return encoder.decode(context)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    crystals = np.array(crystal_values).reshape(n, dimension)\n    attention_scores = np.dot(crystals, crystals.T)\n    attention_weights = softmax(attention_scores)\n    weighted_patterns = np.dot(attention_weights, crystals)\n    final_pattern = [round(num, 4) for num in weighted_patterns.flatten()]\n    return final_pattern\nn = 3\ndimension = 2"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given corpus of documents and a query.\n\n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :param k1: Term frequency saturation parameter. Default is 1.5.\n    :param b: Document length normalization parameter. Default is 0.75.\n    :return: A list of scores for each document in the corpus.\n    \"\"\"\n    if not corpus or not query:\n        return []\n    avgdl = sum((len(doc) for doc in corpus)) / len(corpus)\n    N = len(corpus)\n    idf = calculate_idf(corpus, query)\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query:\n            f_t_d = doc.count(term)\n            score += idf[term] * (f_t_d * (k1 + 1)) / (f_t_d + k1 * (1 - b + b * len(doc) / avgdl))\n        scores.append(round(score, 3))\n    return scores\ndef calculate_idf(corpus, query):\n    \"\"\"\n    Calculates the inverse document frequency for each term in the query from the corpus.\n\n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :return: A dictionary mapping each term in the query to its IDF value.\n    \"\"\"\n    term_in_doc_counts = {term: 0 for term in query}\n    for doc in corpus:\n        for term in query:\n            if term in doc:\n                term_in_doc_counts[term] += 1\n    idf = {term: np.log((N - term_in_doc_counts[term] + 0.5) / (term_in_doc_counts[term] + 0.5)) for term in query}\n    return idf\ncorpus = [['term1', 'term2', 'term3'], ['term1', 'term1', 'term4'], ['term2', 'term5']]\nquery = ['term1', 'term2']"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    TP = FP = FN = 0\n    unique_elements = set(y_true) | set(y_pred)\n    for elem in unique_elements:\n        TP += sum([1 for i in range(len(y_true)) if y_true[i] == elem and y_pred[i] == elem])\n        FP += sum([1 for i in range(len(y_true)) if y_true[i] != elem and y_pred[i] == elem])\n        FN += sum([1 for i in range(len(y_true)) if y_true[i] == elem and y_pred[i] != elem])\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 1, 0, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Remove the known fluctuation from each data point, fit a linear regression model to the detrended data,\n    predict day 15's base consumption, add back the fluctuation for day 15, and finally include a 5% safety margin.\n    :param consumption_data: A list of integers representing the power consumption of the first 10 days.\n    :return: An integer representing the predicted consumption for day 15 with a 5% safety margin.\n    \"\"\"\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([15]).reshape(-1, 1)\n    predicted_base_consumption = model.predict(day_15)\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = predicted_base_consumption + day_15_fluctuation\n    final_prediction = math.ceil(predicted_consumption * 1.05)\n    return final_prediction[0]\nconsumption_data = [100, 110, 115, 120, 130, 140, 150, 160, 170, 180]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('y_true and y_pred should be numpy.ndarray type')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape')\n    errors = np.abs(y_true - y_pred)\n    return round(np.mean(errors), 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the queries, keys, and values for multi-head attention.\n    \n    Parameters:\n    X: Input data of shape (n_tokens, d_model).\n    W_q, W_k, W_v: Weight matrices for queries, keys, and values, each of shape (d_model, d_model).\n\n    Returns:\n    Tuple of Q, K, V matrices of shape (n_tokens, d_model).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention given the queries, keys, and values.\n    \n    Parameters:\n    Q, K, V: Matrices of queries, keys, and values, each of shape (n_tokens, d_model).\n\n    Returns:\n    Output matrix of shape (n_tokens, d_model).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attn = np.dot(np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True), V)\n    return attn\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n    \n    Parameters:\n    X: Input data of shape (n_tokens, d_model).\n    W_q, W_k, W_v: Weight matrices for queries, keys, and values, each of shape (n_heads, d_model, d_model).\n    n_heads: Number of attention heads.\n\n    Returns:\n    List of shape (n_heads, n_tokens, d_model / n_heads) after rounding to the 4th decimal.\n    \"\"\"\n    (n_tokens, d_model) = X.shape\n    d_k = d_model // n_heads\n    results = []\n    for i in range(n_heads):\n        (Q, K, V) = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        Q = Q.reshape(n_tokens, n_heads, -1)[:, i, :]\n        K = K.reshape(n_tokens, n_heads, -1)[:, i, :]\n        V = V.reshape(n_tokens, n_heads, -1)[:, i, :]\n        attn = self_attention(Q, K, V)\n        results.append(attn.reshape(-1, d_k))\n    return np.stack(results, axis=0).round(4).tolist()\nX = np.random.rand(10, 512)\nW_q = np.random.rand(8, 512, 512)\nW_k = np.random.rand(8, 512, 512)\nW_v = np.random.rand(8, 512, 512)"}
{"task_id": 95, "completion_id": 0, "solution": "import numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_array = np.array(x)\n    y_array = np.array(y)\n    n11 = np.sum(x_array * y_array)\n    n10 = np.sum(x_array * (1 - y_array))\n    n01 = np.sum((1 - x_array) * y_array)\n    n00 = np.sum((1 - x_array) * (1 - y_array))\n    numerator = n11 * n00 - n10 * n01\n    denominator = np.sqrt((n10 + n11) * (n00 + n01) * (n10 + n00) * (n11 + n01))\n    if denominator == 0:\n        return 0.0\n    phi_coefficient = numerator / denominator\n    return round(phi_coefficient, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    grpo_val = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    kl_div = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_val -= beta * kl_div\n    return round(grpo_val, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    return round(x * sigmoid_x, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n\n    def sigmoid(z: np.ndarray) -> np.ndarray:\n        \"\"\"Compute the output of the sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n    logits = np.dot(X, weights) + bias\n    probabilities = sigmoid(logits)\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef softmax(Z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the softmax of the input array Z.\n    \"\"\"\n    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\ndef one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:\n    \"\"\"\n    Convert an array of class indices to a one-hot encoded matrix.\n    \"\"\"\n    y_one_hot = np.zeros((num_classes, y.size))\n    y_one_hot[y, np.arange(y.size)] = 1\n    return y_one_hot\ndef cross_entropy_loss(Y_pred: np.ndarray, Y_true: np.ndarray) -> float:\n    \"\"\"\n    Compute the cross entropy loss between predictions and ground-truth labels.\n    \"\"\"\n    epsilon = 1e-15\n    Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n    return -np.sum(Y_true * np.log(Y_pred)) / Y_pred.shape[1]\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    (n_features, n_samples) = X.shape\n    num_classes = len(np.unique(y))\n    B = np.random.rand(num_classes, n_features)\n    Y_hot = one_hot(y, num_classes)\n    losses = []\n    for _ in range(iterations):\n        Z = B @ X\n        Y_pred = softmax(Z)\n        loss = cross_entropy_loss(Y_pred, Y_hot)\n        losses.append(round(loss, 4))\n        grad = 1 / n_samples * (Y_pred - Y_hot) @ X.T\n        B -= learning_rate * grad\n    return (B.round(4).tolist(), [loss for loss in losses])"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function implementation.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    weights = np.zeros(X.shape[1])\n    loss_history = []\n    for _ in range(iterations):\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        gradient = np.dot(X.T, y_pred - y) / y.size\n        weights -= learning_rate * gradient\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_history.append(round(loss, 4))\n    optimized_coeffs = np.round(weights, 4).tolist()\n    return (optimized_coeffs, loss_history)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = np.where(mask, scores, -np.inf)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()\nmask = np.array([[True, False, False], [True, True, False], [True, True, True]])\noutput = masked_attention(Q, K, V, mask)"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is defined as the Shannon entropy of the distribution\n    of colors in the basket of apples.\n    \"\"\"\n    total_apples = len(apples)\n    if total_apples == 0:\n        return 0.0\n    color_counts = Counter(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * p.log2() for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n    \n    Parameters:\n    - X: A 3D array of shape (batch_size, sequence_length, feature_dim)\n    - gamma: A 1D array of scaling parameters of shape (feature_dim,)\n    - beta: A 1D array of shifting parameters of shape (feature_dim,)\n    - epsilon: A small constant for numerical stability\n    \n    Returns:\n    - A 3D array of the same shape as X, with layer normalization applied\n    \"\"\"\n    mean = np.mean(X, axis=2, keepdims=True)\n    var = np.var(X, axis=2, keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled = gamma * X_norm + beta\n    return np.round(X_scaled, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef compute_precision_recall(reference, candidate):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n    common_words = list((ref_counter & cand_counter).elements())\n    precision = len(common_words) / len(cand_words) if cand_words else 0\n    recall = len(common_words) / len(ref_words) if ref_words else 0\n    return (precision, recall)\ndef compute_fmean(precision, recall, beta=3):\n    if precision == 0 or recall == 0:\n        return 0\n    return (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\ndef compute_word_order_penalty(reference, candidate):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    common = set(ref_words) & set(cand_words)\n    ref_positions = {word: [i for (i, x) in enumerate(ref_words) if x == word] for word in common}\n    cand_positions = {word: [i for (i, x) in enumerate(cand_words) if x == word] for word in common}\n    distance_sum = 0\n    for word in common:\n        for (ref_pos, cand_pos) in zip(ref_positions[word], cand_positions[word]):\n            distance_sum += abs(ref_pos - cand_pos)\n    penalty = np.exp(-1 * gamma * distance_sum)\n    return penalty\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    (precision, recall) = compute_precision_recall(reference, candidate)\n    fmean = compute_fmean(precision, recall, beta)\n    penalty = compute_word_order_penalty(reference, candidate)\n    score = (1 - alpha) * precision + alpha * fmean * penalty\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) value for two events given their joint\n    and individual occurrence counts and the total number of samples.\n    \n    Args:\n    joint_counts (int): The number of times both events occurred together.\n    total_counts_x (int): The total number of occurrences of event X.\n    total_counts_y (int): The total number of occurrences of event Y.\n    total_samples (int): The total number of samples in the data.\n    \n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    if p_x == 0 or p_y == 0 or p_xy == 0:\n        return float('-inf')\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\njoint_counts = 10\ntotal_counts_x = 50\ntotal_counts_y = 40\ntotal_samples = 1000"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    This function scales all values to the range [0, 1] and returns a list of floats rounded to 4 decimal places.\n    \"\"\"\n    if len(x) < 2 or min(x) == max(x):\n        return [0.0] * len(x)\n    min_val = min(x)\n    max_val = max(x)\n    range_val = max_val - min_val\n    normalized_x = [(i - min_val) / range_val for i in x]\n    return [round(i, 4) for i in normalized_x]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Parameters:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weight array for first layer. Must be compatible with x for matrix multiplication.\n    w2 (np.ndarray): Weight array for second layer. Must be compatible with w1*x for matrix multiplication.\n    \n    Returns:\n    np.ndarray: The output of the residual block, as a list, rounded to 4 decimal places.\n    \"\"\"\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(z1, 0)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(z2, 0)\n    z_shortcut = a2 + x\n    output = np.maximum(z_shortcut, 0)\n    return np.round(output, 4).tolist()\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nw2 = np.array([[0.7, 0.8], [0.9, 1.0]])"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) containing the average of all\n                elements in each feature map.\n    \"\"\"\n    channels = x.shape[2]\n    return np.array([np.mean(x[:, :, c]) for c in range(channels)])"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n    \n    Parameters:\n    - X: A 4D numpy array of shape (N, C, H, W) representing a batch of feature maps.\n    - gamma: A 1D numpy array of shape (C,) representing the scale parameter per channel.\n    - beta: A 1D numpy array of shape (C,) representing the shift parameter per channel.\n    - epsilon: A small constant for numerical stability.\n    \n    Returns:\n    - A 4D numpy array with batch normalization applied.\n    \"\"\"\n    (N, C, H, W) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = gamma.reshape(1, C, 1, 1) * X_norm + beta.reshape(1, C, 1, 1)\n    return np.round(X_norm, decimals=4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term c * x^n at a point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which the derivative is evaluated.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: A list of 2D vectors (lists of floats).\n    - tol: A tolerance value to determine linear independence.\n    \n    Returns:\n    - A list of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n    matrix = np.array(vectors)\n    (q, r) = np.linalg.qr(matrix.T)\n    independent_columns = np.abs(np.diagonal(r)) > tol\n    orthonormal_vectors = q[:, independent_columns].T\n    return [np.round(vec, 4).tolist() for vec in orthonormal_vectors]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Calculate the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): A 3D vector.\n    b (list/tuple): A 3D vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    result = np.cross(a_np, b_np)\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()\nvector1 = (1.2, 3.5, 4.6)\nvector2 = (2.1, 1.3, 0.8)\nresult = cross_product(vector1, vector2)"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.ndarray): A square coefficient matrix.\n    b (np.ndarray): A constant vector.\n    \n    Returns:\n    list: The solution vector x as a list, or -1 if there is no unique solution.\n    \"\"\"\n    if A.shape[0] != A.shape[1]:\n        return -1\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(A.shape[1]):\n        A_i = np.copy(A)\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x.append(round(det_A_i / det_A, 4))\n    return np.array(x).tolist()\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\nb = np.array([1, -2, 0])"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two probability distributions p and q.\n    \n    Args:\n    p: A list of floats representing the first probability distribution.\n    q: A list of floats representing the second probability distribution.\n    \n    Returns:\n    A float representing the Bhattacharyya distance between p and q, rounded to 4 decimal places.\n    If p and q have different lengths or are empty, returns 0.0.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    bc = -np.log(np.sum(np.sqrt(np.multiply(p, q))))\n    return round(bc, 4)\np = [0.1, 0.6, 0.3]\nq = [0.2, 0.4, 0.4]"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Args:\n    a: A list of integers or floats representing the first vector.\n    b: A list of integers or floats representing the second vector.\n    \n    Returns:\n    A list representing the element-wise sum of the two vectors if they are of the same length,\n    or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    result = [i + j for (i, j) in zip(a, b)]\n    return result"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x: np.ndarray):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta)\n    for episode in episodes:\n        returns = 0\n        returns_list = []\n        for (_, _, reward) in reversed(episode):\n            returns = reward + 0.99 * returns\n            returns_list.append(returns)\n        returns_list.reverse()\n        for ((state, action, _), return_) in zip(episode, returns_list):\n            log_policy = np.log(softmax(theta[state, :]))\n            gradient[state, :] += softmax(theta[state, :]) * return_\n            gradient[state, action] -= return_\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()\ntheta = np.array([[0.1, 0.2, 0.3], [0.3, 0.2, 0.1]])\nepisodes = [[(0, 1, 1.0), (1, 2, 1.0)], [(0, 0, 0.0), (1, 1, 0.5)]]"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Args:\n    n_experts (int): Number of experts.\n    k_active (int): Number of active experts.\n    d_in (int): Dimension of input.\n    d_out (int): Dimension of output.\n    \n    Returns:\n    float: Percentage of computational cost savings.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe_experts = 2 * k_active * d_in * d_out\n    flops_moe_mixing = 2 * k_active * d_out\n    flops_moe = flops_moe_experts + flops_moe_mixing\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nk_active = 2\nd_in = 512\nd_out = 512"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    z = X @ W_g\n    z += N * W_noise\n    p = softmax(z)\n    topk_indices = np.argpartition(-p, k, axis=-1)[:, :k]\n    result = np.zeros_like(p)\n    for i in range(X.shape[0]):\n        result[i, topk_indices[i]] = p[i, topk_indices[i]]\n    row_sums = result.sum(axis=-1, keepdims=True)\n    result = np.nan_to_num(result / row_sums)\n    result = np.round(result, decimals=4)\n    return result.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    gating_probs = softmax(np.dot(x, Wg))\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    expert_outputs = np.dot(x[:, np.newaxis, :], We).squeeze()\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, np.newaxis], axis=1)\n    weighted_expert_outputs = top_k_probs_normalized[:, :, np.newaxis] * top_k_expert_outputs\n    final_outputs = np.sum(weighted_expert_outputs, axis=1)\n    return np.round(final_outputs, decimals=4).tolist()\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / np.sum(e_x, axis=-1, keepdims=True)"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    if C % num_groups != 0:\n        raise ValueError('Number of channels must be divisible by num_groups.')\n    G = num_groups\n    X_reshaped = X.reshape(B, G, -1)\n    mean = np.mean(X_reshaped, axis=2, keepdims=True)\n    var = np.var(X_reshaped, axis=2, keepdims=True)\n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_out = gamma * X_norm + beta\n    X_out_rounded = np.round(X_out, 4)\n    return X_out_rounded.tolist()\ngamma = np.ones(8)\nbeta = np.zeros(8)\nnum_groups = 2"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Implement gradient descent to find the minimum value of x for the function f(x) = x^4 - 3x^3 + 2.\n    \n    Parameters:\n    - start_x: The initial x value to start the search from.\n    - learning_rate: The step size to take during each iteration.\n    - tolerance: The threshold for determining convergence.\n    - max_iters: The maximum number of iterations to perform.\n    \n    Returns:\n    - The x value that minimizes the function, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * x ** 3 - 9 * x ** 2\n        x_new = x - learning_rate * gradient\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which simulates layer normalization in a Transformer architecture.\n    \n    Parameters:\n    - x: An input numpy array of shape (batch_size, hidden_size).\n    - alpha: A scaling factor that controls the slope of the function.\n    - gamma: A scaling factor that controls the width of the 'squashing' behavior.\n    - beta: A bias term that shifts the output along the value range.\n    \n    Returns:\n    - A list of floats where each element corresponds to the DyT transformed element of the input array, rounded to 4 decimal places.\n    \"\"\"\n    dyt = np.tanh(gamma * x + beta)\n    dyt = alpha * dyt\n    dyt_rounded = np.round(dyt, 4)\n    return dyt_rounded.tolist()"}
