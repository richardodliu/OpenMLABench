{"task_id": 1, "completion_id": 0, "completion": "```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float] | int:\n    # Check if the matrix and vector dimensions are compatible\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the resulting vector\n    result = []\n    \n    # Iterate over each row in the matrix\n    for row in a:\n        # Compute the dot product of the current row with the vector\n        dot_product = sum(x * y for x, y in zip(row, b))\n        # Append the result of the dot product to the result list\n        result.append(dot_product)\n    \n    return result\n\n# Check function to test the correctness of the matrix_dot_vector function\ndef check():\n    assert matrix_dot_vector([[1, 2, 3], [4, 5, 6]], [1, 2, 3]) == [14, 32]\n    assert matrix_dot_vector([[1, 2], [3, 4]], [1, 2]) == [5, 11]\n    assert matrix_dot_vector([], [1, 2, 3]) == -1\n    assert matrix_dot_vector([[1, 2, 3], [4, 5, 6]], [1, 2]) == -1\n    print(\"All checks passed successfully!\")\n\n# Run the check function to verify the solution\ncheck()\n```"}
{"task_id": 2, "completion_id": 0, "completion": "```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Computes the transpose of a given matrix.\n    \n    Parameters:\n    a (list of list of int/float): The input matrix.\n    \n    Returns:\n    list of list of int/float: The transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    expected_transpose = [[1, 4], [2, 5], [3, 6]]\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nTransposed Matrix:\")\n    for row in transpose_matrix(matrix):\n        print(row)\n    # Check function\n    assert transpose_matrix(matrix) == expected_transpose, \"The transpose function does not work as expected.\"\n    print(\"\\nThe transpose function works correctly with the provided data points.\")\n```"}
{"task_id": 3, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    \"\"\"\n    Reshapes the given 2D list (matrix) into the specified new shape if possible.\n    Returns the reshaped matrix as a Python list of lists.\n    If the reshaping is not possible (because the total number of elements does not match),\n    returns an empty list.\n    \n    :param a: The original 2D list (matrix) to reshape.\n    :param new_shape: A tuple specifying the new shape (rows, columns).\n    :return: The reshaped matrix as a Python list of lists, or an empty list if reshaping is not possible.\n    \"\"\"\n    # Convert the input list to a numpy array\n    a_array = np.array(a)\n    # Attempt to reshape the array to the new shape\n    try:\n        reshaped_array = np.reshape(a_array, new_shape)\n        # Convert the reshaped numpy array back to a Python list of lists\n        return reshaped_array.tolist()\n    except:\n        # If reshaping is not possible due to size mismatch, return an empty list\n        return []\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example matrix and new shape\n    matrix = [[1, 2], [3, 4]]\n    new_shape = (4, 1)\n    # Reshape the matrix\n    reshaped_matrix = reshape_matrix(matrix, new_shape)\n    # Print the result\n    print(reshaped_matrix)\n```"}
{"task_id": 4, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    Args:\n    matrix: A list of lists of floats, where each inner list represents a row of the matrix.\n    mode: A string that specifies the calculation mode: 'row' for row-wise mean or 'column' for column-wise mean.\n    \n    Returns:\n    A list of means either for each row or each column, depending on the mode.\n    \"\"\"\n    # Convert the list of lists into a numpy array\n    np_matrix = np.array(matrix)\n    \n    if mode == 'row':\n        # Calculate the mean along the columns (axis=1) for each row\n        means = np_matrix.mean(axis=1)\n    elif mode == 'column':\n        # Calculate the mean along the rows (axis=0) for each column\n        means = np_matrix.mean(axis=0)\n    else:\n        raise ValueError(\"Invalid mode. Use 'row' or 'column'.\")\n    \n    return list(means)\n\n# Check function with provided data points\nmatrix_example = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(calculate_matrix_mean(matrix_example, 'row'))  # Expected output: [2.0, 5.0, 8.0]\nprint(calculate_matrix_mean(matrix_example, 'column'))  # Expected output: [4.0, 5.0, 6.0]\n```"}
{"task_id": 5, "completion_id": 0, "completion": "```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element in the given matrix by the scalar value.\n    \n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar value.\n    :return: A 2D list of the same size as the matrix, with each element multiplied by the scalar.\n    \"\"\"\n    return [[elem * scalar for elem in row] for row in matrix]\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define a matrix and a scalar\n    matrix = [[1, 2, 3], [4, 5, 6]]\n    scalar = 2\n    \n    # Call the function and print the result\n    result = scalar_multiply(matrix, scalar)\n    print(\"Original Matrix:\")\n    for row in matrix:\n        print(row)\n    print(\"\\nScalar Multiplied Matrix:\")\n    for row in result:\n        print(row)\n```"}
{"task_id": 6, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate and return the eigenvalues of a 2x2 matrix in a sorted list from highest to lowest.\n    \n    Args:\n    - matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    - list of float: A sorted list of eigenvalues from highest to lowest.\n    \"\"\"\n    # Convert the list of lists to a numpy array\n    np_matrix = np.array(matrix)\n    # Calculate the eigenvalues using numpy's linalg.eig function\n    eigenvalues = np.linalg.eig(np_matrix)[0]\n    # Sort the eigenvalues in descending order and return them as a list\n    return sorted(eigenvalues, reverse=True)\n\n# Test the function\nif __name__ == \"__main__\":\n    test_matrix = [[4, 2], [1, 3]]\n    print(calculate_eigenvalues(test_matrix))  # Expected to print the sorted eigenvalues of the test_matrix\n```"}
{"task_id": 7, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS, where T and S are invertible matrices.\n    If T or S are not invertible, returns -1.\n    \"\"\"\n    try:\n        # Convert lists to numpy arrays\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        \n        # Validate if T and S are invertible\n        np.linalg.inv(T_np)\n        np.linalg.inv(S_np)\n        \n        # Perform the transformation\n        T_inv = np.linalg.inv(T_np)\n        S_inv = np.linalg.inv(S_np)\n        transformed_matrix = T_inv @ A_np @ S_inv\n        \n        # Round to the nearest 4th decimal and return as a list\n        return np.round(transformed_matrix, 4).tolist()\n    \n    except np.linalg.LinAlgError:\n        # Return -1 if the matrices are not invertible\n        return -1\n\n# Example use case\nA = [[1, 2], [3, 4]]\nT = [[2, 3], [1, 2]]\nS = [[4, 3], [2, 1]]\n\nprint(transform_matrix(A, T, S))\n```\n\nThis code defines the `transform_matrix` function which performs the transformation as described, and also includes an example of how to use the function. If the input matrices T or S are not invertible, the function returns -1."}
{"task_id": 8, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the inverse of a 2x2 matrix if it is invertible.\n    \n    Args:\n    - matrix (list[list[float]]): a 2x2 matrix represented as a nested list\n    \n    Returns:\n    - list[list[float]]: the inverse of the given matrix, or None if the matrix is not invertible\n    \"\"\"\n    # Extracting elements from the matrix\n    a, b, c, d = matrix[0][0], matrix[0][1], matrix[1][0], matrix[1][1]\n    \n    # Calculating the determinant\n    det = a * d - b * c\n    \n    # Checking if the matrix is invertible\n    if det == 0:\n        return None\n    \n    # Creating the inverse matrix\n    inverse_matrix = [[d / det, -b / det], [-c / det, a / det]]\n    \n    return inverse_matrix\n\n# Check function to test inverse_2x2 with provided data points\ndef check_inverse_2x2():\n    # Test invertible matrix\n    matrix1 = [[4, 7], [2, 6]]\n    expected1 = [[0.6, -0.7], [-0.2, 0.4]]\n    assert inverse_2x2(matrix1) == expected1, f\"Inverse of {matrix1} should be {expected1}\"\n    \n    # Test non-invertible matrix\n    matrix2 = [[1, 2], [2, 4]]\n    assert inverse_2x2(matrix2) is None, f\"Matrix {matrix2} is not invertible\"\n\n    print(\"All checks passed.\")\n\n# Running the check function\ncheck_inverse_2x2()\n```\n\nNote: This script does not require the `numpy` package and can be run in any Python environment capable of executing standard Python scripts. It includes a check function with provided data points to verify the correctness of the `inverse_2x2` function."}
{"task_id": 9, "completion_id": 0, "completion": "```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies two matrices a and b.\n    \n    Args:\n    a (list of list of int | float): First matrix.\n    b (list of list of int | float): Second matrix.\n    \n    Returns:\n    list of list of int | float: Resulting matrix of the multiplication or -1 if the matrices cannot be multiplied.\n    \"\"\"\n    rows_a = len(a)\n    cols_a = len(a[0])\n    rows_b = len(b)\n    cols_b = len(b[0])\n    \n    if cols_a != rows_b:\n        return -1\n    \n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    \n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):  # cols_a is the same as rows_b\n                result[i][j] += a[i][k] * b[k][j]\n                \n    return result\n\n# Check function to verify the correctness of the solution\ndef check(matrixmul_func):\n    # Test cases\n    test_cases = [\n        ([[1, 2], [3, 4]], [[2, 0], [1, 2]], [[4, 4], [10, 8]]),\n        ([[1]], [[2]], [[2]]),\n        ([[1, 2], [3, 4]], [[5, 6], [7, 8], [9, 10]], -1),\n        ([[1, 2]], [[3, 4, 5]], -1)\n    ]\n    \n    for a, b, expected in test_cases:\n        assert matrixmul_func(a, b) == expected, f\"Failed test with input a={a}, b={b}\"\n    print(\"All test cases passed.\")\n\n# Run the check function\ncheck(matrixmul)\n```"}
{"task_id": 10, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list of list of float): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list of list of float: A covariance matrix.\n    \"\"\"\n    # Convert the list of lists into a numpy array for easier manipulation\n    vectors_np = np.array(vectors)\n    \n    # Calculate the covariance matrix using numpy's cov function\n    # rowvar=False indicates that each column represents a variable (feature) with observations in the rows\n    covariance_matrix = np.cov(vectors_np, rowvar=False)\n    \n    # Convert the numpy array back to a list of lists for the output\n    covariance_matrix_list = covariance_matrix.tolist()\n    \n    return covariance_matrix_list\n\n# Test cases to verify the correctness of the function\ndef check_function():\n    test_vectors = [\n        [1, 2, 3, 4],\n        [2, 3, 4, 5],\n        [3, 4, 5, 6]\n    ]\n    expected_result = [\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25]\n    ]\n    result = calculate_covariance_matrix(test_vectors)\n    # We'll round the results since numpy's computation might introduce some floating point errors\n    rounded_result = [[round(val, 2) for val in row] for row in result]\n    assert rounded_result == expected_result, f\"Test failed: expected {expected_result}, got {rounded_result}\"\n    \n    print(\"All tests passed!\")\n\n# Run the test cases\ncheck_function()\n```\n\nThis provided solution defines a Python function `calculate_covariance_matrix` that calculates the covariance matrix for a given set of vectors. The test function `check_function` ensures the implemented function works correctly by comparing the output against an expected result."}
{"task_id": 11, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    A : np.ndarray\n        The coefficient matrix of the system.\n    b : np.ndarray\n        The constant matrix of the system.\n    n : int\n        The number of iterations for the Jacobi method.\n    \n    Returns:\n    list\n        The approximate solution of the system, rounded to the 4th decimal place.\n    \"\"\"\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be a square matrix.\")\n    \n    # Initialize the solution vector with zeros\n    x = np.zeros_like(b)\n    \n    # Iterate n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        # Rounding all elements to four decimal places\n        x = np.round(x, decimals=4)\n    \n    # Convert numpy array to list and return\n    return x.tolist()\n\n# Example usage\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 25\nprint(solve_jacobi(A, b, n))\n```\n\nThis python script implements the Jacobi method to approximate the solution of a system of linear equations, given the coefficient matrix A and the constant matrix b. The approximation process is iterated n times, and each approximated value is rounded to the fourth decimal place before returning the final solution in the form of a python list."}
{"task_id": 12, "completion_id": 0, "completion": "To approximate the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method without relying on the `numpy.linalg.svd` function, we must outline the steps for the algorithm. The SVD of a matrix \\( A \\) is given by \\( A = U \\Sigma V^T \\), where \\( U \\) and \\( V \\) are orthogonal matrices, and \\( \\Sigma \\) is a diagonal matrix with the singular values on the diagonal. The singular values are the square roots of the eigenvalues of \\( A^T A \\).\n\nThe Jacobian method can be complex for this context, so we're focusing on a simpler approach to find the singular values directly, which are needed here. We will calculate the eigenvalues of the matrix \\( A^T A \\) to get the singular values since the eigenvalues of \\( A^T A \\) are the squares of the singular values.\n\nHere is a self-contained Python script to fulfill your request:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximate the singular values of a 2x2 matrix using the properties of SVD.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 numpy array.\n    \n    Returns:\n    tuple: A tuple containing the two singular values, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Compute A^T A\n    ATA = A.T @ A\n    \n    # Get the eigenvalues of A^T A\n    eigenvalues = np.linalg.eigvals(ATA)\n    \n    # The singular values are the square roots of the eigenvalues\n    singular_values = np.sqrt(eigenvalues)\n    \n    # Round to the nearest 4th decimal and sort in descending order\n    singular_values = np.round(singular_values, 4)\n    singular_values.sort()\n    singular_values = singular_values[::-1]\n    \n    return tuple(singular_values)\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nsigma1, sigma2 = svd_2x2_singular_values(A)\nprint(f\"Singular values: {sigma1}, {sigma2}\")\n```\n\nThis script defines a function `svd_2x2_singular_values` that calculates and returns the singular values of a 2x2 matrix. The singular values are computed from the eigenvalues of \\( A^T A \\), as the square roots of those eigenvalues. The result is rounded to the nearest 4th decimal to meet the output constraints."}
{"task_id": 13, "completion_id": 0, "completion": "Below is the self-contained Python script that solves the given task of calculating the determinant of a 4x4 matrix using Laplace's Expansion method:\n\n```python\nfrom typing import List, Union\n\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    \"\"\"\n    Calculates the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n    def determinant_3x3(submatrix: List[List[Union[int, float]]]) -> float:\n        \"\"\"\n        Calculates the determinant of a 3x3 matrix.\n        \n        :param submatrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the 3x3 matrix.\n        \"\"\"\n        return submatrix[0][0] * (submatrix[1][1] * submatrix[2][2] - submatrix[1][2] * submatrix[2][1]) - \\\n               submatrix[0][1] * (submatrix[1][0] * submatrix[2][2] - submatrix[1][2] * submatrix[2][0]) + \\\n               submatrix[0][2] * (submatrix[1][0] * submatrix[2][1] - submatrix[1][1] * submatrix[2][0])\n    \n    # Laplace's Expansion for 4x4 matrix:\n    det = 0\n    for column in range(4):\n        # Calculate the minor matrix by removing the first row and the current column\n        minor = [row[:column] + row[column+1:] for row in matrix[1:]]\n        # Calculate the determinant of the minor matrix\n        minor_det = determinant_3x3(minor)\n        # Add the contribution of the current element to the determinant\n        det += ((-1) ** (0 + column)) * matrix[0][column] * minor_det\n    \n    return det\n\n# Example usage:\nmatrix_4x4 = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\nprint(determinant_4x4(matrix_4x4))\n```\n\nThis script includes a helper function `determinant_3x3` to compute the determinant of any 3x3 submatrix, which is recursively called from the main function `determinant_4x4`. The main function first initializes the determinant to `0`, then iterates through each element of the first row, computes the minor matrix, and calculates the determinant of this minor, and adds it to the overall determinant with the appropriate sign based on the Laplace's Expansion formula."}
{"task_id": 14, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    Performs linear regression using the normal equation and returns model coefficients.\n    Coefficients are rounded up to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for easier manipulation\n    X_np = np.array(X)\n    y_np = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X_np = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    \n    # Calculate the coefficients using the normal equation\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    \n    # Round the coefficients to 4 decimal places\n    theta_rounded = np.round(theta, decimals=4)\n    \n    # Convert numpy array back to list for output\n    return theta_rounded.tolist()\n\n# Example usage:\nX_example = [[1, 1], [1, 2], [2, 2], [2, 3]]\ny_example = [6, 8, 9, 11]\nprint(linear_regression_normal_equation(X_example, y_example))\n```\n\nThis Python script defines a function `linear_regression_normal_equation` which takes a matrix `X` and a vector `y` as input. The function performs linear regression using the normal equation to calculate the model coefficients and returns them as a list of floats, rounded to four decimal places. The example usage provided demonstrates how to call this function with a simple dataset."}
{"task_id": 15, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input features including a column of ones for the intercept.\n    y (np.ndarray): The target values.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to run the gradient descent.\n    \n    Returns:\n    list: The list of coefficients for the linear regression model, rounded to 4 decimal places.\n    \"\"\"\n    # Initialize coefficients to zero\n    coefficients = np.zeros(X.shape[1])\n    \n    # Perform gradient descent\n    for _ in range(iterations):\n        # Predicted values\n        y_pred = np.dot(X, coefficients)\n        # Errors\n        error = y_pred - y\n        # Gradient of the cost function\n        gradient = np.dot(X.T, error) / len(y)\n        # Update coefficients\n        coefficients -= alpha * gradient\n    \n    # Round coefficients to 4 decimal places and convert to list\n    rounded_coefficients = np.round(coefficients, 4).tolist()\n    \n    return rounded_coefficients\n\n# Example usage\nif __name__ == \"__main__\":\n    np.random.seed(0)  # For reproducibility\n    X = np.random.rand(100, 1)\n    y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5  # y = 3x + 2 + noise\n    \n    # Add intercept term\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # Define parameters for gradient descent\n    alpha = 0.1\n    iterations = 1000\n    \n    # Perform linear regression\n    coefficients = linear_regression_gradient_descent(X, y, alpha, iterations)\n    print(\"Coefficients:\", coefficients)\n```\n\nThis script defines a function `linear_regression_gradient_descent` that performs linear regression using gradient descent. It takes as input the feature matrix `X`, target values `y`, a learning rate `alpha`, and the number of iterations. The function returns the model coefficients rounded to four decimal places. An example usage of the function is also provided, demonstrating how to perform linear regression on a synthetic dataset."}
{"task_id": 16, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Takes a 2D NumPy array and performs both standardization and min-max normalization.\n    Returns two lists: the first is data standardized, the second is data normalized.\n    \"\"\"\n    # Standardization (subtract mean and divide by standard deviation)\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized = (data - mean) / std\n    \n    # Min-Max Normalization (subtract min and divide by max-min)\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    normalized = (data - min_val) / (max_val - min_val)\n    \n    # Rounding to the nearest 4th decimal and converting to list\n    standardized_list = np.round(standardized, 4).tolist()\n    normalized_list = np.round(normalized, 4).tolist()\n    \n    return standardized_list, normalized_list\n\n# Example usage\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nstandardized_data, normalized_data = feature_scaling(data)\nprint(\"Standardized Data:\", standardized_data)\nprint(\"Normalized Data:\", normalized_data)\n```\n\nThis code provides a self-contained Python script that implements feature scaling on a given dataset using both standardization and min-max normalization, adhering to the specified guidelines and constraints."}
{"task_id": 17, "completion_id": 0, "completion": "To accomplish the task of implementing the k-Means clustering algorithm in Python, we need to follow several steps: initialization, assignment, update, and repetition until either convergence or the maximum number of iterations is reached. Below is the self-contained Python script that fulfills the requirements:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and initial centroids to numpy arrays for ease of calculation\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n\n    for iteration in range(max_iterations):\n        # Step 1: Assign each point to the closest centroid\n        distances = np.sqrt(((points - centroids[:, np.newaxis]) ** 2).sum(axis=2))\n        closest_centroid_idx = np.argmin(distances, axis=0)\n\n        # Step 2: Recalculate the centroids as the mean of all points assigned to each cluster\n        new_centroids = np.array([points[closest_centroid_idx == i].mean(axis=0) if np.any(closest_centroid_idx == i) else centroids[i] for i in range(k)])\n\n        # If centroids have not changed, terminate early\n        if np.array_equal(centroids, new_centroids):\n            break\n        else:\n            centroids = new_centroids\n\n    # Round the final centroids to the closest 4th decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids]\n    return final_centroids\n\n# Example usage\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nprint(k_means_clustering(points, k, initial_centroids, max_iterations))\n```\n\nThe script defines the `k_means_clustering` function, which takes `points`, `k`, `initial_centroids`, and `max_iterations` as inputs and returns the list of final centroids, each rounded to the nearest 4th decimal. The example usage at the end demonstrates how to test the function with a set of points and parameters."}
{"task_id": 18, "completion_id": 0, "completion": "Here is a self-contained Python script that implements K-Fold Cross-Validation as per the given task:\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): Input data.\n    y (np.ndarray): Targets for the input data.\n    k (int): Number of folds. Default is 5.\n    shuffle (bool): Whether to shuffle the data before splitting into batches. Default is True.\n    random_seed (int): Seed for the random number generator. Default is None.\n\n    Returns:\n    list of tuples: A list containing tuples of train and test indices for each fold.\n    \"\"\"\n    n_samples = X.shape[0]\n    if shuffle:\n        rng = np.random.default_rng(random_seed)\n        permutation = rng.permutation(n_samples)\n        X = X[permutation]\n        y = y[permutation]\n\n    fold_size = n_samples // k\n    indices = np.arange(n_samples)\n    folds = []\n\n    for i in range(k):\n        test_start = i * fold_size\n        test_end = test_start + fold_size\n        if i == k - 1:\n            test_end = n_samples  # Ensure the last fold includes all remaining samples\n        test_indices = indices[test_start:test_end]\n        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n        folds.append((train_indices, test_indices))\n\n    return folds\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n    y = np.array([0, 1, 0, 1, 0, 1])\n    folds = k_fold_cross_validation(X, y, k=3)\n    for fold_idx, (train_indices, test_indices) in enumerate(folds):\n        print(f\"Fold {fold_idx + 1}:\")\n        print(\"Train indices:\", train_indices)\n        print(\"Test indices:\", test_indices)\n        print()\n```\n\nThis script defines a function `k_fold_cross_validation` that takes input data `X`, target labels `y`, and parameters for the number of folds `k`, shuffling, and a random seed. It returns a list of tuples containing the train and test indices for each fold. The example usage shows how to call the function with a simple dataset and print out the train and test indices for each fold."}
{"task_id": 19, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the given data.\n    \n    Parameters:\n    - data: A 2D NumPy array where each row is a data sample and each column is a feature.\n    - k: The number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the top k principal components.\n    \"\"\"\n    # Standardize the dataset\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Find the eigenvalues and eigenvectors of the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    \n    # Select the first k eigenvectors (principal components)\n    principal_components = sorted_eigenvectors[:, :k]\n    \n    # Convert the principal components to a list of lists and round to 4 decimal places\n    result = [list(map(lambda x: round(x, 4), principal_components[:, i])) for i in range(k)]\n    \n    return result\n\n# Example usage and verification\nif __name__ == \"__main__\":\n    # Generate a random dataset for demonstration\n    np.random.seed(0)  # For reproducibility\n    data = np.random.rand(100, 5)  # 100 samples, 5 features each\n    \n    # Apply PCA to the dataset, reducing it to 2 principal components\n    principal_components = pca(data, 2)\n    \n    # Print the principal components\n    print(\"Principal Components:\")\n    for pc in principal_components:\n        print(pc)\n```\n\nThis code defines a `pca` function that standardizes the input data, computes the covariance matrix, and finds the eigenvalues and eigenvectors. It then sorts these eigenvalues and corresponding eigenvectors in descending order and selects the top `k` eigenvectors to represent the principal components. The principal components are returned as a list of lists, with each list rounded to the nearest 4th decimal."}
{"task_id": 20, "completion_id": 0, "completion": "Below is a self-contained Python script that implements the decision tree learning algorithm using recursive binary splitting based on entropy and information gain.\n\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(examples, target_attr):\n    \"\"\"\n    Calculate the entropy of the examples based on the target attribute.\n    \"\"\"\n    examples_count = len(examples)\n    if examples_count == 0:\n        return 0\n    target_values = [example[target_attr] for example in examples]\n    target_counts = Counter(target_values)\n    entropy_value = 0\n    for count in target_counts.values():\n        probability = count / examples_count\n        entropy_value -= probability * math.log2(probability)\n    return entropy_value\n\ndef information_gain(examples, attribute, target_attr):\n    \"\"\"\n    Calculate the information gain of a specific attribute.\n    \"\"\"\n    total_entropy = entropy(examples, target_attr)\n    examples_count = len(examples)\n    attribute_values = set([example[attribute] for example in examples])\n    weighted_entropy = 0\n    for value in attribute_values:\n        subset = [example for example in examples if example[attribute] == value]\n        subset_entropy = entropy(subset, target_attr)\n        probability = len(subset) / examples_count\n        weighted_entropy += probability * subset_entropy\n    return total_entropy - weighted_entropy\n\ndef majority_value(examples, target_attr):\n    \"\"\"\n    Find the most common target value in the examples.\n    \"\"\"\n    target_values = [example[target_attr] for example in examples]\n    target_counts = Counter(target_values)\n    return target_counts.most_common(1)[0][0]\n\ndef choose_best_attribute(examples, attributes, target_attr):\n    \"\"\"\n    Select the attribute with the maximum information gain.\n    \"\"\"\n    if not attributes:\n        return None\n    gains = {attribute: information_gain(examples, attribute, target_attr) for attribute in attributes}\n    best_attribute = max(gains, key=gains.get)\n    return best_attribute\n\ndef get_sub_examples(examples, attribute, value, target_attr):\n    \"\"\"\n    Filter examples based on the attribute and value.\n    \"\"\"\n    return [example for example in examples if example[attribute] == value]\n\ndef learn_decision_tree(examples, attributes, target_attr):\n    \"\"\"\n    Implement the decision tree learning algorithm.\n    \"\"\"\n    if not examples:\n        return majority_value(examples, target_attr)\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attribute = choose_best_attribute(examples, attributes, target_attr)\n    if not best_attribute:\n        return majority_value(examples, target_attr)\n    tree = {best_attribute: {}}\n    attribute_values = set([example[best_attribute] for example in examples])\n    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n    for value in attribute_values:\n        sub_examples = get_sub_examples(examples, best_attribute, value, target_attr)\n        subtree = learn_decision_tree(sub_examples, remaining_attributes, target_attr)\n        tree[best_attribute][value] = subtree\n    return tree\n\n# Example usage\nexamples = [\n    {\"Age\": \"Young\", \"Income\": \"High\", \"Student\": \"No\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"No\"},\n    {\"Age\": \"Young\", \"Income\": \"High\", \"Student\": \"No\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"No\"},\n    {\"Age\": \"Middle_Aged\", \"Income\": \"High\", \"Student\": \"No\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Senior\", \"Income\": \"Medium\", \"Student\": \"No\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Senior\", \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Senior\", \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"No\"},\n    {\"Age\": \"Middle_Aged\", \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Young\", \"Income\": \"Medium\", \"Student\": \"No\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"No\"},\n    {\"Age\": \"Young\", \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Senior\", \"Income\": \"Medium\", \"Student\": \"Yes\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Young\", \"Income\": \"Medium\", \"Student\": \"Yes\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Middle_Aged\", \"Income\": \"Medium\", \"Student\": \"No\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Middle_Aged\", \"Income\": \"High\", \"Student\": \"Yes\", \"Credit_Rating\": \"Fair\", \"Buys_Computer\": \"Yes\"},\n    {\"Age\": \"Senior\", \"Income\": \"Medium\", \"Student\": \"No\", \"Credit_Rating\": \"Excellent\", \"Buys_Computer\": \"Yes\"}\n]\n\nattributes = [\"Age\", \"Income\", \"Student\", \"Credit_Rating\"]\ntarget_attr = \"Buys_Computer\"\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```\n\nThis script defines a function `learn_decision_tree` that takes a list of examples (each example is a dictionary of attribute-value pairs), a list of attribute names, and a target attribute name, and returns a nested dictionary representing the decision tree. The script includes helper functions to calculate entropy, information gain, and select the best attribute for splitting. An example usage of the function is also provided."}
{"task_id": 21, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_linear(x1, x2):\n    return np.dot(x1, x2)\n\ndef kernel_rbf(x1, x2, sigma=1.0):\n    return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * (sigma ** 2)))\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples, n_features = data.shape\n    alphas = np.zeros(n_samples)\n    b = 0.0\n    kernel_func = kernel_linear if kernel == 'linear' else lambda x, y: kernel_rbf(x, y, sigma)\n    \n    for t in range(1, iterations + 1):\n        lambda_t = 1 / (lambda_val * t)\n        for i in range(n_samples):\n            sum_kernels = np.sum(alphas * labels * np.array([kernel_func(data[j], data[i]) for j in range(n_samples)]))\n            if labels[i] * (sum_kernels + b) < 1:\n                alphas[i] = (1 - lambda_t * lambda_val) * alphas[i] + lambda_t * labels[i]\n                b += lambda_t * (1 - labels[i] * (sum_kernels + b))\n            else:\n                alphas[i] = (1 - lambda_t * lambda_val) * alphas[i]\n    \n    # Calculate bias again after training to get the last value\n    b = 0\n    for i in range(n_samples):\n        sum_kernels = np.sum(alphas * labels * np.array([kernel_func(data[j], data[i]) for j in range(n_samples)]))\n        b += labels[i] - sum_kernels\n    b /= n_samples\n    \n    return alphas.tolist(), round(b, 4)\n\n# Example usage\ndata = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\nlabels = np.array([1, 1, -1, -1])\nalphas, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\nprint(f\"Alpha coefficients: {alphas}\")\nprint(f\"Bias: {bias}\")\n```\n\nThe given script includes a deterministic version of the Pegasos algorithm for training a kernel SVM. The kernel can be selected as either linear or RBF (Radial Basis Function), and the script calculates and returns the alpha coefficients and bias for the trained model. The results are rounded and returned as required."}
{"task_id": 22, "completion_id": 0, "completion": "```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the sigmoid activation function for a given input.\n    \n    Args:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    return round(1 / (1 + math.exp(-z)), 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    z_value = 0.0\n    output = sigmoid(z_value)\n    print(f\"The sigmoid of {z_value} is: {output}\")\n```"}
{"task_id": 23, "completion_id": 0, "completion": "```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    This function takes a list of scores and computes the softmax activation for each score.\n    The softmax function is defined as exp(x)/sum(exp(x)) for each element in the input list.\n    The result is a probability distribution over the scores.\n    \"\"\"\n    # Compute the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Sum up the exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Compute the softmax for each score by dividing each exponential by the sum of exponentials\n    softmax_scores = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_scores\n\n# Test the function with some example scores\nexample_scores = [1.0, 2.0, 3.0]\nprint(softmax(example_scores))\n```\n\nThis solution defines the `softmax` function which correctly computes the softmax values for a list of scores and rounds each value to four decimal places as requested."}
{"task_id": 24, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    This function takes feature vectors, binary labels, and neuron's parameters to return\n    predicted probabilities and mean squared error.\n    \n    :param features: A list of lists, where each sub-list represents a feature vector.\n    :param labels: A list of binary labels (0 or 1).\n    :param weights: A list of weights corresponding to each feature.\n    :param bias: A float representing the bias.\n    :return: A tuple containing a list of predicted probabilities and the mean squared error.\n    \"\"\"\n    # Convert features and labels to numpy arrays for easier manipulation\n    features_np = np.array(features)\n    labels_np = np.array(labels)\n    \n    # Calculate the dot product of features and weights, and add bias\n    z = np.dot(features_np, weights) + bias\n    \n    # Apply sigmoid activation function to calculate predicted probabilities\n    predictions = 1 / (1 + np.exp(-z))\n    \n    # Calculate mean squared error\n    mse = np.mean((predictions - labels_np) ** 2)\n    \n    # Round results\n    predictions_rounded = [round(pred, 4) for pred in predictions.tolist()]\n    mse_rounded = round(mse, 4)\n    \n    return predictions_rounded, mse_rounded\n\n# Example usage:\nfeatures = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\nlabels = [0, 1, 1]\nweights = [0.4, 0.5]\nbias = 0.6\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(f\"Predicted Probabilities: {predictions}\")\nprint(f\"Mean Squared Error: {mse}\")\n```\n\nThis solution follows the provided guidelines, including everything from the function definition to an example usage of the provided function, where the output is rounded to four decimal places as required."}
{"task_id": 25, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred) ** 2)\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_losses = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        \n        # Compute loss\n        mse_loss = mse(labels, predictions)\n        mse_losses.append(round(mse_loss, 4))\n        \n        # Backward pass\n        error = labels - predictions\n        adjustments = error * sigmoid_derivative(predictions)\n        \n        # Update weights and bias\n        weights += learning_rate * np.dot(features.T, adjustments)\n        bias += learning_rate * np.sum(adjustments)\n    \n    return weights.tolist(), round(bias, 4), mse_losses\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([[0], [1], [1], [0]])\ninitial_weights = np.array([[0.5], [0.5]])\ninitial_bias = 0.5\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_losses = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Trained Weights:\", weights)\nprint(\"Trained Bias:\", bias)\nprint(\"MSE Losses:\", mse_losses)\n```\n\nThis script defines a `train_neuron` function which takes in feature vectors, labels, initial parameters for the neuron (weights and bias), a learning rate, and the number of epochs to run. It implements a simple gradient descent algorithm to update the neuron's parameters based on the mean squared error (MSE) loss function and the sigmoid activation function. The trained weights, bias, and list of MSE losses for each epoch are returned by the function, all rounded to four decimal places."}
{"task_id": 26, "completion_id": 0, "completion": "```python\nclass Value:\n    def __init__(self, data, _children=(), _op='', label=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n        self.label = label\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n    \n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self, ), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage\nx = Value(2.0)\ny = Value(-4.0)\nz = x * y\nq = z.relu()\nq.backward()\nprint(f\"x.grad = {x.grad}, y.grad = {y.grad}\")\n```\n\nThis example implements a `Value` class capable of automatic differentiation for scalar values. It supports addition, multiplication, and ReLU operations. The `backward` method computes the gradients of the operations starting from the target `Value` object (in this case `q`). The gradients for inputs `x` and `y` are calculated according to the operations performed (`relu` of `z` where `z` is a product of `x` and `y`)."}
{"task_id": 27, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix from basis B to basis C.\n    \n    Parameters:\n    B (list of lists of int): Basis vectors in basis B.\n    C (list of lists of int): Basis vectors in basis C.\n    \n    Returns:\n    list of lists of float: The transformation matrix from B to C.\n    \"\"\"\n    # Convert lists to numpy arrays for matrix operations\n    B_array = np.array(B)\n    C_array = np.array(C)\n    \n    # The transformation matrix from B to C is given by C^{-1}B\n    # where C^{-1} is the inverse of matrix C\n    C_inv = np.linalg.inv(C_array)\n    transform_matrix = np.dot(C_inv, B_array)\n    \n    # Rounding to 4 decimal places and converting back to list\n    transform_matrix = np.round(transform_matrix, 4).tolist()\n    \n    return transform_matrix\n\n# Example usage\nB = [[1, 2, 0], [0, 1, 3], [2, 0, 1]]\nC = [[-1, 1, 0], [1, 0, 2], [0, 1, 1]]\nprint(transform_basis(B, C))\n```\n\nThis script defines a function `transform_basis` that calculates the transformation matrix from one basis to another, following the description and constraints given."}
{"task_id": 28, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD (U, S, V) of a 2x2 matrix A using the eigenvalues and eigenvectors of A^T A.\n    \n    Args:\n    - A: A 2x2 numpy array.\n    \n    Returns:\n    - U, S, V: The SVD matrices such that A = U * S * V.\n    \"\"\"\n    # Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Perform eigendecomposition of A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(ATA)\n    \n    # Construct the singular values matrix S\n    S = np.sqrt(eigenvalues)\n    \n    # Sort eigenvalues and eigenvectors in descending order\n    idx = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Construct V from eigenvectors\n    V = eigenvectors.T\n    \n    # Compute U using the formula U = A * V * S_inv\n    S_inv = np.diag(1 / np.where(S > 0, S, 1))\n    U = np.dot(A, np.dot(V.T, S_inv))\n    \n    # Ensure that U is a proper orthogonal matrix\n    U = np.round(U, decimals=4).tolist()\n    S = np.round(S, decimals=4).tolist()\n    S_diagonal = np.zeros((2, 2))\n    np.fill_diagonal(S_diagonal, S)\n    S_diagonal = S_diagonal.tolist()\n    V = np.round(V, decimals=4).tolist()\n    \n    return U, S_diagonal, V\n\n# Example usage:\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script provides a function `svd_2x2` that computes the SVD of a 2x2 matrix A. The `svd_2x2` function uses the eigenvalues and eigenvectors of the matrix `A^T A` to construct the SVD matrices U, S, and V. The results are rounded to the nearest 4th decimal as required and returned as lists."}
{"task_id": 29, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the rows of two numpy arrays, X and y, preserving their correspondence.\n    \n    Parameters:\n    - X: numpy array, the feature matrix.\n    - y: numpy array, the label array.\n    - seed: int, optional, the seed for the random number generator for reproducibility.\n    \n    Returns:\n    - X_shuffled as a list, the shuffled feature matrix.\n    - y_shuffled as a list, the shuffled label array.\n    \"\"\"\n    if not isinstance(seed, int):\n        np.random.seed(seed)\n    assert X.shape[0] == y.shape[0], \"X and y must have the same number of rows.\"\n    \n    # Create an array of indices and shuffle it\n    idx = np.arange(X.shape[0])\n    np.random.shuffle(idx)\n    \n    # Shuffle X and y using the shuffled indices\n    X_shuffled = X[idx]\n    y_shuffled = y[idx]\n    \n    # Convert the shuffled arrays to list format\n    return X_shuffled.tolist(), y_shuffled.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])\n\nX_shuffled, y_shuffled = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X:\\n\", X_shuffled)\nprint(\"Shuffled y:\\n\", y_shuffled)\n\n# Check function to verify the correctness of the generated function\ndef check_shuffle_function(func):\n    X_test = np.array([[10, 20], [30, 40], [50, 60]])\n    y_test = np.array([70, 80, 90])\n    X_shuffled, y_shuffled = func(X_test, y_test, seed=42)\n    # Expected results with seed=42\n    X_expected = [[30, 40], [50, 60], [10, 20]]\n    y_expected = [80, 90, 70]\n    assert X_shuffled == X_expected, \"X_shuffled does not match expected output\"\n    assert y_shuffled == y_expected, \"y_shuffled does not match expected output\"\n    print(\"Check passed.\")\n\n# Running the check function\ncheck_shuffle_function(shuffle_data)\n```"}
{"task_id": 30, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    This function creates a batch iterator for a given dataset X and an optional target dataset y.\n    It yields batches of dataset(s) as specified by the batch_size.\n\n    Parameters:\n    X (numpy.ndarray): Input dataset.\n    y (numpy.ndarray, optional): Target dataset. Defaults to None.\n    batch_size (int): Size of the batch to be yielded.\n\n    Returns:\n    tuple[list, list] or list: A batch of X and y (if y is provided), or a batch of X only.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        start_idx = i\n        end_idx = min(i + batch_size, n_samples)\n        if y is not None:\n            yield X[start_idx:end_idx].tolist(), y[start_idx:end_idx].tolist()\n        else:\n            yield X[start_idx:end_idx].tolist()\n\n# Sample usage of the batch_iterator function\nif __name__ == \"__main__\":\n    # Generating sample data\n    X = np.array([[i, i+1] for i in range(100)])\n    y = np.array([1 if i % 2 == 0 else 0 for i in range(100)])\n\n    # Iterating through batches with y data\n    for batch_X, batch_y in batch_iterator(X, y, batch_size=10):\n        print(f\"Batch X: {batch_X}\\nBatch y: {batch_y}\\n\")\n\n    # Iterating through batches without y data\n    for batch_X in batch_iterator(X, batch_size=10):\n        print(f\"Batch X: {batch_X}\\n\")\n```"}
{"task_id": 31, "completion_id": 0, "completion": "Here is the self-contained Python script that solves the given task:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides a dataset based on whether the value of a specified feature is\n    greater than or equal to a given threshold.\n    \n    Parameters:\n    X (numpy.ndarray): The input dataset.\n    feature_i (int): Index of the feature to be used for comparison.\n    threshold (float): Value to compare the feature against.\n    \n    Returns:\n    list: Two subsets of the dataset. First subset contains samples with\n          feature values >= threshold and the second subset has values < threshold.\n    \"\"\"\n    # Ensure X is a numpy array\n    X = np.array(X)\n    \n    # Find the indices where the feature values are greater than or equal to the threshold\n    idx_ge = np.where(X[:, feature_i] >= threshold)\n    # Find the indices where the feature values are less than the threshold\n    idx_lt = np.where(X[:, feature_i] < threshold)\n    \n    # Split the dataset based on the found indices\n    subset_ge = X[idx_ge].tolist()\n    subset_lt = X[idx_lt].tolist()\n    \n    # Return the two subsets\n    return [subset_ge, subset_lt]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    np.random.seed(0)\n    X = np.random.rand(10, 3)  # 10 samples, 3 features\n    \n    # Divide the dataset based on the second feature (index 1) and threshold of 0.5\n    divided_sets = divide_on_feature(X, 1, 0.5)\n    \n    # Output the subsets\n    print(\"Subset with feature >= threshold:\")\n    print(divided_sets[0])\n    print(\"\\nSubset with feature < threshold:\")\n    print(divided_sets[1])\n```\n\nThis script includes the function `divide_on_feature` which takes a numpy array as input and returns two subsets of this array based on a threshold comparison for a specified feature. The example usage at the end demonstrates how to call this function with a sample dataset and prints out the resulting subsets."}
{"task_id": 32, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial and interaction features.\n    \n    :param X: A 2D numpy array of shape (n_samples, n_features).\n    :param degree: An integer representing the highest degree of interaction features.\n    :return: A 2D numpy array with new polynomial features, reshaped and converted to a python list.\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n    n_samples, n_features = X.shape\n    \n    # Get all combinations of feature indices up to the given degree\n    indices = list(combinations_with_replacement(range(n_features), degree))\n    \n    # Create the new feature matrix\n    new_features = np.empty((n_samples, 0))\n    for index in indices:\n        new_feature = np.prod(X[:, index], axis=1).reshape(-1, 1)\n        new_features = np.hstack((new_features, new_feature))\n    \n    return new_features.tolist()\n\n# Example usage:\nX = np.array([[1, 2], [3, 4]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis example code defines a function that takes a 2D numpy array and an integer as parameters to generate the polynomial features up to the specified degree. It uses `combinations_with_replacement` to generate all possible combinations of feature indices for the given degree, then computes the product of the features at these indices for each sample to form the polynomial features. Finally, it converts the numpy array to a Python list and returns it."}
{"task_id": 33, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n        else:\n            indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([0, 1, 0, 1, 1])\n    n_subsets = 2\n    subsets = get_random_subsets(X, y, n_subsets, replacements=False)\n    for i, (X_sub, y_sub) in enumerate(subsets):\n        print(f\"Subset {i+1}:\\nX_subset:\\n{X_sub}\\ny_subset:\\n{y_sub}\\n\")\n```\nIn the provided script, the `get_random_subsets` function generates the specified number of random subsets from the dataset represented by `X` and `y`. Depending on the `replacements` parameter, it either samples with or without replacement. Each subset is created as a tuple of `(X_subset, y_subset)` and then appended to the list of subsets, which is returned at the end of the function. The subsets are converted to lists using `tolist()` to adhere to the output constraints."}
{"task_id": 34, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding on a 1D numpy array of integer values.\n    \n    Parameters:\n    - x: A 1D numpy array of integer values.\n    - n_col: An optional integer. The number of columns in the one-hot encoded array.\n             If not provided, it will be determined based on the maximum value in x.\n    \n    Returns:\n    - A list of lists representing the one-hot encoded data.\n    \"\"\"\n    if not n_col:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\n\n# Example usage\nx = np.array([0, 1, 2, 2, 3])\nprint(to_categorical(x))\n```\n\nThis function `to_categorical` handles the one-hot encoding requirement as described. It checks if `n_col` is provided, and if not, it calculates it based on the maximum value in the input array `x`. The `np.eye` function is used to generate an identity matrix, which is then indexed by `x` to produce the one-hot encoded array. Finally, the array is converted to a list using the `tolist()` method to meet the output format requirement."}
{"task_id": 35, "completion_id": 0, "completion": "Here is a self-contained Python script that accomplishes the task of converting a 1D numpy array into a diagonal matrix and returns the output as a Python list:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    \"\"\"\n    Convert a 1D numpy array into a diagonal matrix and return it as a Python list.\n    \n    Parameters:\n    x (np.array): A 1D numpy array to be converted into a diagonal matrix.\n    \n    Returns:\n    list: The diagonal matrix as a Python list.\n    \"\"\"\n    # Convert the 1D numpy array into a diagonal matrix\n    diagonal_matrix = np.diag(x)\n    # Convert the 2D numpy array into a Python list and return\n    return diagonal_matrix.tolist()\n\n# Example usage:\n# Define a 1D numpy array\nx = np.array([1, 2, 3])\n# Call the function to make a diagonal matrix\ndiagonal_matrix = make_diagonal(x)\n# Print the result\nprint(diagonal_matrix)\n```\n\nThis script defines the `make_diagonal` function which takes a 1D numpy array as input and outputs a diagonal matrix as a Python list. It uses numpy's `diag` function to create the diagonal matrix from the input array and `tolist()` to convert the resulting numpy array into a list. The example usage demonstrates how to call the function and print the result."}
{"task_id": 36, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score as a float rounded to the 4th decimal.\n    \"\"\"\n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n    # Calculate the accuracy score\n    accuracy = correct_predictions / total_predictions\n    # Return the accuracy rounded to the 4th decimal\n    return round(accuracy, 4)\n\n# Example usage\ny_true = np.array([1, 2, 3, 4, 5])\ny_pred = np.array([1, 2, 3, 4, 4])\nprint(accuracy_score(y_true, y_pred))  # Expected output: 0.8\n```"}
{"task_id": 37, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset. If Y is not provided,\n    calculate the correlation matrix of X with itself.\n    \n    :param X: 2D numpy array\n    :param Y: Optional, 2D numpy array\n    :return: 2D numpy array representing the correlation matrix, as a python list\n    \"\"\"\n    if Y is None:\n        Y = X\n    \n    # Centering the data\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    \n    # Calculating covariance matrix\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    \n    # Normalizing to get correlation matrix\n    X_std = np.std(X, axis=0)\n    Y_std = np.std(Y, axis=0)\n    \n    # Avoid division by zero by replacing 0 std with 1\n    X_std[X_std == 0] = 1\n    Y_std[Y_std == 0] = 1\n    \n    corr_matrix = cov_matrix / np.outer(X_std, Y_std)\n    \n    # Return the correlation matrix as a python list with values rounded to 4th decimal\n    return np.round(corr_matrix, 4).tolist()\n\n# Check function with provided data points\ndef check_calculate_correlation_matrix():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[2, 4], [6, 8], [10, 12]])\n    print(\"Correlation matrix of X with itself:\", calculate_correlation_matrix(X))\n    print(\"Correlation matrix of X with Y:\", calculate_correlation_matrix(X, Y))\n    \ncheck_calculate_correlation_matrix()\n```\n\nThis script provides a self-contained Python function to calculate the correlation matrix between two provided datasets or between a dataset and itself if the second array is not provided. It outputs the rounded results in the form of a Python list."}
{"task_id": 38, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Implements the fitting method for an AdaBoost classifier.\n    \n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features) representing the dataset.\n    - y: 1D numpy array of shape (n_samples,) representing the labels.\n    - n_clf: An integer representing the number of classifiers.\n    \n    Returns:\n    - classifiers: A list of dictionaries, each representing a classifier with parameters.\n    \"\"\"\n    n_samples, n_features = X.shape\n    weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    \n    for _ in range(n_clf):\n        min_error = float('inf')\n        best_clf = {}\n        \n        for feature in range(n_features):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                # Predict positive if X[:, feature] > threshold else negative\n                pred = np.where(X[:, feature] > threshold, 1, -1)\n                error = np.sum(weights[y != pred])\n                \n                # Invert prediction if error is less than 0.5\n                if error > 0.5:\n                    error = 1 - error\n                    pred = -pred\n                \n                # Update best classifier parameters\n                if error < min_error:\n                    min_error = error\n                    best_clf = {\n                        'feature': feature,\n                        'threshold': threshold,\n                        'prediction': np.sign(np.sum(weights * y * (pred == y)))\n                    }\n                    best_pred = pred\n        \n        # Calculate alpha and update weights\n        alpha = 0.5 * np.log((1 - min_error) / min_error)\n        weights *= np.exp(-alpha * y * best_pred)\n        weights /= np.sum(weights)\n        \n        best_clf['alpha'] = round(alpha, 4)\n        classifiers.append(best_clf)\n    \n    return classifiers\n\n# Example usage\nX = np.array([[2, 4], [4, 6], [6, 8], [8, 10]])\ny = np.array([1, -1, 1, -1])\nn_clf = 2\nprint(adaboost_fit(X, y, n_clf))\n```\n\nThis script provides a full implementation of the AdaBoost fitting method as requested, including the method's documentation and an example usage scenario. The classifiers' parameters are stored in a list and returned at the end of the function, with each dictionary in the list representing a classifier's parameters, and values rounded to the nearest 4th decimal place as required."}
{"task_id": 39, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a list of scores.\n    \n    The log-softmax function is defined as:\n    log_softmax(i) = log(exp(score[i]) / \u03a3(exp(score[j])) for all j)\n    \n    Args:\n        scores (list): A list of scores to compute the log-softmax for.\n    \n    Returns:\n        list: A list containing the log-softmax values of the original scores, rounded to 4 decimal places.\n    \"\"\"\n    # Convert the scores list to a numpy array for vectorized operations\n    scores_np = np.array(scores)\n    \n    # Compute the softmax in a numerically stable way\n    adjusted_scores = scores_np - np.max(scores_np)\n    exp_scores = np.exp(adjusted_scores)\n    sum_exp_scores = np.sum(exp_scores)\n    \n    softmax = exp_scores / sum_exp_scores\n    \n    # Compute log-softmax\n    log_softmax = np.log(softmax)\n    \n    # Round to 4 decimal places and convert back to list\n    return log_softmax.round(4).tolist()\n\n# Example usage\nscores = [3.0, 1.0, 0.2]\nprint(log_softmax(scores))\n```\n\nThis script defines a function `log_softmax` that takes a list of scores as input and computes the log-softmax of the scores, returning the result rounded to the nearest 4th decimal place. The example usage at the end demonstrates how to use the function with a sample list of scores."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer = None\n\n    def initialize(self, optimizer):\n        # Set input shape\n        if self.input_shape:\n            input_shape = self.input_shape\n        else:\n            raise Exception('Dense layer is not the first layer, therefore it must receive its input_shape')\n        \n        # Initialize weights and bias\n        limit = 1 / math.sqrt(input_shape[0])\n        self.W  = np.random.uniform(-limit, limit, (input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        \n        # Initialize optimizers for weights and bias\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Update weights\n        W = self.optimizer_W.update(self.W, accum_grad.T.dot(self.layer_input))\n        # Update bias\n        w0 = self.optimizer_w0.update(self.w0, accum_grad)\n        self.W = W\n        self.w0 = w0\n        return accum_grad.dot(self.W.T)\n\n    def output_shape(self):\n        return (self.n_units,)\n        \n# Example usage\nif __name__ == '__main__':\n    # Define a simple optimizer (SGD for instance)\n    class SGD:\n        def __init__(self, learning_rate=0.01):\n            self.learning_rate = learning_rate\n\n        def update(self, param, grad):\n            return param - self.learning_rate * grad.mean(axis=0)\n\n    # Create a Dense layer\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    dense_layer.initialize(SGD(learning_rate=0.01))\n\n    # Simulate input data\n    X_input = np.array([[0.1, 0.2], [0.3, 0.4]])\n    \n    # Forward pass\n    output = dense_layer.forward_pass(X_input, training=True)\n    print(\"Forward Pass Output:\\n\", np.round(output, 4).tolist())\n\n    # Backward pass (simulated accum_grad from previous passes)\n    accum_grad = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    output = dense_layer.backward_pass(accum_grad)\n    print(\"Backward Pass Output:\\n\", np.round(output, 4).tolist())\n```\nThis Python script defines the Dense class as required, with appropriate weight initialization, forward and backward pass methods, and trainable parameters count. It also provides an example of setting up a Dense layer and simulating forward and backward passes."}
{"task_id": 41, "completion_id": 0, "completion": "Here is the self-contained Python script that implements a simple 2D convolutional layer according to the given task description:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    \"\"\"\n    Applies a 2D convolution on an input matrix with a specified kernel, padding, and stride.\n    \n    Parameters:\n        input_matrix (np.ndarray): The input 2D matrix on which the convolution is applied.\n        kernel (np.ndarray): The convolutional kernel to use.\n        padding (int): The number of rows and columns of zeros to be added to the edges of the input matrix.\n        stride (int): The stride value for the convolution operation.\n    \n    Returns:\n        np.ndarray: The result of the convolution as a 2D numpy array, rounded to the nearest 4th decimal.\n    \"\"\"\n    # Add padding to the input matrix\n    input_matrix = np.pad(input_matrix, padding, mode='constant')\n\n    # Dimensions of the input matrix and the kernel\n    n_in, m_in = input_matrix.shape\n    n_kernel, m_kernel = kernel.shape\n\n    # Calculate the dimension of the output matrix\n    n_out = (n_in - n_kernel)//stride + 1\n    m_out = (m_in - m_kernel)//stride + 1\n\n    # Initialize the output matrix\n    output_matrix = np.zeros((n_out, m_out))\n\n    # Perform the 2D convolution\n    for i in range(0, n_out):\n        for j in range(0, m_out):\n            # Calculate the matrix to be convoluted\n            current_matrix = input_matrix[i*stride:i*stride+n_kernel, j*stride:j*stride+m_kernel]\n            # Perform the convolution and sum the result\n            output_matrix[i, j] = np.sum(current_matrix * kernel)\n\n    # Round the output matrix to the nearest 4th decimal and return as list\n    return np.round(output_matrix, 4).tolist()\n\n# Example usage\ninput_matrix = np.array([[1, 2, 3, 4],\n                         [5, 6, 7, 8],\n                         [9, 10, 11, 12],\n                         [13, 14, 15, 16]])\nkernel = np.array([[1, 0], [0, -1]])\npadding = 1\nstride = 1\n\noutput = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(output)\n```\n\nThis script defines the `simple_conv2d` function that takes an input matrix, a kernel, padding value, and stride, and applies a 2D convolution. The output of the function is a list of lists (2D list) representing the result of the convolution, rounded to the nearest 4th decimal. The example usage demonstrates how to use the function with a simple input matrix, kernel, padding, and stride."}
{"task_id": 42, "completion_id": 0, "completion": "```python\ndef relu(z: float) -> float:\n    \"\"\"\n    Applies the Rectified Linear Unit (ReLU) function to the input.\n    \n    Parameters:\n    z (float): A single float number to which the ReLU function will be applied.\n    \n    Returns:\n    float: The result after applying the ReLU function. It returns z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)\n\n# Test the relu function\ntest_cases = [-5.0, -1.0, 0.0, 1.0, 5.0]\nfor z in test_cases:\n    print(f\"Relu({z}) = {relu(z)}\")\n```"}
{"task_id": 43, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given the feature matrix, coefficients,\n    true labels, and regularization parameter.\n    \n    :param X: 2D numpy array, feature matrix\n    :param w: 1D numpy array, coefficients\n    :param y_true: 1D numpy array, true labels\n    :param alpha: float, regularization parameter\n    :return: float, Ridge loss rounded to 4 decimal places\n    \"\"\"\n    # Predicted values\n    y_pred = X.dot(w)\n    \n    # Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Regularization term\n    regularization = alpha * np.sum(w ** 2)\n    \n    # Total loss\n    total_loss = mse + regularization\n    \n    # Return the loss rounded to 4 decimals\n    return round(total_loss, 4)\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    w = np.array([0.5, 1.5])\n    y_true = np.array([1, 2, 3])\n    alpha = 0.1\n    print(ridge_loss(X, w, y_true, alpha))\n```"}
{"task_id": 44, "completion_id": 0, "completion": "```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    - z (float): The input value.\n    - alpha (float): The slope for the negative part of the function. Default is 0.01.\n    \n    Returns:\n    - float: The Leaky ReLU output given the input value.\n    \"\"\"\n    return max(alpha * z, z)\n\n# Example usage and testing the function\nif __name__ == \"__main__\":\n    # Test cases\n    test_cases = [-10.0, 0.0, 5.0]\n    alpha = 0.01\n    \n    for z in test_cases:\n        print(f\"leaky_relu({z}, {alpha}) = {leaky_relu(z, alpha)}\")\n        \n    # Expected output:\n    # leaky_relu(-10.0, 0.01) = -0.1\n    # leaky_relu(0.0, 0.01) = 0.0\n    # leaky_relu(5.0, 0.01) = 5.0\n```"}
{"task_id": 45, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n\n    Parameters:\n    x1 (np.array): The first input vector.\n    x2 (np.array): The second input vector.\n\n    Returns:\n    float: The computed linear kernel value.\n\n    Raises:\n    ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError(\"Input vectors must be of the same length.\")\n    \n    return np.dot(x1, x2)\n\n# Example usage\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nprint(kernel_function(x1, x2))\n```"}
{"task_id": 46, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision score.\n    \n    Precision is the ratio of true positive predictions to the total positive predictions (including false positives).\n    \n    Parameters:\n    - y_true: Array of true binary labels (numpy array)\n    - y_pred: Array of predicted binary labels (numpy array)\n    \n    Returns:\n    - Precision score (float)\n    \"\"\"\n    \n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate true positives (TP) and false positives (FP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate precision\n    if TP + FP == 0:\n        return 0  # To avoid division by zero\n    precision_score = TP / (TP + FP)\n    \n    return precision_score\n\n# Example usage\ny_true = np.array([0, 1, 1, 0, 1, 1])\ny_pred = np.array([0, 1, 0, 0, 0, 1])\nprint(precision(y_true, y_pred))\n```\n\nThis code defines a function `precision` which calculates the precision score for binary classification problems. The precision score gives the measure of the accuracy provided that a prediction is positive. The function uses numpy for efficient array operations, handling true positives and false positives to compute the precision."}
{"task_id": 47, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent to minimize the MSE loss function.\n    \n    :param X: numpy array of shape (n_samples, n_features), the input features.\n    :param y: numpy array of shape (n_samples,), the target values.\n    :param weights: numpy array of shape (n_features,), the initial weights.\n    :param learning_rate: float, the step size at each iteration while moving toward a minimum.\n    :param n_iterations: int, the number of iterations to perform.\n    :param batch_size: int, the size of the batch for gradient descent. Default is 1.\n    :param method: str, one of \"batch\", \"stochastic\", or \"mini-batch\". Default is \"batch\".\n    :return: list, the optimized weights.\n    \"\"\"\n    \n    n_samples, n_features = X.shape\n    updated_weights = weights.copy()\n    \n    for iteration in range(n_iterations):\n        if method == 'batch':\n            gradients = np.dot(X.T, (np.dot(X, updated_weights) - y)) / n_samples\n            updated_weights -= learning_rate * gradients\n            \n        elif method == 'stochastic':\n            for sample_index in range(n_samples):\n                gradient = X[sample_index] * (np.dot(X[sample_index], updated_weights) - y[sample_index])\n                updated_weights -= learning_rate * gradient\n                \n        elif method == 'mini-batch':\n            for batch_start in range(0, n_samples, batch_size):\n                batch_end = batch_start + batch_size\n                X_batch = X[batch_start:batch_end]\n                y_batch = y[batch_start:batch_end]\n                gradients = np.dot(X_batch.T, (np.dot(X_batch, updated_weights) - y_batch)) / batch_size\n                updated_weights -= learning_rate * gradients\n                \n    # Return the updated weights rounded to the nearest 4th decimal\n    return np.round(updated_weights, decimals=4).tolist()\n\n# Example use case\nX_example = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny_example = np.dot(X_example, np.array([1, 2])) + 3\nweights_example = np.zeros(X_example.shape[1])\nlearning_rate_example = 0.01\nn_iterations_example = 1000\n\n# Run the gradient descent with different methods\nweights_batch = gradient_descent(X_example, y_example, weights_example, learning_rate_example, n_iterations_example, method='batch')\nweights_stochastic = gradient_descent(X_example, y_example, weights_example, learning_rate_example, n_iterations_example, method='stochastic')\nweights_mini_batch = gradient_descent(X_example, y_example, weights_example, learning_rate_example, n_iterations_example, batch_size=2, method='mini-batch')\n\nprint(\"Batch Gradient Descent Weights:\", weights_batch)\nprint(\"Stochastic Gradient Descent Weights:\", weights_stochastic)\nprint(\"Mini-Batch Gradient Descent Weights:\", weights_mini_batch)\n```"}
{"task_id": 48, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Args:\n    matrix (list of list of float): The matrix to convert into RREF.\n    \n    Returns:\n    list of list of float: The RREF of the given matrix.\n    \"\"\"\n    # Convert the input matrix into a numpy array for easier manipulation\n    mat = np.array(matrix, dtype=np.float64)\n    rows, cols = mat.shape\n    lead = 0\n    \n    # Iterate over the rows of the matrix\n    for r in range(rows):\n        if lead >= cols:\n            break\n        \n        # Find the first non-zero element in the current column if possible\n        i = r\n        while mat[i, lead] == 0:\n            i += 1\n            if i == rows:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return mat.tolist()\n        \n        # Swap the current row with the row containing the first non-zero element\n        mat[[i, r]] = mat[[r, i]]\n        \n        # Divide the current row by the leading coefficient to make it 1\n        lead_coeff = mat[r, lead]\n        mat[r] = mat[r] / lead_coeff\n        \n        # Make all other elements in the lead column 0\n        for i in range(rows):\n            if i != r:\n                mat[i] = mat[i] - mat[i, lead] * mat[r]\n        \n        # Move to the next column\n        lead += 1\n    \n    return mat.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    matrix = [\n        [1, 2, -1, -4],\n        [2, 3, -1, -11],\n        [-2, 0, -3, 22]\n    ]\n    print(rref(matrix))\n```\n\nThis Python script defines a function `rref` that takes a matrix as input and returns its Reduced Row Echelon Form (RREF). The script handles various edge cases such as zero rows, columns with pivots, and non-square matrices. It uses numpy for easier matrix manipulation and returns the final RREF as a list of lists, as specified in the task description."}
{"task_id": 49, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    \"\"\"\n    Adam optimization algorithm implementation.\n    \n    Parameters:\n    f: The objective function to be optimized.\n    grad: A function that computes the gradient of f.\n    x0: Initial parameter values.\n    learning_rate: The step size, default is 0.001.\n    beta1: Exponential decay rate for the first moment estimates, default is 0.9.\n    beta2: Exponential decay rate for the second moment estimates, default is 0.999.\n    epsilon: A small constant for numerical stability, default is 1e-8.\n    num_iterations: Number of iterations to run the optimizer, default is 1000.\n    \n    Returns:\n    The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    x = np.array(x0)\n    \n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        \n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage:\n# Define an example function and its gradient\ndef example_function(x):\n    return x[0] ** 2 + x[1] ** 2\n\ndef example_gradient(x):\n    return np.array([2 * x[0], 2 * x[1]])\n\n# Initial guess\nx0 = [1.0, 2.0]\n\n# Optimize\noptimal_x = adam_optimizer(example_function, example_gradient, x0)\nprint(optimal_x)\n```\n\nThe provided script contains an implementation of the Adam optimization algorithm that takes an objective function and its gradient, alongside initial parameter values, and optimizes the parameters according to Adam's adaptation of learning rates. The `adam_optimizer` function is designed to be flexible with parameters for common hyperparameters used in Adam, and it returns the optimized parameter values as a list, rounded to the nearest fourth decimal for precision control and ease of interpretation."}
{"task_id": 50, "completion_id": 0, "completion": "Below is the Python script that implements Lasso Regression using Gradient Descent as per the provided requirements.\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    m, n = X.shape\n    weights = np.zeros(n)\n    bias = 0\n    prev_cost = float('inf')\n    \n    for iter in range(max_iter):\n        # Predictions\n        y_pred = np.dot(X, weights) + bias\n        \n        # Gradients\n        dw = (-2/m) * (np.dot(X.T, (y - y_pred)) + alpha * np.sign(weights))\n        db = (-2/m) * np.sum(y - y_pred)\n        \n        # Update weights and bias\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        \n        # Cost\n        cost = (1/(2*m)) * np.sum((y - y_pred)**2) + alpha * np.sum(np.abs(weights))\n        \n        # Check for convergence\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    \n    # Ensure the results are rounded to the nearest 4th decimal\n    weights = np.round(weights, decimals=4)\n    bias = np.round(bias, decimals=4)\n    return weights.tolist(), bias.tolist()\n\n# Example usage:\n# X = np.random.rand(100, 3)\n# y = np.random.rand(100)\n# weights, bias = l1_regularization_gradient_descent(X, y)\n# print(f\"Weights: {weights}\")\n# print(f\"Bias: {bias}\")\n```\n\nThe above script defines a function `l1_regularization_gradient_descent` that takes in the feature matrix `X`, the target vector `y`, and the hyperparameters for the Lasso Regression model. It performs gradient descent to minimize the Lasso Regression objective function, applying L1 regularization to the weights. After each iteration, the function checks if the change in the cost is below a certain tolerance level to determine whether to stop early. The function returns the optimized weights and bias as lists after rounding them to 4 decimal places."}
{"task_id": 51, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    \n    Parameters:\n    - source: The source string to be transformed.\n    - target: The target string to transform into.\n    \n    Returns:\n    - The OSA distance between the source and target strings.\n    \"\"\"\n    # Ensure source is the shorter string\n    if len(source) > len(target):\n        source, target = target, source\n    \n    # Initialize distances array\n    distances = np.zeros((2, len(target) + 1), dtype=int)\n    \n    # Initialize the first row\n    for t in range(len(target) + 1):\n        distances[0][t] = t\n    \n    # Fill the distances array\n    for s in range(1, len(source) + 1):\n        distances[s % 2][0] = s\n        for t in range(1, len(target) + 1):\n            substitution_cost = 0 if source[s - 1] == target[t - 1] else 1\n            \n            distances[s % 2][t] = min(\n                distances[(s - 1) % 2][t] + 1,              # Deletion\n                distances[s % 2][t - 1] + 1,                # Insertion\n                distances[(s - 1) % 2][t - 1] + substitution_cost,  # Substitution\n            )\n            \n            if s > 1 and t > 1 and source[s - 1] == target[t - 2] and source[s - 2] == target[t - 1]:\n                distances[s % 2][t] = min(\n                    distances[s % 2][t],\n                    distances[(s - 2) % 2][t - 2] + 1  # Transposition\n                )\n    \n    return distances[len(source) % 2][len(target)]\n\n# Example check function\ndef check_OSA():\n    assert OSA(\"caper\", \"acer\") == 2\n    assert OSA(\"abc\", \"acb\") == 1\n    assert OSA(\"python\", \"typhon\") == 1\n    print(\"All tests passed.\")\n\n# Run the check function to verify correctness\ncheck_OSA()\n```\n\nThis solution provides an efficient way to compute the OSA distance between two strings using dynamic programming with space optimization. It ensures that the algorithm runs in O(nm) time complexity and O(min(m, n)) space complexity, where `m` and `n` are the lengths of the two input strings."}
{"task_id": 52, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for a binary classification problem.\n\n    Parameters:\n    y_true (list of int): The true binary labels.\n    y_pred (list of int): The predicted binary labels.\n\n    Returns:\n    float: The recall value rounded to three decimal places. Returns 0.0 if the denominator is zero.\n    \"\"\"\n    true_positives = sum([1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1])\n    false_negatives = sum([1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0])\n    \n    if true_positives + false_negatives == 0:\n        return 0.0\n    \n    recall_value = true_positives / (true_positives + false_negatives)\n    return round(recall_value, 3)\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\nprint(recall(y_true, y_pred))  # Expected output: 0.6\n```"}
{"task_id": 53, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implements the self-attention mechanism on a batch of input data.\n    \n    Parameters:\n    X (numpy.ndarray): Input tensor of shape (batch_size, seq_length, embedding_dim).\n    W_q (numpy.ndarray): Query weight matrix of shape (embedding_dim, embedding_dim).\n    W_k (numpy.ndarray): Key weight matrix of shape (embedding_dim, embedding_dim).\n    W_v (numpy.ndarray): Value weight matrix of shape (embedding_dim, embedding_dim).\n\n    Returns:\n    numpy.ndarray: Output tensor of shape (batch_size, seq_length, embedding_dim) after applying self-attention.\n    \"\"\"\n    batch_size, seq_length, embedding_dim = X.shape\n    \n    # Compute the queries, keys, and values\n    Q = np.einsum('ijk,kl->ijl', X, W_q)\n    K = np.einsum('ijk,kl->ijl', X, W_k)\n    V = np.einsum('ijk,kl->ijl', X, W_v)\n\n    # Compute the attention scores\n    attention_scores = np.einsum('ijk,ilk->ijl', Q, K) / np.sqrt(embedding_dim)\n    attention_weights = np.softmax(attention_scores, axis=-1)\n    \n    # Compute the weighted sum of values\n    attention_output = np.einsum('ijk,ilk->ijl', attention_weights, V)\n    \n    # Round the output to 4 decimal places\n    rounded_output = np.round(attention_output, 4)\n    \n    # Convert to list for returning\n    return rounded_output.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    batch_size = 2\n    seq_length = 3\n    embedding_dim = 4\n    \n    X = np.random.rand(batch_size, seq_length, embedding_dim)\n    W_q = np.random.rand(embedding_dim, embedding_dim)\n    W_k = np.random.rand(embedding_dim, embedding_dim)\n    W_v = np.random.rand(embedding_dim, embedding_dim)\n    \n    output = self_attention(X, W_q, W_k, W_v)\n    print(output)\n```"}
{"task_id": 54, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a simple RNN cell to compute the final hidden state.\n    \n    Args:\n    input_sequence: A list of input vectors for each time step.\n    initial_hidden_state: The initial hidden state.\n    Wx: The weight matrix for input-to-hidden connections.\n    Wh: The weight matrix for hidden-to-hidden connections.\n    b: The bias vector.\n    \n    Returns:\n    A list of floats representing the final hidden state after processing the entire input sequence.\n    \"\"\"\n    # Convert input parameters to numpy arrays for easier manipulation\n    H = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x in input_sequence:\n        x = np.array(x)\n        # Compute the next hidden state\n        H = np.tanh(np.dot(x, Wx) + np.dot(H, Wh) + b)\n    # Round the final hidden state to 4 decimal places and convert back to python list\n    return np.round(H, decimals=4).tolist()\n\n# Example usage:\ninput_sequence = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.1, 0.05], [0.05, 0.1]]\nb = [0.1, -0.1]\n\nfinal_hidden_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_hidden_state)\n```"}
{"task_id": 55, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation matrix to a set of points.\n    \n    :param points: List of [x, y] coordinates\n    :param tx: Translation distance in the x direction\n    :param ty: Translation distance in the y direction\n    :return: Translated list of [x, y] coordinates\n    \"\"\"\n    # Convert points to a numpy array for easier manipulation\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([\n        [1, 0, tx],\n        [0, 1, ty],\n        [0, 0, 1]\n    ])\n    \n    # Append a column of ones to the points array for matrix multiplication\n    ones_column = np.ones(points_array.shape[0]).reshape(-1, 1)\n    points_homogeneous = np.hstack((points_array, ones_column))\n    \n    # Apply the translation\n    translated_points_homogeneous = np.dot(translation_matrix, points_homogeneous.T).T\n    \n    # Convert back to original form (ignoring the last column of ones)\n    translated_points = translated_points_homogeneous[:, :-1].tolist()\n    \n    return translated_points\n\n# Example usage:\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 1.5\nty = -2.0\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n- In this script, `translate_object` function takes a list of 2D points and applies a translation transformation to each of them.\n- The `points_homogeneous` array is created by appending a column of ones to the original `points_array`. This is necessary to use the translation matrix.\n- The `translation_matrix` is a 3x3 matrix. The final column contains the translation values `tx` and `ty`, and the bottom right element must always be 1 to preserve the translation.\n- The transformed points are then converted back to their original form, ignoring the ones column that was added for calculation purposes.\n- The result is converted to a list using `.tolist()` before returning it, as per the requirement."}
{"task_id": 56, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution (P).\n    sigma_p (float): Standard deviation of the first normal distribution (P).\n    mu_q (float): Mean of the second normal distribution (Q).\n    sigma_q (float): Standard deviation of the second normal distribution (Q).\n    \n    Returns:\n    float: The KL divergence between the two normal distributions.\n    \"\"\"\n    # Asserting that the standard deviations are positive\n    assert sigma_p > 0, \"sigma_p must be positive\"\n    assert sigma_q > 0, \"sigma_q must be positive\"\n    \n    kl_div = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_div\n\n# Example usage\nmu_p, sigma_p = 0, 1  # Mean and standard deviation for distribution P\nmu_q, sigma_q = 2, 4  # Mean and standard deviation for distribution Q\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence: {kl_div}\")\n```"}
{"task_id": 57, "completion_id": 0, "completion": "Below is the self-contained Python script for the task provided:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Apply the Gauss-Seidel iteration method to solve a system of linear equations.\n    \n    Parameters:\n    A (numpy.ndarray): The coefficient matrix of the system.\n    b (numpy.ndarray): The right-hand side vector.\n    n (int): Number of iterations to perform.\n    x_ini (numpy.ndarray or None): Initial guess for the solution vector. If not provided, a zero vector is used.\n    \n    Returns:\n    list: The approximated solution vector after n iterations.\n    \"\"\"\n    # Ensure A is a square matrix\n    assert A.shape[0] == A.shape[1], \"Matrix A must be square.\"\n    # Ensure A, b have compatible dimensions\n    assert A.shape[0] == b.shape[0], \"The number of rows in A must match the size of b.\"\n    \n    # Initialize x if not provided\n    if x_ini is None:\n        x = np.zeros_like(b, dtype=np.double)\n    else:\n        x = np.array(x_ini, dtype=np.double)\n    \n    # Iteratively update the solution vector\n    for it_count in range(n):\n        for i in range(A.shape[0]):\n            s1 = np.dot(A[i, :i], x[:i])  # Sum of the products of the row elements and the solution elements to the left\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])  # Sum of the products of the row elements and the solution elements to the right\n            x[i] = (b[i] - s1 - s2) / A[i, i]  # Update the current solution element\n    # Return the solution vector, rounded to 4 decimal places and converted to a python list\n    return np.round(x, 4).tolist()\n\n# Example Usage\nA = np.array([[10., -1., 2., 0.],\n              [-1., 11., -1., 3.],\n              [2., -1., 10., -1.],\n              [0.0, 3., -1., 8.]])\nb =  np.array([6., 25., -11., 15.])\nn = 10\nx_ini = np.zeros_like(b)\nsolution = gauss_seidel(A, b, n, x_ini)\nprint(solution)\n```\n\nThe function `gauss_seidel` iteratively updates the solution vector `x` using the latest values available from previous updates in the same iteration, which is a key aspect of the Gauss-Seidel method. The function returns the solution vector as a Python list, rounded to the fourth decimal place, as requested."}
{"task_id": 58, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves the linear system Ax = b using Gaussian elimination with partial pivoting.\n    \n    Parameters:\n        A (np.array): Coefficient matrix of the system.\n        b (np.array): Constants vector.\n        \n    Returns:\n        list: Solution vector x rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    # Combining A and b into a single matrix\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    \n    # Iterating through all rows of the matrix\n    for i in range(n):\n        # Finding the pivot row and swap it with the current row\n        pivot_row = np.argmax(abs(Ab[i:, i])) + i\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        \n        # Eliminating the entries below the pivot\n        for j in range(i + 1, n):\n            factor = Ab[j][i] / Ab[i][i]\n            Ab[j] -= factor * Ab[i]\n    \n    # Back substitution\n    x = np.zeros_like(b, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    \n    return np.round(x, decimals=4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=np.float64)\nb = np.array([8, -11, -3], dtype=np.float64)\nprint(gaussian_elimination(A, b))  # Output should be close to [1.0, 3.0, -2.0]\n```\n\nThis solution implements the Gaussian elimination method with partial pivoting, ensuring numerical stability. The `gaussian_elimination` function takes in a coefficient matrix `A` and a constant vector `b`, and returns the solution vector `x` as a list, with each element rounded to 4 decimal places as required."}
{"task_id": 59, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        cell_state = initial_cell_state\n        hidden = initial_hidden_state\n\n        for t in range(T):\n            # Concatenate the input x_t with the previous hidden state\n            xt_h = np.concatenate((x[t].reshape(-1, 1), hidden), axis=0)\n\n            # Compute the gates and candidate cell state\n            forget_gate = self.sigmoid(self.Wf @ xt_h + self.bf)\n            input_gate = self.sigmoid(self.Wi @ xt_h + self.bi)\n            candidate_cell = self.tanh(self.Wc @ xt_h + self.bc)\n            output_gate = self.sigmoid(self.Wo @ xt_h + self.bo)\n\n            # Update cell state and hidden state\n            cell_state = forget_gate * cell_state + input_gate * candidate_cell\n            hidden = output_gate * self.tanh(cell_state)\n\n            # Save the hidden state for this time step\n            hidden_states.append(hidden.flatten())\n\n        # Final results\n        final_hidden_state = hidden.flatten()\n        final_cell_state = cell_state.flatten()\n\n        # Round the results to 4 decimal places\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = final_hidden_state.round(4).tolist()\n        final_cell_state = final_cell_state.round(4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage:\ninput_size = 3\nhidden_size = 5\nlstm = LSTM(input_size, hidden_size)\nx = [np.random.randn(input_size) for _ in range(10)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))\nhidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\nprint(hidden_states)\nprint(final_hidden_state)\nprint(final_cell_state)\n```\n\nThis script implements the LSTM class with initializers for weights and biases inside the `__init__` method. The `forward` method processes a sequence of input vectors through time steps, updating the hidden and cell states according to the LSTM equations. It returns the hidden states at each time step, as well as the final hidden and cell states, rounded to the nearest 4th decimal place as specified."}
{"task_id": 60, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"The corpus cannot be empty.\")\n    \n    num_docs = len(corpus)\n    tf_idfs = []\n    \n    for doc in corpus:\n        doc_tf_idfs = []\n        for term in query:\n            # Calculate Term Frequency (TF)\n            term_freq = doc.count(term) / len(doc) if len(doc) > 0 else 0\n            \n            # Calculate Document Frequency (DF)\n            doc_freq = sum(1 for d in corpus if term in d)\n            if doc_freq == 0:\n                doc_freq = 1  # Smoothing to avoid division by zero\n            \n            # Calculate Inverse Document Frequency (IDF)\n            idf = math.log((num_docs + 1) / doc_freq)\n            \n            # Combine to get TF-IDF\n            tf_idf = term_freq * idf\n            doc_tf_idfs.append(round(tf_idf, 4))\n        \n        tf_idfs.append(doc_tf_idfs)\n    \n    return np.array(tf_idfs).tolist()\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\"],\n    [\"the\", \"lazy\", \"dog\"],\n    [\"the\", \"quick\", \"dog\"],\n]\nquery = [\"the\", \"quick\", \"dog\"]\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script defines the `compute_tf_idf` function as per the guidelines provided, incorporating handling for edge cases and smoothing. The function calculates the TF-IDF scores for the given query against the documents in the corpus and returns the results in a list of lists, with each sublist representing the TF-IDF scores for each document."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n    TP = np.sum((y_pred == 1) & (y_true == 1))\n    FP = np.sum((y_pred == 1) & (y_true == 0))\n    FN = np.sum((y_pred == 0) & (y_true == 1))\n\n    # Calculate Precision and Recall\n    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n\n    # Calculate F-Score\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_score_value, 3)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1  # For F1-Score, use beta=1\nprint(f_score(y_true, y_pred, beta))  # Output should be the F-Score rounded to three decimal places\n```"}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def dsigmoid(self, x):\n        return x * (1 - x)\n\n    def tanh_prime(self, x):\n        return 1 - x ** 2\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        hiddens = [np.zeros((self.hidden_size, 1))]  # Hidden states initialized to zero\n        outputs = []\n        inputs = []\n\n        for x in input_sequence:\n            x = np.reshape(x, (len(x), 1))  # Reshape input for dot product\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, hiddens[-1]) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            hiddens.append(h)\n            outputs.append(y)\n            inputs.append(x)\n\n        return outputs, inputs, hiddens\n\n    def backward(self, input_sequence, expected_output, outputs, inputs, hiddens, learning_rate):\n        \"\"\"\n        Backward pass through the RNN with BPTT.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n\n        d_h_next = np.zeros_like(hiddens[0])\n        loss = 0\n\n        for t in reversed(range(len(input_sequence))):\n            y = outputs[t]\n            x = inputs[t]\n            h = hiddens[t+1]\n\n            # Calculate the loss\n            loss += 0.5 * np.sum((y - np.reshape(expected_output[t], (len(y), 1))) ** 2)\n\n            # Output layer gradients\n            dy = y - np.reshape(expected_output[t], (len(y), 1))\n            dW_hy += np.dot(dy, h.T)\n            db_y += dy\n\n            # Hidden layer gradients\n            dh = np.dot(self.W_hy.T, dy) + d_h_next\n            dh_raw = (1 - h * h) * dh  # derivative of tanh\n            db_h += dh_raw\n            dW_hh += np.dot(dh_raw, hiddens[t].T)\n            dW_xh += np.dot(dh_raw, x.T)\n\n            # Update the gradient for the next step\n            d_h_next = np.dot(self.W_hh.T, dh_raw)\n\n        # Clip to prevent exploding gradients\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -1, 1, out=dparam)\n\n        # Update weights and biases\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n        return loss\n\n# Example of use\ninput_size = 5\nhidden_size = 10\noutput_size = 3\n\nrnn = SimpleRNN(input_size, hidden_size, output_size)\n\n# Dummy data\ninput_sequence = [np.random.rand(input_size) for _ in range(5)]\nexpected_output = [np.random.rand(output_size) for _ in range(5)]\n\noutputs, inputs, hiddens = rnn.forward(input_sequence)\nloss = rnn.backward(input_sequence, expected_output, outputs, inputs, hiddens, learning_rate=0.01)\n\nprint(\"Loss after backpropagation: \", loss)\n```"}
{"task_id": 63, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n\n    x = x0\n    r = np.dot(A, x) - b\n    p = -r\n    rsold = np.dot(r, r)\n\n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r + alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = -r + (rsnew / rsold) * p\n        rsold = rsnew\n\n    return np.round(x, decimals=8).tolist()\n\n# Example usage\nA = np.array([[4, 1], [1, 3]])\nb = np.array([1, 2])\nn = 100\n\nsolution = conjugate_gradient(A, b, n)\nprint(solution)\n```\n\nThis code snippet defines the `conjugate_gradient` function, which implements the Conjugate Gradient method for solving linear systems. The function accepts a matrix `A`, a vector `b`, a maximum number of iterations `n`, an optional initial guess vector `x0`, and a convergence tolerance `tol`. The function iterates up to `n` times or until the residual norm falls below `tol`, returning the solution vector `x` with each component rounded to 8 decimal places."}
{"task_id": 64, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n    \n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each unique element\n    values, counts = np.unique(y, return_counts=True)\n    \n    # Calculate probabilities\n    probabilities = counts / len(y)\n    \n    # Calculate Gini impurity\n    gini_imp = 1 - np.sum(probabilities**2)\n    \n    return round(gini_imp, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_example = [1, 2, 3, 4, 5]\n    print(gini_impurity(y_example))  # Expected output: 0.8\n    y_example2 = [1, 1, 1, 1, 1]\n    print(gini_impurity(y_example2))  # Expected output: 0.0\n```"}
{"task_id": 65, "completion_id": 0, "completion": "```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]  # The first row's pointer is always 0 in CSR format\n    \n    for row in dense_matrix:\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n        row_pointers.append(len(values))  # End of the row indicator\n    \n    return values, column_indices, row_pointers[:-1]  # Exclude the last value which is redundant\n\n# Check function to verify the solution\ndef check_solution():\n    # Test case 1\n    matrix1 = [\n        [1, 0, 0, 0],\n        [0, 0, 2, 0],\n        [3, 0, 0, 0],\n        [0, 4, 0, 0]\n    ]\n    values1, col_indices1, row_ptrs1 = compressed_row_sparse_matrix(matrix1)\n    assert values1 == [1, 2, 3, 4], \"Test case 1 failed: values array incorrect\"\n    assert col_indices1 == [0, 2, 0, 1], \"Test case 1 failed: column indices array incorrect\"\n    assert row_ptrs1 == [0, 1, 2, 3, 4], \"Test case 1 failed: row pointer array incorrect\"\n    \n    # Test case 2\n    matrix2 = [\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]\n    ]\n    values2, col_indices2, row_ptrs2 = compressed_row_sparse_matrix(matrix2)\n    assert not values2, \"Test case 2 failed: values array incorrect\"\n    assert not col_indices2, \"Test case 2 failed: column indices array incorrect\"\n    assert row_ptrs2 == [0, 0, 0, 0], \"Test case 2 failed: row pointer array incorrect\"\n    \n    print(\"All test cases passed!\")\n\ncheck_solution()\n```\n\nThe provided solution function `compressed_row_sparse_matrix` takes a dense matrix in the form of a 2D list and converts it into the Compressed Row Sparse (CSR) format by extracting non-zero values, their corresponding column indices, and row pointers. The check function `check_solution` tests this implementation with two matrices to ensure its correctness."}
{"task_id": 66, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Converting lists to numpy arrays for vector operations\n    vector_v = np.array(v)\n    vector_L = np.array(L)\n\n    # Scalar projection of v onto L\n    scalar_projection = np.dot(vector_v, vector_L) / np.linalg.norm(vector_L)\n\n    # Vector projection of v onto L\n    vector_projection = (scalar_projection * vector_L) / np.linalg.norm(vector_L)\n\n    # Return the result rounded to three decimal places\n    return [round(x, 3) for x in vector_projection]\n\n# Example usage:\n# Projection of vector [1, 2] onto vector [3, 4]\nprint(orthogonal_projection([1, 2], [3, 4]))  # Output should be the projection vector\n```"}
{"task_id": 67, "completion_id": 0, "completion": "```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        raise ValueError(\"Empty matrix\")\n    num_cols = len(dense_matrix[0])\n    \n    values = []\n    row_indices = []\n    column_pointer = [0]  # The first column always starts at index 0\n\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer.append(len(values))\n    \n    return values, row_indices, column_pointer\n\n# Check function to verify the correctness of the converted CSC matrix\ndef check_csc_conversion(dense_matrix):\n    values, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\n    print(\"Values:\", values)\n    print(\"Row Indices:\", row_indices)\n    print(\"Column Pointer:\", column_pointer)\n\n# Test the function with provided data points\ndense_matrix = [\n    [0, 2, 0],\n    [3, 0, 0],\n    [0, 0, 4],\n    [5, 0, 0]\n]\n\ncheck_csc_conversion(dense_matrix)\n```\n\nThis script correctly implements the conversion of a dense matrix into its Compressed Column Sparse (CSC) representation, as per the requirements outlined in the task description."}
{"task_id": 68, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef matrix_image(A):\n    \"\"\"\n    Given a matrix A, returns the basis vectors of its column space.\n    The basis vectors are extracted from the original matrix A and correspond to its independent columns.\n    The returned matrix is rounded to 8 decimal places.\n    \n    Args:\n    A: 2D numpy array representing the matrix.\n    \n    Returns:\n    A list of lists representing the matrix of basis vectors, rounded to 8 decimal places.\n    \"\"\"\n    # Convert input to numpy array for consistency\n    A_np = np.array(A)\n    # Perform row echelon form transformation\n    row_echelon = np.linalg.qr(A_np.T)[0].T\n    # Identify the independent columns\n    independent_cols = []\n    for col in range(row_echelon.shape[1]):\n        if np.linalg.norm(row_echelon[:, col]) > 1e-8:  # threshold to avoid numerical issues\n            independent_cols.append(col)\n    # Extract the corresponding columns from the original matrix\n    basis_vectors = A_np[:, independent_cols]\n    # Round the result and convert to list\n    basis_vectors_rounded = np.round(basis_vectors, 8).tolist()\n    return basis_vectors_rounded\n\n# Function to check the correctness of the solution\ndef check_solution():\n    A1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    A2 = [[1, 0], [0, 1]]\n    print(\"Basis for A1:\", matrix_image(A1))\n    print(\"Basis for A2:\", matrix_image(A2))\n\n# Run the check function\ncheck_solution()\n```\n\nThis solution uses QR decomposition to find the row echelon form of the matrix. It then uses this transformed matrix to identify the independent columns, from which it extracts the corresponding columns from the original matrix to form the basis for the column space. The result is rounded to 8 decimal places as requested and converted into a list format."}
{"task_id": 69, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value (coefficient of determination) for regression analysis.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the total sum of squares (TSS) and residual sum of squares (RSS)\n    tss = np.sum((y_true - np.mean(y_true)) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Calculate R-squared\n    r2 = 1 - (rss / tss)\n    \n    # Return R-squared rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\n\nprint(r_squared(y_true, y_pred))\n```\nThis script implements the calculation of R-squared, using the total sum of squares (TSS) and the residual sum of squares (RSS) to find how well the predicted values explain the variance in the true values."}
{"task_id": 70, "completion_id": 0, "completion": "```python\ndef calculate_brightness(img):\n    \"\"\"\n    Calculate the average brightness of a grayscale image represented by a 2D matrix.\n    \n    Parameters:\n    img (list of list of int): The grayscale image as a 2D matrix.\n    \n    Returns:\n    float: The average brightness rounded to two decimal places, or -1 if the image is invalid.\n    \"\"\"\n    if not img or not all(img):\n        # Edge case: empty matrix or empty rows\n        return -1\n    \n    total_brightness = 0\n    num_pixels = 0\n    \n    for row in img:\n        if len(row) != len(img[0]):\n            # Edge case: inconsistent row lengths\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                # Edge case: invalid pixel value\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    \n    # Calculate average brightness\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)\n\n# Test cases\nprint(calculate_brightness([[100, 100, 100], [100, 100, 100], [100, 100, 100]]))  # Expected output: 100.0\nprint(calculate_brightness([[255], [255], [255]]))  # Expected output: 255.0\nprint(calculate_brightness([[255, -1], [255, 255]]))  # Expected output: -1 (invalid pixel value)\nprint(calculate_brightness([]))  # Expected output: -1 (empty matrix)\nprint(calculate_brightness([[255, 255], [255]]))  # Expected output: -1 (inconsistent row lengths)\n```"}
{"task_id": 71, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual and predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of true target values.\n    y_pred (np.ndarray): Array of predicted target values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays have different shapes or are empty.\n    TypeError: If inputs are not of type np.ndarray or the elements cannot be converted to float.\n    \"\"\"\n    # Input Validation\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError(\"Inputs y_true and y_pred must be of type np.ndarray.\")\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays y_true and y_pred must have the same shape.\")\n    if y_true.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # RMSE Calculation\n    squared_errors = np.square(y_true - y_pred)\n    mse = np.mean(squared_errors)\n    rmse_value = np.sqrt(mse)\n    \n    return round(rmse_value, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = np.array([3.0, -0.5, 2.0, 7.0])\n    y_pred = np.array([2.5, 0.0, 2.0, 8.0])\n    print(\"RMSE:\", rmse(y_true, y_pred))  # Expected RMSE output: 0.612\n```"}
{"task_id": 72, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculates the Jaccard Index, a measure of similarity between two binary sets.\n    \n    Parameters:\n    - y_true: An iterable of true binary labels.\n    - y_pred: An iterable of predicted binary labels.\n    \n    Returns:\n    - The Jaccard Index rounded to three decimal places as a float.\n    \"\"\"\n    # Convert the inputs to numpy arrays if they are not already.\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where both sets are empty\n    if union == 0:\n        return 1.0  # Perfect overlap when both sets are empty\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    return round(jaccard, 3)\n\n# Example check function to test the solution\ndef check_jaccard_index():\n    # Example data points\n    data_points = [([0, 1, 1, 1], [0, 1, 0, 1], 0.750),\n                   ([0, 0, 0, 0], [0, 0, 0, 0], 1.000),\n                   ([1, 0, 0, 0], [0, 0, 0, 1], 0.000),\n                   ([1, 1, 0, 0], [1, 0, 1, 0], 0.333)]\n    \n    for y_true, y_pred, expected in data_points:\n        result = jaccard_index(y_true, y_pred)\n        assert result == expected, f\"Expected {expected}, got {result} for y_true={y_true} and y_pred={y_pred}\"\n    print(\"All tests passed.\")\n\n# Run the check function\ncheck_jaccard_index()\n```"}
{"task_id": 73, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    :param y_true: numpy array of true binary labels.\n    :param y_pred: numpy array of predicted binary labels.\n    :return: Dice Score as a float rounded to 3 decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays for performance and consistency\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection between the two sets\n    intersection = np.sum(y_true * y_pred)\n    \n    # Calculate the number of elements in each set\n    elements_y_true = np.sum(y_true)\n    elements_y_pred = np.sum(y_pred)\n    \n    # Handle edge case where there are no true or predicted positives\n    if (elements_y_true + elements_y_pred) == 0:\n        return 1.0  # If both sets are empty, the score is 1\n    \n    # Calculate the Dice Score\n    dice = (2 * intersection) / (elements_y_true + elements_y_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Test cases to verify the correctness of the function\nassert dice_score([1, 0, 1, 0, 1, 0], [1, 0, 1, 0, 0, 0]) == 0.8\nassert dice_score([1, 1, 1, 1], [0, 1, 0, 1]) == 0.5\nassert dice_score([], []) == 1.0\nassert dice_score([0, 0, 0], [0, 0, 0]) == 1.0\nassert dice_score([1, 0, 0], [0, 0, 0]) == 0.0\n```"}
{"task_id": 74, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef create_hypervector(length, seed):\n    \"\"\"\n    Generates a random hypervector of a given dimension with a specific seed for reproducibility.\n    \"\"\"\n    np.random.seed(seed)\n    hv = np.random.choice([-1, 1], size=length)\n    return hv\n\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row using HDC.\n    Each feature in the row is represented by binding hypervectors for the feature name and its value.\n    The hypervectors for the values are created using the same feature seed provided in the `random_seeds` dictionary.\n    All feature hypervectors are then bundled to create a composite hypervector for the row.\n    \n    :param row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    :param dim: The dimensionality of the hypervectors.\n    :param random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    :return: A composite hypervector representing the entire row as a list.\n    \"\"\"\n    # Initialize the composite hypervector\n    composite_hv = np.zeros(dim)\n    \n    for feature, value in row.items():\n        # Generate the hypervector for the feature name\n        feature_hv = create_hypervector(dim, random_seeds[feature])\n        \n        # Generate the hypervector for the value\n        value_hv = create_hypervector(dim, hash((feature, value)))\n        \n        # Bind the feature and value hypervectors by element-wise multiplication\n        bound_hv = np.multiply(feature_hv, value_hv)\n        \n        # Bundle the bound hypervectors to form the composite hypervector\n        composite_hv = np.add(composite_hv, bound_hv)\n    \n    # Convert the composite hypervector to a list before returning\n    return composite_hv.tolist()\n\n# Example usage:\nrow_example = {'feature1': 0.5, 'feature2': 0.8, 'feature3': 0.3}\ndim_example = 1000\nrandom_seeds_example = {'feature1': 10, 'feature2': 20, 'feature3': 30}\ncomposite_hv_example = create_row_hv(row_example, dim_example, random_seeds_example)\nprint(composite_hv_example)\n```\n\nThis script defines a function `create_row_hv` which takes a dictionary representing a single dataset row, a dimensionality `dim` for the hypervectors, and a dictionary `random_seeds` to ensure the reproducibility of hypervectors for the same features. It generates a composite hypervector for the row by binding hypervectors of feature names and their values, then bundling these bindings. The output is a list representing the composite hypervector."}
{"task_id": 75, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a confusion matrix for binary classification.\n\n    Args:\n    data (list of lists): A list where each element is a list [y_true, y_pred] representing\n                          the true and predicted labels for a single observation.\n\n    Returns:\n    list of lists: A 2x2 confusion matrix as [[TP, FP], [FN, TN]].\n    \"\"\"\n    # Initialize a counter to keep track of the confusion matrix elements\n    cm = Counter()\n    for y_true, y_pred in data:\n        # True Positive (TP): Correctly predicted positive\n        if y_true == 1 and y_pred == 1:\n            cm['TP'] += 1\n        # False Positive (FP): Incorrectly predicted as positive\n        elif y_true == 0 and y_pred == 1:\n            cm['FP'] += 1\n        # False Negative (FN): Incorrectly predicted as negative\n        elif y_true == 1 and y_pred == 0:\n            cm['FN'] += 1\n        # True Negative (TN): Correctly predicted negative\n        elif y_true == 0 and y_pred == 0:\n            cm['TN'] += 1\n\n    # Extract TP, FP, FN, TN from the counter, with default value 0 if not present\n    TP = cm.get('TP', 0)\n    FP = cm.get('FP', 0)\n    FN = cm.get('FN', 0)\n    TN = cm.get('TN', 0)\n\n    # Return the confusion matrix as a list of lists\n    return [[TP, FP], [FN, TN]]\n\n# Example usage\ndata = [[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]]\nprint(confusion_matrix(data))\n# Expected output: [[2, 1], [1, 2]]\n```"}
{"task_id": 76, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors v1 and v2.\n    \n    Parameters:\n    v1 (numpy.ndarray): The first input vector.\n    v2 (numpy.ndarray): The second input vector.\n    \n    Returns:\n    float: The cosine similarity rounded to three decimal places.\n    \"\"\"\n    # Ensure the vectors are of the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape\")\n    \n    # Check for zero magnitude vectors\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Input vectors cannot have zero magnitude\")\n    \n    # Calculate dot product\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate magnitude of vectors\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    \n    # Calculate cosine similarity\n    cos_sim = dot_product / (norm_v1 * norm_v2)\n    \n    return round(cos_sim, 3)\n\n# Example usage:\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))\n```\n\nThis script provides a function `cosine_similarity(v1, v2)` to calculate the cosine similarity between two vectors using NumPy for efficient computations. It includes the necessary error checks and returns the result rounded to three decimal places as requested."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\nimport numpy as np\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    \"\"\"\n    Calculate various performance metrics for a binary classification problem.\n    \"\"\"\n    # Ensure inputs are valid\n    assert len(actual) == len(predicted), \"Both lists must have the same length.\"\n    assert all(x in [0, 1] for x in actual), \"All elements in actual must be either 0 or 1.\"\n    assert all(x in [0, 1] for x in predicted), \"All elements in predicted must be either 0 or 1.\"\n    \n    # Count occurrences\n    counts = Counter(zip(actual, predicted))\n    \n    tp = counts[(1, 1)]  # True Positives\n    tn = counts[(0, 0)]  # True Negatives\n    fp = counts[(0, 1)]  # False Positives\n    fn = counts[(1, 0)]  # False Negatives\n    \n    # Prepare confusion matrix\n    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n    \n    # Calculate metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    specificity = round(specificity, 3)\n    negative_predictive_value = tn / (tn + fn) if (tn + fn) > 0 else 0\n    negative_predictive_value = round(negative_predictive_value, 3)\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage\nactual = [1, 0, 1, 1, 0, 1]\npredicted = [1, 0, 1, 0, 0, 1]\n\nconf_matrix, acc, f1, spec, npv = performance_metrics(actual, predicted)\nprint(f\"Confusion Matrix: \\n{conf_matrix}\")\nprint(f\"Accuracy: {acc}\")\nprint(f\"F1 Score: {f1}\")\nprint(f\"Specificity: {spec}\")\nprint(f\"Negative Predictive Value: {npv}\")\n```\n\nThis script defines a function `performance_metrics` that calculates several performance metrics for a binary classification model based on the provided actual and predicted labels. It uses basic Python and Numpy for the calculations and ensures the return values are rounded to three decimal places as required."}
{"task_id": 78, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates and returns various descriptive statistics for a given dataset.\n    \n    Parameters:\n    data (list or numpy.ndarray): A list or array of numerical values.\n    \n    Returns:\n    dict: A dictionary containing computed statistics.\n    \"\"\"\n    # Convert data to numpy array for consistency\n    data = np.array(data)\n    \n    # Calculate various statistics\n    mean = np.mean(data).round(4)\n    median = np.median(data).round(4)\n    mode = stats.mode(data)[0][0]  # Scipy returns mode with additional information\n    variance = np.var(data, ddof=1).round(4)  # Sample variance\n    standard_deviation = np.std(data, ddof=1).round(4)  # Sample standard deviation\n    percentile_25 = np.percentile(data, 25).round(4)\n    percentile_50 = np.percentile(data, 50).round(4)\n    percentile_75 = np.percentile(data, 75).round(4)\n    \n    # Calculate Interquartile Range (IQR)\n    iqr = (percentile_75 - percentile_25).round(4)\n    \n    # Prepare and return the result as a dictionary\n    statistics_dict = {\n        'mean': mean,\n        'median': median,\n        'mode': mode,\n        'variance': variance,\n        'standard_deviation': standard_deviation,\n        '25th_percentile': percentile_25,\n        '50th_percentile': percentile_50,\n        '75th_percentile': percentile_75,\n        'interquartile_range': iqr\n    }\n    \n    return statistics_dict\n\n# Example usage\ndata = np.random.randn(100)  # Generate some random data\nstats_results = descriptive_statistics(data)\nfor key, value in stats_results.items():\n    print(f\"{key}: {value}\")\n```"}
{"task_id": 79, "completion_id": 0, "completion": "```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials, rounded to five decimal places\n    \"\"\"\n    # Calculate binomial coefficient\n    binom_coeff = math.comb(n, k)\n    # Calculate the probability using Binomial distribution formula\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the rounded probability to five decimal places\n    return round(probability, 5)\n\n# Example usage:\n# n = 10, k = 5, p = 0.5\nprint(binomial_probability(10, 5, 0.5))\n```\n\nThis script defines a function `binomial_probability` that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials, each with a success probability of `p`. The function uses the binomial distribution formula and rounds the result to five decimal places as required."}
{"task_id": 80, "completion_id": 0, "completion": "```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    exponent = math.exp(-((x - mean) ** 2) / (2 * std_dev ** 2))\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = exponent / denominator\n    return round(pdf_value, 5)\n\n# Example checks\nprint(normal_pdf(0, 0, 1))  # Should be close to 0.39894\nprint(normal_pdf(1, 0, 1))  # Should be close to 0.24197\n```"}
{"task_id": 81, "completion_id": 0, "completion": "```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    probability = (math.exp(-lam) * (lam ** k)) / math.factorial(k)\n    return round(probability, 5)\n\n# Example usage\nk = 2\nlam = 3.5\nprint(poisson_probability(k, lam))  # Output should be the calculated probability\n```"}
{"task_id": 82, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a random grayscale image\n    img = np.random.randint(0, 256, size=(100, 100), dtype=np.uint8)\n    print(\"Contrast of the image:\", calculate_contrast(img))\n```"}
{"task_id": 83, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError(\"Both vectors must be of the same length.\")\n    \n    return np.dot(vec1, vec2)\n\n# Example usage:\nvector1 = np.array([1, 3, -5])\nvector2 = np.array([4, -2, -1])\nprint(calculate_dot_product(vector1, vector2)) # Should output 3\n```"}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of a data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the input list to a numpy array for easier manipulation\n    data_np = np.array(data).reshape(-1, 1)\n    \n    # Generate the polynomial features\n    expanded_data = np.hstack([data_np ** d for d in range(1, degree + 1)])\n    \n    # Convert the numpy array back to a list of lists and round the values\n    expanded_data_rounded = [np.round(row, 8).tolist() for row in expanded_data]\n    \n    return expanded_data_rounded\n\n# Check the correctness of the function\ndef check_phi_transform():\n    test_inputs = {\n        ([1, 2, 3], 0): [],\n        ([1, 2, 3], 1): [[1.0], [2.0], [3.0]],\n        ([1, 2, 3], 2): [[1.0, 1.0], [2.0, 4.0], [3.0, 9.0]],\n        ([1, 2, 3], 3): [[1.0, 1.0, 1.0], [2.0, 4.0, 8.0], [3.0, 9.0, 27.0]]\n    }\n    for input_data, expected in test_inputs.items():\n        output = phi_transform(*input_data)\n        assert output == expected, f\"Failed for input {input_data}. Expected {expected}, got {output}\"\n    print(\"All checks passed.\")\n\n# Running the test cases to verify the correctness of the function\ncheck_phi_transform()\n```\n\nThis Python script defines a `phi_transform` function that generates polynomial features for the input data by raising it to the powers from 1 up to the specified degree, and checks the correctness of this function with predefined inputs and outputs."}
{"task_id": 85, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculate positional encodings for a given sequence length and model dimensionality.\n    \n    Args:\n        position (int): The length of the sequence.\n        d_model (int): The dimensionality of the model.\n    \n    Returns:\n        list or int: A list of positional encodings if valid input, otherwise -1.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n\n    # Create a new array of shape (position, d_model)\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    # Calculate the denominator for each dimension\n    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n    \n    # For each position calculate the sin and cos components\n    for pos in range(position):\n        # Apply the sin or cos function based on the dimension's index\n        pe[pos, 0::2] = np.sin(pos * div_term)\n        pe[pos, 1::2] = np.cos(pos * div_term)\n    \n    # Convert the array to a list and return\n    return pe.tolist()\n\n# Example checks\nprint(pos_encoding(3, 4))\nprint(pos_encoding(0, 4))\nprint(pos_encoding(3, 0))\n```"}
{"task_id": 86, "completion_id": 0, "completion": "```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Overfitting condition: Training accuracy is significantly higher than test accuracy\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    # Underfitting condition: Both training and test accuracy are below 0.7\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # Good fit: Neither Overfitting nor Underfitting\n    else:\n        return 0\n\n# Example usage\nif __name__ == \"__main__\":\n    # Test cases to verify the correctness\n    print(model_fit_quality(0.95, 0.75))  # Expected output: 1 (Overfitting)\n    print(model_fit_quality(0.65, 0.60))  # Expected output: -1 (Underfitting)\n    print(model_fit_quality(0.80, 0.78))  # Expected output: 0 (Good fit)\n```"}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    \n    # Update biased second moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    \n    # Bias correction\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    \n    # Update parameter\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    return parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist()\n\n# Example usage:\nparameter = np.array([0.5])\ngrad = np.array([0.2])\nm = np.array([0.0])  # Initial first moment estimate\nv = np.array([0.0])  # Initial second moment estimate\nt = 1  # Current timestep\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(updated_parameter, updated_m, updated_v)\n```\nIn this script, the `adam_optimizer` function implements the update step for the Adam optimizer. It takes the current parameter values, gradients, and moving averages as inputs and returns the updated parameter and moving averages, rounded to 5 decimal places and converted to a list. The example usage demonstrates how to call the function with initial values."}
{"task_id": 88, "completion_id": 0, "completion": "To address this task, I will provide a Python script that includes the simplified GPT-2-like text generation function. The script will be self-contained and include the required function and helper function.\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    context = encoder.encode(prompt)\n    \n    for _ in range(n_tokens_to_generate):\n        # In a real GPT-2 implementation, compute attention and apply transformations.\n        # For this simplified version, let's just randomly select the next token.\n        context.append(np.random.choice([0, 1, 2]))  # 0, 1, 2 are the token ids in the dummy example.\n    \n    return encoder.decode(context)\n\n# Example usage\nprompt_text = \"hello world\"\ngenerated_text = gen_text(prompt_text, n_tokens_to_generate=50)\nprint(generated_text)\n```\n\nThis script includes a simplified version of a GPT-2-like text generation function. The `gen_text` function is designed to handle the generation of text based on a given prompt. For demonstration purposes, the next token in the sequence is selected randomly rather than through an actual GPT-2 attention and feed-forward network.\n\nPlease adapt the `gen_text` function to include the actual attention mechanisms, feed-forward networks, and other components of the GPT-2 architecture according to the available parameters and hyper-parameters in `params` and `hparams` for a more accurate implementation. The provided script serves as a starting point for understanding the sequence of operations involved in GPT-2-like text generation."}
{"task_id": 89, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=-1, keepdims=True)\n    \n    # Convert the list of crystal values into a numpy array\n    crystals = np.array(crystal_values).reshape(n, dimension)\n    \n    # Compute the dot product between each pair of crystals to get raw attention scores\n    attention_scores = np.dot(crystals, crystals.T)\n    \n    # Apply the softmax function to get the attention weights\n    attention_weights = softmax(attention_scores)\n    \n    # Derive the final weighted pattern for each crystal\n    weighted_patterns = np.dot(attention_weights, crystals)\n    \n    # Flatten the result and round to the 4th decimal place\n    final_pattern = [round(num, 4) for num in weighted_patterns.flatten()]\n    \n    return final_pattern\n\n# Example usage:\nn = 3\ncrystal_values = [1, 2, 3, 4, 5, 6]\ndimension = 2\nresult = pattern_weaver(n, crystal_values, dimension)\nprint(result)\n```\n\nIn this script, the `pattern_weaver` function takes in the number of crystals `n`, a list of `crystal_values`, and the `dimension` of each crystal. It first defines a helper function `softmax` to apply the softmax function over the attention scores. It then computes the attention scores as the dot product between each pair of crystals. Afterward, it applies the softmax function to get the attention weights and uses these weights to derive the final weighted pattern for each crystal. The final result is a list of these flattened and rounded weighted patterns."}
{"task_id": 90, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for a given corpus of documents and a query.\n\n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :param k1: Term frequency saturation parameter. Default is 1.5.\n    :param b: Document length normalization parameter. Default is 0.75.\n    :return: A list of scores for each document in the corpus.\n    \"\"\"\n    if not corpus or not query:\n        return []\n\n    avgdl = sum(len(doc) for doc in corpus) / len(corpus)\n    N = len(corpus)\n    idf = calculate_idf(corpus, query)\n\n    scores = []\n    for doc in corpus:\n        score = 0\n        for term in query:\n            f_t_d = doc.count(term)\n            score += idf[term] * (f_t_d * (k1 + 1)) / (f_t_d + k1 * (1 - b + b * len(doc) / avgdl))\n        scores.append(round(score, 3))\n    \n    return scores\n\ndef calculate_idf(corpus, query):\n    \"\"\"\n    Calculates the inverse document frequency for each term in the query from the corpus.\n\n    :param corpus: List of documents, where each document is a list of words.\n    :param query: A list of words representing the query.\n    :return: A dictionary mapping each term in the query to its IDF value.\n    \"\"\"\n    term_in_doc_counts = {term: 0 for term in query}\n    for doc in corpus:\n        for term in query:\n            if term in doc:\n                term_in_doc_counts[term] += 1\n\n    idf = {term: np.log((N - term_in_doc_counts[term] + 0.5) / (term_in_doc_counts[term] + 0.5)) for term in query}\n    return idf\n\n# Example usage:\ncorpus = [[\"term1\", \"term2\", \"term3\"], [\"term1\", \"term1\", \"term4\"], [\"term2\", \"term5\"]]\nquery = [\"term1\", \"term2\"]\nprint(calculate_bm25_scores(corpus, query))\n```\n\nIn the above script, `calculate_bm25_scores` computes the BM25 scores for a given corpus of documents and a query. The `calculate_idf` function is a helper function that computes the inverse document frequency for the terms in the query. The example usage demonstrates how to use the `calculate_bm25_scores` function with a simple corpus and query."}
{"task_id": 91, "completion_id": 0, "completion": "```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize true positives (TP), false positives (FP), and false negatives (FN)\n    TP = FP = FN = 0\n\n    # Generate a set of unique elements from both y_true and y_pred\n    unique_elements = set(y_true) | set(y_pred)\n\n    # Count true positives, false positives, and false negatives for each class\n    for elem in unique_elements:\n        TP += sum([1 for i in range(len(y_true)) if y_true[i] == elem and y_pred[i] == elem])\n        FP += sum([1 for i in range(len(y_true)) if y_true[i] != elem and y_pred[i] == elem])\n        FN += sum([1 for i in range(len(y_true)) if y_true[i] == elem and y_pred[i] != elem])\n\n    # Calculate precision and recall\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n\n    # Calculate and return the F1 score\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\n\n# Example usage\ny_true = [1, 1, 0, 1, 0, 0, 1, 1]\ny_pred = [1, 0, 0, 1, 1, 0, 0, 1]\nprint(calculate_f1_score(y_true, y_pred))  # Expected output: 0.667\n```"}
{"task_id": 92, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Constants\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Remove the known fluctuation from each data point, fit a linear regression model to the detrended data,\n    predict day 15's base consumption, add back the fluctuation for day 15, and finally include a 5% safety margin.\n    :param consumption_data: A list of integers representing the power consumption of the first 10 days.\n    :return: An integer representing the predicted consumption for day 15 with a 5% safety margin.\n    \"\"\"\n    # 1) Subtract the daily fluctuation from each data point\n    detrended_data = [consumption_data[i] - 10 * math.sin(2 * PI * (i + 1) / 10) for i in range(len(consumption_data))]\n    \n    # 2) Perform linear regression on the detrended data\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    \n    # 3) Predict day 15's base consumption\n    day_15 = np.array([15]).reshape(-1, 1)\n    predicted_base_consumption = model.predict(day_15)\n    \n    # 4) Add the day 15 fluctuation back\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = predicted_base_consumption + day_15_fluctuation\n    \n    # 5) Round, then add a 5% safety margin (round up)\n    final_prediction = math.ceil(predicted_consumption * 1.05)\n    \n    # 6) Return the final integer\n    return final_prediction[0]\n\n# Example usage\nconsumption_data = [100, 110, 115, 120, 130, 140, 150, 160, 170, 180]\nprint(power_grid_forecast(consumption_data))\n```\nThis script uses the numpy library for handling arrays and the LinearRegression model from sklearn to perform the linear regression, which is an advanced technique from machine learning but fits perfectly for the task of predicting power consumption on Mars."}
{"task_id": 93, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError(\"y_true and y_pred should be numpy.ndarray type\")\n    \n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"y_true and y_pred must have the same shape\")\n\n    errors = np.abs(y_true - y_pred)\n    return round(np.mean(errors), 3)\n\n# Example usage:\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(mae(y_true, y_pred))  # Expected output: 0.500\n```"}
{"task_id": 94, "completion_id": 0, "completion": "Below is a self-contained Python script that implements the multi-head attention mechanism by leveraging NumPy for numerical computations. The script includes three main functions: `compute_qkv` for computing the queries, keys, and values; `self_attention` for computing the attention scores; and `multi_head_attention` for the overall multi-head attention mechanism.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the queries, keys, and values for multi-head attention.\n    \n    Parameters:\n    X: Input data of shape (n_tokens, d_model).\n    W_q, W_k, W_v: Weight matrices for queries, keys, and values, each of shape (d_model, d_model).\n\n    Returns:\n    Tuple of Q, K, V matrices of shape (n_tokens, d_model).\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention given the queries, keys, and values.\n    \n    Parameters:\n    Q, K, V: Matrices of queries, keys, and values, each of shape (n_tokens, d_model).\n\n    Returns:\n    Output matrix of shape (n_tokens, d_model).\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attn = np.dot(np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True), V)\n    return attn\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Implements the multi-head attention mechanism.\n    \n    Parameters:\n    X: Input data of shape (n_tokens, d_model).\n    W_q, W_k, W_v: Weight matrices for queries, keys, and values, each of shape (n_heads, d_model, d_model).\n    n_heads: Number of attention heads.\n\n    Returns:\n    List of shape (n_heads, n_tokens, d_model / n_heads) after rounding to the 4th decimal.\n    \"\"\"\n    n_tokens, d_model = X.shape\n    d_k = d_model // n_heads\n    results = []\n    for i in range(n_heads):\n        Q, K, V = compute_qkv(X, W_q[i], W_k[i], W_v[i])\n        # Split and reshape for multi-head\n        Q = Q.reshape(n_tokens, n_heads, -1)[:, i, :]\n        K = K.reshape(n_tokens, n_heads, -1)[:, i, :]\n        V = V.reshape(n_tokens, n_heads, -1)[:, i, :]\n        attn = self_attention(Q, K, V)\n        results.append(attn.reshape(-1, d_k))\n    return np.stack(results, axis=0).round(4).tolist()\n\n# Example usage\nX = np.random.rand(10, 512)  # 10 tokens, 512 token features\nW_q = np.random.rand(8, 512, 512)  # 8 heads\nW_k = np.random.rand(8, 512, 512)\nW_v = np.random.rand(8, 512, 512)\n\noutput = multi_head_attention(X, W_q, W_k, W_v, 8)\nprint(output)\n```\n\nThis script generates random input and weight matrices, computes the multi-head attention, and then prints the output. The number of heads (`n_heads`) is set to 8 for demonstration purposes, and the output arrays are rounded to the nearest 4th decimal place as requested."}
{"task_id": 95, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    # Convert lists to numpy arrays for efficient computation\n    x_array = np.array(x)\n    y_array = np.array(y)\n    \n    # Compute the contingency table\n    n11 = np.sum(x_array * y_array)\n    n10 = np.sum(x_array * (1 - y_array))\n    n01 = np.sum((1 - x_array) * y_array)\n    n00 = np.sum((1 - x_array) * (1 - y_array))\n    \n    # Calculate the Phi coefficient formula\n    numerator = n11 * n00 - n10 * n01\n    denominator = np.sqrt((n10 + n11) * (n00 + n01) * (n10 + n00) * (n11 + n01))\n    \n    # Avoid division by zero\n    if denominator == 0:\n        return 0.0\n    \n    phi_coefficient = numerator / denominator\n    \n    # Return the Phi coefficient rounded to 4 decimal places\n    return round(phi_coefficient, 4)\n\n# Test the function to verify its correctness\ndef check_phi_corr():\n    x1 = [1, 0, 1, 0, 1, 0, 1, 0]\n    y1 = [1, 1, 0, 0, 1, 1, 0, 0]\n    print(\"Phi Coefficient (x1, y1):\", phi_corr(x1, y1))\n\n    x2 = [1, 0, 0, 1, 1, 0, 0, 1]\n    y2 = [0, 1, 1, 0, 0, 1, 1, 0]\n    print(\"Phi Coefficient (x2, y2):\", phi_corr(x2, y2))\n\n# Run the test function\ncheck_phi_corr()\n```\n\nThis code snippet defines a function `phi_corr` to calculate the Phi coefficient between two binary variables `x` and `y`. It then tests the function with two sets of data points to verify its correctness. The `check_phi_corr` function is used to output the results, which should show the Phi coefficient for the provided binary variables."}
{"task_id": 96, "completion_id": 0, "completion": "```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n    \n    Args:\n        x (float): Input value\n    \n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5\n\n# Test the function with some input values\ntest_values = [-3.0, -2.5, -1.0, 0.0, 1.0, 2.5, 3.0]\nfor value in test_values:\n    print(f\"hard_sigmoid({value}) = {hard_sigmoid(value)}\")\n```\n\nThis script defines the `hard_sigmoid` function according to its mathematical definition, where the function output is 0 if the input is less than or equal to -2.5, 1 if the input is greater than or equal to 2.5, and linearly interpolates between 0 and 1 otherwise. The function is tested with a set of predefined values to ensure its correctness."}
{"task_id": 97, "completion_id": 0, "completion": "```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)\n        \n# Example usage:\n# Test with default alpha value\nprint(round(elu(-1.0), 4))  # Example input, feel free to change\n\n# Test with custom alpha value\nprint(round(elu(-1.0, 0.5), 4))  # Example input, feel free to change\n```"}
{"task_id": 98, "completion_id": 0, "completion": "```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    return max(0, x) + alpha * min(0, x)\n\n# Example usage\nif __name__ == \"__main__\":\n    test_cases = [\n        (-1, 0.25),\n        (0, 0.25),\n        (1, 0.25),\n        (-2, 0.1),\n        (2, 0.1)\n    ]\n    \n    for x, alpha in test_cases:\n        print(f\"prelu({x}, {alpha}) = {prelu(x, alpha)}\")\n```\n\nThis code snippet provides a straightforward implementation of the PReLU activation function, including a few example cases to demonstrate its usage."}
{"task_id": 99, "completion_id": 0, "completion": "```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n    \n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(0, 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)\n\n# Example usage\nprint(softplus(1.0))  # Example input, can be changed to test with different values\n```"}
{"task_id": 100, "completion_id": 0, "completion": "```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)\n\n# Example usage:\n# print(softsign(0.5))  # Output: 0.3333\n# print(softsign(-2.0)) # Output: -0.6667\n```"}
{"task_id": 101, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for convenience\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Compute clipped likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute GRPO objective value\n    grpo_val = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # Compute KL divergence between old policy and reference policy\n    kl_div = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Apply KL divergence penalty\n    grpo_val -= beta * kl_div\n    \n    # Return rounded result\n    return round(grpo_val, 6)\n```\n\nThis code snippet defines the `grpo_objective` function that calculates the GRPO objective function. It first converts the inputs to numpy arrays, then computes the clipped likelihood ratios. It calculates the GRPO objective using the minimum of the original likelihood ratios and the clipped ones multiplied by the advantage estimates. A KL divergence penalty is applied to stabilize training, with the final result rounded to the nearest 6th decimal."}
{"task_id": 102, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # The Swish function is defined as f(x) = x * sigmoid(x)\n    sigmoid_x = 1 / (1 + math.exp(-x))\n    return round(x * sigmoid_x, 4)\n\n# Example usage\ninput_value = 0.5\noutput_value = swish(input_value)\nprint(f\"Swish({input_value}) =\", output_value)\n```"}
{"task_id": 103, "completion_id": 0, "completion": "```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)\n\n# Example usage and verification:\ndef check_selu():\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    expected_outputs = [0.1754, 0.0838, 0.0, 1.0507, 2.1014]\n    for val, expected in zip(test_values, expected_outputs):\n        computed = round(selu(val), 4)\n        assert computed == expected, f\"Error: selu({val}) = {computed}, expected {expected}\"\n    print(\"All tests passed!\")\n\ncheck_selu()\n```"}
{"task_id": 104, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    def sigmoid(z: np.ndarray) -> np.ndarray:\n        \"\"\"Compute the output of the sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    # Compute the logits\n    logits = np.dot(X, weights) + bias\n    \n    # Apply the sigmoid function to compute probabilities\n    probabilities = sigmoid(logits)\n    \n    # Convert probabilities to binary predictions\n    predictions = (probabilities >= 0.5).astype(int)\n\n    return predictions.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    X = np.array([[1,2,3], [4,5,6], [7,8,9]])\n    weights = np.array([0.1, 0.2, 0.3])\n    bias = 0.5\n    predictions = predict_logistic(X, weights, bias)\n    print(predictions)\n```"}
{"task_id": 105, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(Z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the softmax of the input array Z.\n    \"\"\"\n    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n\ndef one_hot(y: np.ndarray, num_classes: int) -> np.ndarray:\n    \"\"\"\n    Convert an array of class indices to a one-hot encoded matrix.\n    \"\"\"\n    y_one_hot = np.zeros((num_classes, y.size))\n    y_one_hot[y, np.arange(y.size)] = 1\n    return y_one_hot\n\ndef cross_entropy_loss(Y_pred: np.ndarray, Y_true: np.ndarray) -> float:\n    \"\"\"\n    Compute the cross entropy loss between predictions and ground-truth labels.\n    \"\"\"\n    epsilon = 1e-15\n    Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n    return -np.sum(Y_true * np.log(Y_pred)) / Y_pred.shape[1]\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_features, n_samples = X.shape\n    num_classes = len(np.unique(y))\n    \n    # Initialize weights\n    B = np.random.rand(num_classes, n_features)\n    \n    # Convert y to one-hot encoding\n    Y_hot = one_hot(y, num_classes)\n    \n    losses = []\n    for _ in range(iterations):\n        # Forward pass\n        Z = B @ X\n        Y_pred = softmax(Z)\n        \n        # Compute loss\n        loss = cross_entropy_loss(Y_pred, Y_hot)\n        losses.append(round(loss, 4))\n        \n        # Compute gradient\n        grad = (1 / n_samples) * (Y_pred - Y_hot) @ X.T\n        \n        # Update weights\n        B -= learning_rate * grad\n    \n    return B.round(4).tolist(), [loss for loss in losses]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2, 3, 4, 5],\n                  [3, 4, 5, 6, 7]])\n    y = np.array([0, 1, 0, 1, 2])\n    \n    # Training parameters\n    learning_rate = 0.01\n    iterations = 1000\n    \n    # Train the model\n    B, losses = train_softmaxreg(X, y, learning_rate, iterations)\n    print(\"Optimized Coefficients:\\n\", B)\n    print(\"Losses:\\n\", losses)\n```"}
{"task_id": 106, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function implementation.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add intercept term\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    weights = np.zeros(X.shape[1])\n    loss_history = []\n\n    for _ in range(iterations):\n        # Predicted probabilities\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        \n        # Compute gradient\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        \n        # Update weights\n        weights -= learning_rate * gradient\n        \n        # Compute and store loss\n        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        loss_history.append(round(loss, 4))\n    \n    # Collect optimized coefficients\n    optimized_coeffs = np.round(weights, 4).tolist()\n    \n    return (optimized_coeffs, loss_history)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example data\n    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n    y = np.array([0, 0, 1, 1])\n    \n    # Training parameters\n    learning_rate = 0.1\n    iterations = 1000\n    \n    # Train logistic regression\n    optimized_coeffs, loss_history = train_logreg(X, y, learning_rate, iterations)\n    \n    print(\"Optimized Coefficients:\", optimized_coeffs)\n    print(\"Loss History:\", loss_history)\n```\n\nThis code snippet defines a function `train_logreg` that takes as input a dataset `X` (features), `y` (target labels), a `learning_rate`, and the number of `iterations` for gradient descent. It uses the Binary Cross Entropy loss function to optimize the parameters of a logistic regression model and returns the optimized coefficients and the loss values over iterations, both rounded to the nearest 4th decimal. The example usage at the end demonstrates how to call the function with some mock data and print the results."}
{"task_id": 107, "completion_id": 0, "completion": "Below is a self-contained Python script to solve the task of implementing masked self-attention. The script includes the initialization of matrices, the computation of Q, K, V matrices, and the masked self-attention computation with an output in the required format.\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Compute the attention scores\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    \n    # Apply the mask (replace -inf with very small negative value to avoid NaNs)\n    scores = np.where(mask, scores, -np.inf)\n    \n    # Compute softmax across the last dimension\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    \n    # Apply the attention weights on the value matrix\n    output = np.dot(attention_weights, V)\n    return output.tolist()\n\n# Example usage:\n\n# Initialize the input and weight matrices\nX = np.array([[1, 2], [3, 4], [5, 6]])  # Example input matrix\nW_q = np.array([[1, 0], [0, 1]])        # Example weight matrix for Q\nW_k = np.array([[1, 0], [0, 1]])        # Example weight matrix for K\nW_v = np.array([[1, 0], [0, 1]])        # Example weight matrix for V\n\n# Compute Q, K, V matrices\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\n\n# Example attention mask (True for positions to keep, False for positions to mask)\nmask = np.array([[True, False, False],\n                 [True, True, False],\n                 [True, True, True]])\n\n# Compute masked attention\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```\n\nThis script assumes that the provided `mask` in the `masked_attention()` function is a boolean mask, where `True` indicates positions that are not masked and `False` indicates the positions that should be masked. The script outputs a Python list representing the masked self-attention results."}
{"task_id": 108, "completion_id": 0, "completion": "```python\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is defined as the Shannon entropy of the distribution\n    of colors in the basket of apples.\n    \"\"\"\n    total_apples = len(apples)\n    if total_apples == 0:\n        return 0.0\n    \n    # Count the frequency of each color in the basket\n    color_counts = Counter(apples)\n    \n    # Calculate the probabilities of each color\n    probabilities = [count / total_apples for count in color_counts.values()]\n    \n    # Calculate the Shannon entropy (disorder)\n    entropy = -sum(p * (p.log2()) for p in probabilities if p > 0)\n    \n    return round(entropy, 4)\n\n# Test cases\nprint(disorder([0, 0, 0, 0]))  # Expected output: 0.0\nprint(disorder([1, 1, 0, 0]))  # Expected output: 1.0\nprint(disorder([0, 1, 2, 3]))  # Expected output: 2.0\nprint(disorder([0, 0, 1, 1, 2, 2, 3, 3]))  # Expected output: 2.0\nprint(disorder([0, 0, 0, 0, 0, 1, 2, 3]))  # Expected output: 1.2144\n```\n\nThis script implements a function `disorder` to calculate the Shannon entropy, a measure of disorder or uncertainty, based on the distribution of apple colors in a basket. The entropy is calculated as the sum of probabilities of each color multiplied by the logarithm (base 2) of those probabilities, then negated, providing a measure that increases as the variety of colors increases. The results are rounded to the nearest 4th decimal to meet the output constraint."}
{"task_id": 109, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n    \n    Parameters:\n    - X: A 3D array of shape (batch_size, sequence_length, feature_dim)\n    - gamma: A 1D array of scaling parameters of shape (feature_dim,)\n    - beta: A 1D array of shifting parameters of shape (feature_dim,)\n    - epsilon: A small constant for numerical stability\n    \n    Returns:\n    - A 3D array of the same shape as X, with layer normalization applied\n    \"\"\"\n    # Calculate the mean across the feature dimension\n    mean = np.mean(X, axis=2, keepdims=True)\n    # Calculate the variance across the feature dimension\n    var = np.var(X, axis=2, keepdims=True)\n    # Normalize the input tensor X\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    # Apply scaling and shifting\n    X_scaled = gamma * X_norm + beta\n    # Return the normalized tensor\n    return np.round(X_scaled, 5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Dimensions\n    batch_size, sequence_length, feature_dim = 4, 7, 5\n    # Create random input tensor X\n    X = np.random.randn(batch_size, sequence_length, feature_dim)\n    # Create random scaling and shifting parameters\n    gamma = np.random.randn(feature_dim)\n    beta = np.random.randn(feature_dim)\n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    print(normalized_X)\n```"}
{"task_id": 110, "completion_id": 0, "completion": "```python\nimport numpy as np\nfrom collections import Counter\n\ndef compute_precision_recall(reference, candidate):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n\n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n\n    common_words = list((ref_counter & cand_counter).elements())\n    precision = len(common_words) / len(cand_words) if cand_words else 0\n    recall = len(common_words) / len(ref_words) if ref_words else 0\n    return precision, recall\n\ndef compute_fmean(precision, recall, beta=3):\n    if precision == 0 or recall == 0:\n        return 0\n    return ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall)\n\ndef compute_word_order_penalty(reference, candidate):\n    ref_words = reference.split()\n    cand_words = candidate.split()\n\n    common = set(ref_words) & set(cand_words)\n\n    ref_positions = {word: [i for i, x in enumerate(ref_words) if x == word] for word in common}\n    cand_positions = {word: [i for i, x in enumerate(cand_words) if x == word] for word in common}\n\n    distance_sum = 0\n    for word in common:\n        for ref_pos, cand_pos in zip(ref_positions[word], cand_positions[word]):\n            distance_sum += abs(ref_pos - cand_pos)\n\n    penalty = np.exp(-1 * gamma * distance_sum)\n    return penalty\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    precision, recall = compute_precision_recall(reference, candidate)\n    fmean = compute_fmean(precision, recall, beta)\n    penalty = compute_word_order_penalty(reference, candidate)\n    \n    score = (1 - alpha) * precision + alpha * fmean * penalty\n    return round(score, 3)\n\n# Test the function with example data\nprint(meteor_score(\"The cat sat on the mat\", \"The cat is on the mat\"))\n```"}
{"task_id": 111, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) value for two events given their joint\n    and individual occurrence counts and the total number of samples.\n    \n    Args:\n    joint_counts (int): The number of times both events occurred together.\n    total_counts_x (int): The total number of occurrences of event X.\n    total_counts_y (int): The total number of occurrences of event Y.\n    total_samples (int): The total number of samples in the data.\n    \n    Returns:\n    float: The PMI value rounded to 3 decimal places.\n    \"\"\"\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Avoiding division by zero and log of zero issues\n    if p_x == 0 or p_y == 0 or p_xy == 0:\n        return float('-inf')\n    \n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)\n\n# Example usage\njoint_counts = 10\ntotal_counts_x = 50\ntotal_counts_y = 40\ntotal_samples = 1000\n\nprint(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\nThis script defines a function `compute_pmi` that calculates the Pointwise Mutual Information for two events given their joint occurrence count, their individual counts, and the total number of samples in the dataset. The result is rounded to three decimal places as per the specification."}
{"task_id": 112, "completion_id": 0, "completion": "```python\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers.\n    This function scales all values to the range [0, 1] and returns a list of floats rounded to 4 decimal places.\n    \"\"\"\n    if len(x) < 2 or min(x) == max(x):  # Handle edge cases: empty list, single value, or all values are equal.\n        return [0.0000] * len(x)\n    \n    min_val = min(x)\n    max_val = max(x)\n    range_val = max_val - min_val\n    \n    normalized_x = [(i - min_val) / range_val for i in x]\n    return [round(i, 4) for i in normalized_x]\n\n# Check function to verify the correctness of the generated function.\ndef check_min_max_normalization():\n    data_points = [\n        ([1, 2, 3, 4, 5], [0.0, 0.25, 0.5, 0.75, 1.0]),\n        ([10, 20, 30], [0.0, 0.5, 1.0]),\n        ([5], [0.0]),\n        ([-1, -2, -3, -4], [1.0, 0.5, 0.0, 0.0]),\n        ([100, 100, 100, 100], [0.0, 0.0, 0.0, 0.0])\n    ]\n    for i, (input_data, expected_output) in enumerate(data_points):\n        result = min_max(input_data)\n        assert result == expected_output, f\"Test case {i} failed: expected {expected_output}, got {result}\"\n    print(\"All test cases passed.\")\n\ncheck_min_max_normalization()\n```\n\nThis script defines the `min_max` function which performs Min-Max normalization on a given list of integers. It then checks the correctness of this function using predefined data points."}
{"task_id": 113, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Parameters:\n    x (np.ndarray): 1D input array.\n    w1 (np.ndarray): Weight array for first layer. Must be compatible with x for matrix multiplication.\n    w2 (np.ndarray): Weight array for second layer. Must be compatible with w1*x for matrix multiplication.\n    \n    Returns:\n    np.ndarray: The output of the residual block, as a list, rounded to 4 decimal places.\n    \"\"\"\n    \n    # First layer: matrix multiply with w1 and apply ReLU\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(z1, 0)\n    \n    # Second layer: matrix multiply with w2 and apply ReLU\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(z2, 0)\n    \n    # Shortcut connection: element-wise addition of input x and output a2\n    z_shortcut = a2 + x\n    \n    # Final ReLU activation\n    output = np.maximum(z_shortcut, 0)\n    \n    # Return the result as a list, rounded to 4 decimal places\n    return np.round(output, 4).tolist()\n\n# Example usage\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\nw2 = np.array([[0.7, 0.8], [0.9, 1.0]])\n\n# Call the function and print the result\nprint(residual_block(x, w1, w2))\n```"}
{"task_id": 114, "completion_id": 0, "completion": "Here's a self-contained Python script that defines a function to perform Global Average Pooling on a 3D NumPy array:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n\n    Returns:\n    np.ndarray: 1D array of shape (channels,) containing the average of all\n                elements in each feature map.\n    \"\"\"\n    # Get the number of channels from the input shape.\n    channels = x.shape[2]\n    \n    # Compute the average of all elements in each channel and return the result.\n    return np.array([np.mean(x[:, :, c]) for c in range(channels)])\n\n# Example usage\nif __name__ == \"__main__\":\n    # Generate a sample 3D array with shape (4, 4, 3) for example.\n    example_input = np.arange(48).reshape((4, 4, 3))\n    print(\"Input array shape:\", example_input.shape)\n    \n    # Perform global average pooling on the example input.\n    pooled_output = global_avg_pool(example_input)\n    \n    # Print the results.\n    print(\"Pooled output shape:\", pooled_output.shape)\n    print(\"Pooled output:\", pooled_output)\n```\n\nIn this script, the `global_avg_pool` function takes an input array `x` and computes the average of all elements for each channel separately. The pooling is done by iterating over each channel, calculating the mean, and returning these averages as a 1D numpy array. An example usage of the function is also provided in the script, which generates a sample input array and applies the global average pooling function to it."}
{"task_id": 115, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n    \n    Parameters:\n    - X: A 4D numpy array of shape (N, C, H, W) representing a batch of feature maps.\n    - gamma: A 1D numpy array of shape (C,) representing the scale parameter per channel.\n    - beta: A 1D numpy array of shape (C,) representing the shift parameter per channel.\n    - epsilon: A small constant for numerical stability.\n    \n    Returns:\n    - A 4D numpy array with batch normalization applied.\n    \"\"\"\n    N, C, H, W = X.shape\n    \n    # Calculate the mean and variance for each channel across the batch and spatial dimensions\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Scale and shift\n    X_norm = gamma.reshape(1, C, 1, 1) * X_norm + beta.reshape(1, C, 1, 1)\n    \n    return np.round(X_norm, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define an example input and parameters\n    np.random.seed(0)  # For reproducibility\n    X = np.random.randn(2, 3, 4, 4)\n    gamma = np.random.randn(3)\n    beta = np.random.randn(3)\n    \n    # Perform batch normalization\n    normalized_output = batch_normalization(X, gamma, beta)\n    \n    # Output the result\n    print(normalized_output)\n```"}
{"task_id": 116, "completion_id": 0, "completion": "```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term c * x^n at a point x.\n    \n    Parameters:\n    c (float): Coefficient of the polynomial term.\n    x (float): Point at which the derivative is evaluated.\n    n (float): Exponent of the polynomial term.\n    \n    Returns:\n    float: The value of the derivative at x, rounded to 4 decimal places.\n    \"\"\"\n    # Derivative of c * x^n is n * c * x^(n-1)\n    derivative_value = n * c * (x ** (n - 1))\n    return round(derivative_value, 4)\n\n# Test cases to verify the correctness of the function\nassert poly_term_derivative(3.0, 2.0, 2.0) == 12.0000\nassert poly_term_derivative(5.0, 3.0, 3.0) == 135.0000\nassert poly_term_derivative(0.5, 4.0, 1.0) == 0.5000\nassert poly_term_derivative(2.5, -2.0, 3.0) == -30.0000\n\nprint(\"All test cases passed successfully.\")\n```\nThis Python script defines a function `poly_term_derivative` which computes the derivative of a given polynomial term at a specific point and returns the derivative value rounded to 4 decimal places. It also includes test cases to verify the correctness of the implementation."}
{"task_id": 117, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10):\n    \"\"\"\n    Compute an orthonormal basis for a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    - vectors: A list of 2D vectors (lists of floats).\n    - tol: A tolerance value to determine linear independence.\n    \n    Returns:\n    - A list of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n    # Convert list of vectors to a numpy array\n    matrix = np.array(vectors)\n    # Gram-Schmidt process\n    q, r = np.linalg.qr(matrix.T)\n    # Find the independent (non-zero) vectors\n    independent_columns = np.abs(np.diagonal(r)) > tol\n    orthonormal_vectors = q[:, independent_columns].T\n    # Convert numpy array to list of lists, rounding to 4 decimal places\n    return [np.round(vec, 4).tolist() for vec in orthonormal_vectors]\n\n# Check function with provided data points\ndef check():\n    vectors = [[1, 2], [2, 4], [1, 3]]\n    expected_output_length = 2  # There should be 2 independent vectors in 2D space\n    output = orthonormal_basis(vectors)\n    assert len(output) == expected_output_length, f\"Expected {expected_output_length} orthonormal vectors, got {len(output)}\"\n    print(\"Output for provided data points:\")\n    print(output)\n\ncheck()\n```\n\nThis script defines the `orthonormal_basis` function as per the task description and includes a check with provided data to verify the correctness of the generated function. The QR decomposition method from numpy's linear algebra module is used as a more stable and efficient implementation of the Gram-Schmidt process."}
{"task_id": 118, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cross_product(a, b):\n    \"\"\"\n    Calculate the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): A 3D vector.\n    b (list/tuple): A 3D vector.\n    \n    Returns:\n    list: The cross product of vectors a and b, rounded to 4 decimal places.\n    \"\"\"\n    # Convert the input lists/tuples to numpy arrays for easier manipulation\n    a_np = np.array(a)\n    b_np = np.array(b)\n    \n    # Compute the cross product\n    result = np.cross(a_np, b_np)\n    \n    # Round the result to 4 decimal places\n    result_rounded = np.round(result, 4)\n    \n    # Convert the result to a list before returning\n    return result_rounded.tolist()\n\n# Example usage\nvector1 = (1.2, 3.5, 4.6)\nvector2 = (2.1, 1.3, 0.8)\nresult = cross_product(vector1, vector2)\nprint(result)\n\n# Check function to verify the correctness of the cross_product function\ndef check_function():\n    test_cases = [\n        ((1, 0, 0), (0, 1, 0), [0.0, 0.0, 1.0]),\n        ((0, 0, 1), (1, 0, 0), [0.0, 1.0, 0.0]),\n        ((1.0, 2.0, 3.0), (4.0, 5.0, 6.0), [-3.0, 6.0, -3.0]),\n    ]\n    \n    for a, b, expected in test_cases:\n        result = cross_product(a, b)\n        assert result == expected, f\"cross_product({a}, {b}) returned {result}, but expected {expected}\"\n    print(\"All test cases passed!\")\n\n# Run the check function\ncheck_function()\n```"}
{"task_id": 119, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.ndarray): A square coefficient matrix.\n    b (np.ndarray): A constant vector.\n    \n    Returns:\n    list: The solution vector x as a list, or -1 if there is no unique solution.\n    \"\"\"\n    # Check if the matrix A is square\n    if A.shape[0] != A.shape[1]:\n        return -1\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # If the determinant is 0, there is no unique solution\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Solve for each variable using Cramer's Rule\n    for i in range(A.shape[1]):\n        # Replace the i-th column of A with b\n        A_i = np.copy(A)\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the new matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Append the i-th solution to the solution vector\n        x.append(round(det_A_i / det_A, 4))\n    \n    # Return the solution vector as a list\n    return np.array(x).tolist()\n\n# Example usage:\nA = np.array([[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]])\nb = np.array([1, -2, 0])\nsolution = cramers_rule(A, b)\nprint(solution)\n```\nIn this Python script, the `cramers_rule` function takes a square matrix `A` and a vector `b`, then solves the system of linear equations using Cramer's Rule, returning the solution vector as a list. If the matrix has no unique solution, it returns `-1`. The example usage demonstrates how to call the function with a specific system of equations and prints the solution."}
{"task_id": 120, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculates the Bhattacharyya distance between two probability distributions p and q.\n    \n    Args:\n    p: A list of floats representing the first probability distribution.\n    q: A list of floats representing the second probability distribution.\n    \n    Returns:\n    A float representing the Bhattacharyya distance between p and q, rounded to 4 decimal places.\n    If p and q have different lengths or are empty, returns 0.0.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    bc = -np.log(np.sum(np.sqrt(np.multiply(p, q))))\n    return round(bc, 4)\n\n# Example usage\np = [0.1, 0.6, 0.3]\nq = [0.2, 0.4, 0.4]\nprint(bhattacharyya_distance(p, q))  # Output should be the calculated Bhattacharyya distance\n```\n\nThis provided Python script follows the task instructions closely, implementing a function that calculates the Bhattacharyya distance between two probability distributions, with considerations for input validation and formatting the output as required."}
{"task_id": 121, "completion_id": 0, "completion": "```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Args:\n    a: A list of integers or floats representing the first vector.\n    b: A list of integers or floats representing the second vector.\n    \n    Returns:\n    A list representing the element-wise sum of the two vectors if they are of the same length,\n    or -1 if the vectors have incompatible dimensions.\n    \"\"\"\n    \n    # Check if both vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Compute the element-wise sum\n    result = [i + j for i, j in zip(a, b)]\n    \n    return result\n\n# Example checks\nprint(vector_sum([1, 2, 3], [4, 5, 6]))  # Should print [5, 7, 9]\nprint(vector_sum([1.5, 2.5], [1, 2]))    # Should print [2.5, 4.5]\nprint(vector_sum([1, 2], [1, 2, 3]))     # Should print -1 due to incompatible dimensions\n```"}
{"task_id": 122, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x: np.ndarray):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n\n    for episode in episodes:\n        returns = 0\n        returns_list = []\n        for _, _, reward in reversed(episode):\n            returns = reward + 0.99 * returns\n            returns_list.append(returns)\n        returns_list.reverse()\n\n        for (state, action, _), return_ in zip(episode, returns_list):\n            log_policy = np.log(softmax(theta[state, :]))\n            gradient[state, :] += softmax(theta[state, :]) * return_\n            gradient[state, action] -= return_\n\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()\n\n# Example usage:\ntheta = np.array([[0.1, 0.2, 0.3], [0.3, 0.2, 0.1]])\nepisodes = [\n    [(0, 1, 1.0), (1, 2, 1.0)],\n    [(0, 0, 0.0), (1, 1, 0.5)]\n]\nprint(compute_policy_gradient(theta, episodes))\n```\n\nThis script defines a function `compute_policy_gradient` which computes the average gradient for the policy gradient method using the REINFORCE algorithm. It implements a softmax function to compute the policy from `theta`, calculates the returns for each state-action pair in the episodes, and then calculates the gradient for each episode to be averaged over all episodes provided. The results are rounded to four decimal places and returned as a list."}
{"task_id": 123, "completion_id": 0, "completion": "```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Args:\n    n_experts (int): Number of experts.\n    k_active (int): Number of active experts.\n    d_in (int): Dimension of input.\n    d_out (int): Dimension of output.\n    \n    Returns:\n    float: Percentage of computational cost savings.\n    \"\"\"\n    \n    # FLOPs for dense layer\n    flops_dense = 2 * d_in * d_out\n\n    # FLOPs for MoE layer\n    # Each active expert performs d_in * d_out FLOPs, and there is an additional mixing cost of 2 * k_active * d_out\n    flops_moe_experts = 2 * k_active * d_in * d_out\n    flops_moe_mixing = 2 * k_active * d_out\n    flops_moe = flops_moe_experts + flops_moe_mixing\n\n    # Calculate savings\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    \n    # Round to the nearest 1th decimal\n    return round(savings, 1)\n\n# Example use case\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 512\n\nsavings_percentage = compute_efficiency(n_experts, k_active, d_in, d_out)\nprint(f\"Computational Cost Savings: {savings_percentage}%\")\n```\n\nThis script calculates the computational cost savings of an MoE layer compared to a dense layer based on the input parameters and prints the savings percentage rounded to the nearest 1th decimal."}
{"task_id": 124, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n):\n    # Compute logits using the input and gating weights\n    z = X @ W_g\n    \n    # Add noise to the logits\n    z += N * W_noise\n    \n    # Apply softmax to turn logits into probabilities\n    p = softmax(z)\n    \n    # Get the indices of the top-k values along the last axis\n    topk_indices = np.argpartition(-p, k, axis=-1)[:, :k]\n    \n    # Initialize the result array with zeros\n    result = np.zeros_like(p)\n    \n    # For each element, set the top-k elements to their respective probabilities\n    for i in range(X.shape[0]):\n        result[i, topk_indices[i]] = p[i, topk_indices[i]]\n    \n    # Normalize top-k probabilities within each row so they sum to 1\n    row_sums = result.sum(axis=-1, keepdims=True)\n    result = np.nan_to_num(result / row_sums)\n    \n    # Round the results to the nearest 4th decimal\n    result = np.round(result, decimals=4)\n    \n    # Convert to list and return\n    return result.tolist()\n\n# Example usage:\n# X = np.random.rand(2, 5)  # 2 samples, each with 5 features\n# W_g = np.random.rand(5, 3)  # 3 experts, 5 input features\n# W_noise = np.random.rand(3)  # Noise for each expert\n# N = np.random.rand(2, 3)  # Pre-sampled noise for each sample and expert\n# k = 2  # Sparsity constraint\n# print(noisy_topk_gating(X, W_g, W_noise, N, k))\n```"}
{"task_id": 125, "completion_id": 0, "completion": "Here is a Python script that implements the Sparse Mixture of Experts Layer as described:\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Compute gating probabilities: (batch_size, n_experts)\n    gating_probs = softmax(np.dot(x, Wg))\n    \n    # Select top-k experts per token: (batch_size, top_k)\n    top_k_indices = np.argsort(-gating_probs, axis=1)[:, :top_k]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    \n    # Normalize the top-k probabilities: (batch_size, top_k)\n    top_k_probs_normalized = top_k_probs / np.sum(top_k_probs, axis=1, keepdims=True)\n    \n    # Compute expert outputs: (batch_size, n_experts, expert_output_dim)\n    expert_outputs = np.dot(x[:, np.newaxis, :], We).squeeze()\n    \n    # Select top-k expert outputs per token: (batch_size, top_k, expert_output_dim)\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, np.newaxis], axis=1)\n    \n    # Weight expert outputs by their normalized gating probabilities and sum across top-k experts\n    weighted_expert_outputs = top_k_probs_normalized[:, :, np.newaxis] * top_k_expert_outputs\n    final_outputs = np.sum(weighted_expert_outputs, axis=1)\n    \n    # Round to the nearest 4th decimal and return as a python list\n    return np.round(final_outputs, decimals=4).tolist()\n\ndef softmax(x):\n    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example inputs\n    x = np.array([[1, 2], [3, 4]])  # Input tensor of shape (batch_size, input_dim)\n    We = np.array([[[0.1, 0.2], [0.3, 0.4]], [[0.5, 0.6], [0.7, 0.8]], [[0.9, 1.0], [1.1, 1.2]]])  # Expert weight matrices of shape (n_experts, input_dim, expert_output_dim)\n    Wg = np.array([[1, 0], [0, 1]])  # Gating weight matrix of shape (input_dim, n_experts)\n    n_experts = 3\n    top_k = 2\n    \n    # Call moe function\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```\n\nThis script defines a `moe` function that performs the Mixture of Experts operation as described, using numpy for the computations and softmax for the gating mechanism. The top-k routing is implemented by selecting the top-k expert indices and their corresponding gating probabilities. The final output is computed by weighting the expert outputs by the normalized top-k probabilities and summing over the top-k dimension. The output is rounded to the nearest 4th decimal and returned as a python list."}
{"task_id": 126, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    B, C, H, W = X.shape\n    if C % num_groups != 0:\n        raise ValueError(\"Number of channels must be divisible by num_groups.\")\n    \n    G = num_groups\n    X_reshaped = X.reshape(B, G, -1)\n    mean = np.mean(X_reshaped, axis=2, keepdims=True)\n    var = np.var(X_reshaped, axis=2, keepdims=True)\n    \n    X_norm = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    \n    X_out = gamma * X_norm + beta\n    X_out_rounded = np.round(X_out, 4)\n    return X_out_rounded.tolist()\n\n# Example usage:\n# Define a tensor X, scale gamma, shift beta, number of groups G\nX = np.random.rand(2, 8, 4, 4)\ngamma = np.ones(8)\nbeta = np.zeros(8)\nnum_groups = 2\n\n# Perform group normalization\nnormalized_X = group_normalization(X, gamma, beta, num_groups)\nprint(normalized_X)\n```\n\nThis script defines a `group_normalization` function that takes as input a 4D numpy array `X`, scale factors `gamma`, shift factors `beta`, the number of groups to split the channels into, and an optional `epsilon` for numerical stability. The function first checks if the number of channels (`C`) is divisible by the number of groups (`num_groups`). Then, it reshapes the input tensor to facilitate computation of the mean and variance for each group. The normalization process involves subtracting the mean and dividing by the square root of the variance plus a small epsilon value. Finally, it applies the learned scale and shift, rounds the results, and returns the output in the form of a Python list."}
{"task_id": 127, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    \"\"\"\n    Implement gradient descent to find the minimum value of x for the function f(x) = x^4 - 3x^3 + 2.\n    \n    Parameters:\n    - start_x: The initial x value to start the search from.\n    - learning_rate: The step size to take during each iteration.\n    - tolerance: The threshold for determining convergence.\n    - max_iters: The maximum number of iterations to perform.\n    \n    Returns:\n    - The x value that minimizes the function, rounded to the nearest 4th decimal.\n    \"\"\"\n    x = start_x\n    for _ in range(max_iters):\n        # Calculate the derivative of the function at the current x\n        gradient = 4 * x**3 - 9 * x**2\n        \n        # Update the position according to the learning rate and gradient\n        x_new = x - learning_rate * gradient\n        \n        # Check for convergence based on change in x\n        if abs(x_new - x) < tolerance:\n            break\n        \n        x = x_new\n    \n    return round(x, 4)\n\n# Testing the function\nif __name__ == \"__main__\":\n    test_x = 3.5  # Random initial position\n    print(f\"Starting search from x = {test_x}\")\n    treasure_location = find_treasure(test_x)\n    print(f\"The treasure is located at x = {treasure_location}\")\n```"}
{"task_id": 128, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implements the Dynamic Tanh (DyT) function, which simulates layer normalization in a Transformer architecture.\n    \n    Parameters:\n    - x: An input numpy array of shape (batch_size, hidden_size).\n    - alpha: A scaling factor that controls the slope of the function.\n    - gamma: A scaling factor that controls the width of the 'squashing' behavior.\n    - beta: A bias term that shifts the output along the value range.\n    \n    Returns:\n    - A list of floats where each element corresponds to the DyT transformed element of the input array, rounded to 4 decimal places.\n    \"\"\"\n    # Compute the dynamic tanh function\n    dyt = np.tanh(gamma * x + beta)\n    dyt = alpha * dyt\n    \n    # Round to the nearest 4th decimal place and convert to list\n    dyt_rounded = np.round(dyt, 4)\n    return dyt_rounded.tolist()\n\n# Example check function to verify the correctness\ndef check_function():\n    x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    alpha = 2.0\n    gamma = 0.5\n    beta = 0.2\n    result = dynamic_tanh(x, alpha, gamma, beta)\n    expected_output = [\n        [1.5372, 1.8702, 1.9927],\n        [1.9982, 1.9998, 1.9999]\n    ]\n    assert result == expected_output, f\"Expected {expected_output}, but got {result}\"\n    print(\"The check is successful!\")\n\n# Uncomment to run the check function\n# check_function()\n```"}
