{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    \"\"\"\n    Computes the dot product of a matrix and a vector.\n    \n    Args:\n    a: A list of lists representing the matrix.\n    b: A list representing the vector.\n    \n    Returns:\n    A list representing the resulting vector or -1 if the operation is invalid.\n    \"\"\"\n    if len(a) == 0 or len(a[0]) != len(b):\n        return -1\n    result = []\n    for row in a:\n        dot_product = sum((x * y for (x, y) in zip(row, b)))\n        result.append(dot_product)\n    return result"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    \"\"\"\n    Computes the transpose of a given matrix `a`.\n    \n    Parameters:\n    - a: A list of lists where each inner list represents a row of the matrix.\n    \n    Returns:\n    - A list of lists representing the transposed matrix.\n    \"\"\"\n    return [[a[j][i] for j in range(len(a))] for i in range(len(a[0]))]"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    \"\"\"\n    Reshape the given matrix 'a' into the specified 'new_shape' if possible.\n    If reshaping is not possible, return an empty list.\n    \n    :param a: List of lists representing the matrix to be reshaped.\n    :param new_shape: Tuple representing the desired shape (rows, columns).\n    :return: Reshaped matrix as a list of lists, or an empty list if reshaping is not possible.\n    \"\"\"\n    try:\n        np_array = np.array(a)\n        reshaped_array = np_array.reshape(new_shape)\n        return reshaped_array.tolist()\n    except ValueError:\n        return []"}
{"task_id": 4, "completion_id": 0, "solution": "import numpy as np\ndef calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculates the mean of a matrix either by row or by column.\n    \n    Args:\n    - matrix: A list of lists of floats representing the matrix.\n    - mode: A string, either 'row' or 'column', specifying the mean calculation mode.\n    \n    Returns:\n    - A list of floats representing the calculated means.\n    \"\"\"\n    if mode.lower() == 'row':\n        return [np.mean(row) for row in matrix]\n    elif mode.lower() == 'column':\n        transposed_matrix = list(zip(*matrix))\n        return [np.mean(column) for column in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of a given matrix by a scalar and returns the resulting matrix.\n    \n    :param matrix: A list of lists where each sublist represents a row of the matrix.\n    :param scalar: An integer or float representing the scalar value to multiply by.\n    :return: A new matrix (list of lists) with each element of the original matrix multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import numpy as np\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    \"\"\"\n    Calculates the eigenvalues of a 2x2 matrix and returns them in a sorted list\n    from highest to lowest.\n    \n    Parameters:\n    matrix (list of list of float|int): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list of float: A list containing the eigenvalues sorted from highest to lowest.\n    \"\"\"\n    matrix_np = np.array(matrix)\n    eigenvalues = np.linalg.eigvals(matrix_np)\n    sorted_eigenvalues = sorted(eigenvalues, reverse=True)\n    return sorted_eigenvalues\nmatrix = [[4, 2], [3, 1]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> int | list[list[int | float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1}AS.\n    Validates if matrices T and S are invertible before performing the transformation.\n    Returns -1 if any of the matrices T or S are not invertible.\n    \"\"\"\n    try:\n        A_np = np.array(A)\n        T_np = np.array(T)\n        S_np = np.array(S)\n        np.linalg.inv(T_np)\n        np.linalg.inv(S_np)\n        transformed_matrix = np.linalg.inv(T_np) @ A_np @ S_np\n        transformed_matrix_rounded = np.round(transformed_matrix, 4).tolist()\n        return transformed_matrix_rounded\n    except np.linalg.LinAlgError:\n        return -1\nA = [[1, 2], [3, 4]]\nT = [[4, 3], [2, 1]]\nS = [[1, 2], [3, 5]]"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the inverse of a 2x2 matrix.\n    \n    Args:\n    matrix (list[list[float]]): A 2x2 matrix represented as a list of lists.\n    \n    Returns:\n    list[list[float]]: The inverse of the given matrix, or None if the matrix is not invertible.\n    \"\"\"\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse_matrix = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse_matrix"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    \"\"\"\n    Multiplies two matrices a and b.\n    \n    Args:\n    a: A matrix represented as a list of lists of integers or floats.\n    b: A matrix represented as a list of lists of integers or floats.\n    \n    Returns:\n    The product of the two matrices as a list of lists of integers or floats, or -1 if the matrices cannot be multiplied.\n    \"\"\"\n    if len(a[0]) != len(b):\n        return -1\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "import numpy as np\ndef calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculates the covariance matrix for a given set of vectors.\n    \n    :param vectors: A list of lists, where each inner list represents a feature with its observations.\n    :return: A covariance matrix as a list of lists.\n    \"\"\"\n    data = np.array(vectors)\n    cov_matrix = np.cov(data, bias=True)\n    return cov_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solve a system of linear equations using the Jacobi iterative method.\n    \n    Parameters:\n    A (np.ndarray): Coefficient matrix.\n    b (np.ndarray): Constant vector.\n    n (int): Number of iterations.\n    \n    Returns:\n    list: The approximate solution vector x.\n    \"\"\"\n    assert A.shape[0] == A.shape[1], 'Matrix A must be a square matrix.'\n    assert A.shape[0] == b.shape[0], 'Matrix A and vector b must have compatible dimensions.'\n    x = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(A.shape[0]):\n            s = np.dot(A[i], x) - A[i][i] * x[i]\n            x_new[i] = (b[i] - s) / A[i][i]\n        x = x_new\n        x = np.round(x, decimals=4)\n    return x.tolist()\nA = np.array([[4, 1], [1, 3]], dtype=np.float64)\nb = np.array([1, 2], dtype=np.float64)\nn = 25"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2x2 matrix A using an iterative method.\n    :param A: A 2x2 numpy array\n    :return: A tuple containing the singular values of A rounded to the nearest 4th decimal.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (a, b) = (ATA[0, 0], ATA[0, 1])\n    (c, d) = (ATA[1, 0], ATA[1, 1])\n    trace = a + d\n    det = a * d - b * c\n    eigenvalue1 = (trace + np.sqrt(trace ** 2 - 4 * det)) / 2\n    eigenvalue2 = (trace - np.sqrt(trace ** 2 - 4 * det)) / 2\n    singular_value1 = np.sqrt(eigenvalue1)\n    singular_value2 = np.sqrt(eigenvalue2)\n    singular_value1 = round(singular_value1, 4)\n    singular_value2 = round(singular_value2, 4)\n    return (singular_value1, singular_value2)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "from typing import List, Union\ndef determinant_4x4(matrix: List[List[Union[int, float]]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \n    :param matrix: A 4x4 matrix represented as a list of lists.\n    :return: The determinant of the matrix.\n    \"\"\"\n\n    def determinant_3x3(sub_matrix: List[List[Union[int, float]]]) -> float:\n        \"\"\"\n        Calculate the determinant of a 3x3 matrix.\n        \n        :param sub_matrix: A 3x3 matrix represented as a list of lists.\n        :return: The determinant of the sub_matrix.\n        \"\"\"\n        return sub_matrix[0][0] * (sub_matrix[1][1] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][1]) - sub_matrix[0][1] * (sub_matrix[1][0] * sub_matrix[2][2] - sub_matrix[1][2] * sub_matrix[2][0]) + sub_matrix[0][2] * (sub_matrix[1][0] * sub_matrix[2][1] - sub_matrix[1][1] * sub_matrix[2][0])\n    if len(matrix) != 4 or any((len(row) != 4 for row in matrix)):\n        raise ValueError('Input must be a 4x4 matrix')\n    det = 0\n    for col in range(4):\n        sub_matrix = [row[:col] + row[col + 1:] for row in matrix[1:]]\n        det += (-1) ** col * matrix[0][col] * determinant_3x3(sub_matrix)\n    return det"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    \"\"\"\n    This function performs linear regression using the normal equation method.\n    It takes as input a matrix X representing the features and a vector y representing the target values.\n    It returns the coefficients of the linear regression model.\n    The coefficients are rounded to the nearest 4th decimal.\n    \"\"\"\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_np = np.c_[np.ones(X_np.shape[0]), X_np]\n    theta = np.linalg.inv(X_np.T.dot(X_np)).dot(X_np.T).dot(y_np)\n    theta_rounded = [round(t, 4) for t in theta]\n    return theta_rounded"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    \"\"\"\n    Perform linear regression using gradient descent.\n    \n    Parameters:\n    X (np.ndarray): The input feature matrix with an added column of ones for the intercept.\n    y (np.ndarray): The target values.\n    alpha (float): The learning rate.\n    iterations (int): The number of iterations to run the gradient descent algorithm.\n    \n    Returns:\n    list: The coefficients of the linear regression model rounded to 4 decimal places.\n    \"\"\"\n    (num_samples, num_features) = X.shape\n    theta = np.zeros(num_features)\n    for _ in range(iterations):\n        predictions = np.dot(X, theta)\n        errors = predictions - y\n        gradient = np.dot(X.T, errors) / num_samples\n        theta -= alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    \"\"\"\n    Perform feature scaling on a dataset using standardization and min-max normalization.\n    \n    Parameters:\n    data (np.ndarray): A 2D numpy array where each row is a data sample and each column is a feature.\n    \n    Returns:\n    tuple: A tuple of two 2D lists, the first scaled by standardization and the second by min-max normalization.\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4).tolist()\n    min_values = np.min(data, axis=0)\n    max_values = np.max(data, axis=0)\n    min_max_scaled_data = (data - min_values) / (max_values - min_values)\n    min_max_scaled_data = np.round(min_max_scaled_data, 4).tolist()\n    return (standardized_data, min_max_scaled_data)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis]) ** 2).sum(axis=2))\n        labels = np.argmin(distances, axis=0)\n        new_centroids = np.array([points_array[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids_array == new_centroids):\n            break\n        centroids_array = new_centroids\n    final_centroids = [tuple((round(coord, 4) for coord in centroid)) for centroid in centroids_array.tolist()]\n    return final_centroids\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Function to perform k-fold cross-validation on a given dataset.\n    \n    Parameters:\n    X (np.ndarray): Features of the dataset.\n    y (np.ndarray): Labels of the dataset.\n    k (int): Number of folds.\n    shuffle (bool): Whether to shuffle the dataset before splitting.\n    random_seed (int): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of tuples, each containing the train and test indices for each fold.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_indices = np.array_split(indices, k)\n    splits = []\n    for i in range(k):\n        test_indices = fold_indices[i]\n        train_indices = np.concatenate(fold_indices[:i] + fold_indices[i + 1:])\n        splits.append((train_indices, test_indices))\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"\n    Perform Principal Component Analysis (PCA) on the input data.\n    \n    Parameters:\n    - data: 2D NumPy array, where each row is a sample and each column is a feature.\n    - k: Number of principal components to return.\n    \n    Returns:\n    - A list of lists containing the principal components (eigenvectors corresponding to the largest eigenvalues).\n    \"\"\"\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.around(principal_components, decimals=4).tolist()\n    return principal_components_rounded"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"\n    Build a decision tree from the given examples using specified attributes.\n    \n    Args:\n    examples: A list of dictionaries where each dictionary represents an example with attribute-value pairs.\n    attributes: A list of attribute names to be used for building the decision tree.\n    target_attr: The name of the target attribute for classification.\n    \n    Returns:\n    A nested dictionary representing the decision tree.\n    \"\"\"\n    if len(set((e[target_attr] for e in examples))) == 1:\n        return examples[0][target_attr]\n    if not attributes:\n        majority_class = Counter((e[target_attr] for e in examples)).most_common(1)[0][0]\n        return majority_class\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    for value in set((e[best_attr] for e in examples)):\n        subset = [e for e in examples if e[best_attr] == value]\n        if not subset:\n            majority_class = Counter((e[target_attr] for e in examples)).most_common(1)[0][0]\n            tree[best_attr][value] = majority_class\n        else:\n            remaining_attributes = [a for a in attributes if a != best_attr]\n            subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n            tree[best_attr][value] = subtree\n    return tree\ndef choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n    \"\"\"\n    Choose the attribute with the highest information gain.\n    \n    Args:\n    examples: A list of dictionaries where each dictionary represents an example with attribute-value pairs.\n    attributes: A list of attribute names to be used for building the decision tree.\n    target_attr: The name of the target attribute for classification.\n    \n    Returns:\n    The attribute with the highest information gain.\n    \"\"\"\n    base_entropy = calculate_entropy(examples, target_attr)\n    max_gain = -1\n    best_attr = None\n    for attr in attributes:\n        attr_entropy = 0\n        for value in set((e[attr] for e in examples)):\n            subset = [e for e in examples if e[attr] == value]\n            attr_entropy += len(subset) / len(examples) * calculate_entropy(subset, target_attr)\n        gain = base_entropy - attr_entropy\n        if gain > max_gain:\n            max_gain = gain\n            best_attr = attr\n    return best_attr\ndef calculate_entropy(examples: list[dict], target_attr: str) -> float:\n    \"\"\"\n    Calculate the entropy of the given examples based on the target attribute.\n    \n    Args:\n    examples: A list of dictionaries where each dictionary represents an example with attribute-value pairs.\n    target_attr: The name of the target attribute for classification.\n    \n    Returns:\n    The entropy of the examples.\n    \"\"\"\n    class_counts = Counter((e[target_attr] for e in examples))\n    entropy = 0\n    for count in class_counts.values():\n        probability = count / len(examples)\n        entropy -= probability * math.log2(probability)\n    return entropy\nexamples = [{'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'}, {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'}, {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'}, {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'}, {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'\ntree = learn_decision_tree(examples, attributes, target_attr)"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    n_samples = data.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    if kernel == 'linear':\n        K = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                K[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma ** 2))\n    return K\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    (n_samples, n_features) = data.shape\n    K = kernel_matrix(data, kernel, sigma)\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    for t in range(1, iterations + 1):\n        lambda_t = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            if labels[i] * (np.sum(alpha * labels * K[:, i]) + bias) < 1:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i] + lambda_t * labels[i]\n                bias += lambda_t * (1 - labels[i] * (np.sum(alpha * labels * K[:, i]) + bias))\n            else:\n                alpha[i] = (1 - lambda_t * lambda_val) * alpha[i]\n    alpha = np.round(alpha, 4)\n    bias = np.round(bias, 4)\n    return (alpha.tolist(), bias.tolist())"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function, rounded to four decimal places.\n    \"\"\"\n    output = 1 / (1 + math.exp(-z))\n    return round(output, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Computes the softmax of a given list of scores.\n    \n    Args:\n    - scores (list of float): A list of scores to apply the softmax function to.\n    \n    Returns:\n    - list of float: A list of softmax values rounded to four decimal places.\n    \"\"\"\n    exp_scores = [math.exp(score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    return softmax_values"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n    predictions = []\n    for feature in features:\n        weighted_sum = sum((feat * weight for (feat, weight) in zip(feature, weights))) + bias\n        prediction = sigmoid(weighted_sum)\n        predictions.append(round(prediction, 4))\n    mse = np.mean((np.array(predictions) - np.array(labels)) ** 2)\n    return (predictions, round(float(mse), 4))\nfeatures = [[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]]\nlabels = [0, 1, 1]\nweights = [0.5, -0.5]\nbias = 0.1"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the sigmoid of x element-wise.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Compute the derivative of the sigmoid function.\"\"\"\n    return sigmoid(x) * (1 - sigmoid(x))\ndef mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the mean squared error.\"\"\"\n    return np.mean((y_true - y_pred) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single neuron using backpropagation and return the updated weights, bias, and MSE values.\n    \n    :param features: np.ndarray, the feature vectors.\n    :param labels: np.ndarray, the true binary labels.\n    :param initial_weights: np.ndarray, the initial weights.\n    :param initial_bias: float, the initial bias.\n    :param learning_rate: float, the learning rate for gradient descent.\n    :param epochs: int, the number of epochs for training.\n    :return: tuple, containing the updated weights, bias, and list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    for _ in range(epochs):\n        linear_output = np.dot(features, weights) + bias\n        predictions = sigmoid(linear_output)\n        mse = mean_squared_error(labels, predictions)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        gradient_weights = np.dot(features.T, error * sigmoid_derivative(linear_output))\n        gradient_bias = np.sum(error * sigmoid_derivative(linear_output))\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    return (weights.tolist(), round(bias, 4), mse_values)\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 0, 0, 1])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of list of int): The basis vectors of B as columns.\n    C (list of list of int): The basis vectors of C as columns.\n    \n    Returns:\n    list of list of float: The transformation matrix P from B to C.\n    \"\"\"\n    B_np = np.array(B)\n    C_np = np.array(C)\n    P = np.linalg.solve(B_np.T, C_np.T).T\n    P_rounded = np.round(P, decimals=4).tolist()\n    return P_rounded\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix A using eigendecomposition.\n    Returns U, S, V such that A \u2248 U * S * V.T, where S is a diagonal matrix.\n    \"\"\"\n    ATA = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(ATA)\n    sorted_indices = eigenvalues.argsort()[::-1]\n    eigenvalues = eigenvalues[sorted_indices]\n    eigenvectors = eigenvectors[:, sorted_indices]\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    V = eigenvectors\n    U = np.zeros(A.shape)\n    for i in range(A.shape[1]):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    return (U.tolist(), S.tolist(), V.tolist())\nA = np.array([[1, 2], [3, 4]], dtype=float)"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Randomly shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    :param X: Numpy array, feature dataset.\n    :param y: Numpy array, labels corresponding to the dataset.\n    :param seed: Optional; int seed for random number generator for reproducibility.\n    :return: Two python lists (shuffled X and y) after reshaping.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    assert len(X) == len(y), 'X and y must have the same number of samples'\n    p = np.random.permutation(len(X))\n    shuffled_X = X[p]\n    shuffled_y = y[p]\n    return (shuffled_X.tolist(), shuffled_y.tolist())\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Generates batches of data from X and y, if y is provided,\n    batches will contain pairs (X, y). Otherwise, batches will\n    only contain X. Each batch is returned as a Python list.\n    \n    Parameters:\n    X (numpy.ndarray): Input features.\n    y (numpy.ndarray): Target labels (optional).\n    batch_size (int): The size of each batch.\n    \n    Yields:\n    list: A batch of X (and y, if provided) as Python lists.\n    \"\"\"\n    n_samples = X.shape[0]\n    for i in range(0, n_samples, batch_size):\n        batch_X = X[i:i + batch_size].tolist()\n        if y is not None:\n            batch_y = y[i:i + batch_size].tolist()\n            yield (batch_X, batch_y)\n        else:\n            yield batch_X"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of the feature at index\n    feature_i is greater than or equal to the threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset.\n    feature_i (int): The index of the feature to check the threshold against.\n    threshold (float): The threshold value.\n\n    Returns:\n    list: Two subsets of the dataset. The first subset contains samples where the value of the specified\n          feature is greater than or equal to the threshold. The second subset contains the remaining samples.\n    \"\"\"\n    if feature_i >= X.shape[1]:\n        raise ValueError(f'feature_i is out of range. It should be less than {X.shape[1]}')\n    subset1 = X[X[:, feature_i] >= threshold].tolist()\n    subset2 = X[X[:, feature_i] < threshold].tolist()\n    return [subset1, subset2]"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate a new feature matrix consisting of all polynomial combinations of the features\n    with degree less than or equal to the specified degree. For example, if an input sample is two dimensional\n    and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n    \n    Parameters:\n    X : numpy array of shape (n_samples, n_features)\n        The data.\n    degree : int\n        The degree of the polynomial features.\n        \n    Returns:\n    numpy array of shape (n_samples, new_n_features)\n        The matrix of polynomial features.\n    \"\"\"\n    if X.ndim == 1:\n        X = X[:, np.newaxis]\n    (n_samples, n_features) = X.shape\n    indices = np.arange(n_features)\n    combos = [combinations_with_replacement(indices, i) for i in range(0, degree + 1)]\n    flatten_combos = [item for sublist in combos for item in sublist]\n    n_output_features = len(flatten_combos)\n    poly_features = np.empty((n_samples, n_output_features))\n    for (i, combo) in enumerate(flatten_combos):\n        poly_features[:, i] = np.prod(X[:, combo], axis=1)\n    return poly_features.tolist()\nX = np.array([[1, 2], [3, 4]])\ndegree = 2"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate a list of random subsets of the dataset (X, y).\n\n    Parameters:\n    - X: 2D numpy array, the feature matrix.\n    - y: 1D numpy array, the target vector.\n    - n_subsets: int, the number of subsets to generate.\n    - replacements: bool, if True, create subsets with replacements.\n    - seed: int, the random seed for reproducibility.\n\n    Returns:\n    - A list of n_subsets random subsets, each subset is a tuple of (X_subset, y_subset).\n    \"\"\"\n    np.random.seed(seed)\n    subsets = []\n    n_samples = X.shape[0]\n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets\nX = np.array([[10, 20], [30, 40], [50, 60], [70, 80]])\ny = np.array([1, 2, 3, 4])\nn_subsets = 2\nsubsets = get_random_subsets(X, y, n_subsets, replacements=False)"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of a 1D numpy array of integer values.\n    \n    Parameters:\n    x (np.array): A 1D numpy array of integer values to be encoded.\n    n_col (int, optional): The number of columns for the one-hot encoded array.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot = np.eye(n_col)[x]\n    return one_hot.tolist()\nx = np.array([0, 2, 1, 3])"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix.\n    \n    Parameters:\n    - x (numpy.ndarray): A 1D numpy array.\n    \n    Returns:\n    - list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    x = np.array(x)\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score given true labels and predicted labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true labels.\n    y_pred (numpy.ndarray): An array of predicted labels.\n    \n    Returns:\n    float: The accuracy score rounded to 4 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    accuracy = np.mean(y_true == y_pred)\n    return round(accuracy, 4)\ny_true = np.array([0, 1, 2, 2, 2])\ny_pred = np.array([0, 0, 2, 2, 1])"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculates the correlation matrix for a given dataset.\n    \n    Parameters:\n    - X: 2D numpy array\n    - Y: 2D numpy array (optional). If not provided, Y will be set to X.\n    \n    Returns:\n    - A 2D numpy array representing the correlation matrix, rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - X.mean(axis=0)\n    Y_centered = Y - Y.mean(axis=0)\n    cov_matrix = np.dot(X_centered.T, Y_centered)\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    std_matrix = np.outer(std_X, std_Y)\n    std_matrix[std_matrix == 0] = 1\n    corr_matrix = cov_matrix / std_matrix\n    return np.round(corr_matrix, 4).tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\ndef adaboost_fit(X, y, n_clf):\n    sample_weights = np.full(X.shape[0], 1 / X.shape[0])\n    classifiers = []\n    for _ in range(n_clf):\n        error_min = float('inf')\n        best_threshold = 0\n        best_feature = 0\n        best_polarity = 1\n        classifier = {}\n        for feature_idx in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                for polarity in [1, -1]:\n                    prediction = np.where(polarity * X[:, feature_idx] > polarity * threshold, 1, -1)\n                    error = np.sum(sample_weights * (prediction != y))\n                    if error < error_min:\n                        error_min = error\n                        best_threshold = threshold\n                        best_feature = feature_idx\n                        best_polarity = polarity\n        classifier_weight = 0.5 * np.log((1 - error_min) / error_min)\n        prediction = np.where(best_polarity * X[:, best_feature] > best_polarity * best_threshold, 1, -1)\n        sample_weights *= np.exp(-classifier_weight * y * prediction)\n        sample_weights /= np.sum(sample_weights)\n        classifier['weight'] = round(classifier_weight, 4)\n        classifier['feature'] = best_feature\n        classifier['threshold'] = round(best_threshold, 4)\n        classifier['polarity'] = best_polarity\n        classifiers.append(classifier)\n    return classifiers\nX = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\ny = np.array([-1, -1, 1, 1, 1])\nn_clf = 2"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    \"\"\"\n    Computes the log-softmax of a given list of scores.\n    \n    Args:\n    scores (list): A list of scores.\n    \n    Returns:\n    list: A list of log-softmax values corresponding to the input scores.\n    \"\"\"\n    scores_array = np.array(scores)\n    stable_scores = scores_array - np.max(scores_array)\n    softmax_values = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax_values)\n    log_softmax_rounded = np.round(log_softmax_values, 4)\n    return log_softmax_rounded.tolist()\nscores = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nfrom optimizers import Sgd\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        input_shape = self.input_shape[0]\n        limit = 1 / math.sqrt(input_shape)\n        self.W = np.random.uniform(-limit, limit, (input_shape, self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.optimizer_W = copy.copy(optimizer)\n        self.optimizer_w0 = copy.copy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return X.dot(self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W = self.W\n        if self.trainable:\n            grad_w0 = np.sum(accum_grad, axis=0, keepdims=True)\n            grad_w = self.layer_input.T.dot(accum_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, grad_w0)\n            self.W = self.optimizer_W.update(self.W, grad_w)\n        accum_grad = accum_grad.dot(W.T)\n        return accum_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    padded_height = input_height + 2 * padding\n    padded_width = input_width + 2 * padding\n    padded_input = np.zeros((padded_height, padded_width))\n    padded_input[padding:padding + input_height, padding:padding + input_width] = input_matrix\n    output_height = (padded_height - kernel_height) // stride + 1\n    output_width = (padded_width - kernel_width) // stride + 1\n    output_matrix = np.zeros((output_height, output_width))\n    for y in range(0, output_height):\n        for x in range(0, output_width):\n            output_matrix[y, x] = np.sum(padded_input[y * stride:y * stride + kernel_height, x * stride:x * stride + kernel_width] * kernel)\n    output_matrix = np.round(output_matrix, 4)\n    return output_matrix.tolist()\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input to the ReLU function.\n    \n    Returns:\n    float: The output of the ReLU function, which is z if z > 0, otherwise 0.\n    \"\"\"\n    return max(0, z)"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss given the feature matrix X, coefficients w,\n    true labels y_true, and regularization parameter alpha.\n    \n    Parameters:\n    X (np.ndarray): 2D array of features.\n    w (np.ndarray): 1D array of coefficients.\n    y_true (np.ndarray): 1D array of true labels.\n    alpha (float): Regularization parameter.\n\n    Returns:\n    float: Ridge loss rounded to 4 decimal places.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(loss, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Args:\n    z (float): The input value.\n    alpha (float): The slope of the function for z < 0. Default is 0.01.\n\n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    return max(alpha * z, z)"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors.\n    \n    The linear kernel is the dot product of the two input vectors.\n    \n    Parameters:\n    x1 (np.ndarray): The first input vector.\n    x2 (np.ndarray): The second input vector.\n    \n    Returns:\n    float: The computed linear kernel (dot product) of x1 and x2.\n    \n    Raises:\n    ValueError: If the input vectors are not of the same length.\n    \"\"\"\n    if len(x1) != len(x2):\n        raise ValueError('Input vectors must be of the same length.')\n    return np.dot(x1, x2)\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true binary labels and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy.ndarray): An array of true binary labels.\n    y_pred (numpy.ndarray): An array of predicted binary labels.\n    \n    Returns:\n    float: The precision value.\n    \"\"\"\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The shape of y_true and y_pred should be the same.')\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    predicted_positives = np.sum(y_pred == 1)\n    if predicted_positives == 0:\n        return 1.0\n    precision_value = true_positives / predicted_positives\n    return precision_value\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent using Mean Squared Error (MSE) as the loss function.\n    \n    Parameters:\n    X (np.array): Input features.\n    y (np.array): Target values.\n    weights (np.array): Initial weights.\n    learning_rate (float): Learning rate for the gradient descent.\n    n_iterations (int): Number of iterations to perform.\n    batch_size (int): Size of the mini-batch. Default is 1.\n    method (str): Method to use, one of 'batch', 'stochastic', or 'mini-batch'.\n    \n    Returns:\n    np.array: Optimized weights.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    updated_weights = np.copy(weights)\n    for _ in range(n_iterations):\n        if method == 'stochastic':\n            for i in range(n_samples):\n                y_pred = np.dot(X[i], updated_weights)\n                error = y_pred - y[i]\n                gradient = 2 * error * X[i]\n                updated_weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            for i in range(0, n_samples, batch_size):\n                X_batch = X[i:i + batch_size]\n                y_batch = y[i:i + batch_size]\n                y_pred = np.dot(X_batch, updated_weights)\n                error = y_pred - y_batch\n                gradient = 2 * np.dot(X_batch.T, error) / batch_size\n                updated_weights -= learning_rate * gradient\n        else:\n            y_pred = np.dot(X, updated_weights)\n            error = y_pred - y\n            gradient = 2 * np.dot(X.T, error) / n_samples\n            updated_weights -= learning_rate * gradient\n    return np.round(updated_weights, 4).tolist()\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nweights = np.array([0.5, -0.5])\nlearning_rate = 0.01\nn_iterations = 1000"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    \"\"\"\n    Convert the given matrix into its Reduced Row Echelon Form (RREF).\n    \n    Parameters:\n    matrix (list of lists): A 2D matrix represented as a list of lists.\n    \n    Returns:\n    list: The matrix in RREF as a list of lists.\n    \"\"\"\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    lead = 0\n    for r in range(rows):\n        if cols <= lead:\n            return A.tolist()\n        i = r\n        while A[i, lead] == 0:\n            i += 1\n            if rows == i:\n                i = r\n                lead += 1\n                if cols == lead:\n                    return A.tolist()\n        A[[i, r]] = A[[r, i]]\n        A[r] = A[r] / A[r, lead]\n        for i in range(rows):\n            if i != r:\n                A[i] = A[i] - A[i, lead] * A[r]\n        lead += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    \"\"\"\n    Optimizes the parameters using the Adam algorithm.\n    \n    Parameters:\n    f (callable): The objective function to be optimized.\n    grad (callable): A function that computes the gradient of f.\n    x0 (np.array): Initial parameter values.\n    learning_rate (float): The step size. Default is 0.001.\n    beta1 (float): Exponential decay rate for the first moment estimates. Default is 0.9.\n    beta2 (float): Exponential decay rate for the second moment estimates. Default is 0.999.\n    epsilon (float): A small constant for numerical stability. Default is 1e-8.\n    num_iterations (int): Number of iterations to run the optimizer. Default is 1000.\n    \n    Returns:\n    list: The optimized parameters.\n    \"\"\"\n    m = np.zeros_like(x0)\n    v = np.zeros_like(x0)\n    t = 0\n    for _ in range(num_iterations):\n        t += 1\n        g = grad(x0)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x0 -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return np.round(x0, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent with L1 regularization.\n    \n    Parameters:\n    X (np.array): Feature matrix.\n    y (np.array): Target vector.\n    alpha (float): Regularization parameter.\n    learning_rate (float): Learning rate for gradient descent.\n    max_iter (int): Maximum number of iterations for gradient descent.\n    tol (float): Tolerance for stopping criterion.\n    \n    Returns:\n    tuple: A tuple containing the weights and bias.\n    \"\"\"\n    (m, n) = X.shape\n    w = np.zeros(n)\n    b = 0\n    prev_cost = np.inf\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        dw = -2 / m * np.dot(X.T, y - y_pred) + alpha * np.sign(w)\n        db = -2 / m * np.sum(y - y_pred)\n        w -= learning_rate * dw\n        b -= learning_rate * db\n        cost = 1 / (2 * m) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(w))\n        if abs(prev_cost - cost) < tol:\n            break\n        prev_cost = cost\n    return (w.round(4).tolist(), round(b, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Calculates the Optimal String Alignment (OSA) distance between two strings.\n    The OSA distance is the minimum number of edits (insert, delete, substitute, transpose)\n    required to transform one string into another, where each operation costs 1 unit.\n    \"\"\"\n    (len_source, len_target) = (len(source), len(target))\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + cost)\n    return dp[len_source][len_target]"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall for a binary classification problem.\n    \n    Parameters:\n    - y_true: List[int], true binary labels (0 or 1) for the dataset.\n    - y_pred: List[int], predicted binary labels (0 or 1) from the model.\n    \n    Returns:\n    - float: Recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    recall_value = TP / (TP + FN)\n    return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1, 1]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n    \n    :param X: Input sequence as a numpy array of shape (sequence_length, embedding_dim)\n    :param W_q: Query weight matrix as a numpy array of shape (embedding_dim, embedding_dim)\n    :param W_k: Key weight matrix as a numpy array of shape (embedding_dim, embedding_dim)\n    :param W_v: Value weight matrix as a numpy array of shape (embedding_dim, embedding_dim)\n    :return: Self-attention output as a numpy array of shape (sequence_length, embedding_dim)\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T)\n    scaled_attention_scores = attention_scores / np.sqrt(K.shape[1])\n    attention_weights = np.exp(scaled_attention_scores)\n    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return np.round(output, 4).tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    \"\"\"\n    Implements a forward pass through a simple RNN cell.\n    \n    Parameters:\n    - input_sequence: A list of input vectors for each time step.\n    - initial_hidden_state: The initial hidden state of the RNN.\n    - Wx: Weight matrix for input-to-hidden connections.\n    - Wh: Weight matrix for hidden-to-hidden connections.\n    - b: Bias vector for the hidden state update.\n    \n    Returns:\n    - The final hidden state after processing the entire sequence, rounded to 4 decimal places.\n    \"\"\"\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(x, Wx) + np.dot(h, Wh) + b)\n    final_hidden_state = np.round(h, decimals=4).tolist()\n    return final_hidden_state\ninput_sequence = [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by the specified tx and ty values.\n    \n    Args:\n    points (list of lists): A list of [x, y] points to be translated.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: The translated points as a list of [x, y] coordinates.\n    \"\"\"\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    ones = np.ones((points_array.shape[0], 1))\n    points_homogeneous = np.hstack((points_array, ones))\n    translated_points_homogeneous = np.dot(translation_matrix, points_homogeneous.T).T\n    translated_points = translated_points_homogeneous[:, :2]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence between the two normal distributions.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError('Standard deviations must be positive')\n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2)\n    kl_div = term1 + term2 - 0.5\n    return kl_div"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations using the Gauss-Seidel method.\n    \n    :param A: A square matrix of coefficients\n    :param b: Right-hand side vector\n    :param n: Number of iterations\n    :param x_ini: Initial guess for the solution vector (defaults to a vector of zeros if not provided)\n    :return: Approximated solution vector x as a python list\n    \"\"\"\n    A = np.array(A)\n    b = np.array(b).reshape((-1, 1))\n    size = A.shape[0]\n    if x_ini is None:\n        x = np.zeros((size, 1))\n    else:\n        x = np.array(x_ini).reshape((-1, 1))\n    for _ in range(n):\n        for i in range(size):\n            s = 0\n            for j in range(size):\n                if j != i:\n                    s += A[i, j] * x[j]\n            x[i] = (b[i] - s) / A[i, i]\n    x = np.round(x, 4).tolist()\n    return [item for sublist in x for item in sublist]\nA = [[4, -1, 0, 0], [-1, 4, -1, 0], [0, -1, 4, -1], [0, 0, -1, 3]]\nb = [20, 24, 24, 15]\nn = 5\nx_ini = [0, 0, 0, 0]"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    \"\"\"\n    Solves a system of linear equations using Gaussian Elimination with partial pivoting.\n    \n    Parameters:\n    A (np.array): Coefficient matrix.\n    b (np.array): Constant terms.\n    \n    Returns:\n    list: Solution vector x as a list rounded to 4 decimal places.\n    \"\"\"\n    n = len(A)\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    for i in range(n):\n        pivot_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, pivot_row]] = Ab[[pivot_row, i]]\n        pivot = Ab[i, i]\n        for row in range(i + 1, n):\n            factor = Ab[row, i] / pivot\n            Ab[row, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    return np.round(x, 4).tolist()\nA = np.array([[1, 2, 3], [2, 5, 2], [2, 3, 8]], dtype=float)"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    @staticmethod\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n\n    @staticmethod\n    def tanh(x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        T = len(x)\n        hidden_states = []\n        h_t = initial_hidden_state\n        c_t = initial_cell_state\n        for t in range(T):\n            inputs = np.concatenate((x[t], h_t), axis=0).reshape(-1, 1)\n            f_t = self.sigmoid(self.Wf @ inputs + self.bf)\n            i_t = self.sigmoid(self.Wi @ inputs + self.bi)\n            c_tilde = self.tanh(self.Wc @ inputs + self.bc)\n            c_t = f_t * c_t + i_t * c_tilde\n            o_t = self.sigmoid(self.Wo @ inputs + self.bo)\n            h_t = o_t * self.tanh(c_t)\n            hidden_states.append(h_t.flatten())\n        final_hidden_state = h_t\n        final_cell_state = c_t\n        hidden_states = np.round(np.array(hidden_states), 4).tolist()\n        final_hidden_state = np.round(final_hidden_state, 4).tolist()\n        final_cell_state = np.round(final_cell_state, 4).tolist()\n        return (hidden_states, final_hidden_state, final_cell_state)\ninput_size = 3\nhidden_size = 5\nx = [np.random.randn(input_size, 1) for _ in range(10)]\ninitial_hidden_state = np.zeros((hidden_size, 1))\ninitial_cell_state = np.zeros((hidden_size, 1))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus cannot be empty.')\n    num_docs = len(corpus)\n    query_words = set(query)\n    df = {word: 0 for word in query_words}\n    for doc in corpus:\n        for word in set(doc):\n            if word in df:\n                df[word] += 1\n    tf_idf_scores = []\n    for doc in corpus:\n        doc_word_count = len(doc)\n        tf_idf_doc = []\n        for word in query_words:\n            tf = doc.count(word) / doc_word_count\n            idf = np.log((num_docs + 1) / (df[word] + 1))\n            tf_idf = tf * idf\n            tf_idf_doc.append(round(tf_idf, 4))\n        tf_idf_scores.append(tf_idf_doc)\n    return np.array(tf_idf_scores).tolist()\ncorpus = [['the', 'quick', 'brown', 'fox'], ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'quick', 'red', 'fox']]\nquery = ['the', 'quick', 'fox', 'lazy', 'dog']\ntf_idf_scores = compute_tf_idf(corpus, query)"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    tp = np.sum((y_true == 1) & (y_pred == 1))\n    fp = np.sum((y_true == 0) & (y_pred == 1))\n    fn = np.sum((y_true == 1) & (y_pred == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    if precision == 0 or recall == 0:\n        return 0.0\n    f_beta = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)\n    return round(f_beta, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\ndef tanh(x):\n    return np.tanh(x)\ndef dtanh(y):\n    return 1 - y ** 2\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def rnn_forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        time_steps = len(input_sequence)\n        hidden_states = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        for t in range(time_steps):\n            x = np.array(input_sequence[t]).reshape(-1, 1)\n            h_prev = hidden_states[-1]\n            h = tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            hidden_states.append(h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n        return (outputs, hidden_states)\n\n    def rnn_backward(self, input_sequence, expected_output, outputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN using BPTT.\n        \"\"\"\n        time_steps = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n        loss = 0\n        for t in reversed(range(time_steps)):\n            y = outputs[t]\n            expected = np.array(expected_output[t]).reshape(-1, 1)\n            dy = y - expected\n            loss += 0.5 * np.sum(dy ** 2)\n            dW_hy += np.dot(dy, hidden_states[t + 1].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = dh * dtanh(hidden_states[t + 1])\n            dW_xh += np.dot(dh_raw, np.array(input_sequence[t]).reshape(1, -1))\n            dW_hh += np.dot(dh_raw, hidden_states[t].T)\n            db_h += dh_raw\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = b - np.dot(A, x)\n    p = r\n    rsold = np.dot(r, r)\n    for i in range(n):\n        Ap = np.dot(A, p)\n        alpha = rsold / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        rsnew = np.dot(r, r)\n        if np.sqrt(rsnew) < tol:\n            break\n        p = r + rsnew / rsold * p\n        rsold = rsnew\n    return x.round(8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (classes, counts) = np.unique(y, return_counts=True)\n    probabilities = counts / len(y)\n    gini = 1 - np.sum(probabilities ** 2)\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointers = [0]\n    for row in dense_matrix:\n        non_zero_count = 0\n        for (col_idx, element) in enumerate(row):\n            if element != 0:\n                values.append(element)\n                column_indices.append(col_idx)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    return (values, column_indices, row_pointers[:-1])\ndense_matrix = [[1, 0, 0, 2], [0, 3, 0, 0], [4, 0, 5, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "import numpy as np\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    v = np.array(v)\n    L = np.array(L)\n    projection = (np.dot(v, L) / np.dot(L, L) * L).round(3)\n    return projection.tolist()"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    values = []\n    row_indices = []\n    col_ptr = [0]\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        col_ptr.append(len(values))\n    return (values, row_indices, col_ptr)\ndense_matrix = [[0, 2, 0], [3, 0, 4], [0, 5, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Calculates the basis vectors of the column space (image) of matrix A.\n    \n    Args:\n    - A: A numpy array representing the matrix.\n    \n    Returns:\n    - A list of basis vectors that span the column space of A.\n    \"\"\"\n    A_echelon = np.array(A, dtype=float)\n    (n_rows, n_cols) = A_echelon.shape\n    lead = 0\n    for r in range(n_rows):\n        if n_cols == lead:\n            break\n        i = r\n        while A_echelon[i, lead] == 0:\n            i += 1\n            if n_rows == i:\n                i = r\n                lead += 1\n                if n_cols == lead:\n                    break\n        A_echelon[[i, r]] = A_echelon[[r, i]]\n        pivot = A_echelon[r, lead]\n        A_echelon[r, :] /= pivot\n        for i in range(n_rows):\n            if i != r:\n                A_echelon[i, :] -= A_echelon[i, lead] * A_echelon[r, :]\n        lead += 1\n    pivot_indices = []\n    for col in A_echelon.T:\n        if any((col[:i] == 0 and col[i] == 1 for i in range(n_rows))):\n            pivot_indices.append(np.argmax(col != 0))\n    basis_vectors = A[:, pivot_indices]\n    basis_vectors = np.round(basis_vectors, decimals=8).tolist()\n    return basis_vectors\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    \"\"\"\n    Calculate the R-squared value for a given set of true and predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): An array of true values.\n    y_pred (np.ndarray): An array of predicted values.\n    \n    Returns:\n    float: The R-squared value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n    r_squared_value = 1 - ss_res / ss_tot\n    return round(r_squared_value, 3)\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image.\n    \n    Parameters:\n    img (list of list of int): A 2D matrix representing the grayscale image.\n    \n    Returns:\n    float: The average brightness of the image rounded to two decimal places,\n           or -1 if the image is invalid.\n    \"\"\"\n    if not img or not all((isinstance(row, list) for row in img)):\n        return -1\n    total_brightness = 0\n    num_pixels = 0\n    first_row_length = len(img[0])\n    for row in img:\n        if len(row) != first_row_length:\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            num_pixels += 1\n    if num_pixels == 0:\n        return -1\n    average_brightness = total_brightness / num_pixels\n    return round(average_brightness, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the actual values and the predicted values.\n    \n    Parameters:\n    y_true (np.ndarray): Array of actual values.\n    y_pred (np.ndarray): Array of predicted values.\n    \n    Returns:\n    float: RMSE value rounded to three decimal places.\n    \n    Raises:\n    ValueError: If input arrays are of mismatched shapes or empty.\n    TypeError: If input is not an array or contains non-numeric values.\n    \"\"\"\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise TypeError('Inputs must be numpy arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Input arrays must have the same shape.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays must not be empty.')\n    squared_errors = (y_true - y_pred) ** 2\n    rmse_value = np.sqrt(np.mean(squared_errors))\n    return round(rmse_value, 3)\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    - y_true: np.array, true binary labels.\n    - y_pred: np.array, predicted binary labels.\n    \n    Returns:\n    - float, the Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    return round(intersection / union, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n\n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n\n    Returns:\n    float: The Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    elements_true = np.sum(y_true)\n    elements_pred = np.sum(y_pred)\n    if elements_true + elements_pred == 0:\n        dice = 1.0\n    else:\n        dice = 2.0 * intersection / (elements_true + elements_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generates a composite hypervector for a given dataset row.\n    \n    Parameters:\n    row (dict): A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    dim (int): The dimensionality of the hypervectors.\n    random_seeds (dict): A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    list: A composite hypervector representing the entire row.\n    \"\"\"\n\n    def create_hv(seed, dim):\n        \"\"\"Creates a hypervector with a given seed and dimension.\"\"\"\n        np.random.seed(seed)\n        hv = np.where(np.random.rand(dim) > 0.5, 1, -1)\n        return hv\n\n    def bind_hv(hv1, hv2):\n        \"\"\"Binds two hypervectors by element-wise multiplication.\"\"\"\n        return hv1 * hv2\n\n    def bundle_hvs(hvs):\n        \"\"\"Bundles a list of hypervectors into one composite hypervector.\"\"\"\n        composite_hv = np.zeros(dim)\n        for hv in hvs:\n            composite_hv += hv\n        composite_hv = np.where(composite_hv > 0, 1, -1)\n        return composite_hv\n    feature_hvs = []\n    for (feature, value) in row.items():\n        feature_seed = random_seeds[feature]\n        feature_name_hv = create_hv(feature_seed, dim)\n        value_seed = hash(value) + feature_seed\n        value_hv = create_hv(value_seed, dim)\n        feature_value_hv = bind_hv(feature_name_hv, value_hv)\n        feature_hvs.append(feature_value_hv)\n    composite_hv = bundle_hvs(feature_hvs)\n    return composite_hv.tolist()\nrow = {'age': 30, 'income': 50000, 'education': 'bachelor'}\ndim = 1000\nrandom_seeds = {'age': 123, 'income': 456, 'education': 789}\ncomposite_hv = create_row_hv(row, dim, random_seeds)"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Calculate the confusion matrix for binary classification.\n    \n    Parameters:\n    data (list of lists): A list of [y_true, y_pred] pairs.\n    \n    Returns:\n    list of lists: A 2x2 confusion matrix, where index [0][0] is TN, [0][1] is FP, [1][0] is FN, [1][1] is TP.\n    \"\"\"\n    tn_count = fp_count = fn_count = tp_count = 0\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            tn_count += 1\n        elif y_true == 0 and y_pred == 1:\n            fp_count += 1\n        elif y_true == 1 and y_pred == 0:\n            fn_count += 1\n        elif y_true == 1 and y_pred == 1:\n            tp_count += 1\n    confusion_mat = [[tn_count, fp_count], [fn_count, tp_count]]\n    return confusion_mat\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    v1 (numpy.ndarray): The first vector.\n    v2 (numpy.ndarray): The second vector.\n    \n    Returns:\n    float: The cosine similarity between v1 and v2, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or (np.linalg.norm(v2) == 0):\n        raise ValueError('Input vectors cannot be empty or have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cos_sim, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from typing import List, Tuple\nfrom collections import Counter\nimport numpy as np\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[np.ndarray, float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    confusion_matrix = np.zeros((2, 2), dtype=int)\n    for (a, p) in zip(actual, predicted):\n        confusion_matrix[a, p] += 1\n    tp = confusion_matrix[1, 1]\n    tn = confusion_matrix[0, 0]\n    fp = confusion_matrix[0, 1]\n    fn = confusion_matrix[1, 0]\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = round(tp / (tp + fp), 3) if tp + fp > 0 else 0\n    recall = round(tp / (tp + fn), 3) if tp + fn > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if precision + recall > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if tn + fp > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if tn + fn > 0 else 0\n    return (confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value)\nactual = [1, 0, 1, 1, 0, 1, 0, 1, 0, 0]\npredicted = [1, 0, 1, 0, 0, 1, 1, 1, 0, 0]"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    \"\"\"\n    Calculates various descriptive statistics for a given dataset.\n    \n    :param data: list or numpy array of numerical values\n    :return: dictionary containing mean, median, mode, variance, standard deviation,\n             25th percentile, 50th percentile, 75th percentile, and interquartile range.\n    \"\"\"\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]\n    variance = np.var(data)\n    standard_deviation = np.std(data)\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    iqr = percentile_75 - percentile_25\n    stats_dict = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': mode, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\n    return stats_dict\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the average rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability rounded to 5 decimal places\n    \"\"\"\n    factorial_k = math.factorial(k)\n    probability = math.pow(lam, k) * math.exp(-lam) / factorial_k\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        float: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    img = np.array(img)\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    contrast = max_pixel_value - min_pixel_value\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    if vec1.shape != vec2.shape:\n        raise ValueError('Both vectors must be of the same length')\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    exponents = np.arange(degree + 1)\n    data_expanded = np.expand_dims(data_array, axis=-1)\n    exponents_expanded = np.expand_dims(exponents, axis=0)\n    transformed_data = np.power(data_expanded, exponents_expanded)\n    transformed_data = np.round(transformed_data, 8)\n    transformed_data_list = transformed_data.tolist()\n    return transformed_data_list"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    \"\"\"\n    Calculates the positional encodings for a sequence length and model dimensionality.\n    \n    Parameters:\n    - position (int): The sequence length.\n    - d_model (int): The model dimensionality.\n    \n    Returns:\n    - list: A list of positional encodings if position and d_model are valid, otherwise -1.\n    \"\"\"\n    if position <= 0 or d_model <= 0:\n        return -1\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    position_enc = np.arange(position, dtype=np.float16)[:, np.newaxis]\n    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float16) * -(np.log(10000.0) / d_model))\n    pe[:, 0::2] = np.sin(position_enc * div_term)\n    pe[:, 1::2] = np.cos(position_enc * div_term)\n    return pe.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return (np.round(parameter, 5).tolist(), np.round(m, 5).tolist(), np.round(v, 5).tolist())\nparameter = np.array([1.0, 2.0])\ngrad = np.array([0.1, 0.2])\nm = np.array([0.0, 0.0])\nv = np.array([0.0, 0.0])\nt = 1"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    input_ids = encoder.encode(prompt)\n    max_context_length = hparams['n_ctx']\n\n    def get_embeddings(ids):\n        token_embeddings = params['wte'][ids]\n        pos_embeddings = params['wpe'][:len(ids)]\n        return token_embeddings + pos_embeddings\n\n    def multi_head_attention(x):\n        return x\n\n    def feed_forward(x):\n        return np.tanh(x)\n\n    def layer_norm(x, g, b, epsilon=1e-05):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        x = (x - mean) / np.sqrt(var + epsilon)\n        return x * g + b\n    for _ in range(n_tokens_to_generate):\n        embeddings = get_embeddings(input_ids[-max_context_length:])\n        attention_output = multi_head_attention(embeddings)\n        feed_forward_output = feed_forward(attention_output)\n        norm_output = layer_norm(feed_forward_output, params['ln_f']['g'], params['ln_f']['b'])\n        next_token = np.argmax(np.sum(norm_output, axis=0))\n        input_ids.append(next_token)\n    return encoder.decode(input_ids)\nprompt = 'hello world'"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum()\n    crystals = np.array(crystal_values)\n    weights = np.random.rand(dimension, dimension)\n    projected_crystals = np.array([np.dot(weights, [value] * dimension) for value in crystals])\n    weighted_patterns = []\n    for i in range(n):\n        attention_scores = []\n        for j in range(n):\n            if i != j:\n                score = np.dot(projected_crystals[i], projected_crystals[j])\n                attention_scores.append(score)\n            else:\n                attention_scores.append(0)\n        attention_scores = softmax(attention_scores)\n        weighted_pattern = np.zeros(dimension)\n        for j in range(n):\n            if i != j:\n                weighted_pattern += attention_scores[j] * projected_crystals[j]\n        weighted_patterns.append(weighted_pattern)\n    return [np.round(pattern, 4).tolist() for pattern in weighted_patterns]\nn = 4\ncrystal_values = [1, 2, 3, 4]\ndimension = 2"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus given a query.\n    \n    :param corpus: List of documents, each document is a list of words.\n    :param query: List of words representing the query.\n    :param k1: Free parameter to control non-linearity of term frequency.\n    :param b: Free parameter to control the effect of document length.\n    :return: List of BM25 scores for each document in the corpus.\n    \"\"\"\n    avg_doc_len = sum((len(doc) for doc in corpus)) / len(corpus)\n    term_freqs = [Counter(doc) for doc in corpus]\n    doc_freqs = Counter()\n    for doc in corpus:\n        doc_freqs.update(set(doc))\n    scores = []\n    for (doc_idx, doc) in enumerate(corpus):\n        score = 0\n        for term in query:\n            if term in doc:\n                tf = term_freqs[doc_idx][term]\n                df = doc_freqs[term]\n                idf = np.log((len(corpus) - df + 0.5) / (df + 0.5))\n                score += idf * (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * len(doc) / avg_doc_len))\n        scores.append(round(score, 3))\n    return scores\ncorpus = [['information', 'retrieval', 'system'], ['information', 'retrieval', 'query', 'system'], ['information', 'retrieval', 'system', 'evaluation'], ['query', 'system', 'retrieval'], ['system', 'information', 'query']]\nquery = ['information', 'query', 'system']\nscores = calculate_bm25_scores(corpus, query)"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if not y_true or not y_pred:\n        raise ValueError('Input lists cannot be empty.')\n    tp = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 1))\n    fp = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 0 and yp == 1))\n    fn = sum((1 for (yt, yp) in zip(y_true, y_pred) if yt == 1 and yp == 0))\n    precision = tp / (tp + fp) if tp + fp > 0 else 0\n    recall = tp / (tp + fn) if tp + fn > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Removes daily fluctuation from consumption_data, fits a linear regression model,\n    predicts day 15's base consumption, adds back the fluctuation for day 15, and\n    applies a 5% safety margin. Returns the final integer value.\n    \"\"\"\n    days = np.array([i for i in range(1, len(consumption_data) + 1)]).reshape(-1, 1)\n    fluctuation = np.array([10 * math.sin(2 * PI * i / 10) for i in range(1, len(consumption_data) + 1)])\n    detrended_data = np.array(consumption_data) - fluctuation\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([15]).reshape(-1, 1)\n    prediction = model.predict(day_15)\n    day_15_fluctuation = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption = prediction + day_15_fluctuation\n    final_consumption = math.ceil(predicted_consumption * 1.05)\n    return int(final_consumption)\nconsumption_data = [102, 105, 108, 110, 112, 114, 115, 116, 117, 118]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be equal.')\n    absolute_errors = np.abs(y_true - y_pred)\n    mae_value = np.mean(absolute_errors)\n    return round(mae_value, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query, Key, and Value matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, n_heads: int) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention for one head.\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n    attention_weights = np.apply_along_axis(lambda x: np.exp(x) / np.sum(np.exp(x)), axis=1, arr=scores)\n    output = np.matmul(attention_weights, V)\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes multi-head attention.\n    Splits the input matrices Q, K, V into multiple heads and then concatenates them after self-attention.\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    d_model = Q.shape[-1]\n    d_k = d_model // n_heads\n    Q = np.array_split(Q, n_heads, axis=-1)\n    K = np.array_split(K, n_heads, axis=-1)\n    V = np.array_split(V, n_heads, axis=-1)\n    heads = [self_attention(q, k, v, n_heads) for (q, k, v) in zip(Q, K, V)]\n    concatenated_heads = np.concatenate(heads, axis=-1)\n    return np.round(concatenated_heads, decimals=4).tolist()\nX = np.random.randn(5, 10)\nW_q = np.random.randn(10, 10)\nW_k = np.random.randn(10, 10)\nW_v = np.random.randn(10, 10)\nn_heads = 2"}
{"task_id": 95, "completion_id": 0, "solution": "import numpy as np\ndef phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    x_arr = np.array(x)\n    y_arr = np.array(y)\n    n11 = np.sum((x_arr == 1) & (y_arr == 1))\n    n10 = np.sum((x_arr == 1) & (y_arr == 0))\n    n01 = np.sum((x_arr == 0) & (y_arr == 1))\n    n00 = np.sum((x_arr == 0) & (y_arr == 0))\n    numerator = n11 * n00 - n10 * n01\n    denominator = np.sqrt((n11 + n10) * (n01 + n00) * (n11 + n01) * (n10 + n00))\n    if denominator == 0:\n        return 0.0\n    else:\n        phi = numerator / denominator\n        return round(phi, 4)\nx = [0, 1, 1, 0, 1, 0, 0, 1]\ny = [1, 0, 1, 0, 1, 1, 0, 0]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return 0.2 * x + 0.5"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * (math.exp(x) - 1)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 100:\n        return round(x, 4)\n    elif x < -100:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    rhos_clipped = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    objective_clipped = np.minimum(rhos * A, rhos_clipped * A)\n    kl_penalty = beta * (pi_theta_ref / pi_theta_old - 1)\n    grpo_objective_value = np.mean(objective_clipped + kl_penalty)\n    return round(grpo_objective_value, 6)\nrhos = [0.9, 1.1, 1.2]\nA = [0.5, -0.8, 1.0]\npi_theta_old = [0.3, 0.4, 0.6]\npi_theta_ref = [0.4, 0.5, 0.7]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    return round(x / (1 + math.exp(-x)), 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        return scale * x\n    else:\n        return scale * alpha * (math.exp(x) - 1)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities > 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    K = np.unique(y).size\n    D = X.shape[1]\n    B = np.zeros((D, K))\n    Y = np.eye(K)[y]\n    losses = []\n    for i in range(iterations):\n        scores = np.dot(X, B)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(Y * np.log(probs))\n        losses.append(round(loss, 4))\n        dW = np.dot(X.T, probs - Y)\n        B -= learning_rate * dW\n    return (B.round(4).tolist(), [loss for loss in losses])"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sigmoid function to map predicted values to probabilities.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"\n    Binary Cross Entropy loss function.\n    \"\"\"\n    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    bias = 0\n    losses = []\n    for _ in range(iterations):\n        z = np.dot(X, theta) + bias\n        y_pred = sigmoid(z)\n        d_theta = 1 / m * np.dot(X.T, y_pred - y)\n        d_bias = 1 / m * np.sum(y_pred - y)\n        theta -= learning_rate * d_theta\n        bias -= learning_rate * d_bias\n        loss = binary_cross_entropy(y, y_pred)\n        losses.append(round(loss, 4))\n    return (theta.tolist(), bias, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    masked_scores = np.where(mask, -np.inf, attention_scores)\n    attention_weights = np.exp(masked_scores) / np.sum(np.exp(masked_scores), axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "from collections import Counter\nfrom math import log\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    The measure of disorder is based on the Shannon entropy formula, which is a measure of uncertainty or information entropy.\n    \"\"\"\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * log(p, 2) for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Performs Layer Normalization on a 3D array across the feature dimension for each sequence.\n    \n    Parameters:\n    - X: np.ndarray, a 3D tensor of shape (batch_size, seq_length, feature_dim).\n    - gamma: np.ndarray, scaling parameter of shape (feature_dim,).\n    - beta: np.ndarray, shifting parameter of shape (feature_dim,).\n    - epsilon: float, a small constant for numerical stability.\n    \n    Returns:\n    - np.ndarray, the normalized tensor of the same shape as X.\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    std = np.std(X, axis=-1, keepdims=True)\n    normalized_X = (X - mean) / (std + epsilon)\n    normalized_X = normalized_X * gamma + beta\n    return np.round(normalized_X, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    \"\"\"\n    Calculate the METEOR score between a reference and a candidate string.\n    :param reference: str, the reference translation.\n    :param candidate: str, the candidate translation.\n    :param alpha: float, parameter to control precision vs recall.\n    :param beta: float, parameter to control the penalty for fragmentation.\n    :param gamma: float, parameter to control the penalty for word order.\n    :return: float, the METEOR score.\n    \"\"\"\n    ref_words = reference.split()\n    cand_words = candidate.split()\n    matches = min(len(ref_words), len(cand_words))\n    precision = matches / len(cand_words) if cand_words else 0\n    recall = matches / len(ref_words) if ref_words else 0\n    fmean = (1 + alpha ** 2) * precision * recall / (alpha ** 2 * precision + recall) if precision + recall != 0 else 0\n    ref_counter = Counter(ref_words)\n    cand_counter = Counter(cand_words)\n    word_order_penalty = 0\n    for word in ref_counter.keys():\n        if word in cand_counter:\n            ref_indices = [i for (i, x) in enumerate(ref_words) if x == word]\n            cand_indices = [i for (i, x) in enumerate(cand_words) if x == word]\n            for i in ref_indices:\n                min_dist = float('inf')\n                for j in cand_indices:\n                    dist = abs(i - j)\n                    if dist < min_dist:\n                        min_dist = dist\n                word_order_penalty += min_dist\n    fragmentation_penalty = abs(len(ref_words) - len(cand_words))\n    meteor = (1 - gamma) * fmean + gamma * (1 - np.exp(-beta * fragmentation_penalty)) * (1 - np.exp(-gamma * word_order_penalty / matches)) if matches != 0 else 0\n    return round(meteor, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the number of times the two events occur together.\n    - total_counts_x: int, the total number of occurrences of event x.\n    - total_counts_y: int, the total number of occurrences of event y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the computed PMI value rounded to 3 decimal places.\n    \"\"\"\n    if joint_counts == 0:\n        return float('-inf')\n    p_xy = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    pmi = np.log2(p_xy / (p_x * p_y))\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "import numpy as np\ndef min_max(x: list[int]) -> list[float]:\n    \"\"\"\n    Perform Min-Max Normalization on a list of integers, scaling all values to the range [0, 1].\n    Each value x is transformed to (x - min(x)) / (max(x) - min(x)), then rounded to 4 decimal places.\n    :param x: List of integers to be normalized.\n    :return: List of floats with normalized values rounded to 4 decimal places.\n    \"\"\"\n    if not x or max(x) == min(x):\n        return [0.0] * len(x)\n    x_min = min(x)\n    x_max = max(x)\n    normalized = [float(i - x_min) / (x_max - x_min) for i in x]\n    return [round(i, 4) for i in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    \"\"\"\n    Implements a simple residual block with shortcut connection using NumPy.\n    \n    Parameters:\n    - x: np.ndarray, the input array.\n    - w1: np.ndarray, the first weight matrix.\n    - w2: np.ndarray, the second weight matrix.\n    \n    Returns:\n    - Output list after processing the input through the residual block.\n    \"\"\"\n    assert x.shape[0] == w1.shape[0], 'Input and weight1 dimensions do not match for matrix multiplication.'\n    assert w1.shape[1] == w2.shape[0], 'Weight1 and weight2 dimensions do not match for matrix multiplication.'\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(z1, 0)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(z2, 0)\n    shortcut_output = a2 + x\n    output = np.maximum(shortcut_output, 0)\n    return output.round(4).tolist()\nx = np.array([1, 2, 3, 4])\nw1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]])\nw2 = np.array([[0.9, 0.1], [0.2, 0.3]])\noutput = residual_block(x, w1, w2)"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform global average pooling on a 3D numpy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D numpy array of shape (height, width, channels)\n    \n    Returns:\n    np.ndarray: A 1D numpy array of shape (channels,) with the average of\n                each channel across height and width.\n    \"\"\"\n    channels = x.shape[2]\n    return np.array([np.mean(x[:, :, c]) for c in range(channels)])"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05) -> np.ndarray:\n    \"\"\"\n    Perform batch normalization on a 4D input array in the BCHW format.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, channels, height, width), the input to normalize.\n    - gamma: np.ndarray of shape (channels,), the scale parameter.\n    - beta: np.ndarray of shape (channels,), the shift parameter.\n    - epsilon: float, a small value to avoid division by zero.\n    \n    Returns:\n    - np.ndarray: The normalized, scaled, and shifted input, rounded to 4 decimal places.\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_scaled_shifted = gamma.reshape(1, channels, 1, 1) * X_norm + beta.reshape(1, channels, 1, 1)\n    return np.round(X_scaled_shifted, decimals=4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    \"\"\"\n    Calculate the derivative of a polynomial term at a given point x.\n    \n    Args:\n    c (float): The coefficient of the polynomial term.\n    x (float): The point at which to evaluate the derivative.\n    n (float): The exponent of the term.\n    \n    Returns:\n    float: The value of the derivative at point x, rounded to 4 decimal places.\n    \"\"\"\n    derivative_value = n * c * x ** (n - 1)\n    return round(derivative_value, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Args:\n        vectors: A list of 2D vectors represented as lists.\n        tol: Tolerance value to determine linear independence.\n    \n    Returns:\n        A list of orthonormal vectors that span the same subspace as the input vectors.\n    \"\"\"\n    V = np.array(vectors).T\n    (m, n) = V.shape\n    U = np.zeros((m, n))\n    for i in range(n):\n        projection = U[:, :i] @ (U[:, :i].T @ V[:, i])\n        w = V[:, i] - projection.sum(axis=1)\n        if np.linalg.norm(w) > tol:\n            U[:, i] = w / np.linalg.norm(w)\n    U = U[:, np.linalg.norm(U, axis=0) > tol]\n    return [list(map(lambda x: round(x, 4), U[:, i].tolist())) for i in range(U.shape[1])]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    \"\"\"\n    Computes the cross product of two 3D vectors.\n    \n    Parameters:\n    a (list/tuple): First 3D vector.\n    b (list/tuple): Second 3D vector.\n    \n    Returns:\n    list: The cross product vector as a list rounded to 4 decimal places.\n    \"\"\"\n    a_np = np.array(a)\n    b_np = np.array(b)\n    cross_product_np = np.cross(a_np, b_np)\n    cross_product_rounded = np.round(cross_product_np, 4)\n    return cross_product_rounded.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n    \n    Parameters:\n    A (np.ndarray): A square coefficient matrix.\n    b (np.ndarray): A constant vector.\n    \n    Returns:\n    list: The solution vector x rounded to 4th decimal places, or -1 if no unique solution exists.\n    \"\"\"\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    x = []\n    for i in range(A.shape[1]):\n        Ai = A.copy()\n        Ai[:, i] = b\n        det_Ai = np.linalg.det(Ai)\n        x.append(round(det_Ai / det_A, 4))\n    return np.array(x).tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions p and q.\n    \n    Args:\n    p (list[float]): The first probability distribution.\n    q (list[float]): The second probability distribution.\n    \n    Returns:\n    float: The Bhattacharyya distance between p and q, rounded to 4 decimal places.\n           Returns 0.0 if p and q have different lengths or are empty.\n    \"\"\"\n    if len(p) != len(q) or len(p) == 0:\n        return 0.0\n    coeff = np.sqrt(np.multiply(p, q)).sum()\n    dist = -np.log(coeff)\n    return round(dist, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float] | int:\n    \"\"\"\n    Computes the element-wise sum of two vectors.\n    \n    Parameters:\n    a (list of int|float): The first vector.\n    b (list of int|float): The second vector.\n    \n    Returns:\n    list of int|float: A new vector representing the sum of the two vectors.\n    int: -1 if the operation cannot be performed due to incompatible dimensions.\n    \"\"\"\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    \"\"\"\n    Computes the policy gradient using the REINFORCE algorithm.\n\n    Parameters:\n    - theta (np.ndarray): A 2D array of shape (num_states, num_actions) representing the policy parameters.\n    - episodes (list[list[tuple[int, int, float]]]): A list of episodes. Each episode is a list of (state, action, reward) tuples.\n\n    Returns:\n    - list: The average policy gradient, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def softmax(x):\n        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum(axis=0)\n\n    def compute_returns(ep):\n        \"\"\"Compute cumulative returns for the given episode.\"\"\"\n        returns = []\n        cumulative_return = 0\n        for (_, _, reward) in reversed(ep):\n            cumulative_return += reward\n            returns.append(cumulative_return)\n        returns = list(reversed(returns))\n        return returns\n    gradient = np.zeros_like(theta, dtype=np.float64)\n    num_episodes = len(episodes)\n    for ep in episodes:\n        returns = compute_returns(ep)\n        for (t, (state, action, _)) in enumerate(ep):\n            state_policy = softmax(theta[state, :])\n            log_policy = np.log(state_policy[action])\n            gradient[state, action] += log_policy * returns[t]\n    average_gradient = gradient / num_episodes\n    rounded_gradient = np.round(average_gradient, 4)\n    return rounded_gradient.tolist()\ntheta = np.array([[1.0, 2.0], [3.0, 4.0]])\nepisodes = [[(0, 1, 1.0), (1, 0, 1.0)], [(0, 0, 0.5), (1, 1, 0.5)]]"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculates the computational cost savings of an MoE layer compared to a dense layer.\n    \n    Parameters:\n    n_experts (int): The total number of experts in the MoE layer.\n    k_active (int): The number of active experts in the MoE layer.\n    d_in (int): The input dimension for both the MoE and dense layers.\n    d_out (int): The output dimension for both the MoE and dense layers.\n    \n    Returns:\n    float: The percentage of computational cost savings, rounded to the nearest 1th decimal.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = 2 * (k_active * d_in * d_out + d_in * n_experts)\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    return round(savings, 1)\nn_experts = 128\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K gating mechanism for Mixture-of-Experts models.\n    :param X: Input matrix of shape (batch_size, input_features).\n    :param W_g: Weight matrix for gating of shape (input_features, num_experts).\n    :param W_noise: Weight matrix for noise scaling of shape (input_features, num_experts).\n    :param N: Pre-sampled noise of shape (batch_size, num_experts).\n    :param k: Sparsity constraint, number of experts to select.\n    :return: Gating probabilities matrix of shape (batch_size, num_experts).\n    \"\"\"\n    logits = X @ W_g\n    noise = N * (X @ W_noise)\n    noisy_logits = logits + noise\n    exp_noisy_logits = np.exp(noisy_logits)\n    sum_exp_noisy_logits = np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    softmax_noisy_logits = exp_noisy_logits / sum_exp_noisy_logits\n    topk_indices = np.argpartition(softmax_noisy_logits, -k, axis=1)[:, -k:]\n    softmax_noisy_logits[np.arange(softmax_noisy_logits.shape[0])[:, None], topk_indices] -= 1\n    softmax_noisy_logits[softmax_noisy_logits < 0] = 0\n    softmax_noisy_logits[softmax_noisy_logits >= 0] = 1\n    sum_topk = np.sum(softmax_noisy_logits, axis=1, keepdims=True)\n    gating_probabilities = softmax_noisy_logits / sum_topk\n    return np.round(gating_probabilities, 4).tolist()\nbatch_size = 2\ninput_features = 3\nnum_experts = 4\nk = 2\nX = np.random.rand(batch_size, input_features)\nW_g = np.random.rand(input_features, num_experts)\nW_noise = np.random.rand(input_features, num_experts)\nN = np.random.rand(batch_size, num_experts)\ngating_probabilities = noisy_topk_gating(X, W_g, W_noise, N, k)"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Implements a Mixture-of-Experts (MoE) layer using softmax gating and top-k routing.\n    \n    Parameters:\n    x (np.ndarray): Input tensor of shape (batch_size, input_dim).\n    We (np.ndarray): Expert weight matrices of shape (n_experts, input_dim, output_dim).\n    Wg (np.ndarray): Gating weight matrix of shape (input_dim, n_experts).\n    n_experts (int): Number of experts.\n    top_k (int): Number of top experts to select per token.\n    \n    Returns:\n    np.ndarray: Output tensor of the MoE layer.\n    \"\"\"\n    (batch_size, input_dim) = x.shape\n    output_dim = We.shape[2]\n    gating_scores = np.dot(x, Wg)\n    gating_probs = np.exp(gating_scores - gating_scores.max(axis=-1, keepdims=True))\n    gating_probs /= gating_probs.sum(axis=-1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, -top_k:]\n    top_k_gating_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    top_k_gating_probs /= top_k_gating_probs.sum(axis=-1, keepdims=True)\n    expert_outputs = np.einsum('bij,jkl->bikl', x[:, np.newaxis, :], We)\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, top_k_indices[:, :, np.newaxis, np.newaxis], axis=2)\n    aggregated_output = (top_k_expert_outputs * top_k_gating_probs[:, :, :, np.newaxis]).sum(axis=2)\n    aggregated_output = np.round(aggregated_output, decimals=4)\n    return aggregated_output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    (B, C, H, W) = X.shape\n    G = num_groups\n    X = X.reshape(B, G, C // G, H, W)\n    mean = np.mean(X, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X, axis=(2, 3, 4), keepdims=True)\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    X_norm = X_norm.reshape(B, C, H, W)\n    X_norm = gamma * X_norm + beta\n    return np.round(X_norm, 4).tolist()\nnum_groups = 4\nX = np.random.randn(B, C, H, W)\ngamma = np.random.randn(C)\nbeta = np.random.randn(C)"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    \"\"\"\n    Implement a function that finds the value of x where f(x) = x^4 - 3x^3 + 2 reaches its minimum.\n    \n    Parameters:\n    - start_x: Initial starting point for x.\n    - learning_rate: Step size for each iteration.\n    - tolerance: The threshold for determining when to stop the iterations.\n    - max_iters: Maximum number of iterations to perform.\n    \n    Returns:\n    - The value of x where f(x) is minimized, rounded to the nearest 4th decimal.\n    \"\"\"\n\n    def f_prime(x):\n        \"\"\"Derivative of the function f(x) = x^4 - 3x^3 + 2.\"\"\"\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for _ in range(max_iters):\n        gradient = f_prime(x)\n        x_new = x - learning_rate * gradient\n        if abs(x_new - x) < tolerance:\n            break\n        x = x_new\n    return round(x, 4)\nstart_x = np.random.uniform(-10, 10)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) function to the input array x.\n    \n    Parameters:\n    x (np.ndarray): The input array to which the DyT function will be applied.\n    alpha (float): The scaling factor for the output of the tanh function.\n    gamma (float): The scaling factor for the input x.\n    beta (float): The offset added to the scaled input x.\n    \n    Returns:\n    list[float]: A list of floats representing the DyT function output, rounded to 4 decimal places.\n    \"\"\"\n    dyt_output = alpha * np.tanh(gamma * x + beta)\n    return np.round(dyt_output, 4).tolist()\nx = np.array([0.1, 0.5, 1.0, 2.0, 3.0])\nalpha = 1.5\ngamma = 0.8\nbeta = 0.3"}
