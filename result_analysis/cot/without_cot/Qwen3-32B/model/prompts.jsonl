{"task_id": 0, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Ridge Regression Closed-Form Solver\n# Description:\nImplement Ridge (L2-regularised) Linear Regression using its closed-form solution.  \nGiven an ndarray where the last column is the target $\\mathbf y$ and all preceding columns are the features $\\mathbf X$, add a bias (intercept) term, then compute the weight vector  \n\n$$\\hat\\mathbf w=(\\mathbf X^\\top\\mathbf X+\\alpha\\,\\mathbf I)^{-1}\\,\\mathbf X^\\top\\mathbf y,$$\n\nwhere $\\alpha\\ge 0$ is the regularisation strength and $\\mathbf I$ is the identity matrix with a size equal to the number of columns in $\\mathbf X$ (after adding the bias).  \n\nIf the matrix $(\\mathbf X^\\top\\mathbf X+\\alpha\\,\\mathbf I)$ is not invertible (numerical determinant equal to 0), return **-1**.  \n\nReturn the weight vector rounded to **4 decimal places** as a Python list.\n# Your code should start with:\n```python\nimport numpy as np\ndef ridge_regression(data: np.ndarray, alpha: float = 0.0) -> list[float]:\n    \"\"\"Your docstring here.\"\"\"\n    pass\n```\n# Output Constraints:\nRound every coefficient to the nearest 4th decimal and return a Python list (not an ndarray).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 1, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Dual-Form Perceptron Learning\n# Description:\nImplement the dual-form perceptron learning algorithm.\n\nThe classic perceptron learns a linear classifier of the form  f(x)=sign(w\u00b7x+b).  In its **dual formulation** the weight vector w is expressed as a linear combination of training samples\n\n            w = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62\n\nwhere \u03b1\u1d62 \u2265 0 are the dual parameters that are updated during training.  All computations that involve x appear only through the inner product K(x\u1d62,x\u2c7c)=x\u1d62\u00b7x\u2c7c (i.e. the **Gram matrix**), so the algorithm is a first step towards kernel methods.\n\nWrite a function `perceptron_dual` that, given a training set `X_train` (shape `(n_samples, n_features)`) and a label vector `y_train` (values must be **+1 or \u20111**), learns the classifier with the following rules:\n\n1. Initialise `\u03b1 = 0`,  `b = 0`.\n2. Scan the samples in the order 0 \u2026 n-1.\n3. For the i-th sample compute  \n      activation = \u03a3\u2c7c \u03b1\u2c7c y\u2c7c K(x\u2c7c,x\u1d62)\n   and test the margin  y\u1d62 (activation + b).\n4. If the margin is \u2264 0 the sample is mis-classified \u2013 update  \n      \u03b1\u1d62 \u2190 \u03b1\u1d62 + \u03b7,\n      b   \u2190 b + \u03b7 y\u1d62,\n   then restart the scan from i = 0.\n5. Stop when an entire pass over the data finishes with **no** update or after `n_iter` updates (the latter prevents an infinite loop on inseparable data).\n6. After training compute the primal weight vector w from the final \u03b1.\n\nReturn the tuple `(w, b)` where  \u2022 `w` is returned as a Python list rounded to four decimals,  \u2022 `b` is a scalar rounded to four decimals.\n\nIf the data are linearly separable the algorithm is guaranteed to converge in finite time.\n# Your code should start with:\n```python\nimport numpy as np\ndef perceptron_dual(X_train: np.ndarray,\n                    y_train: np.ndarray,\n                    eta: float = 1.0,\n                    n_iter: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Dual-form perceptron.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training samples of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Binary labels (+1 or \u20111) of length n_samples.\n    eta : float, optional\n        Learning rate, by default 1.0.\n    n_iter : int, optional\n        Maximum number of updates, by default 10 000.\n\n    Returns\n    -------\n    tuple[list[float], float]\n        The weight vector (as a list) and the bias; both rounded to 4 decimals.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every component of w as well as b to 4 decimal places before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 2, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement Standard GLM Link Functions\n# Description:\nIn Generalized Linear Models (GLMs) the relationship between the expected value of the response variable, \u03bc, and the linear predictor, \u03b7 = X\u03b2, is controlled by a *link* function g(\u00b7).\n\nFor the three most common GLM instances you will implement a small helper that returns numpy-aware callables for\n\u2022 the link\u2003\u2003\u2003\u2003\u2003  g(\u03bc)\n\u2022 its inverse \u2003\u2003\u2003\u2003g\u207b\u00b9(\u03b7)\n\u2022 the first derivative g\u2032(\u03bc).\n\nRequired links\n1. identity\u2003g(\u03bc)=\u03bc\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003(for Gaussian family)\n2. log\u2003\u2003\u2003g(\u03bc)=log \u03bc\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003 (for Poisson family)\n3. logit\u2003\u2003g(\u03bc)=log(\u03bc/(1-\u03bc))\u2003\u2003\u2003\u2003\u2003 (for Bernoulli / Binomial)\n\nThe function must return a dictionary that can be used like the snippet below:\n```\nlinks = glm_links()\nmu  = np.array([0.2, 0.8])\neta = links[\"logit\"][\"link\"](mu)        # \u2192 [-1.3863, 1.3863]\nmu2 = links[\"logit\"][\"inv_link\"](eta)    # \u2192 [0.2, 0.8]\n```\nAll returned functions have to work with numpy scalars **and** 1-D/2-D numpy arrays via element-wise operations.\n# Your code should start with:\n```python\nimport numpy as np\ndef glm_links():\n    \"\"\"Construct and return standard GLM link functions.\n\n    Returns\n    -------\n    dict\n        A three-entry dictionary (identity, log, logit) where each entry is a\n        dictionary containing callables for the link, its inverse and its\n        derivative with respect to \u03bc. All functions must work with numpy\n        scalars as well as 1-D/2-D numpy arrays via element-wise operations.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nRound every numerical output inside your examples and in the public test-cases to four decimal places when showing them, but the functions themselves must work with full floating-point precision.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 3, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Isolation Tree Path Lengths\n# Description:\nIn **Isolation Forests** each sample is isolated by recursively partitioning the data with random splits.  A single randomly\u2013grown binary tree that performs this procedure is called an *isolation tree*.\n\nGrow the following kind of isolation tree for a given data matrix `data` (each row is a sample, each column a feature):\n1. The node receives the set of row-indices that reach it.\n2. If fewer than three samples reach the node it becomes a *leaf* and stores the indices it contains.\n3. Otherwise pick a split as follows\n   \u2022 choose a feature index `f` uniformly at random from all available features;\n   \u2022 let `down = min(data[indices, f])` and `up = max(data[indices, f])`;\n   \u2022 draw a real number `v` uniformly from `[down, up]`.\n4. Send every sample whose feature value is `\u2264 v` to the *left* child, all others to the *right* child and continue recursively.\n\nAfter the tree has been built, traverse it and record, for every original sample, the *depth* (number of edges from the root) of the leaf in which the sample ends up.\n\n\u2022 Always invoke `numpy.random.seed(0)` **inside** the main function before any random call so the result is reproducible.\n\u2022 Return the depths as a Python `list` where the *i-th* element corresponds to the *i-th* row of the input matrix.\n\nExample\nInput\n    data = np.arange(5).reshape(-1, 1)\nOutput\n    [2, 2, 2, 1, 1]\nReasoning\n    The tree is built with the random seed fixed to 0.  The first split separates samples `[0,1,2]` from `[3,4]`.  The left side is split once more while the right side is already a leaf.  Hence the first three samples reach depth 2 while the last two stop at depth 1.\n# Your code should start with:\n```python\nimport numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom collections import Counter\n\ndef isolation_tree_path_length(data: np.ndarray) -> list[int]:\n    \"\"\"Compute the depth of the leaf reached by every sample in a random isolation tree.\n\n    The tree is built following the rules described in the task statement and the\n    random seed must be set to 0 so that results are reproducible.\n\n    Args:\n        data: A 2-D NumPy array of shape (n_samples, n_features).\n\n    Returns:\n        A list of length *n_samples* where the *i-th* element is the number of\n        edges from the root to the leaf that contains the *i-th* sample.\n    \"\"\"\n    # TODO: implement the function according to the specification\n    pass\n```\n# Output Constraints:\nReturn a Python list of non-negative integers whose length equals the number of rows in the input array.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 7, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Frequent Itemset Mining\n# Description:\nIn market basket analysis one often needs to discover **all item combinations that occur frequently enough** in a transactional data set.  \nYour task is to write a function that, given a list of transactions and a minimum support threshold, returns every frequent itemset together with its absolute support (the number of transactions that contain the itemset).\n\nDefinitions\n1. A *transaction* is a list of hashable items (strings, numbers, \u2026).\n2. The *support* of an itemset is the number of transactions that contain **all** the items in the set (duplicates inside the same transaction are ignored).\n3. An itemset is *frequent* if  \n   support \\(\\ge\\lceil \\text{minsup}\\times N\\rceil\\) where \\(N\\) is the total number of transactions.\n\nRequirements\n\u2022 Return the result as a dictionary `dict[tuple, int]` where each key is the itemset written as a **tuple sorted in ascending order** and the value is its support count.  \n\u2022 If no itemset satisfies the threshold return the empty dictionary `{}`.  \n\u2022 The algorithm must work for any 0 < `minsup` \u2264 1.  \n\u2022 Do **not** use third-party libraries such as *pandas*, *sklearn*, *torch*, *tensorflow* \u2026 \u2013 only Python standard library modules are allowed.\n\nExample\nInput\ntransactions = [\n    ['bread', 'milk'],\n    ['bread', 'diaper', 'beer', 'egg'],\n    ['milk', 'diaper', 'beer', 'coke'],\n    ['bread', 'milk', 'diaper', 'beer'],\n    ['bread', 'milk', 'diaper', 'coke']\n]\nminsup = 0.6\n\nOutput\n{\n ('bread',): 4,\n ('milk',): 4,\n ('diaper',): 4,\n ('beer',): 3,\n ('bread', 'milk'): 3,\n ('bread', 'diaper'): 3,\n ('diaper', 'milk'): 3,\n ('beer', 'diaper'): 3\n}\n\nReasoning\nThere are 5 transactions, so \\(\\lceil0.6\\times5\\rceil = 3\\).  \nAll single items that appear in at least three transactions are frequent.  \nLikewise, pairs such as (bread, milk) appear together in 3 transactions and are also frequent.  \nNo triplet reaches a support of 3, therefore none is returned.\n# Your code should start with:\n```python\nimport math\nfrom collections import Counter\nfrom itertools import combinations\nfrom typing import List, Dict, Tuple, Hashable\n\ndef find_frequent_itemsets(transactions: List[List[Hashable]], minsup: float) -> Dict[Tuple[Hashable, ...], int]:\n    \"\"\"Return all frequent itemsets in **transactions**.\n\n    Args:\n        transactions: List where each element represents a transaction and is\n                       itself a list of items (hashable Python objects).\n        minsup:       Minimum support given as a fraction between 0 and 1.\n\n    Returns:\n        A dictionary that maps every frequent itemset (stored as a tuple of\n        sorted items) to its absolute support count.  If no itemset reaches\n        the threshold the function must return an empty dictionary.\n    \"\"\"\n    # Write your code below\n    pass\n```\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 11, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: K-Means Clustering from Scratch\n# Description:\nImplement the K-Means clustering algorithm **without relying on any external machine-learning library**.  \nThe function must repeatedly  \n1. choose initial cluster centres,  \n2. assign every sample to its nearest centre (using the squared Euclidean distance),  \n3. recompute each centre as the arithmetic mean of all samples currently assigned to it,  \n4. stop when the maximum change of any centre between two consecutive iterations becomes smaller than `epsilon` **or** when `max_iter` iterations have been executed.  \n\nRequirements\n\u2022   The initial centres are simply the first `k` samples of the input array (deterministic and therefore testable).  \n\u2022   If during the iterations a cluster becomes empty, immediately re-initialise its centre with a random sample from the dataset (use `np.random.randint`) so the algorithm can continue.  \n\u2022   After convergence round every coordinate of every centre to **4 decimal places** and return them together with the list of cluster labels for the samples.  \n\u2022   No object-oriented code (classes) or external ML libraries such as *scikit-learn* are allowed.\n# Your code should start with:\n```python\nimport numpy as np\ndef kmeans(data: np.ndarray, k: int, epsilon: float = 1e-3, max_iter: int = 2000) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Cluster *data* into *k* groups using the K-Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column a feature.\n    k : int\n        Desired number of clusters (1 \u2264 k \u2264 number of samples).\n    epsilon : float, optional (default=1e-3)\n        Threshold on the maximum centre movement used as the convergence\n        criterion.\n    max_iter : int, optional (default=2000)\n        Maximum number of iterations allowed.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[int]]\n        A pair consisting of\n        \u2022 a list with *k* centres (each rounded to 4 decimals) and\n        \u2022 a list with the cluster index of every input sample.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nEvery coordinate of every returned centre must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 13, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Item-based k-NN Collaborative Filtering Recommender\n# Description:\nYou are asked to implement a *pure* Python / NumPy version of an **item\u2013based k-nearest neighbour (k-NN) collaborative filtering recommender**.\n\nGiven\n\u2022 a user\u2013item rating matrix `data` where each row represents a user and each column an item,\n\u2022 the index of an *active* user `user_ind`,\n\u2022 the number `k` of items that have to be proposed, and\n\u2022 a similarity measure `criterion` that can be either `\"cosine\"` (default) or `\"pearson\"`,\n\nwrite a function that returns the indices of at most **k** items that the active user has **not** yet rated but are predicted to be the most attractive to him / her.\n\nAlgorithm to follow  (exactly replicates the logic of the reference implementation shown in the original code snippet):\n1. Build an *item\u2013item similarity matrix* `S` of shape `(n_item, n_item)`.\n   \u2022 For every unordered pair of items `(i , j)` collect all users that rated **both** items (ratings > 0).\n   \u2022 If the intersection is empty set `S[i,j] = S[j,i] = 0`.\n   \u2022 Otherwise form the two rating vectors `v1 , v2`.\n        \u2013 If `criterion == \"cosine\"` first *mean centre* each vector **only** when its sample standard deviation is larger than `1e-3` and then compute the cosine similarity.\n        \u2013 If `criterion == \"pearson\"` compute the usual sample Pearson correlation (`np.corrcoef`).\n2. For the active user collect the indices of the items he / she has already rated (`r > 0`). Denote the ratings with the vector `r`.\n3. For every yet unrated item `t` compute the *predicted attractiveness*\n        score(t) = \u03a3\u1d62  r\u1d62 \u00b7 S[t,i]   /   \u03a3\u1d62 |S[t,i]|\n   where the summation runs over the rated items `i` only.  If the denominator is `0`, the score is defined to be `0`.\n4. Return a list with the at most **k** unseen items sorted by decreasing predicted score.  If two items obtain exactly the same score keep the one with the **smaller** column index first (Python\u2019s sort stability guarantees this when the dictionary is filled in ascending order).\n5. If the user has already rated *all* items return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\nfrom collections import defaultdict\nimport numpy as np\nfrom collections import defaultdict\n\ndef knn_recommend(data: np.ndarray,\n                  user_ind: int,\n                  k: int,\n                  criterion: str = 'cosine') -> list[int]:\n    \"\"\"Item-based k-NN collaborative filtering recommender.\n\n    The function must build an item\u2013item similarity matrix and then predict\n    the attractiveness of every yet unrated item for the specified user.  It\n    finally returns the indices of the *k* most promising items.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D rating matrix of shape (n_user, n_item) containing **positive**\n        ratings; a value of *0* means *not rated*.\n    user_ind : int\n        Index (0-based) of the active user for whom we want to obtain\n        recommendations.\n    k : int\n        Maximal number of items that must be recommended.\n    criterion : str, optional\n        Similarity metric to employ \u2013 either ``'cosine'`` (default) or\n        ``'pearson'``.\n\n    Returns\n    -------\n    list[int]\n        A list with at most ``k`` item indices ordered from the highest to\n        the lowest predicted score.\n    \"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list whose length is at most k and that is sorted according to the rules stated in the description.  No rounding of the scores is required (only the indices are returned).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 19, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Best Gain Split for Gradient-Boosting Tree\n# Description:\nGradient boosting trees evaluate candidate feature thresholds by how much they decrease the regularised loss function.  \n\nFor a leaf that contains a set of training instances \\(\\mathcal{I}\\) with first-order (gradient) statistics \\(g_i\\) and second-order (Hessian) statistics \\(h_i\\), the regularised objective of that leaf is  \n\\[\\mathcal{L}(\\mathcal{I})\\;=\\;-\\,\\frac{1}{2}\\,\\frac{\\big(\\sum_{i\\in\\mathcal{I}} g_i\\big)^2}{\\sum_{i\\in\\mathcal{I}} h_i\\; +\\;\\lambda}\\; +\\;\\gamma\\]  \nwhere \\(\\lambda\\) and \\(\\gamma\\) are regularisation hyper-parameters.\n\nIf a node is split into a left child \\(\\mathcal{I}_L\\) and a right child \\(\\mathcal{I}_R\\), the **gain** obtained from the split is  \n\\[\\text{gain}\\;=\\;\\mathcal{L}(\\mathcal{I})\\; -\\;\\mathcal{L}(\\mathcal{I}_L)\\; -\\;\\mathcal{L}(\\mathcal{I}_R).\\]\n\nA positive gain implies the split reduces the overall loss.\n\nWrite a function `best_split` that, given\n1. a feature matrix `X` (shape *n_samples \u00d7 n_features*),\n2. the corresponding first-order gradients `g`,\n3. the corresponding second-order gradients `h`,\n4. the regularisation constants `gamma` and `lam`,\n\nreturns the best split **(feature_index, threshold)** that maximises the gain.  \n\nRules:\n\u2022 Consider every unique value of every feature as a possible threshold.  \n\u2022 A valid split must leave **at least two** training instances on each side.  \n\u2022 If no split yields a strictly positive gain, return `None`.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef best_split(X: np.ndarray,\n               g: np.ndarray,\n               h: np.ndarray,\n               gamma: float,\n               lam: float) -> tuple[int, float] | None:\n    \"\"\"Return the best (feature, threshold) split for a tree node.\n\n    The split is chosen to maximise the reduction in the regularised loss used\n    by gradient-boosting decision-trees.  If no split achieves a positive gain\n    the function returns ``None``.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape *(n_samples, n_features)* containing the feature\n        values of all training instances that reach the current node.\n    g : np.ndarray\n        1-D array with the first-order gradients for each training instance.\n    h : np.ndarray\n        1-D array with the second-order gradients (Hessians).\n    gamma : float\n        Complexity regularisation term added to every leaf.\n    lam : float\n        L2 regularisation term added to the denominator when computing the\n        weight of a leaf.\n\n    Returns\n    -------\n    tuple[int, float] | None\n        A pair *(feature_index, threshold)* describing the optimal split, or\n        ``None`` if no valid split yields a positive gain.\n    \"\"\"\n    # >>>>>>>>>>  Write your code here  <<<<<<<<<<\n    pass\n```\n# Output Constraints:\nReturn `None` or a tuple `(feature_index, threshold)`.\nIf a split exists, the tuple must correspond to the split with the largest positive gain.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 20, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement Sigmoid Activation and Its Gradient\n# Description:\nThe sigmoid (logistic) activation function is widely used in neural networks where it maps any real-valued input into the interval (0,1).  Its derivative (gradient) is equally important during back-propagation.  \n\nWrite a function that takes a single numeric value, a Python list, or a NumPy array *x* and returns **both** the element-wise sigmoid values and their corresponding gradients.\n\nBehaviour requirements\n1. The function must work with:\n   \u2022 an `int`/`float` scalar  \n   \u2022 a 1-D/2-D NumPy array  \n   \u2022 a Python list (which you may internally convert to a NumPy array)\n2. The return type must be a tuple `(sigmoid_x, gradient_x)` where\n   \u2022 if the input is a scalar, both items are rounded `float`s  \n   \u2022 if the input is an array/list, both items are *Python lists* of the same shape and rounded element-wise.\n3. All results must be rounded to **4 decimal places**.\n4. Use only the standard library and **NumPy**.\n# Your code should start with:\n```python\nimport numpy as np\ndef sigmoid_activation(x):\n    \"\"\"Compute the sigmoid of *x* and its gradient.\n\n    Parameters\n    ----------\n    x : float | int | list | numpy.ndarray\n        Input data that can be a scalar, a Python list, or a NumPy array.\n\n    Returns\n    -------\n    tuple\n        A tuple (sigmoid_x, gradient_x)\n        where each element is rounded to 4 decimal places and returned as:\n        \u2022 float when *x* is scalar\n        \u2022 Python list when *x* is array-like\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every numeric result to 4 decimal places and, for non-scalar inputs, convert NumPy arrays to Python lists using `.tolist()`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 21, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost with One-Dimensional Decision Stumps\n# Description:\nImplement a **from-scratch** version of the AdaBoost learning algorithm when the weak learner is a one\u2013dimensional decision stump.  \n\nThe decision stump (weak classifier) is defined by a pair `(d, \u03b8)` where  \n\u2022 `d = 0` means it predicts **+1** when the sample value is *\u2264 \u03b8* and **\u20131** otherwise  \n\u2022 `d = 1` means it predicts **+1** when the sample value is *> \u03b8* and **\u20131** otherwise  \n\nDuring training the algorithm must:  \n1.  Start with uniform sample weights.  \n2.  Enumerate every possible stump obtained by putting the threshold halfway between every two consecutive training points (after the data are sorted).  \n3.  Repeatedly pick the stump with the minimum weighted error, compute its coefficient  \n      \u03b1 = \u00bd\u00b7ln((1\u2013err)/err)  \n     (update rules are those of standard AdaBoost).  \n4.  Update the sample weights and normalise them.  \n5.  Stop when the **training** error of the current ensemble is not larger than the user supplied value `epsilon`.\n\nAfter training, the strong classifier\u2019s prediction for a point *x* is  \n             sign( \u03a3 \u03b1\u1d62 \u00b7 h\u1d62(x) )  \nwhere *h\u1d62* are the selected decision stumps.\n\nWrite a single function that trains the ensemble **and** returns the predictions for a given test set.\n\nIf several stumps reach exactly the same weighted error you may return any of them \u2013 results for the provided tests are unique anyway.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\nfrom collections import defaultdict\ndef adaboost_1d_predict(x_train: list[float],\n                        y_train: list[int],\n                        x_test: list[float],\n                        epsilon: float = 0.0) -> list[int]:\n    \"\"\"Trains a 1-D AdaBoost ensemble and returns predictions.\n\n    Parameters\n    ----------\n    x_train : list[float]\n        Training sample values (one-dimensional).\n    y_train : list[int]\n        Labels corresponding to *x_train* (each value must be 1 or \u20131).\n    x_test : list[float]\n        Sample values to classify after training.\n    epsilon : float, default 0.0\n        Upper bound on the allowed training error.  Training stops once\n        the ensemble\u2019s training error \u2264 *epsilon*.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (1 or \u20131) for every value in *x_test*.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list of integers, each element being either **1** or **-1**.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 25, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gaussian Kernel SVM Prediction\n# Description:\nYou are given everything that is already needed to make predictions with a pre-trained Support Vector Machine that uses a Gaussian (a.k.a. Radial Basis Function \u2013 RBF) kernel:\n1. X_train \u2013 the training samples used when the model was fitted (shape n\u00d7d)\n2. y_train \u2013 their binary class labels (only \u22121 or 1, length n)\n3. alpha \u2013 the final Lagrange multipliers returned by the training algorithm (length n)\n4. b \u2013 the bias (intercept) term\n5. gamma \u2013 the Gaussian kernel hyper-parameter\n6. X_test \u2013 samples whose classes have to be predicted (shape m\u00d7d)\n\nFor a test vector z the SVM decision function is\n    g(z) = \u03a3_{i=1..n} \u03b1_i \u00b7 y_i \u00b7 exp( \u2212\u03b3 \u00b7 ||x_i \u2212 z||\u00b2 )  +  b\nwhere ||\u00b7|| denotes the Euclidean norm.  The predicted class is sign(g(z)).  Implement a function that computes this value for every row in X_test and returns the corresponding predicted labels as a Python list of integers (each element must be either 1 or \u22121).\n\nThe implementation must work for arbitrary numbers of training and test samples, be fully vectorised (only NumPy, math are allowed) and must not rely on any external ML library. Do NOT raise exceptions; you may assume the inputs are valid.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef gaussian_svm_predict(X_train: np.ndarray,\n                         y_train: np.ndarray,\n                         alpha: np.ndarray,\n                         b: float,\n                         gamma: float,\n                         X_test: np.ndarray) -> list[int]:\n    \"\"\"Predict labels for test samples using a Gaussian-kernel SVM.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        The (n, d) matrix of training samples used to fit the SVM.\n    y_train : numpy.ndarray\n        The length-n vector of training labels. Each entry is either 1 or -1.\n    alpha : numpy.ndarray\n        The length-n vector of Lagrange multipliers obtained during training.\n    b : float\n        The scalar bias term obtained during training.\n    gamma : float\n        The positive Gaussian (RBF) kernel parameter.\n    X_test : numpy.ndarray\n        The (m, d) matrix of samples whose labels must be predicted.\n\n    Returns\n    -------\n    list[int]\n        The predicted labels for all m test samples. Each element is exactly\n        1 or -1.\n    \"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list of integers of length m, where m is the number of test samples. Each element must be exactly 1 or -1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 28, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Linear Autoencoder Reconstruction\n# Description:\nAn autoencoder can be interpreted as a neural network that tries to reproduce its input at the output.  \nIf the network is linear and we only keep **k** latent dimensions, then the optimal reconstruction (with minimum squared error) is given by keeping the first **k** singular values/vectors of the data matrix \u2013 i.e. by a truncated Singular Value Decomposition (SVD).\n\nWrite a Python function that:\n1. Receives a two-dimensional list **X** (shape \\(m\\times n\\)) and an integer **k** (\\(1\\le k\\le \\min(m,n)\\)).  \n2. Computes the rank-\\(k\\) reconstruction \\(\\hat X\\) using the truncated SVD (this is equivalent to the best linear auto-encoder with **k** latent units).\n3. Returns a tuple `(X_hat, mse)` where  \n   \u2022 **X_hat** is the reconstructed matrix rounded to four decimals and converted back to a list of lists,  \n   \u2022 **mse** is the mean squared reconstruction error, also rounded to four decimals.\n\nIf **k** is smaller than 1 or greater than `min(m, n)` return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef linear_autoencoder(X: list[list[int | float]], k: int) -> tuple[list[list[float]], float]:\n    \"\"\"Return the optimal rank-k reconstruction of X using truncated SVD.\n\n    Parameters\n    ----------\n    X : list[list[int | float]]\n        Two-dimensional numeric data matrix (m \u00d7 n).\n    k : int\n        Number of latent dimensions to retain.\n\n    Returns\n    -------\n    tuple[list[list[float]], float]\n        A tuple (X_hat, mse) where `X_hat` is the reconstructed matrix and\n        `mse` is the mean squared reconstruction error.  Both are rounded to\n        four decimals.  If `k` is invalid the function returns -1.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every reconstructed value and the MSE to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 29, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: One-Dimensional Gradient Boosting with Stumps\n# Description:\nYou are asked to implement a very small-scale gradient boosting regressor that only works on one-dimensional data and that uses decision stumps (a single split with a constant value on each side) as weak learners.  The algorithm is as follows:\n1. Sort the training samples by the single feature $x$ (a scalar).\n2. Candidate split points are the mid-points between every two consecutive feature values.\n3. While the residual sum of squares (RSS) of the current ensemble is larger than a tolerance $\\varepsilon$  \n   \u2022  For every candidate split **s**\n      \u2013 let $c_1$ be the mean of the current residuals whose feature values are $\\le s$  \n      \u2013 let $c_2$ be the mean of the current residuals whose feature values are $>  s$  \n      \u2013 compute the RSS that would be obtained by adding the stump defined by *(s, c1, c2)*  \n   \u2022  Add to the ensemble the stump that produces the smallest RSS.\n   \u2022  Update the residuals (real target minus current ensemble prediction).\n4. After the loop finishes, predictions for a new point *x* are obtained by summing the constant contributions of all learned stumps (add $c_1$ if *x* is on the left of the corresponding split, otherwise add $c_2$).\n\nWrite the function `predict_boosting_tree` that\n\u2022  receives the training feature list/array `x_train`, the training target list/array `y_train`, a query point `x_query`, and an optional tolerance `epsilon` (default $10^{-2}$),\n\u2022  trains the ensemble described above on the training data, and\n\u2022  returns the prediction for `x_query`.\n\nReturn value must be rounded to 4 decimal places.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef predict_boosting_tree(x_train: list[float] | np.ndarray,\n                           y_train: list[float] | np.ndarray,\n                           x_query: float,\n                           epsilon: float = 1e-2) -> float:\n    \"\"\"Fit a simple 1-D gradient boosting model (decision stumps) and predict a value.\n\n    The function must:  \n    1. Determine all possible split points (mid-points between consecutive *x_train* values).  \n    2. Iteratively add the stump that minimises the squared residuals until the total\n       residual sum of squares becomes smaller than *epsilon*.  \n    3. Return the prediction for *x_query* obtained by summing the constants contributed\n       by every learnt stump.  \n\n    Args:\n        x_train: One-dimensional training features.\n        y_train: Training targets (same length as *x_train*).\n        x_query: Feature value to predict.\n        epsilon: Stopping tolerance on the residual sum of squares (default 1e-2).\n\n    Returns:\n        Single floating point number \u2013 the predicted target for *x_query*, rounded to\n        four decimal places.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturn a single floating-point value rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 34, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Random Binary Tensor Generator\n# Description:\nYou are asked to write a utility that produces a NumPy tensor filled with 0.0s and 1.0s.  The caller specifies the desired shape and a sparsity value (probability of generating a **1**).  Optionally a seed can be provided to obtain reproducible results.\n\nThe function must obey the following rules:\n1. \"sparsity\" is a real number in the closed interval \\([0,1]\\).\n2. Each entry of the returned tensor is 1.0 with probability equal to \"sparsity\" and 0.0 otherwise.\n3. If a seed is supplied, the procedure must first call ``np.random.seed(seed)`` so that the result is deterministic.\n4. When ``sparsity`` is outside the legal range the function must return **-1**.\n5. The output must be a ``numpy.ndarray`` whose ``dtype`` is a floating type (``0.0`` and ``1.0`` values only).\n\nExample behaviour (with seed for reproducibility):\nInput: ``shape = (2,3)``, ``sparsity = 0.3``, ``seed = 42``  \nRandom array produced by ``np.random.rand`` starts with\n[[0.3745 , 0.9507 , 0.7320 ],\n [0.5987 , 0.1560 , 0.1560 ]].  \nThe threshold is ``1 - 0.3 = 0.7`` so entries \\(\\ge 0.7\\) become 1.0, others 0.0, giving\n[[0.0, 1.0, 1.0],\n [0.0, 0.0, 0.0]].\n# Your code should start with:\n```python\nimport numpy as np\ndef random_binary_tensor(shape: tuple[int, ...],\n                         sparsity: float = 0.5,\n                         seed: int | None = None):\n    \"\"\"TODO: Complete docstring and implement the function\"\"\"\n    pass\n```\n# Output Constraints:\nReturn a NumPy array containing only the float values 0.0 and 1.0.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 39, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Median Split for KD-Tree Construction\n# Description:\nImplement a function that performs a median split on a dataset \u2013 the basic operation that underlies KD-Tree construction. For a given two-dimensional NumPy array `data` (shape `(n_samples, n_features)`) and a column index `d`, the function has to\na. find the sample whose value in column `d` is the median ( for even *n*, use position `n//2` )\nb. return its **row index** (in the original array)\nc. return the row indices of all samples that fall strictly to the *left* (smaller values) of the median and the row indices that fall strictly to the *right* (larger values).\nThe split must be executed in **O(n)** time by using `numpy.argpartition` (do **not** sort the whole column). The lists of indices in the result must be sorted increasingly to make the output deterministic.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef get_split(data: np.ndarray, d: int) -> tuple[int, list[int], list[int]]:\n    \"\"\"Split *data* along column *d* by its median value.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array of shape (n_samples, n_features).\n    d : int\n        Index of the column to use for the split.\n\n    Returns\n    -------\n    tuple\n        (pivot, left, right) where\n        \u2022 pivot is the row index whose value in column *d* is the median;\n        \u2022 left  is a list of row indices with smaller values;\n        \u2022 right is a list of row indices with larger  values.\n    \"\"\"\n    # ===== write your code below =====\n    pass\n```\n# Output Constraints:\n\u2022 Return a 3-tuple (pivot, left, right)\n\u2022 `pivot` is an int, `left` and `right` are **Python lists** of ints\n\u2022 The two lists must be sorted increasingly\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 40, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Overlapping Signal Framing\n# Description:\nImplement a NumPy-based utility that breaks a one-dimensional signal into overlapping frames.  Given a 1-D NumPy array `x`, a positive integer window length `frame_width`, and a positive hop length `stride`, the function must return a view on `x` with shape `(n_frames, frame_width)` such that consecutive rows are separated by `stride` samples.  The number of frames is defined as  \n\n    n_frames = (len(x) - frame_width) // stride + 1\n\nIf `(len(x) - frame_width) % stride != 0`, the trailing samples that cannot form a complete frame are dropped.  \n\nThe implementation **must** rely on low-level stride manipulation (i.e. `numpy.lib.stride_tricks.as_strided`) so that the result is a *view* on the original signal, not a copy.  A Boolean argument `writeable` controls whether the returned view can be written to.  When `writeable=False` the returned array must have `arr.flags.writeable == False`, otherwise it inherits the writability of the original array.\n\nReturn the resulting framed signal as a NumPy array.\n\nIf any of the following pre-conditions are violated the function should fail with an `AssertionError` (use `assert`):\n1. `x` is not one-dimensional.\n2. `stride < 1`.\n3. `len(x) < frame_width`.\n# Your code should start with:\n```python\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\ndef to_frames(x: np.ndarray, frame_width: int, stride: int, writeable: bool = False) -> np.ndarray:\n    \"\"\"Convert a 1-D signal into overlapping frames.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        One-dimensional input signal of length *N*.\n    frame_width : int\n        The length (*in samples*) of each output frame.\n    stride : int\n        Hop length \u2013 number of samples between the starts of successive frames.\n    writeable : bool, default=False\n        If *False* the returned view is read-only; if *True* it is writeable\n        whenever the source array was writeable.\n\n    Returns\n    -------\n    np.ndarray\n        A view on *x* with shape ``(n_frames, frame_width)`` where\n        ``n_frames = (len(x) - frame_width) // stride + 1``.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nThe function returns a NumPy ndarray view on the original data.  Its shape must be `(n_frames, frame_width)` as defined in the description.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 48, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Validate Row-Stochastic Matrix\n# Description:\nA stochastic (probability) matrix is a two-dimensional array whose elements are valid probabilities (each element lies in the closed interval [0, 1]) and whose rows each sum to 1.  \n\nWrite a Python function that verifies whether a given matrix is row-stochastic.\n\nThe function must:\n1. Accept the matrix as a *list of lists* or a `numpy.ndarray` of numeric values.\n2. Return **True** if **all** of the following hold, otherwise return **False**:\n   \u2022 Every element is between 0 and 1 inclusive.  \n   \u2022 The matrix is two-dimensional.  \n   \u2022 Each row sums to 1 up to a numerical tolerance of `1e-8` (use `numpy.allclose`).\n\nNo exceptions should be raised by the function \u2013 just return the Boolean result.\n# Your code should start with:\n```python\nimport numpy as np\ndef is_stochastic(X: list[list[float]] | \"np.ndarray\") -> bool:\n    \"\"\"Return True if *X* is a row-stochastic matrix, otherwise False.\n\n    A matrix is row-stochastic when every element is a probability (0 \u2264 p \u2264 1)\n    and each row sums to 1 (within a small numerical tolerance).\n\n    Args:\n        X: Matrix given as a list of lists or a NumPy array.\n\n    Returns:\n        bool: True if *X* is row-stochastic, False otherwise.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single Boolean value: True if the matrix is row-stochastic, otherwise False.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 55, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: RMSprop Optimiser for Linear Regression\n# Description:\nImplement the RMSprop optimisation algorithm for ordinary least-squares (OLS) linear regression.\n\nGiven a design matrix X\u2208\u211d^{n\u00d7d} (each row is a training sample and each column is a feature) and a target vector y\u2208\u211d^{n}, the goal is to find a weight vector w\u2208\u211d^{d} that minimises the mean\u2013squared error\n\n    L(w)=1/(2n)\u2016Xw\u2212y\u2016\u00b2.\n\nWrite a function rms_prop that starts from the all-zero weight vector and iteratively updates the parameters using the RMSprop rule\n\n    s   \u2190 \u03c1\u00b7s +(1\u2212\u03c1)\u00b7g\u00b2            (element-wise)\n    w   \u2190 w \u2212 \u03b7 \u00b7 g /(\u221as+\u03f5_station)\n\nwhere\n    g  = \u2207L(w) = (1/n)\u00b7X\u1d40(Xw\u2212y)\n    s  is the running average of squared gradients (initialised with zeros),\n    \u03c1  is the decay rate,\n    \u03b7  is the learning rate, and\n    \u03f5_station is a tiny constant to avoid division by zero.\n\nStop the optimisation **early** when the \u2113\u2082-norm of the gradient becomes smaller than epsilon or when the number of iterations reaches max_iter.\nReturn the final weight vector rounded to four decimal places and converted to a Python list.\n\nIf n<batch_size, simply use the full data set as one batch; otherwise process mini-batches by slicing successive blocks of rows (wrap around when the end of the matrix is reached).\n# Your code should start with:\n```python\nimport numpy as np\ndef rms_prop(\n    X: np.ndarray,\n    y: np.ndarray,\n    epsilon: float = 1e-4,\n    max_iter: int = 10_000,\n    eta: float = 0.01,\n    rho: float = 0.9,\n    batch_size: int = 32,\n    eps_station: float = 1e-8,\n) -> list[float]:\n    \"\"\"Train a linear regression model with RMSprop.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix where each row is a sample and each column is a feature.\n    y : np.ndarray\n        Target values.\n    epsilon : float, optional\n        Norm threshold for early stopping.\n    max_iter : int, optional\n        Maximum number of iterations.\n    eta : float, optional\n        Learning rate.\n    rho : float, optional\n        Decay factor for the squared gradient running average.\n    batch_size : int, optional\n        Number of samples per mini-batch.\n    eps_station : float, optional\n        Small constant added for numerical stability.\n\n    Returns\n    -------\n    list[float]\n        The learned weight vector rounded to four decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the weight vector as a Python list with every element rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 56, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: SoftPlus Activation with Gradient\n# Description:\nImplement the SoftPlus activation function that is widely used in deep-learning models.  \nThe SoftPlus of a real number $x$ is defined as  \nSoftPlus$(x)=\\log\\bigl(1+e^{x}\\bigr)$.  \nIts derivative with respect to $x$ is the logistic (sigmoid) function  \n$\\sigma(x)=\\dfrac{1}{1+e^{-x}}$.  \n\nWrite a Python function `softplus` that\n1. Accepts a scalar, Python list, or NumPy array `x` containing real values, and a Boolean flag `deriv` (default `False`).\n2. When `deriv=False` it returns **SoftPlus(x)** for every element of `x`.\n3. When `deriv=True` it returns the **gradient**, i.e. the element-wise sigmoid of `x`.\n4. Uses a numerically stable formulation so that very large positive or negative inputs do not overflow (hint: `log1p` and a piece-wise expression help).\n5. Rounds every resulting value to the nearest 4th decimal and returns the results as a Python list.  If a scalar is provided, return the rounded scalar **float**.\n# Your code should start with:\n```python\nimport numpy as np\ndef softplus(x, deriv: bool = False):\n    \"\"\"TODO: implement\"\"\"\n    pass\n```\n# Output Constraints:\nRound every value to the nearest 4th decimal.\nReturn a Python list (or a single float when the input is a scalar).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 58, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Unsmoothed Maximum-Likelihood N-gram Log-Probability\n# Description:\nImplement an unsmoothed, Maximum-Likelihood Estimation (MLE) N-gram language model.  \nGiven a training corpus (a list of word tokens), an integer order **N** (\u22651) and a target **sequence** (also a list of word tokens), write a function that returns the total log-probability (natural logarithm) of the sequence under the **N**-gram MLE model trained on the corpus.\n\nFor a fixed order **N** the probability of an N-gram `(w\u2081 \u2026 w_N)` is estimated by\n\n \u2022 N = 1 (unigram):\u2003P(w\u2081) = count(w\u2081) / |corpus|\n \u2022 N > 1           :\u2003P(w\u2081 \u2026 w_N) = count(w\u2081 \u2026 w_N) / count(w\u2081 \u2026 w_{N-1})\n\nThe log-probability of the whole sequence is the sum of the log-probabilities of every length-**N** sliding window inside the sequence:\n\n\u2003log P(sequence) = \u03a3 log P(sequence[i : i+N])\u2003for i = 0 \u2026 len(sequence)\u2212N\n\nIf at any point either the numerator or the denominator is zero (i.e. the n-gram or its prefix was not observed in the corpus) the function must return `float('-inf')`.\n# Your code should start with:\n```python\nimport numpy as np\nfrom collections import Counter\nfrom collections import Counter\nimport numpy as np\n\ndef unsmoothed_ngram_log_prob(corpus: list[str], sequence: list[str], N: int) -> float:\n    \"\"\"Compute the unsmoothed MLE N-gram log-probability of *sequence*.\n\n    Your task is to complete this function so that it builds N-gram count\n    tables from *corpus* and then returns the total log-probability of\n    *sequence* under the resulting unsmoothed language model.\n\n    The return value must be rounded to 4 decimal places.  If any required\n    count is zero you should immediately return ``float('-inf')``.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the log-probability rounded to 4 decimal places using `round(value, 4)`.  \nIf the probability is zero return `float('-inf')` (negative infinity) exactly.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 62, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Univariate Lasso Regression with Polynomial Features\n# Description:\nImplement a univariate Lasso regression learner that supports polynomial feature expansion. The implementation must use **coordinate descent** to minimise the following objective function  \n\n    1/2m * \\sum_{i=1}^{m} ( \\hat y_i- y_i )^2  + \\lambda * \\sum_{j=1}^{d} |w_j| ,\n\nwhere\n\u2022 m is the number of training examples,  \n\u2022 d is the chosen polynomial degree,  \n\u2022 w\u2080 is the bias (intercept) and is **not** regularised,  \n\u2022 w\u2c7c   ( j \u2265 1 ) are the coefficients of the j-th polynomial term x\u2c7c (x raised to power j),  \n\u2022 \u03bb is the supplied regularisation strength `reg_factor`.\n\nThe function must\n1. Accept one\u2013dimensional input `X`, target values `y`, a polynomial degree `degree`, a regularisation strength `reg_factor`, an optional maximum number of iterations `n_iterations`, and an optional tolerance `tol` used for early stopping.\n2. Build a design matrix that contains a column of ones followed by x\u00b9, x\u00b2 \u2026 x\u1d48. \n3. Optimise the weights with coordinate descent\n   \u2022 Update the bias exactly in every iteration:   \n     w\u2080 \u2190 mean( y \u2212 X_{\u00ac0}\u00b7w_{\u00ac0} )\n   \u2022 For every other coefficient compute  \n     \u03c1 = x\u2c7c\u1d40 (y \u2212 (X\u00b7w) + w\u2c7c x\u2c7c)  \n     w\u2c7c \u2190 soft_threshold(\u03c1 , \u03bb) / (x\u2c7c\u1d40x\u2c7c) ,  \n     where soft_threshold(\u03c1 , \u03bb) = sign(\u03c1)\u00b7max(|\u03c1|\u2212\u03bb, 0).\n4. Stop when the largest absolute weight change falls below `tol` or after `n_iterations` passes.  \n5. Return **all** coefficients `[w\u2080, w\u2081, \u2026, w_d]` rounded to 4 decimal places as a regular Python list.\n\nIf `reg_factor` is 0 the algorithm must converge to the ordinary least-squares solution.\n# Your code should start with:\n```python\nimport numpy as np\ndef lasso_regression(X: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\", degree: int, reg_factor: float, n_iterations: int = 1000, tol: float = 1e-6) -> list[float]:\n    \"\"\"Train a univariate Lasso regression model.\n\n    The function must build polynomial features up to *degree*, optimise the\n    Lasso objective with coordinate descent, and return the learned\n    coefficients rounded to four decimal places.\n\n    Parameters\n    ----------\n    X : list[float] | np.ndarray\n        One-dimensional input samples.\n    y : list[float] | np.ndarray\n        Target values with the same length as *X*.\n    degree : int\n        Highest exponent of *x* to include (gives *degree+1* coefficients in\n        total counting the bias).\n    reg_factor : float\n        L1 regularisation strength \u03bb.\n    n_iterations : int, default=1000\n        Maximum number of coordinate descent passes.\n    tol : float, default=1e-6\n        Early stopping criterion. The algorithm terminates when the largest\n        absolute change in any coefficient between two consecutive passes is\n        smaller than *tol*.\n\n    Returns\n    -------\n    list[float]\n        The learned weights [w0, w1, \u2026, w_degree] rounded to 4 decimals.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the list of coefficients rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 63, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Hidden Markov Model \u2013 Backward Probability Vector\n# Description:\nIn a discrete Hidden Markov Model (HMM) the backward variable \\(\\beta_t(i)\\) expresses the probability of seeing the remaining observations from time \\(t+1\\) onward given that the system is in state \\(i\\) at time \\(t\\):\n\\[\n\\beta_t(i)=\\sum_{j=1}^{N}a_{ij}\\,b_j(o_{t+1})\\,\\beta_{t+1}(j)\\, ,\\qquad \\beta_{T-1}(i)=1\\,\\forall i.\n\\]\nHere\n\u2022 \\(a_{ij}\\) is the transition probability from state \\(i\\) to state \\(j\\),\n\u2022 \\(b_j(o_{t+1})\\) is the emission probability of observing symbol \\(o_{t+1}\\) in state \\(j\\),\n\u2022 \\(T\\) is the length of the observation sequence.\n\nWrite a function that returns the backward probability vector \\(\\beta_t\\) for a given time index \\(t\\).\n\nThe function receives\n1. A \u2014 transition-probability matrix of shape (N, N),\n2. B \u2014 emission-probability matrix of shape (N, M),\n3. obs \u2014 list of observation indices (length T),\n4. t \u2014 integer time index (0 \u2264 t < T).\n\nIt must output a Python list containing the \\(N\\) backward probabilities rounded to 4 decimal places.\n# Your code should start with:\n```python\nimport numpy as np\ndef backward_beta(A: list[list[float]], B: list[list[float]], obs: list[int], t: int) -> list[float]:\n    \"\"\"YOUR DOCSTRING HERE\"\"\"\n```\n# Output Constraints:\nMake sure all results are rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 65, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Hidden Markov Model \u2013 Backward Algorithm Probability\n# Description:\nIn a Hidden Markov Model (HMM) the probability that a particular observation sequence $O=(o_0,o_1,\\dots ,o_{T-1})$ is generated by the model $\\lambda=(\\pi ,A,B)$ can be computed efficiently with the backward algorithm.\n\nThe backward variables are defined as\n$$\n\\beta_t(i)=P(o_{t+1},o_{t+2},\\dots,o_{T-1}\\mid q_t=i,\\lambda),\\qquad t=T-1,\\dots ,0.\n$$\nThey can be calculated recursively\n$$\n\\beta_{T-1}(i)=1,\\qquad\n\\beta_t(i)=\\sum_{j=0}^{N-1}A_{ij}\\,B_{j,o_{t+1}}\\,\\beta_{t+1}(j) \\quad (t<T-1).\n$$\nFinally the sequence probability is\n$$\nP(O\\mid\\lambda)=\\sum_{i=0}^{N-1}\\pi_i\\,B_{i,o_0}\\,\\beta_0(i).\n$$\nWrite a function that receives the three HMM parameters and an observation sequence (as lists) and returns this probability using the backward algorithm.\n\nIf any of the input lists are empty you should return **0.0** because no valid probability can be computed.\n# Your code should start with:\n```python\nimport numpy as np\ndef backward_prob(A: list[list[float]], B: list[list[float]], pi: list[float], obs: list[int]) -> float:\n    \"\"\"Hidden Markov Model backward algorithm.\n\n    Given an HMM defined by transition matrix `A`, emission matrix `B`, and\n    initial distribution `pi`, compute the probability that the model\n    generates the observation sequence `obs`.\n\n    The method uses the recursive backward procedure and returns the result\n    rounded to six decimal places.\n\n    Args:\n        A: Square matrix where `A[i][j]` is the transition probability from\n           state *i* to state *j*.\n        B: Matrix where `B[i][k]` is the probability of emitting symbol *k*\n           from state *i*.\n        pi: Initial probability distribution over states.\n        obs: List of integer observation indices.\n\n    Returns:\n        A float \u2013 the sequence probability rounded to 6 decimals.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 6th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 69, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Hidden Markov Model \u2013 Forward Algorithm\n# Description:\nHidden Markov Models (HMMs) are widely used to model sequential data whose underlying system is assumed to be a Markov process with unobservable (hidden) states.  \n\nWrite a function that implements the *forward algorithm* to compute the likelihood of an observation sequence given an HMM.  \nThe model is fully specified by\n\u2022 the initial\u2010state probability vector S (length n),  \n\u2022 the state\u2013transition matrix A (n\u00d7n), and  \n\u2022 the emission matrix B (n\u00d7m) where B[i][k] is the probability of emitting observation symbol k from state i.  \n\nGiven S, A, B and a list of integer observations, return the probability that the model generates exactly that sequence.  \nThe function must:\n1. Validate the input dimensions.  \n2. Check that every observation index is in the valid range [0, m\u22121].  \n3. Return \u22121 when the input is invalid (dimension mismatch, empty sequence, or out-of-range index).  \n4. Otherwise implement the forward algorithm and return the result rounded to 4 decimal places.\n# Your code should start with:\n```python\nimport numpy as np\ndef forward_algorithm(S: list[float],\n                      A: list[list[float]],\n                      B: list[list[float]],\n                      observations: list[int]) -> float:\n    \"\"\"Forward algorithm for Hidden Markov Models.\n\n    Args:\n        S (list[float]): Initial state probabilities.\n        A (list[list[float]]): State\u2013transition probabilities.\n        B (list[list[float]]): Emission probabilities.\n        observations (list[int]): Observation index sequence.\n\n    Returns:\n        float: Sequence likelihood rounded to 4 decimals, or \u22121 on invalid input.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal.  Use Python\u2019s built-in round(value, 4).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 70, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Frequent Pattern Growth (FP-Growth) Implementation\n# Description:\nWrite a Python function that discovers every frequent item-set that appears in a transactional data base using the **FP-Growth** algorithm (Han et al., 2000).  \nThe function receives  \n\u2022 `transactions` \u2013 a list whose elements are themselves lists; each inner list contains the items purchased in a single transaction.  \n\u2022 `min_support` \u2013 an integer \u2265 1 that states how many transactions an item-set has to appear in before it is considered frequent.\n\nThe algorithm must  \n1. Count the support of every single item and discard infrequent ones.  \n2. Build one FP-tree (a prefix tree in which every node stores *item name* and *support count*).  \n3. Recursively mine conditional FP-trees to obtain larger item-sets.  \n\nReturn a list of all frequent item-sets.  To make automatic testing possible the result has to be **deterministic**:\n\u2022 Inside every item-set the items must be sorted in lexicographic (alphabetical) order.  \n\u2022 The outer list must be sorted first by increasing item-set length and then lexicographically; in Python this is achieved with\n```python\nfrequent_itemsets.sort(key=lambda x: (len(x), x))\n```\nIf no item-set satisfies the support threshold return an empty list.\n# Your code should start with:\n```python\nfrom collections import Counter, defaultdict\ndef fp_growth(transactions: list[list[str]], min_support: int) -> list[list[str]]:\n    \"\"\"Discover every frequent item-set in *transactions* with FP-Growth.\n\n    A *transaction* is represented by a list of items (strings).  `min_support`\n    is the minimum number of transactions an item-set has to appear in.  The\n    function returns **all** frequent item-sets where\n        support(itemset) >= min_support.\n\n    The result must be deterministic:\n      \u2022 Inside each item-set the items have to be sorted alphabetically.\n      \u2022 The outer list has to be sorted by `(len(itemset), itemset)`.\n    If *transactions* is empty or no item-set meets the threshold return an\n    empty list.\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\n1. Each item inside an item-set must be sorted lexicographically.\n2. The returned list must be sorted by `(len(itemset), itemset)` so that calling `sort` with that key does **not** change the order.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 75, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: k-Nearest Neighbour Classifier\n# Description:\nImplement a simple k-Nearest Neighbour (k-NN) classifier.\n\nGiven a labelled training set `(X_train, y_train)` and an unlabeled test set `X_test`, the classifier must assign a class to every test sample by majority voting among its **k** closest training samples (Euclidean distance).  \n\nRules:\n1. Distances are computed with the ordinary Euclidean metric (you may omit the square-root because it is monotone).  \n2. If several classes are tied for the highest vote, return the **smallest** class label among the tied ones.  \n3. If *k* is not a positive integer or *k* is larger than the number of training samples, return `-1`.\n\nThe function must return the predicted classes as a Python list of integers.\n# Your code should start with:\n```python\nimport numpy as np\ndef knn_predict(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], k: int) -> list[int]:\n    \"\"\"Predicts class labels for a test set using the k-Nearest Neighbour algorithm.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training samples where each inner list is a feature vector.\n    y_train : list[int]\n        Integer class labels corresponding to `X_train`.\n    X_test : list[list[float]]\n        Samples to classify.\n    k : int\n        Number of neighbours to use (must satisfy 1 \u2264 k \u2264 len(X_train)).\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for every sample in `X_test`.\n        If `k` is invalid the function returns -1.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a one-dimensional Python list of integers (not a NumPy array).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 76, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Categorical Cross-Entropy Loss\n# Description:\nImplement a function that calculates the (unnormalised) categorical cross-entropy loss for a batch of one-hot encoded targets.\n\nGiven  \n\u2022 y \u2013 the true class labels, encoded as a 2-D list/NumPy array of shape (n_samples, n_classes) where each row is one-hot (exactly one element equals 1, all others are 0);\n\u2022 y_pred \u2013 the predicted class probabilities, a 2-D list/NumPy array of the same shape produced by a soft-max layer (each row sums to 1).\n\nThe categorical cross-entropy loss for the whole batch is\n\n    L = -\u2211_{i=1}^{n_samples} \u2211_{j=1}^{n_classes} y_{ij}\u00b7log(y\u0302_{ij}+\u03b5)\n\nwhere \u03b5 (machine epsilon) is added for numerical stability so that log(0) never occurs.\n\nReturn L rounded to four decimal places as a Python float.\n\nIf the shapes of y and y_pred differ, or any probability in y_pred is negative or greater than 1, the behaviour is undefined (you may assume the input is valid).\n# Your code should start with:\n```python\nimport numpy as np\ndef cross_entropy_loss(y: list | 'np.ndarray', y_pred: list | 'np.ndarray') -> float:\n    \"\"\"Compute the unnormalised categorical cross-entropy loss.\n\n    Parameters\n    ----------\n    y : list | np.ndarray\n        One-hot encoded true labels of shape (n_samples, n_classes).\n    y_pred : list | np.ndarray\n        Predicted probabilities of the same shape produced by a model.\n\n    Returns\n    -------\n    float\n        Total cross-entropy loss for the batch, rounded to 4 decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 77, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Forward Propagation for an L-Layer Neural Network\n# Description:\nIn a fully-connected feed-forward neural network each layer performs the following two steps\n\n1. Linear step:          Z = W\u00b7A_prev + b\n2. Non-linear step:      A = g(Z)\n\nwhere A_prev is the activation coming from the previous layer (the input matrix X for the first layer), W and b are the layer\u2019s parameters and g is an activation function. In this task you have to implement the forward propagation for an L-layer network that\n\n\u2022 uses ReLU in every hidden layer (layers 1 \u2026 L-1)\n\u2022 uses the sigmoid function in the output layer (layer L)\n\nThe network parameters are stored in a dictionary\n    parameters = {\n        'W1': \u2026, 'b1': \u2026,\n        'W2': \u2026, 'b2': \u2026,\n        \u2026\n        'WL': \u2026, 'bL': \u2026\n    }\nwhere W\u1dab has shape (n\u1dab, n\u1dab\u207b\u00b9) and b\u1dab has shape (n\u1dab, 1).\n\nYour function must\n1. iterate through all layers, applying a linear step followed by the correct activation;\n2. collect a cache (you may store anything that is useful for a backward pass) for each layer in a list called caches;\n3. finally return a tuple (AL, caches) where AL is the activation produced by the last layer.\n\nFor grading we only inspect AL, but caches must still be produced so that the tuple structure is preserved.\n# Your code should start with:\n```python\nimport numpy as np\ndef L_model_forward(X: np.ndarray, parameters: dict[str, np.ndarray]) -> list[list[float]]:\n    \"\"\"Forward propagation for an L-layer neural network (ReLU\u2026ReLU \u2192 Sigmoid).\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Input matrix of shape (n_x, m).\n    parameters : dict[str, np.ndarray]\n        Dictionary containing the network parameters W1\u2026WL and b1\u2026bL.\n\n    Returns\n    -------\n    list[list[float]]\n        The final activation AL rounded to 4 decimals and converted to a plain\n        Python list. The shape is (1, m).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound the final activation matrix AL to 4 decimal places and convert it to a regular Python list via ndarray.tolist() before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 81, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Binary Cross-Entropy Cost Computation\n# Description:\nYou are given two NumPy arrays that come from the forward-propagation of a binary classifier.\n\n\u2022 A2 \u2013 predicted probabilities returned by the model.  \n\u2022 Y  \u2013 ground-truth binary labels (0 or 1).\n\nYour task is to write a function compute_cost that returns the binary cross-entropy (a.k.a. log-loss) between A2 and Y\n\n            1  m\n    J =  \u2013 \u2500 \u03a3  [ y\u1d62\u00b7ln(a\u1d62) + (1\u2013y\u1d62)\u00b7ln(1\u2013a\u1d62) ]\n            m i=1\n\nwhere m is the number of samples.  \nBecause taking log(0) is undefined, first clip every element of A2 to the interval [\u03b5, 1\u2013\u03b5] where \u03b5 = 1e-15.  \nReturn the final cost rounded to **six** decimal places.\n# Your code should start with:\n```python\nimport numpy as np\ndef compute_cost(A2: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"Compute the binary cross-entropy cost.\n\n    Args:\n        A2 (np.ndarray): Predicted probabilities, shape (1, m) or (m,).\n        Y  (np.ndarray): Ground-truth labels (0 or 1), same shape as ``A2``.\n\n    Returns:\n        float: The cross-entropy cost rounded to 6 decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python float rounded to 6 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 82, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement \u03b5-Soft Exploration Policy\n# Description:\nIn reinforcement-learning, an \u0003b5-soft (a.k.a. \u0003b5-greedy) exploration policy guarantees that every action has a non-zero probability of being selected while still favouring the greedy (best) action.  \n\nFor a set of Q\u2013values Q(s, a) that quantify how good each action a is in a state s, the \u0003b5-soft probabilities are defined as follows\n\n    let n = number of actions\n    let a* = argmax_a Q(s, a)                   # first occurrence in case of ties\n    p(a*)   = 1 - \u0003b5 + (\u0003b5 / n)\n    p(a\u2260a*) = \u0003b5 / n\n\nWrite a function `epsilon_soft` that, given a 1-D list/NumPy array of Q-values and a scalar 0 \u2264 \u0003b5 \u2264 1, returns the probability of choosing every action under the \u0003b5-soft policy.\n\nIf the greediest action is not unique, pick the first one (smallest index).\n\nAll returned probabilities must sum to 1 (within 1 \u00d7 10\u207b\u00b9\u00b2 numerical tolerance) and be rounded to 4 decimal places.\n\nExample:\n    Q   = [1.2, 0.3, 1.2, -0.1]\n    \u0003b5   = 0.1\n    n   = 4\n    greedy index = 0 (first maximum)\n    base = 0.1 / 4 = 0.025\n    output = [0.925, 0.025, 0.025, 0.025]\n# Your code should start with:\n```python\nimport numpy as np\ndef epsilon_soft(Q: list[float] | \"np.ndarray\", epsilon: float) -> list[float]:\n    \"\"\"Return the \u03b5-soft probabilities for a set of Q-values.\n\n    Q is a sequence containing the Q-values for each possible action in a\n    single state.  epsilon (0 \u2264 \u03b5 \u2264 1) is the exploration parameter.\n\n    The returned list must contain the probability of selecting every action\n    under the \u03b5-soft policy, rounded to 4 decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nAll probabilities must be rounded to 4 decimal places and their sum must equal 1 (within 1\u00d710\u207b\u00b9\u00b2).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 86, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Random Forest Majority Vote Aggregator\n# Description:\nIn a Random Forest classifier each decision tree makes its own prediction for every input sample, and the forest\u2019s final prediction is obtained by taking a *majority vote* across all trees.  \n\nWrite a Python function that aggregates these individual predictions.\n\nThe function receives a two-dimensional list (or list of lists) **predictions** where:\n\u2022 Each inner list contains the predictions produced by **one** tree for **all** samples in the data set.  \n\u2022 All inner lists have the same length (equal to the number of samples).  \n\nThe task is to return a single list containing the forest\u2019s final prediction for every sample, obtained as follows:\n1. For every sample (i.e. for every column of the 2-D structure) count how many trees voted for every class label.\n2. Select the class label with the highest vote count.  \n3. If two or more class labels are tied for the highest count, break the tie by choosing the *smallest* label. For numeric labels choose the smaller numeric value, for string labels use standard lexicographic order.\n\nAssume that for any individual sample all votes have the same data type (all numbers or all strings).\n# Your code should start with:\n```python\nfrom collections import Counter\ndef aggregate_random_forest_votes(predictions: list[list[int | float | str]]) -> list:\n    \"\"\"Aggregate individual tree predictions using majority voting.\n\n    Parameters\n    ----------\n    predictions : list[list[int | float | str]]\n        A two-dimensional list where each inner list holds the predictions of a\n        single decision tree for **all** samples. All inner lists have the same\n        length.\n\n    Returns\n    -------\n    list\n        A list with the final prediction for every sample after majority\n        voting. In case of ties the smallest label is chosen.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn the aggregated predictions as a Python list with the same length as the number of samples.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 88, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Softplus Activation Function\n# Description:\nThe **softplus** function is a smooth approximation of the ReLU activation that is widely used in neural-network libraries.  It is defined element-wise as\n\nsoftplus(z)=ln(1+e\u1dbb).\n\nA direct implementation with `np.exp` may overflow for very large positive numbers, while very small negative numbers need to stay accurate.  NumPy provides the numerically stable helper `np.logaddexp(a, b)` which computes ln(e\u1d43+e\u1d47) without overflow.\n\nWrite a Python function that\n1. Accepts a scalar, Python list, or NumPy `ndarray` `z` containing real values.\n2. Returns the element-wise softplus values rounded to **4 decimal places**.\n3. For any array or list input, the result must be converted to a *pure* Python list via NumPy\u2019s `tolist()` method.  For a scalar input, return a single `float`.\n\nYour implementation **must** rely on the numerically stable identity\n\nsoftplus(z)=np.logaddexp(0.0, z).\n\nExample\n-------\nInput\n    z = np.array([-1000, 0, 3])\n\nOutput\n    [0.0, 0.6931, 3.0486]\n\nReasoning\n---------\nsoftplus(\u22121000)\u2248ln(1+e^{-1000})\u22480.0 (underflow to 0 when rounded)\nsoftplus(0)=ln(1+e^{0})=ln(2)=0.693147\u2026\u21920.6931 (rounded)\nsoftplus(3)=ln(1+e^{3})=ln(1+20.085\u2026)\u22483.048587\u2026\u21923.0486 (rounded)\n# Your code should start with:\n```python\nimport numpy as np\ndef softplus(z):\n    \"\"\"Compute the numerically stable softplus activation.\n\n    The softplus function is defined as ln(1 + e**z).  This implementation\n    uses ``numpy.logaddexp`` to avoid overflow/underflow issues.\n\n    Args:\n        z (int | float | list | np.ndarray): Scalar or array-like input.\n\n    Returns:\n        float | list: Softplus value(s) rounded to 4 decimal places. For\n        array-like inputs the returned structure mirrors the input\u2019s shape but\n        is converted to a pure Python ``list``. For scalar inputs a single\n        ``float`` is returned.\n    \"\"\"\n    pass  # your code here\n```\n# Output Constraints:\nRound every value to 4 decimal places.  For array or list inputs, return a Python list (possibly nested) obtained with NumPy\u2019s `tolist()`; for scalar inputs, return a single `float`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 90, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Bandit Policy Mean-Squared Error\n# Description:\nIn a multi-armed bandit problem each arm has an (unknown) expected payout.  \nA policy tries to learn these expectations while interacting with the bandit.\n\nWrite a function `mse` that evaluates how good a policy\u2019s current estimates are by computing the **mean-squared error (MSE)** between\n1. the true expected payouts of every arm (provided by the bandit) and\n2. the policy\u2019s estimates of those expectations.\n\nInput objects\n\u2022 **bandit** \u2013 has a field/entry `arm_evs`, a list/tuple of real numbers.  `arm_evs[i]` is the true expected value of arm *i*.\n\u2022 **policy** \u2013 has a field/entry `ev_estimates`, a dictionary that maps an arm index to the policy\u2019s current estimate of that arm\u2019s expectation.\n\nThe function must\n1. return **`numpy.nan`** if the policy does not contain any estimates (attribute missing or empty dictionary);\n2. otherwise compute the squared error for every arm, average these values, round the result to 4 decimal places and return it.\n\nArm indices in `policy.ev_estimates` can come in any order \u2013 sort them before comparing so that the *i*-th estimate is matched with `arm_evs[i]`.\n\nExample\nbandit = {\"arm_evs\": [0.5, 0.2, 0.9]}\npolicy = {\"ev_estimates\": {0: 0.4, 1: 0.25, 2: 0.8}}\n\nTrue vs. estimated expectations:\narm 0 \u2192 0.5 vs 0.4  \u27f9 (0.4 \u2212 0.5)\u00b2 = 0.01\narm 1 \u2192 0.2 vs 0.25 \u27f9 (0.25 \u2212 0.2)\u00b2 = 0.0025\narm 2 \u2192 0.9 vs 0.8  \u27f9 (0.8 \u2212 0.9)\u00b2 = 0.01\n\nMean-squared error = (0.01 + 0.0025 + 0.01)/3 = 0.0075\n\nHence `mse(bandit, policy)` returns **0.0075**.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\nfrom typing import Any\n\ndef mse(bandit: Any, policy: Any) -> float:\n    \"\"\"Compute the mean-squared error between a policy's estimates and truth.\n\n    Parameters\n    ----------\n    bandit : Any\n        Object or dictionary that stores the true expected payout of each arm\n        under the key/attribute ``arm_evs``.\n    policy : Any\n        Object or dictionary that stores the policy's current estimate of each\n        arm's expectation under the key/attribute ``ev_estimates``. The field\n        must be a dictionary mapping an arm index (int) to a float value.\n\n    Returns\n    -------\n    float\n        The mean-squared error rounded to 4 decimal places. If the policy does\n        not provide any estimates the function returns ``numpy.nan``.\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\nReturn a float rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 96, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Additive-Smoothed N-gram Log-Probability\n# Description:\nImplement an N-gram language-model function that returns the additive-smoothed log\u2013probability of a sentence.\n\nGiven a training corpus (a list of sentences, each sentence is a white-space separated string) and a target sentence, the function must\n1. build all 1-,\u2026,N-gram frequency tables from the corpus,\n2. add an explicit \u201c<UNK>\u201d token to the vocabulary to handle unseen words,\n3. estimate the probability of every contiguous N-gram in the target sentence with additive (a.k.a. Lidstone/Laplace) smoothing\n   P(w_i|context) = (count(context\u25e6w_i)+K) / (count(context)+K\u00b7|V|),\n   where |V| is the vocabulary size **including** \u201c<UNK>\u201d,\n4. return the natural logarithm of the sentence probability (i.e. the sum of log-probabilities of all N-grams) rounded to 4 decimals.\n\nNotes\n\u2022  All words are kept exactly as they appear (no punctuation/stop-word filtering required).\n\u2022  Any word that never occurs in the training corpus is mapped to \u201c<UNK>\u201d.\n\u2022  If the sentence length is smaller than N, no N-grams exist\u037e in that case return 0.0.\n\u2022  Use only the libraries stated in the import section.\n# Your code should start with:\n```python\nimport math\nfrom collections import Counter, defaultdict\ndef additive_ngram_log_prob(corpus: list[str], sequence: str, N: int, K: float = 1.0) -> float:\n    \"\"\"Compute the additive-smoothed log-probability of *sequence* given a corpus.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        Training sentences (white-space separated).\n    sequence : str\n        Sentence whose probability must be evaluated.\n    N : int\n        Order of the N-gram model.\n    K : float, default = 1.0\n        Smoothing constant (Laplace when 1.0).\n\n    Returns\n    -------\n    float\n        Natural logarithm of the sentence probability rounded to 4 decimals.\n    \"\"\"\n    # TODO: implement the function following the description\n    pass\n```\n# Output Constraints:\nReturn the natural logarithm rounded to 4 decimal places as a Python float.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 108, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Regularised Alternating Least Squares Matrix Factorisation\n# Description:\nImplement the regularized Alternating Least Squares (ALS) algorithm to factorize a real-valued matrix.  \nGiven a data matrix X \u2208 \u211d^{N\u00d7M}, the goal is to find two low-rank factor matrices W \u2208 \u211d^{N\u00d7K} and H \u2208 \u211d^{K\u00d7M} that minimise the regularised Frobenius reconstruction loss\n\n\u2016X \u2212 WH\u2016\u00b2_F + \u03b1(\u2016W\u2016\u00b2_F + \u2016H\u2016\u00b2_F),\n\nwhere K is the desired latent rank and \u03b1 \u2265 0 is a Tikhonov (L2) regularisation weight.  \nALS optimises W and H in turn: keeping one fixed while solving a regularised least-squares problem for the other.  \nFor deterministic grading, the factor matrices must be initialised with a fixed random seed (0).  \nYour task is to write a function als_factorization that:\n1. Factorises X with the above objective using ALS.\n2. Stops when either the loss drops below tol or max_iter iterations have been executed.\n3. Returns the reconstructed matrix X\u0302 = WH rounded to 4 decimal places and converted to a standard Python list of lists.\n\nIf the algorithm does not converge within max_iter, simply return the best reconstruction obtained.\n# Your code should start with:\n```python\nimport numpy as np\ndef als_factorization(X: np.ndarray,\n                      K: int,\n                      alpha: float = 1.0,\n                      max_iter: int = 200,\n                      tol: float = 1e-4) -> list[list[float]]:\n    \"\"\"Factorise a real-valued matrix using regularised Alternating Least Squares.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The input matrix of shape (N, M).\n    K : int\n        Target rank (number of latent factors).\n    alpha : float, optional\n        L2 regularisation weight. Default is 1.0.\n    max_iter : int, optional\n        Maximum number of ALS iterations. Default is 200.\n    tol : float, optional\n        Desired value of the regularised loss at which to stop. Default is 1e-4.\n\n    Returns\n    -------\n    list[list[float]]\n        The reconstructed matrix X_hat rounded to 4 decimals.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the reconstructed matrix rounded to the 4th decimal place and cast to a Python list of lists using ndarray.round(4).tolist().\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 109, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: K-Means++ Clustering\n# Description:\nImplement the K-Means clustering algorithm with K-Means++ initialisation.\n\nWrite a function that receives a two-dimensional NumPy array X (shape = m\u00d7n) containing m samples with n features and an integer K representing the desired number of clusters.  \nThe algorithm must:\n1. Set both Python\u2019s `random` and NumPy\u2019s random generator with a provided `random_state` value (so that the results are reproducible).  \n2. Choose the initial centroids with the K-Means++ procedure.  \n3. Perform Lloyd\u2019s iterations (\"assign\u2013update\" steps) until either the cluster assignments stop changing or the maximum number of iterations `max_iter` is reached.  \n4. Return the final centroids **rounded to four decimal places**, sorted in ascending order by their first coordinate (use the complete centroid tuple as a secondary key to break ties).\n\nIf an empty cluster is produced during the update step, keep its centroid unchanged.\n\nExample behaviour (see the worked example below) must be reproduced when `random_state` is set to the same value.\n# Your code should start with:\n```python\nimport numpy as np\nimport random\nimport numpy as np\nimport random\n\ndef kmeans(X: np.ndarray, K: int, max_iter: int = 100, random_state: int | None = None) -> list[list[float]]:\n    \"\"\"Perform K-Means clustering with K-Means++ initialisation.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array with shape (m, n) where *m* is the number of samples and *n* is the\n        number of features.\n    K : int\n        The number of clusters to form.\n    max_iter : int, default = 100\n        Maximum number of iterations for the Lloyd refinement loop.\n    random_state : int | None, default = None\n        Seed for both Python's `random` module and NumPy's RNG to make the result\n        reproducible. If *None*, no seed is set.\n\n    Returns\n    -------\n    list[list[float]]\n        A list of K centroids (each centroid is a list of floats) rounded to 4 decimal\n        places and sorted in ascending order by their first coordinate.\n    \"\"\"\n    # TODO: implement your solution here\n    pass\n```\n# Output Constraints:\nReturn a list of K centroids (each centroid is a list of floats) rounded to the nearest 4th decimal and sorted by their first coordinate.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 111, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Character-Level Sequence Encoding and Decoding\n# Description:\nIn many character-level sequence models we need a small helper routine that\n(1) converts a raw text string into a fixed-length list of integer token\nids, (2) feeds the encoded sequence to a neural model and finally\n(3) converts the model prediction back to a human-readable string.\n\nWrite three helper functions that accomplish exactly this:\n\n1. `string_to_int` \u2013 encodes every character of the input string\n   using a provided vocabulary (``dict[str, int]``). The encoded list\n   must have a **fixed length** ``TIME_STEPS``. If the input is shorter\n   than ``TIME_STEPS`` append the padding token ``0`` on the right; if it\n   is longer, truncate the sequence.\n   Any character that does **not** exist in the vocabulary is also\n   encoded as the padding token ``0``.\n\n2. `int_to_string` \u2013 converts a list/array of integer ids back to text\n   using an *inverse* vocabulary (``dict[int, str]``). Padding tokens\n   (``0``) must be ignored during decoding \u2013 they must **not** appear in\n   the returned string.\n\n3. `run_example` \u2013 puts everything together. It\n   \u2022 encodes the raw text with `string_to_int`,\n   \u2022 calls ``model.predict`` on the encoded batch (batch size 1),\n   \u2022 applies ``argmax`` over the last axis to obtain one predicted id per\n     time step, and finally\n   \u2022 decodes the ids with `int_to_string`.\n\nThe function returns the decoded prediction string.\n\nYou may **only** use NumPy; external libraries such as *TensorFlow* or\n*PyTorch* are **not** allowed.  The constant ``TIME_STEPS`` is fixed to\n``20``.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\nTIME_STEPS = 20  # length of the fixed-size sequence expected by the model\n\ndef string_to_int(text: str, time_steps: int, vocabulary: dict[str, int]) -> list[int]:\n    \"\"\"TODO: implement\"\"\"\n    pass\n\ndef int_to_string(indices, inverse_vocab: dict[int, str]) -> str:\n    \"\"\"TODO: implement\"\"\"\n    pass\n\ndef run_example(model, input_vocabulary: dict[str, int], inv_output_vocabulary: dict[int, str], text: str) -> str:\n    \"\"\"TODO: implement\"\"\"\n    pass\n```\n# Output Constraints:\nReturn the decoded string predicted by the model. Padding tokens (id 0) must be omitted.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 113, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Batch Example Runner\n# Description:\nIn many sequence-to-sequence or language\u2013generation projects you often want to try a trained model on several input strings and very quickly look at the produced predictions.  \n\nWrite a helper function `run_examples` that automates this small piece of workflow.  \n\nThe function receives four arguments:\n1. `model` \u2013 **a callable** that takes one string and returns another string (the model\u2019s prediction).  \n2. `input_vocabulary` \u2013 a dictionary that maps characters to integer indices (kept only for API compatibility; it is **not** used inside this utility).  \n3. `inv_output_vocabulary` \u2013 the inverse mapping from indices to characters (also kept for API compatibility; it is **not** used here either).  \n4. `examples` \u2013 an iterable of input strings.  If this argument is omitted the function must fall back to a global constant `EXAMPLES` that is assumed to exist in the user\u2019s environment.\n\nFor every example in `examples` the function must\n\u2022 call another helper `run_example(model, input_vocabulary, inv_output_vocabulary, example)` that is expected to return a list of characters representing the model\u2019s output,\n\u2022 concatenate the returned characters into a single string,  \n\u2022 print the pair\n```\ninput:  <the original string>\noutput: <the predicted string>\n```\n\u2022 collect the predicted string in a list.\n\nFinally, the list of all predictions (in the same order as the inputs) must be returned.\n\nYou do **not** have to implement `run_example`; you only have to rely on it being available in the runtime.\n# Your code should start with:\n```python\ndef run_examples(model, input_vocabulary, inv_output_vocabulary, examples):\n    \"\"\"Run a prediction model on multiple examples and collect its outputs.\n\n    Parameters\n    ----------\n    model : callable\n        A function that receives a single input string and returns the\n        corresponding predicted string.\n    input_vocabulary : dict\n        Mapping from characters to integer indices.  Provided only for API\n        compatibility \u2013 *run_examples* does not need it.\n    inv_output_vocabulary : dict\n        Mapping from integer indices back to characters.  Also unused inside\n        this helper but kept for API compatibility.\n    examples : iterable[str]\n        A collection of input strings.  If *None*, the function should use the\n        global constant `EXAMPLES`.\n\n    Returns\n    -------\n    list[str]\n        The list of model predictions, one for each input example, in the same\n        order.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a list containing the predicted strings in the same order as the supplied examples.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 115, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Binary Cross-Entropy Loss & Gradient for Logistic Regression\n# Description:\nImplement a utility function that, given a feature matrix X, a binary target vector y and a weight vector w, computes both the average binary cross-entropy loss and its gradient with respect to w for logistic regression.\n\nFor a sample (x, y) the logistic model predicts the probability that the sample belongs to the positive class (y = 1) as\n\n    p = \u03c3(z) = 1 / (1 + e^(\u2013z)),   where z = x \u00b7 w.\n\nThe average binary cross-entropy loss over the whole dataset is\n\n    J(w) = \u2013 1/m \u00b7 \u03a3 [ y \u00b7 ln(p) + (1 \u2013 y) \u00b7 ln(1 \u2013 p) ],\n\nwhere m is the number of samples.  The gradient of the loss with respect to the weights is\n\n    \u2207J(w) = 1/m \u00b7 X\u1d40 (p \u2013 y).\n\nYour task is to write a function logistic_loss_and_gradient that returns\n1. the loss rounded to 4 decimals and\n2. the gradient rounded to 4 decimals and converted to a (nested) Python list via ndarray.tolist().\n\nIf any predicted probability becomes exactly 0 or 1, replace it by a small constant \u03b5 = 1e-20 before using it inside the logarithm to avoid numerical issues.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef logistic_loss_and_gradient(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, list[list[float]]]:\n    \"\"\"Compute binary cross-entropy loss and its gradient for logistic regression.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (m, n).\n        y (np.ndarray): Binary target vector of shape (m,) or (m, 1).\n        w (np.ndarray): Weight vector of shape (n,) or (n, 1).\n\n    Returns:\n        tuple: A tuple containing\n            1. The average cross-entropy loss rounded to 4 decimals (float).\n            2. The gradient of the loss with respect to the weights rounded to 4 decimals and\n               converted to a (nested) Python list via ``tolist()``.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturn a tuple where\n1. the first element is the loss rounded to the 4th decimal place (type float)\n2. the second element is the gradient rounded to the 4th decimal place and converted to a (nested) Python list, e.g. [[0.1234], [-0.5678]]\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 118, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost with Decision Stumps\n# Description:\nImplement the AdaBoost (Adaptive Boosting) algorithm **from scratch** using decision stumps (one\u2013level decision trees) as weak learners.  \nThe function must:\n1. Take a training set `(X_train, y_train)` where `X_train` is a 2-D NumPy array of shape `(m, n)` and `y_train` is a 1-D NumPy array of length `m` whose elements are **only** `-1` or `1`.\n2. Re-weight training examples iteratively and build `n_clf` decision stumps, each time choosing the stump that minimises the weighted classification error.\n3. Store each stump\u2019s weight (often denoted as $\\alpha_t$) computed as  \n$\\alpha_t = \\frac12 \\ln\\!\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$, where $\\varepsilon_t$ is the stump\u2019s weighted error.\n4. For every sample in `X_test` aggregate all stump votes by the sign of the weighted sum $\\sum_{t=1}^{n_{clf}} \\alpha_t h_t(\\mathbf x)$ and output `-1` or `1` accordingly.\n\nReturn a Python **list** of predicted labels for the given `X_test`.  \nIf `n_clf` is smaller than 1, treat it as 1.\n# Your code should start with:\n```python\nimport numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list with each element being either -1 or 1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 128, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Multi-class Linear Discriminant Analysis (LDA) Transformation\n# Description:\nImplement the classical Fisher\u2019s Linear Discriminant Analysis (LDA) for the multi-class case.  \nThe goal is to find a linear projection that maximises the between\u2013class scatter while simultaneously minimising the within\u2013class scatter.  \nGiven a data matrix X\u2208\u211d^{m\u00d7d} (m samples, d features) and the corresponding label vector y, compute the projection matrix W whose columns are the eigenvectors corresponding to the largest eigen-values of S_W^{-1}S_B ( S_W \u2013 within-class scatter, S_B \u2013 between-class scatter).  \nReturn the data projected on the first ``n_components`` discriminant directions.\n\nSpecifically you must:\n1. Compute the within-class scatter matrix   S_W = \u03a3_c \u03a3_{x\u2208c} (x\u2212\u03bc_c)(x\u2212\u03bc_c)^T.\n2. Compute the between-class scatter matrix  S_B = \u03a3_c N_c (\u03bc_c\u2212\u03bc)(\u03bc_c\u2212\u03bc)^T, where \u03bc is the global mean and N_c the number of samples in class c.\n3. Form the matrix A = pinv(S_W)\u00b7S_B (use the Moore\u2013Penrose pseudo-inverse to stay numerically stable when S_W is singular).\n4. Perform eigen-decomposition of A (use ``numpy.linalg.eigh`` because A is symmetric) and sort the eigen-pairs in descending order of the eigen-values.\n5. (Deterministic sign) For every chosen eigenvector flip its sign if the first non-zero element is negative.  \n   This removes the sign ambiguity and makes the results deterministic across different machines.\n6. Project X on the first ``n_components`` eigenvectors and round every element to four decimal places.\n7. Return the projected data as a Python *list of lists* obtained via ``ndarray.tolist()``.\n\nIf ``n_components`` equals the number of original features, the full projection is returned.  \nAll inputs are assumed to be valid.\n\nIn case you could not compute any eigen-vector (e.g. ``n_components`` is 0) return an empty list.\n\nExample\n-------\nInput\nX = np.array([[1,1],[1,2],[2,1],[2,2],[8,8],[9,8],[8,9],[9,9]])\ny = np.array([0,0,0,0,1,1,1,1])\nn_components = 1\n\nOutput\n[[1.4142], [2.1213], [2.1213], [2.8284], [11.3137], [12.0208], [12.0208], [12.7279]]\n\nReasoning\nThe algorithm first builds the scatter matrices, then solves the generalised eigen-value problem S_W^{-1}S_B w = \u03bb w.  \nThe dominant eigen-vector (after the deterministic sign fix) is [0.7071, 0.7071].  \nProjecting every sample on this vector and rounding to four decimals gives the shown result.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef multi_class_lda(X: np.ndarray, y: np.ndarray, n_components: int) -> list[list[float]]:\n    \"\"\"Perform multi-class Linear Discriminant Analysis and project the data.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Two-dimensional array of shape (n_samples, n_features) containing the\n        input data.\n    y : np.ndarray\n        One-dimensional array of shape (n_samples,) containing the integer\n        class labels.\n    n_components : int\n        The number of discriminant components to keep (must be between 1 and\n        ``n_features``).\n\n    Returns\n    -------\n    list[list[float]]\n        The data projected onto the first ``n_components`` LDA directions. Each\n        inner list corresponds to one sample. All values are rounded to four\n        decimal places.\n    \"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nEvery element of the returned list must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 140, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Escape from Fire Maze\n# Description:\nYou are given an \\(n\\times n\\) maze.  Each row of the maze is represented by a string that only contains the following characters:\n\n* \".\" \u2014 free cell, both the agent and fire can enter it;\n* \"#\" \u2014 wall (obstacle), neither the agent nor the fire can enter it;\n* \"F\" \u2014 a cell that is on fire at time \\(t = 0\\).\n\nThe agent starts in the upper-left corner, i.e. at cell \\((0,0)\\), and wants to reach the lower-right corner \\((n-1,n-1)\\).  Both the agent and the fire can move to the four neighbouring cells (up, down, left, right) in one minute.  The fire spreads first, **then** the agent moves.  The agent may enter a free cell only if that cell is **not** on fire at the moment the agent arrives. \n\nYour task is to write a function that returns the minimum number of minutes the agent needs to reach the goal while staying safe.  If it is impossible to reach the goal, return **-1**.\n\nImportant notes\n1. The start or the goal cell may already be on fire \u2013 in that case the answer is immediately **-1**.\n2. If the maze has size 1\u00d71 and the single cell is \"\\.\" the answer is **0** (the agent is already at the goal).\n3. There can be several initial fire sources \u2013 every cell marked with \"F\" burns at time 0.\n# Your code should start with:\n```python\nfrom collections import deque\nimport math\nfrom collections import deque\nimport math\n\ndef escape_fire_maze(grid: list[str]) -> int:\n    \"\"\"Escape from a maze with spreading fire.\n\n    Parameters\n    ----------\n    grid : list[str]\n        Square maze represented as a list of strings. Each character must be\n        '.', '#', or 'F'.  The agent starts at the upper-left corner (0,0) and\n        wishes to reach the lower-right corner (n-1,n-1).  Fire starts in every\n        cell marked with 'F' and spreads to the four neighbouring cells every\n        minute.  The fire spreads first, then the agent moves.\n\n    Returns\n    -------\n    int\n        Minimum number of minutes required for the agent to reach the goal\n        without entering a burning cell, or -1 if this is impossible.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn an integer \u2013 the minimum number of moves (minutes) needed to reach the goal, or -1 if it is impossible.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 141, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: k-Nearest Neighbors Classifier\n# Description:\nImplement the **k-Nearest Neighbors (k-NN) classifier** as a single function.  \nGiven a labelled training set `(X_train, y_train)` and an unlabeled test set `X_test`, the function must:\n1. Compute the distance between each test sample and every training sample using one of the three metrics:\n   \u2022 `'euclidean'`   \u2013 \u2113\u2082 distance  \n   \u2022 `'manhattan'`   \u2013 \u2113\u2081 distance  \n   \u2022 `'cosine'`      \u2013 cosine distance ( 1 \u2212 cosine-similarity )\n2. For every test sample find the *k* training samples with the smallest distance.\n3. Predict the class by majority vote among those k neighbours. In case of a tie return the smallest label value.\n4. If an unknown metric string is supplied, fall back to the Euclidean metric.\n\nReturn a **1-D NumPy array** of the predicted labels.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef knn_predict(\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_test: np.ndarray,\n        k: int,\n        metric: str = 'euclidean') -> np.ndarray:\n    \"\"\"Implement your code here.\"\"\"\n```\n# Output Constraints:\nReturn a 1-D NumPy array containing the predicted labels.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 146, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: k-Nearest Neighbours (k-NN) Classifier\n# Description:\nImplement the classic k-Nearest Neighbours (k-NN) classifier from scratch.  \nYour function must accept a training set (features and labels), a test set, a neighbourhood size k, and one of three distance metrics \u2013  \n\u2022 **euclidean**: $\\sqrt{\\sum_i (x_i-\\hat x_i)^2}$  \n\u2022 **manhattan**: $\\sum_i |x_i-\\hat x_i|$  \n\u2022 **cosine**: $1-\\dfrac{\\mathbf x\\cdot \\hat{\\mathbf x}}{\\|\\mathbf x\\|\\,\\|\\hat{\\mathbf x}\\|}$ (use an $\\varepsilon=10^{-12}$ to avoid division by zero).\n\nFor every test sample you must  \n1. compute its distance to every training sample with the chosen metric,  \n2. pick the *k* closest neighbours (if *k* exceeds the number of training samples, use all samples),  \n3. perform a majority vote on their labels (in case of a tie return the **smallest** label),  \n4. return the predicted labels for all test samples.\n\nDo **not** use any third-party machine-learning libraries such as *scikit-learn* \u2013 only basic packages like **NumPy** are allowed.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef knn_predict(X: np.ndarray,\n                y: np.ndarray,\n                X_test: np.ndarray,\n                k: int = 3,\n                metric: str = 'euclidean') -> list:\n    \"\"\"Predict labels for *X_test* using the k-Nearest Neighbours algorithm.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training features.\n        y: 1-D NumPy array of length *n_samples* containing the training labels.\n        X_test: 2-D NumPy array of shape (m_samples, n_features) with the test\n                 samples whose labels are to be predicted.\n        k: Number of neighbours to consider (default: 3).  If *k* exceeds the\n           number of training samples, use all samples instead.\n        metric: Distance metric to use \u2013 'euclidean', 'manhattan', or 'cosine'.\n\n    Returns:\n        A Python list containing the predicted label for each test sample, in\n        the same order as *X_test*.\n    \"\"\"\n    # TODO: complete the implementation\n    pass\n```\n# Output Constraints:\nReturn a plain Python list obtained via NumPy\u2019s `.tolist()` method; the list length must equal the number of test samples.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 155, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Forward Pass of an Actor Network\n# Description:\nIn many reinforcement-learning (RL) algorithms an *actor* network converts an environment state into an action.  A very common architecture is a fully-connected network with two hidden layers followed by a tanh output layer (tanh keeps the actions inside the range [\u22121,1]).  In this exercise you will write the **forward pass** of such a network using nothing more than NumPy.\n\nThe network topology is\nstate  \u2192  Linear(W1,b1) \u2192 ReLU \u2192 Linear(W2,b2) \u2192 ReLU \u2192 Linear(W3,b3) \u2192 tanh \u2192  action\n\nThe parameters (weight matrices and bias vectors) are supplied through a dictionary that has the following keys:\n\u2022  \"W1\" \u2013 first-layer weight matrix of shape (state_dim, hidden1)\n\u2022  \"b1\" \u2013 first-layer bias vector of shape (hidden1,)\n\u2022  \"W2\" \u2013 second-layer weight matrix of shape (hidden1, hidden2)\n\u2022  \"b2\" \u2013 second-layer bias vector of shape (hidden2,)\n\u2022  \"W3\" \u2013 output-layer weight matrix of shape (hidden2, action_dim)\n\u2022  \"b3\" \u2013 output-layer bias vector of shape (action_dim,)\n\nYour task is to implement a function that\n1.  accepts a one-dimensional state vector and the parameter dictionary,\n2.  performs the three affine transformations and the two nonlinearities (ReLU and tanh),\n3.  returns the resulting action vector as a Python list rounded to **four decimal places**.\n\nIf the input dimensions do not agree with the provided weight shapes, simply let NumPy raise an error (no explicit error handling is required).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef actor_forward(state, weights: dict) -> list[float]:\n    \"\"\"Perform the forward pass of a two-hidden-layer actor network.\n\n    The network architecture is:  Linear \u2192 ReLU \u2192 Linear \u2192 ReLU \u2192 Linear \u2192 tanh.\n\n    Args:\n        state (list[float] | np.ndarray): 1-D vector representing the state.\n        weights (dict): Dictionary with NumPy arrays under the keys\n            'W1', 'b1', 'W2', 'b2', 'W3', 'b3'.\n\n    Returns:\n        list[float]: Action vector (each component rounded to 4 decimals).\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\nMake sure all results are rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 160, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Lasso Regression via Coordinate Descent\n# Description:\nImplement **Lasso regression** (L1-regularised linear regression) using the **coordinate\u2013descent** optimisation strategy.\n\nGiven a design matrix $X\\;(m\\times n)$ and a target vector $\\mathbf y\\;(m)$, Lasso learns weight vector $\\mathbf w$ and optional intercept $b$ that minimise\n\\[\n\\frac1m\\sum_{i=1}^{m}\\bigl(y_i-(b+\\mathbf w^\\top\\mathbf x_i)\\bigr)^2+\\lambda\\,\\|\\mathbf w\\|_1,\n\\]\nwhere $\\|\\mathbf w\\|_1$ is the L1-norm and $\\lambda\\ge 0$ the regularisation strength.\n\nUse the following steps.\n1. If `fit_intercept=True` add an all-ones column to $X$; otherwise add an all-zeros column so that the first coordinate is always the intercept and is **not** included in the L1 penalty.\n2. Initialise all parameters to zero and, if an intercept is fitted, recompute it in every outer loop as the mean residual.\n3. For `max_iters` iterations repeat a **coordinate loop** over every weight (excluding the intercept):  \n   \u2022 Temporarily set the current weight to 0,  \n   \u2022 compute the partial residual $r_j=y-Xw_{\\neg j}$,  \n   \u2022 update weight $w_j$ with the *soft-thresholding* operator\n     \\[ w_j\\leftarrow S\\!\\bigl(\\langle x_j,r_j\\rangle,\\;\\lambda m\\bigr)\\;/\\;\\sum_i x_{ij}^2 \\]\n     where\n     \\[ S(a,\\tau)=\\text{sign}(a)\\cdot\\max(|a|-\\tau,0). \\]\n4. After finishing all iterations, return the final intercept and weight vector.\n\nIf the algorithm converges correctly the resulting model should give a low mean-squared error on the provided data.\n\nThe function must **only use NumPy** (no scikit-learn or other libraries).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef fit_lasso(X: np.ndarray,\n              y: np.ndarray,\n              lambda_param: float = 1.0,\n              max_iters: int = 100,\n              fit_intercept: bool = True) -> tuple[list[float], float]:\n    \"\"\"Fit Lasso (L1-regularised) linear regression using coordinate descent.\n\n    The function should learn a weight vector and optional intercept that\n    minimise squared loss + \u03bb\u2006\u00b7\u2006L1-norm.  **Do not** use scikit-learn; rely\n    solely on NumPy and the algorithm described in the task description.\n\n    Args:\n        X:   2-D array of shape (m, n) \u2013 feature matrix.\n        y:   1-D array of length m \u2013 target values.\n        lambda_param: Regularisation strength \u03bb (non-negative).\n        max_iters: Number of full passes over the coordinates.\n        fit_intercept: Whether to fit an intercept term.\n\n    Returns:\n        Tuple (weights, bias) where `weights` is a list of length n and `bias`\n        is a float.  Round all returned numbers to 4 decimal places.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a tuple (**weights_list, bias_float**) where weights_list is rounded to 4 decimals.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 165, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Normalized Hamming Distance\n# Description:\nHamming distance is a simple yet powerful distance metric that counts how many positions two equal-length sequences differ. It is widely used in information theory, coding theory, and many data-science applications (e.g., comparing binary hash codes).\n\nWrite a Python function that computes the normalized Hamming distance between two 1-D integer vectors.  The distance is defined as\n\n    d(x, y) = (1 / N) * \u03a3 \ud835\udfd9[x_i \u2260 y_i]\n\nwhere N is the common length of the vectors, and \ud835\udfd9 is the indicator function that equals 1 when the two elements differ and 0 otherwise.\n\nThe function must satisfy the following requirements:\n1. Accept either Python lists or NumPy `ndarray`s containing integers.\n2. If the two vectors have different lengths, immediately return **-1**.\n3. Otherwise, return the Hamming distance rounded to **4 decimal places**.\n\nIn cases where all corresponding elements are identical, the distance is 0. When all elements differ, the distance is 1.\n# Your code should start with:\n```python\nimport numpy as np\ndef hamming_distance(x: list[int] | \"np.ndarray\", y: list[int] | \"np.ndarray\") -> float | int:\n    \"\"\"Compute the normalized Hamming distance between two equal-length integer vectors.\n\n    The distance is the proportion of indices at which the corresponding\n    elements are different. If the two vectors have unequal length, the\n    function must return \u20111.\n\n    Args:\n        x: A 1-D Python list or NumPy array of integers.\n        y: A 1-D Python list or NumPy array of integers.\n\n    Returns:\n        A float rounded to 4 decimal places representing the Hamming distance,\n        or \u20111 if the inputs have different lengths.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a float rounded to 4 decimal places. If the input vectors do not share the same length return -1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 169, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Window Function Generator\n# Description:\nIn digital signal-processing many algorithms start by tapering the input data with a window function.  Implement a utility that can directly generate the most common symmetric windows.\n\nWrite a function `generate_window` that creates a list containing **N** window coefficients for one of the following window types\n\n\u2022 \"hamming\" \u2003(Hamming window)\n\u2022 \"hann\" \u2003\u2003 (Hann window)\n\u2022 \"blackman_harris\" (4-term Blackman\u2013Harris window)\n\u2022 \"generalized_cosine\" (arbitrary even cosine series)\n\nThe mathematical definitions are\n\nHamming              :  w[n] = 0.54 \u2212 0.46 cos(2\u03c0n/(N\u22121))\nHann                 :  w[n] = 0.5  \u2212 0.5  cos(2\u03c0n/(N\u22121))\nBlackman\u2013Harris :  w[n] = a\u2080 \u2212 a\u2081 cos(2\u03c0n/(N\u22121)) + a\u2082 cos(4\u03c0n/(N\u22121)) \u2212 a\u2083 cos(6\u03c0n/(N\u22121))\n                        with a\u2080 = 0.35875, a\u2081 = 0.48829, a\u2082 = 0.14128, a\u2083 = 0.01168\nGeneralized cosine :  w[n] = \u2211\u2096 a\u2096 cos(2\u03c0kn/(N\u22121)),  where the list *coefficients* supplies a\u2096.\n\nIf `window == \"generalized_cosine\"` the caller **must** supply the list `coefficients` that contains the series coefficients a\u2080 \u2026 a_M. For all other window types that argument is ignored.\n\nSpecial cases\n1. N must be a positive integer; otherwise raise `ValueError`.\n2. For N = 1 every window reduces to a single value 1.0 (the conventional definition for one-sample windows).\n3. If an unknown window name is passed raise `ValueError`.\n\nAll coefficients have to be rounded to **4 decimal places** and the function must return a Python `list` (not a NumPy array).\n# Your code should start with:\n```python\nimport numpy as np\ndef generate_window(window: str, N: int, coefficients: list[float] | None = None) -> list[float]:\n    \"\"\"Generate coefficients for several common symmetric window functions.\n\n    Parameters\n    ----------\n    window : str\n        Name of the desired window. Supported values are\n        \"hamming\", \"hann\", \"blackman_harris\" and \"generalized_cosine\".\n    N : int\n        Number of coefficients to generate. Must be a positive integer.\n    coefficients : list[float] | None, optional\n        List of cosine\u2013series coefficients used **only** when\n        window == \"generalized_cosine\". The default is ``None``.\n\n    Returns\n    -------\n    list[float]\n        List with *N* floats rounded to 4 decimal places.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nRound every coefficient to the nearest 4th decimal before returning and return a regular Python list.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 171, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Binary Logistic Regression \u2013 Mini-Batch Gradient Descent\n# Description:\nImplement a binary Logistic Regression classifier **from scratch** using mini-batch Gradient Descent.  \nThe function must:  \n1. Accept a training set `(X_train, y_train)` and a test set `X_test`.  \n2. Automatically add an intercept term (bias) to the data.  \n3. Work with any two distinct numeric labels (e.g. `{-1,1}`, `{0,1}`, `{3,7}`); internally map them to `{0,1}` and map predictions back to the original labels before returning.  \n4. Train the weight vector by minimizing the negative log-likelihood (cross-entropy) loss with mini-batch Gradient Descent.  \n5. Return a Python `list` with the predicted labels (same label set as `y_train`) for every sample in `X_test` using a decision threshold of **0.5** on the estimated probability of the positive class.  \n6. Handle the special case where all training labels are identical by skipping training and simply predicting that unique label for every test sample.  \n\nIf the algorithm is implemented correctly it will separate linearly-separable data and give reasonable predictions on simple toy problems.\n# Your code should start with:\n```python\nimport numpy as np\ndef logistic_regression_train_predict(\n    X_train: list[list[float]],\n    y_train: list[int],\n    X_test: list[list[float]],\n    epochs: int = 5000,\n    learning_rate: float = 0.1,\n    batch_size: int = 32,\n) -> list[int]:\n    \"\"\"Train a binary Logistic Regression classifier using mini-batch Gradient\n    Descent and return predictions for the provided test set.\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature matrix where each inner list corresponds to one sample.\n    y_train : list[int]\n        Training labels \u2013 exactly two distinct numeric values are required.\n    X_test  : list[list[float]]\n        Feature matrix for which predictions are requested.\n    epochs : int, default 5000\n        Number of passes over the training data during optimization.\n    learning_rate : float, default 0.1\n        Step size used in Gradient Descent updates.\n    batch_size : int, default 32\n        Number of samples per mini-batch.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels for each sample in `X_test`, expressed in the same\n        value set that appears in `y_train`.\n    \"\"\"\n    pass  # implement your solution here\n```\n# Output Constraints:\nReturn a Python list of integers having the same two distinct values that appear in `y_train`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 176, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost with Decision Stumps From Scratch\n# Description:\nImplement the AdaBoost ensemble algorithm from scratch using decision stumps (one-level decision trees) as weak learners.\n\nThe function must:\n1. Accept a training set (feature matrix X_train and label vector y_train) whose labels are \u201c0\u201d for the negative class and \u201c1\u201d for the positive class.\n2. Train *n_estimators* decision stumps, updating the sample weights after every round according to AdaBoost (Freund & Schapire, 1997).\n3. Produce predictions for an arbitrary test-set X_test by aggregating the weak learners\u2019 weighted votes and converting the aggregated sign back to class labels {0,1}.\n\nA decision stump is defined by\n\u2022 feature_index \u2013 which column is used,\n\u2022 threshold \u2013 the cut value,\n\u2022 polarity \u2013 whether the class *1* is predicted for values **smaller** than the threshold (polarity = 1) or for values **greater or equal** to the threshold (polarity = \u20131).\n\nIn every boosting round the stump with the smallest *weighted* classification error must be selected (ties are broken by the smallest feature index, then the smallest threshold, then polarity 1 before \u20131, in order to keep the behaviour deterministic).  \nIf a perfect stump is found (weighted error = 0) the training may stop early.\n\nReturn the predictions for *X_test* as a plain Python list of integers (0 or 1).\n\nHint: use the standard AdaBoost weight and vote update rules:\n    error_t      = \u03a3_i w_i * [y_i \u2260 h_t(x_i)]\n    \u03b1_t          = \u00bd \u00b7 ln((1 \u2013 error_t) / (error_t + 1e-10))\n    w_i \u2190 w_i \u00b7 exp(\u2013\u03b1_t \u00b7 y_i \u00b7 h_t(x_i))  (with y_i, h_t(x_i) \u2208 {\u20131,1})\n    normalise w so that \u03a3 w_i = 1\n# Your code should start with:\n```python\nimport numpy as np\ndef adaboost_predict(X_train: list[list[int | float]],\n                     y_train: list[int],\n                     X_test: list[list[int | float]],\n                     n_estimators: int = 10) -> list[int]:\n    \"\"\"Fill in here. The final implementation must follow the specification given in the task\n    description and return a list with the predicted class labels for *X_test*.\"\"\"\n```\n# Output Constraints:\nReturn a Python list of integers \u2013 *not* a NumPy array.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 178, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Particle Swarm Optimisation of the Sphere Function\n# Description:\nImplement Particle Swarm Optimization (PSO) from scratch to minimise the Sphere function  \n\n        f(\\mathbf x) = \\sum_{i=1}^{n} x_i^2,\\qquad -1 \\le x_i \\le 1.\n\nThe algorithm keeps a swarm of particles, each with a position \\(\\mathbf x\\), a velocity \\(\\mathbf v\\), its own best known position (personal best) and the globally best known position (global best).  \nAt every iteration the velocity and the position of every particle are updated as follows  \n\n        v \\leftarrow w v + c_1 r_1 (p_{best} - x) + c_2 r_2 (g_{best} - x)  \n        x \\leftarrow \\operatorname{clip}(x + v,\\; \\text{lower\\_bound},\\; \\text{upper\\_bound})\n\nwhere  \n\u2022 *w*  \u2013 inertia weight (0.5)  \n\u2022 *c\u2081* \u2013 cognitive weight (1.5)  \n\u2022 *c\u2082* \u2013 social weight   (1.5)  \n\u2022 *r\u2081,r\u2082* \u2013 independent uniform random numbers in \\([0,1]\\).  \n\nThe function must be fully deterministic with respect to *seed*; use `numpy.random.default_rng(seed)` for all random numbers.\n\nArguments\nn_dims          (int)   \u2013 dimensionality of the search space (>0)  \nnum_particles   (int)   \u2013 size of the swarm  (>0)  \nnum_iterations  (int)   \u2013 optimisation steps   (>0)  \nseed            (int)   \u2013 RNG seed (default 1)\n\nReturn value  \nThe best Sphere-function value encountered, rounded to four decimals.\n\nIf any argument is non-positive, return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef particle_swarm_optimisation(n_dims: int,\n                                num_particles: int,\n                                num_iterations: int,\n                                seed: int = 1) -> float:\n    \"\"\"Minimises the n-dimensional Sphere function using Particle Swarm Optimisation.\n\n    Args:\n        n_dims: Dimensionality of the search space (>0).\n        num_particles: Number of particles in the swarm (>0).\n        num_iterations: Number of optimisation iterations (>0).\n        seed: Random-number-generator seed for reproducibility.\n\n    Returns:\n        The best objective value found, rounded to four decimals, or -1 on\n        invalid input.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the best value rounded to the nearest 4th decimal.   \nReturn -1 if *n_dims*, *num_particles* or *num_iterations* are not positive integers.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 180, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: k-Nearest Neighbours Predictor\n# Description:\nImplement a pure-function version of the k-Nearest Neighbours (k-NN) algorithm that can work both as a classifier (majority vote) and as a regressor (average). The function receives a training feature matrix, the corresponding target vector, a test feature matrix, the integer k and a string that specifies the task type (\"classification\" or \"regression\").\n\nRules & details\n1. Use the Euclidean distance.\n2. If k is 0 or larger than the number of training samples, use all training samples.\n3. For classification, return the most frequent label among the k neighbours. In case of a tie, return the smallest label according to standard Python ordering (this works for both numeric and string labels).\n4. For regression, return the arithmetic mean of the neighbours\u2019 target values rounded to 4 decimal places.\n5. Preserve the order of the test samples when producing the output.\n# Your code should start with:\n```python\nimport numpy as np\nfrom collections import Counter\ndef knn_predict(X_train: list[list[float]],\n                y_train: list,\n                X_test: list[list[float]],\n                k: int = 5,\n                task: str = \"classification\") -> list:\n    \"\"\"k-Nearest Neighbours prediction (classification or regression).\n\n    Parameters\n    ----------\n    X_train : list[list[float]]\n        Training feature vectors.\n    y_train : list\n        Targets associated with *X_train*.\n    X_test : list[list[float]]\n        Feature vectors for which predictions are requested.\n    k : int, default 5\n        Number of neighbours to consider; if 0 uses every training sample.\n    task : str, default \"classification\"\n        Either \"classification\" for majority voting or \"regression\" for\n        numeric averaging.\n\n    Returns\n    -------\n    list\n        Predicted labels/values for every vector in *X_test*.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nFor regression, each predicted numeric value must be rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 184, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: CART Decision Tree Classifier (from Scratch)\n# Description:\nImplement a binary decision-tree classifier (CART algorithm) **from scratch**, using Gini impurity and recursive binary splitting.  \nThe function receives three NumPy arrays:\n1. `X_train` \u2013 shape `(n_samples, n_features)` containing the training features (real numbers).\n2. `y_train` \u2013 shape `(n_samples,)` containing integer class labels starting from **0**.\n3. `X_test`  \u2013 shape `(m_samples, n_features)` containing the unseen samples.\n\nYour task is to build a decision tree on `(X_train, y_train)` and return the predicted class label for every row in `X_test`.\n\nStopping criteria:\n\u2022 If all labels at a node are identical \u2013 create a leaf with that label.  \n\u2022 If `max_depth` is reached (when supplied, otherwise unlimited) \u2013 create a leaf with the majority class of the node.  \n\u2022 If a split produces an empty child \u2013 also create a leaf with the majority class.\n\nSplitting rule:\n\u2022 For each feature sort the samples, evaluate every mid-point lying between two consecutive, different feature values.  \n\u2022 Pick the (feature, threshold) pair with the largest Gini-gain (parent Gini \u2013 weighted children Gini).  \n\u2022 In case no gain can be achieved return a leaf with the majority class.\n\nThe final model must make deterministic predictions purely based on the above rules (no randomness).\n# Your code should start with:\n```python\nimport numpy as np\ndef decision_tree_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    max_depth: int | None = None,\n) -> list[int]:\n    \"\"\"Build a CART decision tree on (X_train, y_train) and predict labels for X_test.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        Training feature matrix of shape (n_samples, n_features).\n    y_train : np.ndarray\n        Integer class labels for the training data, shape (n_samples,).\n    X_test : np.ndarray\n        Feature matrix to classify, shape (m_samples, n_features).\n    max_depth : int | None, optional\n        Maximum allowed depth of the tree. If None the depth is unlimited.\n\n    Returns\n    -------\n    list[int]\n        Predicted class label for each row in X_test.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a **Python list** of integers, one label per test sample.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 190, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Best Gini Split Finder\n# Description:\nThe Gini impurity is one of the most popular criteria for choosing a split in a decision\u2013tree classifier.  \n\nWrite a Python function that, given a numerical feature matrix X (shape n_samples \u00d7 n_features) and the corresponding class labels y, finds the **single best binary split** of the data that minimises the weighted Gini impurity.\n\nFor every feature `j` and every unique value `v` appearing in that feature, form the split\n```\nleft  =  samples with X[i, j] \u2264 v\nright =  samples with X[i, j] > v\n```\nSkip a candidate split if either child node is empty.  Compute the weighted Gini impurity\n```\nG_split = (n_left / n_total) * G(left) + (n_right / n_total) * G(right)\n```\nwhere\n```\nG(node) = 1 \u2212 \u03a3_k p_k\u00b2\n```\nand `p_k` is the proportion of class *k* in the node.\n\nReturn a three-tuple\n```\n(best_feature_index, best_threshold_value, best_gini)\n```\ncontaining the index (0-based) of the feature that yields the minimum `G_split`, the corresponding threshold value `v`, and the split\u2019s Gini impurity rounded to **4 decimal places**.\n\nTie-breaking rules\n1. Prefer the split with the strictly smaller `G_split`.\n2. If the impurities are equal (difference < 1e-12), choose the smaller feature index.\n3. If the feature index is also equal, choose the smaller threshold value.\n\nIf no valid split exists (e.g. every feature takes a constant value or all labels belong to one class) return\n```\n(-1, None, round(G_whole_dataset, 4))\n```\nwhere `G_whole_dataset` is the Gini impurity of the whole, unsplit data.\n# Your code should start with:\n```python\nimport numpy as np\ndef best_gini_split(X, y):\n    \"\"\"Find the best feature index and threshold that minimise the weighted Gini impurity.\n\n    Parameters\n    ----------\n    X : list[list[float]] or numpy.ndarray\n        A 2-D structure where each inner list/row contains the numerical\n        feature values of one sample.\n    y : list[int] or numpy.ndarray\n        A 1-D structure containing the class labels corresponding to the rows\n        of X.\n\n    Returns\n    -------\n    tuple\n        A tuple `(best_feature_index, best_threshold_value, best_gini)` where\n        `best_gini` is rounded to 4 decimal places.  If no valid split exists\n        the function must return `(-1, None, round(G_whole_dataset, 4))`.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn exactly a tuple `(feature_index, threshold, gini)` with `gini` rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 191, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Distance Metric Factory with Validation\n# Description:\nYou are asked to build a very small factory that delivers several classical distance (or dissimilarity) measures.  \n\nImplement the function **metric** that receives a string *name* and returns a callable *d*.  The callable *d* must accept **exactly two** one-dimensional numeric vectors (list, tuple or NumPy array) and compute the corresponding distance, rounded to four decimal places.\n\nSupported metric names and their definitions (for vectors \\(\\mathbf{x},\\mathbf{y}\\) of equal length \\(n\\)):\n\n1. **euclidean** \u2013 \\(\\sqrt{\\sum_{i=1}^{n}(x_i-y_i)^2}\\)\n2. **manhattan** \u2013 \\(\\sum_{i=1}^{n}|x_i-y_i|\\)\n3. **chebyshev** \u2013 \\(\\max_{i}|x_i-y_i|\\)\n4. **cosine** \u2013 cosine **distance**, i.e. \\(1-\\dfrac{\\mathbf{x}\\cdot\\mathbf{y}}{\\|\\mathbf{x}\\|\\,\\|\\mathbf{y}\\|}\\)\n\nBefore the requested value is returned, the vectors have to be validated:\n\u2022 both arguments must be lists, tuples or NumPy arrays that can be converted to `float`\n\u2022 vectors must be one-dimensional, of the **same** length and non-empty\n\u2022 for the cosine metric the two norms must be non-zero\n\nIf either the metric name is not supported or the input validation fails, the callable must return **-1**.\n\nExample call:\n```\nmetric('euclidean')([1, 2, 3], [4, 5, 6]) \u279e 5.1962\n```\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef metric(name: str):\n    \"\"\"Factory producing a validated distance function.\n\n    The function creates and returns a callable *d* that computes one of four\n    classical distances (Euclidean, Manhattan, Chebyshev, Cosine) between two\n    numeric vectors.  All numeric outputs are rounded to four decimal places.\n\n    Validation rules inside the returned callable:\n    * Both arguments must be one-dimensional, non-empty, equal-length numeric\n      iterables (list, tuple or NumPy array).\n    * Metric *name* must be one of the supported strings.\n    * For the cosine distance, zero-norm vectors are rejected.\n\n    If the metric name is unsupported or validation fails, *d* returns -1.\n\n    Args:\n        name (str): Name of the desired metric.\n\n    Returns:\n        Callable[[Iterable, Iterable], float | int]: A distance function with\n        integrated validation.\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\nRound every valid numeric result to the nearest 4th decimal; return -1 otherwise.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 197, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Action Space Statistics\n# Description:\nIn Reinforcement Learning (RL) it is common to work with a variety of action\u2013space types (continuous vs. discrete, single\u2013 vs. multi\u2013dimensional).  \nWrite a function `action_stats` that, given an environment object `env` and two Boolean indicators \u2013 `md_action` (multi-dimensional action space?) and `cont_action` (continuous action space?) \u2013 returns basic statistics about the environment\u2019s action space.\n\nThe function must\n1. distinguish between continuous and discrete spaces,\n2. handle both single\u2013 and multi\u2013dimensional cases, and\n3. summarise the space with the following values:\n   \u2022 `n_actions_per_dim` \u2013 a list whose *i-th* element is the number of distinct actions in dimension *i*; use `math.inf` (or `numpy.inf`) for continuous dimensions,\n   \u2022 `action_ids` \u2013 a list containing every valid discrete action (cartesian product of all dimensions) **or** `None` when at least one dimension is continuous,\n   \u2022 `action_dim` \u2013 the total number of action dimensions.\n\nThe environment is assumed to expose its action space in a way that mimics OpenAI Gym:\n\u2022 `env.action_space.n` \u2013 number of actions for a 1-D discrete space,\n\u2022 `env.action_space.shape` \u2013 tuple whose first element is the dimensionality of a continuous space,\n\u2022 `env.action_space.spaces` \u2013 list-like container holding each sub-space for a multi-dimensional space.  \nEvery sub-space again carries either the attribute `n` (discrete) **or** `shape` (continuous).\n\nReturn the three values **in the above order**. The function must not mutate its inputs.\n\nIf the action space is continuous in *any* dimension the function should:  \n\u2022 set the corresponding entries in `n_actions_per_dim` to `numpy.inf`,  \n\u2022 return `action_ids = None` (because there are infinitely many actions).\n\nWhen the space is fully discrete and multi-dimensional, `action_ids` must contain **all** possible actions represented as tuples, obtained via the cartesian product of the ranges for each dimension.\n# Your code should start with:\n```python\nimport numpy as np\nfrom itertools import product\nfrom itertools import product\nimport numpy as np\nfrom typing import Any, List, Tuple, Union\n\ndef action_stats(env: Any, md_action: bool, cont_action: bool) -> Tuple[List[Union[int, float]], Union[List[Tuple[int, ...]], None], int]:\n    \"\"\"Summarise an RL environment's action space.\n\n    Args:\n        env: Environment instance exposing an ``action_space`` attribute mimicking OpenAI Gym.\n        md_action: ``True`` if the action space is multi-dimensional.\n        cont_action: ``True`` if the action space is continuous (infinite number of actions).\n\n    Returns:\n        A tuple ``(n_actions_per_dim, action_ids, action_dim)`` where\n            \u2022 n_actions_per_dim: list with the number of actions in every dimension\n                                  (``numpy.inf`` for continuous ones),\n            \u2022 action_ids: list of all discrete actions (cartesian product) or ``None`` if\n                           any dimension is continuous,\n            \u2022 action_dim: number of action dimensions.\n    \"\"\"\n    pass\n```\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 198, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Updating the Word\u2013Topic Matrix \u03b2 in Latent Dirichlet Allocation\n# Description:\nIn Latent Dirichlet Allocation (LDA) the word\u2013topic matrix $\\beta\\in\\mathbb{R}^{V\\times T}$ (sometimes also called *topic\u2013word distribution*) stores, for every vocabulary term $v\\in\\{0,\\dots ,V-1\\}$ and every topic $t\\in\\{0,\\dots ,T-1\\}$, the probability $p(w=v\\mid z=t)$.  \n\nDuring the variational M-step the maximum\u2013likelihood estimate of $\\beta$ is obtained from the current variational parameter $\\varphi$ (here denoted **phi**) via\n\n$$\n\\beta_{v,t}\\;\\propto\\;\\sum_{d=0}^{D-1}\\sum_{n=0}^{N_d-1}\\;\\varphi^{(d)}_{n,t}\\,[\\,w^{(d)}_n=v\\,],\n$$\nwhere $[\\,w^{(d)}_n=v\\,]$ is an indicator that the $n$-th token of document $d$ is the word $v$.  After the proportionality is computed the columns of $\\beta$ are normalised so that, for every topic $t$, $\\sum_{v=0}^{V-1}\\beta_{v,t}=1$ holds.\n\nYour task is to implement this **\u03b2-maximisation step**.\n\nFunction requirements\n1. `phi`\u2003\u2013 list of `numpy.ndarray`s.  The *d*-th element has shape `(N_d, T)` and stores the current values of the variational parameter $\\varphi^{(d)}$ for document *d*.\n2. `corpus` \u2013 list of documents.  The *d*-th document is a list of length `N_d` containing integer word indices.\n3. `V`\u2003 \u2013 size of the vocabulary (the number of rows of the returned matrix).\n\nReturn the updated $\\beta$ *as a Python list of lists* such that every column sums to one and every entry is **rounded to 4 decimal places**.\n\nIf a word index from `0 \u2026 V-1` never occurs in the corpus the corresponding row in $\\beta$ must contain only zeros (but columns must still sum to one after normalisation of the non-zero rows).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef update_beta(phi: list[np.ndarray], corpus: list[list[int]], V: int) -> list[list[float]]:\n    \"\"\"Update the word\u2013topic distribution \u03b2 in Latent Dirichlet Allocation.\n\n    Parameters\n    ----------\n    phi : list[np.ndarray]\n        A list with one 2-D NumPy array per document. The array of document *d*\n        has shape (N_d, T) and stores the variational parameter \u03d5 for this\n        document. Row *n* contains the probabilities that token *n* is\n        generated by each of the *T* topics.\n    corpus : list[list[int]]\n        Tokenised corpus. ``corpus[d][n]`` is the integer index of the *n*-th\n        token of document *d*.\n    V : int\n        Vocabulary size, i.e. the number of distinct word indices (rows of \u03b2).\n\n    Returns\n    -------\n    list[list[float]]\n        The updated \u03b2 matrix as a (V \u00d7 T) nested list, column-normalised and\n        rounded to four decimal places.\n    \"\"\"\n    # ===== write your code below =====\n    pass\n```\n# Output Constraints:\nEach inner list corresponds to one vocabulary word and each column to a topic.\nEvery column must sum to exactly 1 (up to 1e-4 rounding error).\nAll returned numbers must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 202, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Partitioning Around Medoids (PAM) Clustering\n# Description:\nImplement the Partitioning Around Medoids (PAM) clustering algorithm.\n\nGiven a data matrix X\u2208\u211d^{n\u00d7d} (n samples, d features) and an integer k (1\u2264k\u2264n), your task is to group the samples into k clusters by iteratively improving a set of representative points called medoids.  \n\nThe algorithm you must follow is strictly deterministic so that the returned result can be tested:\n1. Initialise the medoids as the first k samples of X (i.e. the samples with indices 0,\u2026,k\u22121).\n2. Repeatedly attempt to reduce the total clustering cost \u2013 defined as the sum of the Euclidean distances between every sample and the medoid of the cluster it belongs to \u2013 by swapping any current medoid with any non-medoid sample.  Perform the swap that gives the largest cost reduction in the current iteration, but only accept it if the cost is strictly lower than before the swap.  When no swap can further decrease the cost, stop.\n3. After convergence assign each sample the label of the closest medoid (if two medoids are at exactly the same distance, choose the one that appears first in the current medoid list).\n4. Return the cluster labels as a Python list of integers starting at 0.\n\nExample\n-------\nInput\nX = np.array([[0,0], [1,0], [0,1], [5,5], [6,5], [5,6]]), k = 2\n\nOutput\n[0, 0, 0, 1, 1, 1]\n\nReasoning\nInitial medoids are the first two samples: [0,0] and [1,0].  Swapping the second medoid with the fourth sample [5,5] reduces the total distance from 21.89 to 4.0, which cannot be further improved by any other possible swap.  Finally every sample is assigned to the nearer of the two medoids \u2013 the first three samples to [0,0] (label 0) and the last three samples to [5,5] (label 1).\n# Your code should start with:\n```python\nimport numpy as np\ndef pam_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster *X* into *k* groups using a deterministic PAM algorithm.\n\n    The function must follow these rules:\n    1. Use the first *k* samples of *X* as the initial medoids.\n    2. Repeatedly try all possible single swaps between a medoid and a non-\n       medoid, accepting the swap that strictly reduces the total cost the\n       most.  Stop when no swap can further decrease the cost.\n    3. After convergence label every sample with the index (0 \u2026 k\u22121) of the\n       closest medoid (ties are broken by the medoid that appears first in the\n       current medoid list).\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features).\n        k: Number of clusters to form.\n\n    Returns:\n        A Python list of length *n_samples* containing integer cluster labels.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list of length n where each element is an integer label in the range 0 \u2026 k\u22121.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 216, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Elastic Net Regression from Scratch\n# Description:\nImplement Elastic Net linear regression from scratch using batch gradient descent. The model must be able to 1) generate optional polynomial features of the given degree, 2) standard-score (zero-mean / unit-variance) every non-bias feature, 3) learn the weight vector by minimising the mean\u2013squared error augmented with an Elastic-Net penalty (combined L1 and L2 regularisation) and 4) return predictions for an unseen set of samples.  \n\nWrite a single function `elastic_net_regression` that receives a training design matrix `x_train`, its corresponding target vector `y_train`, and a matrix `x_test` whose targets are to be predicted.  Hyper-parameters controlling the regression are passed with the same names that appear in the signature.  The function must \n\u2022 build the feature matrix (bias term included), \n\u2022 train the model for exactly `n_iterations` passes of batch gradient descent,\n\u2022 regularise every weight except the bias term, and\n\u2022 return the predictions for `x_test`, rounded to four decimals.  \n\nFor the L1 part use the sub-gradient `sign(w_j)` (with `sign(0)=0`).\n\nIf the shapes of `x_train` and `x_test` are incompatible, or if `n_iterations` is smaller than 1, return `-1`.\n# Your code should start with:\n```python\nimport numpy as np\ndef elastic_net_regression(x_train, y_train, x_test, degree=1, reg_factor=0.05, l1_ratio=0.5, n_iterations=3000, learning_rate=0.01):\n    \"\"\"Elastic Net regression implemented with batch gradient descent.\n\n    Parameters\n    ----------\n    x_train : list[list[float]] | np.ndarray\n        Training feature matrix where each sub-list is a sample.\n    y_train : list[float] | np.ndarray\n        Target values for every row in `x_train`.\n    x_test : list[list[float]] | np.ndarray\n        Matrix of samples to predict after training.\n    degree : int, default 1\n        Degree of polynomial expansion applied to every original feature.\n    reg_factor : float, default 0.05\n        Overall regularisation strength (alpha).\n    l1_ratio : float, default 0.5\n        Portion of L1 penalty in Elastic Net (0 = pure ridge, 1 = pure lasso).\n    n_iterations : int, default 3000\n        Number of gradient descent iterations.\n    learning_rate : float, default 0.01\n        Step size used in each gradient update.\n\n    Returns\n    -------\n    list[float]\n        Predictions for `x_test` rounded to 4 decimals. Returns `-1` if the\n        inputs are invalid (different feature counts, or non-positive\n        `n_iterations`).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a python list rounded to 4 decimal places (use numpy.round(arr,4).tolist()).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 217, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Logistic Loss \u2013 Gradient, Hessian & Sigmoid\n# Description:\nImplement three core components of the logistic (sigmoid) loss that are widely used in binary-classification algorithms such as Gradient Boosting and Newton based optimisation.\n\nWrite a Python function that receives two equally-sized one-dimensional containers \u2013 ``actual`` and ``predicted`` \u2013 and returns a tuple containing three lists:\n1. the first list is the gradient of the logistic loss for every observation,\n2. the second list is the Hessian (second derivative) of the logistic loss for every observation,\n3. the third list is the probability obtained by applying the logistic (sigmoid) transformation to every element of ``predicted``.\n\nDefinitions (for every observation *i*):\n    sigmoid(z) = 1 / (1 + e^(\u2212z))\n    grad_i      = actual_i * sigmoid( \u2212 actual_i * predicted_i )\n    hess_i      = sigmoid(predicted_i) * ( 1 \u2212 sigmoid(predicted_i) )\n    prob_i      = sigmoid(predicted_i)\n\nThe labels in ``actual`` are expected to be either +1 or \u22121 (standard representation for logistic loss). The function must:\n\u2022 work with Python lists, tuples, *or* NumPy arrays;\n\u2022 convert the inputs to ``numpy.ndarray`` for vectorised computation;\n\u2022 round every return value to **six (6) decimal places**; and\n\u2022 finally convert the NumPy results back to plain Python lists before returning.\n# Your code should start with:\n```python\nimport numpy as np\ndef logistic_components(actual, predicted):\n    \"\"\"Compute gradient, Hessian and probability for logistic loss.\n\n    The function receives the ground-truth labels (expected to be +1 or \u22121) and\n    the raw model scores, and returns three lists:\n        1. gradient of the logistic loss for each observation,\n        2. Hessian (second derivative) for each observation,\n        3. sigmoid transformation (probability) of each raw score.\n\n    All outputs must be rounded to exactly 6 decimal places.\n\n    Args:\n        actual: 1-D container (list, tuple, or NumPy array) of integers.\n        predicted: 1-D container (list, tuple, or NumPy array) of floats.\n\n    Returns:\n        A tuple (gradient_list, hessian_list, probability_list).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nAll returned numbers must be rounded to exactly 6 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 218, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Blackjack Hand Outcome Evaluation\n# Description:\nIn the casino game *Blackjack* the pay-off for a finished round is determined by very simple rules.  \nYou will write a function `blackjack_outcome` that receives the **final** hands of the player and the dealer and has to return the reward for the player according to the rules below.\n\nCard encoding  \n\u2022 All cards are encoded by an integer in the closed range **1 \u2026 10**.  \n\u2003\u2013 1 represents an **Ace**.  \n\u2003\u2013 2 \u2026 9 keep their numeric value.  \n\u2003\u2013 10 represents the card \u201c10\u201d and all face cards (Jack, Queen, King).  \n\u2022 A hand is a Python `list[int]` of such integers.\n\nHand value  \n\u2022 The value of a hand is the sum of its cards.  \n\u2022 If the hand contains at least one Ace and the sum +10 is **\u2264 21**, exactly one Ace can be counted as 11 instead of 1 (this is called **usable ace**).  \n\u2022 The highest legal value not larger than 21 is the hand\u2019s score.\n\nSpecial terms  \n\u2022 **Bust** \u2013 a hand whose score is larger than 21.  \n\u2022 **Natural blackjack** \u2013 a two-card hand that consists of one Ace (1) and one 10-value card (10).  \n\nReward rules  \n1. If the player busts the reward is **-1** (player loses immediately).\n2. If the dealer busts (and the player did not) the reward is **+1** (player wins).\n3. Otherwise compare both scores:  \n\u2003\u2022 higher score wins (**+1** for player, **-1** for dealer)  \n\u2003\u2022 equal scores lead to a draw (**0**)\n4. A player who wins with a *natural blackjack* is paid **+1.5** instead of +1.  \n   (No extra bonus is given for a drawn natural blackjack.)\n\nYou may assume the lists always contain at least two cards and only valid integers.\n# Your code should start with:\n```python\ndef blackjack_outcome(player: list[int], dealer: list[int]) -> float:\n    \"\"\"Evaluate the outcome of a finished round of Blackjack.\n\n    The function **must** follow the rules presented in the task description.\n\n    Args:\n        player: List of integers (1\u201310) representing the player's final hand. 1 is Ace.\n        dealer: List of integers (1\u201310) representing the dealer's final hand.\n\n    Returns:\n        The player's reward as a float. Possible values are -1, 0, 1 or 1.5.\n    \"\"\"\n    # TODO: Implement your solution here\n    pass\n```\n# Output Constraints:\nReturn a Python `float`. Use the exact values -1, 0, 1 or 1.5.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 221, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Mean Squared Logarithmic Error (MSLE) Implementation\n# Description:\nIn supervised learning projects it is common to evaluate a regression model with the **Mean Squared Logarithmic Error (MSLE)**.  Given two equal-length sequences (lists, tuples or NumPy arrays) \u2013 the *actual* target values and the *predicted* values produced by a model \u2013 the MSLE is defined as  \n\nMSLE = mean\\_i \\[ log(1 + actual\\_i) \u2212 log(1 + predicted\\_i) \\]^2.  \n\nYour task is to implement this metric.\n\nRequirements\n1. Implement a helper function `squared_log_error(actual, predicted)` that returns a NumPy array containing the element-wise squared logarithmic errors.  \n2. Implement the main function `mean_squared_log_error(actual, predicted)` that calls the helper and returns the average of those squared errors, **rounded to 4 decimal places** (as a regular Python `float`).  \n3. Input validation:  \n   \u2022 The two inputs must have the same shape/length.  \n   \u2022 All values must be non-negative (MSLE is undefined for negatives).  \n   If any check fails, the function must return **-1**.\n\nExample\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef squared_log_error(actual, predicted):\n    \"\"\"TODO: implement helper that returns element-wise squared log errors.\"\"\"\n    pass\n\n\ndef mean_squared_log_error(actual, predicted):\n    \"\"\"Calculate Mean Squared Logarithmic Error (MSLE).\n\n    Parameters\n    ----------\n    actual : list | tuple | np.ndarray\n        Sequence of true values.  All elements must be non-negative.\n    predicted : list | tuple | np.ndarray\n        Sequence of predicted values.  Must be the same length as\n        `actual` and contain only non-negative numbers.\n\n    Returns\n    -------\n    float\n        The MSLE rounded to 4 decimals.  If the inputs are invalid the\n        function returns \u20111.\n    \"\"\"\n    # TODO: implement this function\n    pass\n```\n# Output Constraints:\nReturn a Python float rounded to the nearest 4th decimal place.  Return -1 when the input validation fails.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 222, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Sigmoid Activation Function\n# Description:\nImplement the **sigmoid (logistic)** activation function.  The function must accept either a single numeric value (int or float), a Python list of numbers, or a NumPy array of numbers and return the value(s) after applying the sigmoid transformation\n\n                         1\n  sigmoid(z) = ------------------------\n                 1 + exp(-z)\n\nYour implementation has two additional requirements:\n1. It **must work element-wise** for any 1-D or 2-D array-like input (vectorised implementation).\n2. It **must remain numerically stable** for very large positive or negative numbers (e.g. \u00b11000).  A common trick is to compute the expression differently for `z \u2265 0` and `z < 0`.\n\nReturn type rules\n\u2022 If the input is a single scalar, return a single `float` rounded to 4 decimal places.\n\u2022 If the input is a list/NumPy array, return a **Python list** with the same nested structure, every element rounded to 4 decimal places.\n# Your code should start with:\n```python\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the element-wise sigmoid (logistic) function.\n\n    The function must work for scalar numbers, Python lists and NumPy arrays and\n    must be numerically stable for very large positive or negative inputs.\n\n    Args:\n        z: A scalar (int/float) or array-like object (list or np.ndarray)\n           containing numeric values.\n\n    Returns:\n        float | list: The sigmoid of the input rounded to 4 decimal places.\n        If `z` is a scalar the return value is a float, otherwise it is a Python\n        list with the same shape as the input.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nRound every value to the nearest 4th decimal place.  For lists/arrays convert the final NumPy array back to a Python list via `.tolist()`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 224, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Leaky ReLU Activation Function\n# Description:\nIn neural networks the Leaky ReLU activation is often preferred to the ordinary ReLU because it avoids \"dying\" neurons by allowing a small, non-zero gradient when the unit is not active.  \n\nWrite a Python function that applies the Leaky ReLU activation to every element of an input tensor.  \n\nDefinition  \nFor a slope parameter \\(a\\;\\in\\;[0,1)\\) the activation is defined element-wise as\n\n\\[\\operatorname{LeakyReLU}(x)=\\begin{cases}x,&x\\ge 0\\\\a\\,x,&x<0\\end{cases}\\]\n\nThe function must\n1. accept the input `z` as either a Python scalar, a (nested) list, or a `numpy.ndarray` of arbitrary dimension,\n2. accept an optional positive float `a` (default **0.01**),\n3. return the activated values **with the same shape** as `z`, converted to a plain Python list with `numpy.ndarray.tolist()` when necessary.\n\nNo other behaviour is required.\n# Your code should start with:\n```python\nimport numpy as np\ndef leaky_relu(z, a=0.01):\n    \"\"\"Apply the Leaky ReLU activation to every element in *z*.\n\n    Args:\n        z: A scalar, list (possibly nested) or ``numpy.ndarray`` of numbers.\n        a: Optional float in [0,1) \u2014 the slope for negative inputs. Defaults to 0.01.\n\n    Returns:\n        A Python list with the same structure as *z* where each value has been transformed\n        by the Leaky ReLU activation.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list containing the activated values.  The returned list must have the same nested structure as the input.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 226, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost with Decision Stumps\n# Description:\nImplement the AdaBoost ensemble algorithm from scratch using decision stumps (one\u2013level decision trees) as weak learners.\\n\\nThe function must\\n1. Train *n_clf* decision stumps on a binary labelled training set *(X_train, y_train)* where the labels are **-1** and **1**.\\n2. Use the trained ensemble to predict the labels of an unseen data matrix *X_test*.\\n\\nFor every boosting round you have to:\\n\u2022 choose the stump that minimises the weighted classification error; the stump is described by a tuple *(feature_index, threshold, polarity)* where\\n    \u2013 *feature_index* is the column in **X_train** that is inspected,\\n    \u2013 *threshold* is the value that splits the data,\\n    \u2013 *polarity* \\(either 1 or \u22121\\) tells whether values lower than the threshold are classified as **-1** (*polarity*\u2006=\u20061) or **1** (*polarity*\u2006=\u2006\u22121).\\n\u2022 compute the learner weight (``alpha``)\\n    alpha = 0.5 * ln((1 \u2212 error) / (error + 1e-10))\\n\u2022 update the sample weights so that misclassified samples receive higher weights.\\n\\nAt prediction time your ensemble must output the sign of the weighted sum of stump decisions. The returned predictions have to be a **list** of integers each being either **-1** or **1**.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train an AdaBoost ensemble of decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D numpy array of shape (n_samples, n_features) with training data.\n        y_train: 1-D numpy array of shape (n_samples,) containing class labels (-1 or 1).\n        X_test:  2-D numpy array of unseen samples to classify.\n        n_clf:   Number of weak learners (decision stumps) to use in the ensemble.\n\n    Returns:\n        A Python list with one element per row in *X_test*. Each element must be\n        either -1 or 1 indicating the predicted class label.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python **list** of ints consisting solely of -1 and 1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 241, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Orthogonal Weight Initialiser\n# Description:\nIn many deep\u2013learning libraries the weights of a layer are initialised with an **orthogonal matrix** because an orthogonal weight-matrix keeps the activations from shrinking or exploding at the beginning of training.\n\nWrite a function that returns an orthogonally initialised NumPy array with a user specified shape.  The algorithm you have to reproduce is the one popularised by Saxe et al. (2014):\n\n1.  Let the requested tensor shape be `(d0, d1, \u2026, dn)` with `len(shape) \u2265 2`.\n2.  Create a 2-D matrix  `A \u2208 \u211d^{d0\u00d7(d1\u22efdn)}` filled with i.i.d. samples from the standard normal distribution.\n3.  Compute the singular value decomposition (SVD) of `A`\n      \u2003`A = U \u03a3 V\u1d40`  with `U \u2208 \u211d^{d0\u00d7k}` and `V\u1d40 \u2208 \u211d^{k\u00d7(d1\u22efdn)}` where `k = min(d0 , d1\u22efdn)`.\n4.  Choose the SVD factor that has the same size as `A`:\n      \u2003`Q = U`\u2003if `U.shape == A.shape`  *else*  `Q = V\u1d40`.\n5.  Reshape `Q` back to the requested tensor `shape` and multiply it by `scale`.\n\nThe returned tensor must fulfil the orthogonality condition\n```\nflat = result.reshape(shape[0], -1)\nif shape[0] <= flat.shape[1]:\n    flat @ flat.T \u2248 scale**2 \u22c5 I_d0\nelse:\n    flat.T @ flat \u2248 scale**2 \u22c5 I_{d1\u22efdn}\n```\n(i.e. its rows or its columns \u2013 whichever are fewer \u2013 form an orthonormal set up to the given scaling factor).\n\nIf `len(shape) < 2` the function should return `-1`.\n\nExample (fixed random seed)\nInput:  `np.random.seed(0); shape = (2, 2); scale = 0.5`\nOutput:\n```\n[[0.259 , 0.426 ],\n [0.426 ,-0.260 ]]\n```\nReasoning:  with the given seed the algorithm first draws the 2\u00d72 matrix\n`[[1.7641, 0.4002],[0.9787, 2.2409]]`, its left singular vectors form the orthogonal matrix `Q`; after reshaping and scaling by `0.5` we obtain the shown result, whose rows are orthonormal up to the factor `0.5`.\n# Your code should start with:\n```python\nimport numpy as np\ndef orthogonal(shape: tuple[int, ...], scale: float = 0.5) -> list[list[float]]:\n    \"\"\"Initialise a tensor with an orthogonal matrix.\n\n    Args:\n        shape: The desired output shape as a tuple of integers.  Must have\n            length \u2265 2.\n        scale: A scaling factor that is multiplied with the orthogonal\n            matrix.  Defaults to 0.5.\n\n    Returns:\n        A python list representing the tensor whose first two dimensions are\n        orthogonal up to the given scaling factor.  If `shape` has fewer than\n        two dimensions the function returns -1.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturned value must be a python list (use ndarray.tolist()) and satisfy the orthogonality condition described above (within an absolute tolerance of 1e-6).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 243, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Feed-Forward Actor\u2013Critic Forward Pass\n# Description:\nIn many Actor\u2013Critic agents the policy (actor) and the state-value function (critic) share the same feature extractor while having two separate output heads.  In this task you will implement the forward pass of a very small fully-connected Actor\u2013Critic network using nothing more than basic NumPy operations.\n\nNetwork architecture (all layers are fully\u2013connected):\n1. Dense-1 : input \u2192 4 neurons, ReLU activation\n2. Dense-2 : 4 \u2192 4 neurons, ReLU activation\n3. Dense-3 : 4 \u2192 4 neurons, ReLU activation\n4. Actor head  : 4 \u2192 3 neurons, Softmax activation (yields action probabilities)\n5. Critic head : 4 \u2192 1 neuron  (yields a single state value, no activation)\n\nWeights and biases are fixed and **identical to one** (all weights = 1.0, all biases = 0.0).  Because of this choice the network behaves deterministically and its output can be calculated exactly for any input state *s = [s\u2080, s\u2081, s\u2082]* of length 3:\n\u2022 z\u2081 = ReLU(s  \u00b7 W\u2081 + b\u2081)  \u2013 every component equals  max(0, s\u2080+s\u2081+s\u2082)\n\u2022 z\u2082 = ReLU(z\u2081 \u00b7 W\u2082 + b\u2082)  \u2013 every component equals  4\u00b7z\u2081\n\u2022 z\u2083 = ReLU(z\u2082 \u00b7 W\u2083 + b\u2083)  \u2013 every component equals  4\u00b7z\u2082 = 16\u00b7z\u2081\n\u2022 logits = z\u2083 \u00b7 W\u2090 + b\u2090     \u2013 every component equals  4\u00b7z\u2083 = 64\u00b7z\u2081\n\u2022 action_probs = Softmax(logits) \u2013 because all logits are identical the probability of each of the three actions is 1\u20443\n\u2022 state_value  = (z\u2083 \u00b7 W_c + b_c)[0] = 4\u00b7z\u2083 = 64\u00b7z\u2081  \n\nWrite a function that receives a state vector, performs the above computations, and returns \u2013 rounded to four decimals \u2013\n1. a list of the 3 action probabilities, and\n2. the scalar state value.\n# Your code should start with:\n```python\nimport numpy as np\ndef actor_critic_forward(state: list[float]) -> tuple[list[float], float]:\n    \"\"\"Forward pass of a small fully-connected Actor\u2013Critic network.\n\n    Args:\n        state: List of three floats representing the environment state.\n\n    Returns:\n        Tuple containing:\n            1. List of three action probabilities (rounded to 4 decimals).\n            2. Scalar state value (rounded to 4 decimals).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every action probability and the state value to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 249, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Actor\u2013Critic Forward Pass\n# Description:\nIn many reinforcement-learning algorithms the policy (actor) and the state-value estimator (critic) share a large part of the neural network.  A very common layout is three fully\u2013connected layers with ReLU activations followed by two independent output heads:\n\u2022 an \u201cactor head\u201d that converts the last hidden representation into a vector of action scores and then a probability distribution by means of the soft-max function;\n\u2022 a \u201ccritic head\u201d that converts the same hidden representation into a single scalar \u2013 the estimated value of the current state.\n\nYour task is to reproduce the forward pass of such an Actor\u2013Critic network using nothing but NumPy.  All network parameters (weights and biases) are provided in a dictionary.\n\nImplement a function `actor_critic_forward` that\n1. takes the current environment state (a 1-D list of floats) and a dictionary that stores\n   W1, b1, W2, b2, W3, b3  \u2013 the three shared dense layers,\n   Wa, ba               \u2013 actor head,\n   Wc, bc               \u2013 critic head;\n2. performs three affine transformations followed by ReLU on the shared part;\n3. feeds the final hidden vector into the actor head and converts the resulting raw scores into a probability distribution with the soft-max function;\n4. feeds the same hidden vector into the critic head to obtain the scalar state value;\n5. rounds the action probabilities and the state value to four decimal places and returns them.\n\nIf the numerical result is exactly 0 or 1, keep the single decimal place (e.g. `1.0`, `0.0`).\n# Your code should start with:\n```python\nimport numpy as np\ndef actor_critic_forward(state: list[float], params: dict[str, list]) -> tuple[list[float], float]:\n    \"\"\"Compute a forward pass through a three-layer Actor\u2013Critic network.\n\n    The network topology is\n        state \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192 Dense \u2192 ReLU \u2192\n        \u251c\u2500 actor head  (Dense \u2192 soft-max)  \u2192 action probabilities\n        \u2514\u2500 critic head (Dense)             \u2192 state value\n\n    Args:\n        state: 1-D list or array containing the current environment state.\n        params: Dictionary with the following keys (all values are Python lists\n                 that describe NumPy-compatible arrays):\n                 'W1', 'b1', 'W2', 'b2', 'W3', 'b3' \u2013 shared layers\n                 'Wa', 'ba'                    \u2013 actor head\n                 'Wc', 'bc'                    \u2013 critic head\n\n    Returns:\n        A tuple (probabilities, value)\n        probabilities : list of floats \u2013 soft-max of the actor head\n        value         : float          \u2013 scalar output of the critic head\n    \"\"\"\n    # WRITE YOUR CODE HERE\n    pass\n```\n# Output Constraints:\nRound every probability and the state value to 4 decimal places before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 253, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Elastic-Net Penalty and Gradient\n# Description:\nElastic-Net is a convex combination of L1 and L2 regularisation that is widely used to reduce model complexity and prevent over-fitting.  \nWrite a function that can compute both the Elastic-Net penalty value and its analytical gradient for a given weight vector.\n\nThe penalty is defined as\n\n    R(w) = \u03b1 \u00b7 [ \u03bb\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006\u2006 \u00b7 ||w||\u2082 + (1\u2212\u03bb)\u00b70.5\u00b7w\u1d40w ],\n\nwhere\n  \u2022 w is the weight vector,  \n  \u2022 \u03b1 (alpha) is the overall regularisation strength (\u03b1 \u2265 0),  \n  \u2022 \u03bb (lambda) is the L1 ratio (0 \u2264 \u03bb \u2264 1).\n\nThe gradient with respect to w is\n\n    \u2207R(w) = \u03b1 \u00b7 [ \u03bb \u00b7 sign(w) + (1\u2212\u03bb) \u00b7 w ].\n\nImplement a single function `elastic_net_regularization` that\n1. accepts a weight vector (list or 1-D NumPy array), `alpha`, `l1_ratio`, and a Boolean flag `gradient`,\n2. when `gradient=False` (default) returns the scalar penalty value, rounded to 4 decimals, and\n3. when `gradient=True` returns the gradient as a Python list rounded element-wise to 4 decimals.\n# Your code should start with:\n```python\nimport numpy as np\ndef elastic_net_regularization(w, alpha, l1_ratio=0.5, gradient=False):\n    \"\"\"Compute Elastic-Net penalty or its gradient.\n\n    Args:\n        w: 1-D weight vector (list or NumPy array).\n        alpha: Regularisation strength (non-negative float).\n        l1_ratio: Fraction of L1 component (float in [0, 1]).\n        gradient: If True, return gradient, else return penalty value.\n\n    Returns:\n        float if *gradient* is False; list[float] if *gradient* is True.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound the returned float or every element of the returned list to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 256, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Numerical Gradient Check for a Vanilla RNN Parameter\n# Description:\nGradient checking is a simple but extremely useful debugging technique.  When we implement back-propagation we usually derive the analytical gradients (produced by the chain rule) by hand and then code them.  A tiny typo in the algebra or in the code is enough to ruin the whole learning process.  \n\nIn this exercise you will implement a numerical gradient checker for a (vanilla) Recurrent Neural Network (RNN) using the centred finite-difference formula.  The function must work with every trainable parameter stored in the model\u2019s **parameters** dictionary.\n\nGiven\n1. a model that provides\n   \u2022 `model.parameters` \u2013 a dict that maps a parameter name to a NumPy array,\n   \u2022 `model.forward(X_t)` \u2013 performs the forward pass for one time\u2013step and returns the current prediction,\n   \u2022 `model.flush_gradients()` \u2013 resets every internally stored gradient (a no-op in many toy models),\n2. a loss function that takes the list of predictions obtained over all time-steps and returns a scalar loss,\n3. the name of the parameter that has to be checked,\n4. a 3-D input array `X` of shape **(batch, input_dim, n_t)**,\n5. the number of time-steps `n_t`,\n6. a small perturbation `\u03b5`,\n\nyou have to:\n\u2022 iterate over every element of the chosen parameter,\n\u2022 perturb it by **+\u03b5** and **\u2013\u03b5**,\n\u2022 run the forward loop `n_t` times for each perturbation, collect the predictions and evaluate the loss,\n\u2022 approximate the partial derivative with\n\n        \u2202L/\u2202\u03b8\u1d62 \u2248 ( L(\u03b8\u1d62+\u03b5) \u2013 L(\u03b8\u1d62\u2013\u03b5) ) / (2\u03b5)\n\n\u2022 store the numerical gradient in `grads` **at the same index but finally return `grads.T` (transpose of the accumulated array)**.\n\nSpecial cases\n\u2022 If `param_name` is \"Ba\" or \"Bx\" the real key stored in the dictionary is the lower-case variant (\"ba\" or \"bx\") \u2013 handle this automatically.\n\u2022 If `param_name` is \"X\" or \"y\" the function should immediately return `None` \u2013 those are not trainable parameters.\n\nKeep every intermediate tensor in `float64` to avoid unnecessary numerical noise.\n# Your code should start with:\n```python\nimport numpy as np\nfrom copy import deepcopy\ndef grad_check_RNN(model,\n                   loss_func,\n                   param_name: str,\n                   n_t: int,\n                   X,\n                   epsilon: float = 1e-7):\n    \"\"\"Numerically estimate the gradient of an RNN parameter using centred finite differences.\n\n    Args:\n        model:        A model exposing a ``parameters`` dict, a ``forward``\n                       method (single time-step) and a ``flush_gradients``\n                       method.\n        loss_func:    Callable that maps the list of predictions to a scalar\n                       loss value.\n        param_name:   Name of the parameter to be checked.  \"Ba\" and \"Bx\" must\n                       be redirected to the lower-case keys.  If the name is\n                       \"X\" or \"y\" the function should immediately return None.\n        n_t:          Number of time-steps to unroll the network for.\n        X:            Input tensor of shape (batch, input_dim, n_t).\n        epsilon:      Small perturbation added/subtracted to the parameter.\n\n    Returns:\n        NumPy array containing the *transposed* numerical gradient of the\n        chosen parameter or None for the non-trainable names.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nReturn a NumPy array with the same shape as the chosen parameter (but transposed) and dtype float64.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 257, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost with Decision Stumps\n# Description:\nImplement the AdaBoost (Adaptive Boosting) algorithm **from scratch** using decision stumps (one\u2013level decision trees) as weak learners.  \nThe function must:\n1. Take a training set `(X_train, y_train)` where `X_train` is a 2-D NumPy array of shape `(m, n)` and `y_train` is a 1-D NumPy array of length `m` whose elements are **only** `-1` or `1`.\n2. Re-weight training examples iteratively and build `n_clf` decision stumps, each time choosing the stump that minimises the weighted classification error.\n3. Store each stump\u2019s weight (often denoted as $\\alpha_t$) computed as  \n$\\alpha_t = \\frac12 \\ln\\!\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$, where $\\varepsilon_t$ is the stump\u2019s weighted error.\n4. For every sample in `X_test` aggregate all stump votes by the sign of the weighted sum $\\sum_{t=1}^{n_{clf}} \\alpha_t h_t(\\mathbf x)$ and output `-1` or `1` accordingly.\n\nReturn a Python **list** of predicted labels for the given `X_test`.  \nIf `n_clf` is smaller than 1, treat it as 1.\n# Your code should start with:\n```python\nimport numpy as np\ndef adaboost_predict(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_clf: int = 5) -> list[int]:\n    \"\"\"Train AdaBoost with decision stumps and predict labels for X_test.\n\n    Args:\n        X_train: 2-D NumPy array of shape (m, n) containing the training features.\n        y_train: 1-D NumPy array of length m with labels **-1** or **1**.\n        X_test: 2-D NumPy array of shape (k, n) containing test features.\n        n_clf:   Number of weak classifiers (decision stumps) to build. Must be > 0.\n\n    Returns:\n        A Python list of length k, each element being either -1 or 1, the\n        predicted class for the corresponding row in `X_test`.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list with each element being either -1 or 1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 261, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Glorot Xavier Normal Initialisation\n# Description:\nImplement the Glorot (also called Xavier) normal weight-initialisation function that is widely used when training neural networks.  For a requested tensor shape, the function has to\n\n1. compute the so-called *fan in* and *fan out* values\n   \u2022  For a 2-D shape ``(fan_in, fan_out)`` (e.g. a fully\u2013connected layer\u2019s weight\n      matrix) the numbers are given directly by the two dimensions.\n   \u2022  For a shape with more than two dimensions (e.g. convolutional kernels\n      ``(out_channels, in_channels, k1, k2, \u2026)``) the receptive-field size is the\n      product of all dimensions **after** the first two.  In this case\n      ``fan_in  = in_channels  \u00d7 receptive_field_size`` and\n      ``fan_out = out_channels \u00d7 receptive_field_size``.\n2. calculate the standard deviation\n              s = sqrt( 2 / (fan_in + fan_out) ).\n3. return a NumPy array whose elements are independently drawn from a normal\n   distribution with mean 0 and standard deviation ``s``.\n\nThe function must not modify the global NumPy random state apart from using it\nfor sampling.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef glorot_normal(shape: tuple[int, ...]) -> np.ndarray:\n    \"\"\"Generate a NumPy array with Glorot/Xavier normal initialisation.\n\n    Args:\n        shape: Tuple describing the desired tensor shape.  Must have at least\n            two dimensions for well-defined fan_in and fan_out.\n\n    Returns:\n        NumPy ndarray of floats initialised with mean 0 and variance\n        2/(fan_in + fan_out).\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturned NumPy array must have the exact requested shape and dtype float.  The sample mean should be very close to 0 and the sample standard deviation should be close to the theoretical value \u221a[2/(fan_in+fan_out)].\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 266, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Build Adjacency List for an Undirected Graph\n# Description:\nWrite a Python function that converts an undirected, un-weighted graph given by its vertex list `V` and edge list `E` into an adjacency\u2013list representation.\n\nYou are provided\n1. a list `V` containing **unique** vertex identifiers (vertices can be of any hashable type, e.g. `int`, `str`).  The order in `V` must be preserved in the returned structure.\n2. a list `E` where each element is a 2-tuple `(u, v)` that denotes an (undirected) edge connecting vertices `u` and `v`.\n\nYour task is to return a *list of lists* `G` so that\n\u2022 `G[i]` contains all vertices adjacent to `V[i]`.\n\u2022 Every neighbour appears **exactly once** (remove parallel or duplicated edges).\n\u2022 The neighbours inside each `G[i]` must be sorted according to their order of appearance in `V` (this keeps the output deterministic and easy to test).\n\nIf a vertex has no neighbours, its entry must be the empty list `[]`.\n\nExample explanation, detailed constraints, starter code and tests are given below.\n# Your code should start with:\n```python\nfrom typing import Any, List, Tuple\n\ndef build_adj_list(V: List[Any], E: List[Tuple[Any, Any]]) -> List[List[Any]]:\n    \"\"\"Convert an undirected graph given by (V, E) to an adjacency list.\n\n    Parameters\n    ----------\n    V : List[Any]\n        A list of *unique* vertex identifiers. The order in this list must be\n        preserved in the returned adjacency list.\n    E : List[Tuple[Any, Any]]\n        A list of 2-tuples `(u, v)` describing undirected edges that connect\n        vertices `u` and `v`.\n\n    Returns\n    -------\n    List[List[Any]]\n        A list `G` where `G[i]` contains all vertices adjacent to `V[i]`.\n        \u2022 The outer list has the same length and order as `V`.\n        \u2022 Inner lists contain no duplicates and are ordered by their\n          appearance order in `V`.\n    \"\"\"\n    # TODO: Write your code here\n    pass\n```\n# Output Constraints:\nReturn a list of lists. Order of outer list must follow `V`; order inside each inner list must follow `V` as well. No duplicate neighbours allowed.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 267, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Weighted Decision Stump Learning\n# Description:\nA decision stump is the simplest possible decision tree \u2013 it makes its prediction by comparing a single feature to a threshold and optionally flipping the sign (polarity).  In boosting algorithms such as AdaBoost the stump is **learnt with respect to a weight distribution** over the training samples: mis-classified examples with large weights should influence the choice of threshold much more than correctly classified examples with very small weights.\n\nWrite a function that **finds the optimal weighted decision stump** for a binary classification task with class labels \u22121 and 1.\n\nGiven\n1. a data matrix `X \\in \\mathbb{R}^{n\\times d}` (`n` samples, `d` features),\n2. a label vector `y \\in \\{-1,1\\}^n`, and\n3. a non-negative weight vector `w \\in \\mathbb{R}_{\\ge 0}^{n}` (`\\sum_i w_i = 1` is *not* required),\n\nthe function has to examine **all features** and **all unique feature values** as candidate thresholds and return the stump that minimises the **weighted classification error**\n\n    err = \\sum_{i=1}^{n} w_i  \u00b7  \\mathbb{1}[ \\hat y_i \\neq y_i ]\n\nwhere the prediction of a stump is defined as\n\n    \\hat y_i =  \\begin{cases}\n                 \\phantom{-}1  &\\text{if }\\; (x_{ij} <  \u03b8)           \\text{ and } p =  1\\\\\n                 -1            &\\text{if }\\; (x_{ij} \\ge \u03b8)          \\text{ and } p =  1\\\\[4pt]\n                 -1            &\\text{if }\\; (x_{ij} <  \u03b8)           \\text{ and } p = -1\\\\\n                 \\phantom{-}1  &\\text{if }\\; (x_{ij} \\ge \u03b8)          \\text{ and } p = -1\n               \\end{cases}\n\n`p\u2208{1,\u22121}` is the polarity of the stump.\n\nReturn a dictionary with the keys\n```\n{\n    \"feature_index\" : int,   # best feature (0-based)\n    \"threshold\"     : float, # optimal threshold, rounded to 4 decimals\n    \"polarity\"      : int,   # either 1 or -1 as defined above\n    \"weighted_error\": float  # minimal weighted error (rounded to 4 decimals)\n}\n```\nIf several stumps achieve the same minimal error, **any one of them may be returned**.\n# Your code should start with:\n```python\nimport numpy as np\ndef train_decision_stump(X: np.ndarray, y: np.ndarray, sample_weights: np.ndarray) -> dict:\n    \"\"\"Find the optimal weighted decision stump.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features).\n    y : np.ndarray\n        Binary label vector with values -1 or 1.\n    sample_weights : np.ndarray\n        Non-negative weight for every sample.\n\n    Returns\n    -------\n    dict\n        Dictionary describing the best stump (see task description).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound the returned \u201cthreshold\u201d and \u201cweighted_error\u201d to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 273, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Optimal Numerical Threshold Selection for a Decision-Tree Split\n# Description:\nIn a (binary or multi-class) classification decision tree every internal node splits the training data on a single numerical attribute.  For a one\u2013dimensional feature vector X=[x1,\u2026,xn] a candidate split point is chosen exactly half-way between two consecutive **distinct** sorted values of X.\n\nFor every candidate threshold t the dataset is divided into two parts\n  \u2022  Left  = {i | xi < t}\n  \u2022  Right = {i | xi \u2265 t}\n\nThe quality of the split is measured with **information gain**\n\n        IG(t) = H(parent) \u2212 (|L|/n)\u00b7H(L) \u2212 (|R|/n)\u00b7H(R)\n\nwhere H(\u00b7) is the Shannon entropy of the class labels contained in the corresponding subset.\n\nWrite a function best_split that\n1. receives\n   \u2022 feature \u2013 a list (or 1-D NumPy array) of ints/floats,\n   \u2022 target  \u2013 a list (or 1-D NumPy array) of integer class labels,\n2. evaluates every legal threshold and returns the one that maximises the information gain together with the gain itself.\n\nIf several thresholds yield exactly the same (maximal) information gain, return the **smallest** threshold.  If\n \u2022 there are no legal thresholds (all feature values identical), or\n \u2022 no threshold provides a positive information gain (e.g. all labels belong to the same class),\nreturn (None, 0.0).\n\nBoth the chosen threshold and the information gain must be rounded to 4 decimal places with Python\u2019s built-in round function before being returned.\n# Your code should start with:\n```python\nimport math\nfrom collections import Counter\ndef best_split(feature, target):\n    \"\"\"Determine the numerical threshold that produces the highest information gain.\n\n    Parameters\n    ----------\n    feature : list[int | float] | 1-D numpy.ndarray\n        Numerical values of a single attribute.\n    target  : list[int] | 1-D numpy.ndarray\n        Corresponding class labels.\n\n    Returns\n    -------\n    tuple\n        (threshold, information_gain) where\n        * threshold \u2013 float rounded to 4 decimals or None when no useful split exists;\n        * information_gain \u2013 float rounded to 4 decimals.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a tuple (threshold, information_gain) where\n\u2022 threshold is either a float rounded to 4 decimal places or None,\n\u2022 information_gain is a float rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 286, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Dynamic Weight Initializer Retrieval\n# Description:\nIn many deep-learning and numerical libraries the user can specify the **name** of a weight-initialisation routine (e.g. \"zeros\", \"ones\", \"uniform\") and let the framework map that string to the actual Python function that creates the tensor.  \n\nYour task is to implement such a utility.\n\nImplement the function `get_initializer(name)` that receives a string and returns the corresponding *callable* weight-initializer that lives in the module\u2019s **global namespace**.\n\nThe module already contains three simple initializer functions:\n\u2022 `zeros_init(shape)`\u2003\u2013\u2002returns a matrix of zeros of a given shape (tuple `(rows, cols)`).\n\u2022 `ones_init(shape)`\u2003\u2013\u2002returns a matrix of ones of a given shape.\n\u2022 `random_uniform_init(shape, low = 0.0, high = 1.0, seed = 42)` \u2013 returns a matrix whose elements are drawn uniformly from the interval `[low , high]` (the default seed keeps the result deterministic).\n\n`get_initializer` must:\n1. Look for an object whose **name** matches the supplied string inside `globals()`.\n2. Make sure that the found object is *callable*.\n3. Return the callable if it exists.\n4. Otherwise raise a `ValueError` with the exact message:\n   \n   ``Invalid initialization function.``\n\nExample\n-------\nInput:\nname = \"ones_init\"  \nshape = (2, 2)\n\nExecution:\n```\ninit_fn = get_initializer(name)      # returns the function ones_init\noutput  = init_fn(shape)             # [[1.0, 1.0], [1.0, 1.0]]\n```\n\nOutput: \n``[[1.0, 1.0], [1.0, 1.0]]``\n# Your code should start with:\n```python\nimport random\nimport random\nfrom typing import Callable, Tuple, List\n\ndef zeros_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with zeros of the requested shape.\"\"\"\n    # TODO: complete implementation in the reference solution\n    pass\n\ndef ones_init(shape: Tuple[int, int]) -> List[List[float]]:\n    \"\"\"Returns a matrix filled with ones of the requested shape.\"\"\"\n    # TODO: complete implementation in the reference solution\n    pass\n\ndef random_uniform_init(\n        shape: Tuple[int, int],\n        low: float = 0.0,\n        high: float = 1.0,\n        seed: int = 42) -> List[List[float]]:\n    \"\"\"Returns a matrix with uniformly distributed random numbers.\"\"\"\n    # TODO: complete implementation in the reference solution\n    pass\n\ndef get_initializer(name: str):\n    \"\"\"Returns the initializer function that matches *name*.\n\n    Args:\n        name: The name of the initializer (e.g. \"zeros_init\").\n    Returns:\n        A callable initializer.\n    Raises:\n        ValueError: If the name does not correspond to a valid initializer.\n    \"\"\"\n    # TODO: implement this function\n    pass\n```\n# Output Constraints:\nThe returned object must be a callable. When this callable is executed it should strictly follow the behaviour described for each initializer.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 287, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement 2-D Average Pooling Forward & Backward Pass\n# Description:\nIn convolutional neural networks, **average pooling** downsamples an input feature map by sliding a fixed-size window over it and replacing every window with the arithmetic mean of its elements. During back-propagation the gradient that arrives at the pooled output must be distributed _equally_ to every element that took part in each average.\n\nWrite a function that performs **both** the forward and the backward pass of a 2-D average-pooling layer.\n\nForward pass\n  \u2022 Input  X \u2013 a 4-D NumPy array with shape (N, C, H, W) where N is the batch size, C the number of channels, and H\u00d7W the spatial dimensions.\n  \u2022 A pooling window size  pool_shape = (p_h, p_w).\n  \u2022 A stride  stride = (s_h, s_w).\n\nBackward pass\n  \u2022 accum_grad \u2013 a NumPy array with shape identical to the forward output. It stores the gradient of the loss with respect to every pooled value.\n\nYour function must\n 1. Compute the pooled output.\n 2. Propagate the gradient back to the input, i.e. return an array `grad_input` that has the same shape as **X** and whose entries are the sum of all gradients that flow through them.\n 3. Round **both** returned arrays to the nearest 4-th decimal and convert them to Python lists with `tolist()`.\n\nIf the provided shapes do not match (for instance the window does not fit when stepping with the given stride) you may assume that inputs are always valid (no need for error handling).\n# Your code should start with:\n```python\nimport numpy as np\ndef average_pool2d(X: np.ndarray, pool_shape: tuple[int, int], stride: tuple[int, int], accum_grad: np.ndarray) -> tuple[list, list]:\n    \"\"\"Performs forward and backward passes of a 2-D average-pooling layer.\n\n    The function must return a tuple (pooled_output, grad_input), both rounded\n    to the nearest 4-th decimal and converted to regular Python lists using\n    `tolist()`.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every entry of the returned NumPy arrays to the nearest 4-th decimal and convert them to Python lists using `tolist()`. The function must return a **tuple** `(output, grad_input)`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 290, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Compare Two Decision Trees\n# Description:\nYou are given two binary decision trees that are built from the following very small internal representation:\n\n1. Node \u2013 an internal (non-terminal) node that contains\n   \u2022 feature (int): the index of the feature to test\n   \u2022 threshold (float): the threshold that splits the data\n   \u2022 left  (Node | Leaf): the left child (samples with feature value < threshold)\n   \u2022 right (Node | Leaf): the right child (samples with feature value \u2265 threshold)\n\n2. Leaf \u2013 a terminal node that contains\n   \u2022 value (int | float | np.ndarray | list[float]):  the prediction produced by the tree in that region\n\nTwo trees are considered *equivalent* when\n\u2022 they have exactly the same shape (internal nodes at the same places, leaves at the same places),\n\u2022 all internal nodes use the same feature index, and their thresholds are numerically equal up to a tolerance of 1 \u00d7 10\u207b\u2078,\n\u2022 all leaf values are equal within the same tolerance (use numpy.allclose).\n\nWrite a function `compare_trees(tree_a, tree_b)` that returns **True** if the two trees are equivalent and **False** otherwise.  You may assume that `tree_a` and `tree_b` are composed only of the classes `Node` and `Leaf` given in the starter code.\n\nYou must solve the task **recursively** \u2013 do not use any global variables, loops, or external libraries other than *numpy* and *dataclasses*.\n# Your code should start with:\n```python\nimport numpy as np\nfrom dataclasses import dataclass\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass Leaf:\n    \"\"\"A terminal node that stores a prediction value.\"\"\"\n    value: object  # int, float, list or np.ndarray\n\n@dataclass\nclass Node:\n    \"\"\"An internal decision-tree node.\n\n    Attributes:\n        feature (int):   Index of the feature to test.\n        threshold (float): Threshold that splits the data.\n        left (Node | Leaf):  Sub-tree for samples with feature value < threshold.\n        right (Node | Leaf): Sub-tree for samples with feature value \u2265 threshold.\n    \"\"\"\n    feature: int\n    threshold: float\n    left: object   # Node | Leaf\n    right: object  # Node | Leaf\n\ndef compare_trees(tree_a, tree_b):\n    \"\"\"Recursively checks whether *tree_a* and *tree_b* are equivalent.\n\n    Args:\n        tree_a: Root node of the first decision tree (Node or Leaf).\n        tree_b: Root node of the second decision tree (Node or Leaf).\n\n    Returns:\n        True if the two trees are equivalent, False otherwise.\n    \"\"\"\n    # TODO: implement your solution here\n    pass\n```\n# Output Constraints:\nReturn the boolean built-in objects *True* or *False* only (not 0/1, strings, etc.).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 292, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Single-Point Column Crossover for Neural-Network Weights\n# Description:\nIn many evolutionary and genetic-algorithm based neuro-evolution systems, the weights of two parent neural networks are mixed to create new offspring.  A very common operator is the single\u2013point column crossover: a random cut\u2013off column is drawn and all columns **after** that cut-off are swapped between the two parents.\n\nWrite a Python function that performs this single-point column crossover for a single layer\u2019s weight matrix.\n\nThe function must\n1. accept two 2-D weight matrices (``parent1`` and ``parent2``) of identical shape and an integer ``cutoff``,\n2. validate that the two parent matrices have the same shape; if not, return **-1**,\n3. create two new children matrices:\n   \u2022 every column **before** ``cutoff`` is copied from its own parent,\n   \u2022 every column **from** ``cutoff`` (inclusive) to the end is copied from the **other** parent,\n4. return a tuple ``(child1, child2)`` where each child is provided as a nested Python list obtained with NumPy\u2019s ``tolist`` method.\n\nNotes\n\u2022 ``cutoff`` is allowed to be ``0`` (swap all columns) or equal to the number of columns (swap none).\n\u2022 Use NumPy for fast slicing but make sure to convert the final results back to ordinary Python lists.\n\u2022 Do **not** modify the input parents in-place.\n# Your code should start with:\n```python\nimport numpy as np\ndef single_point_crossover(parent1: list[list[int | float]],\n                            parent2: list[list[int | float]],\n                            cutoff: int) -> tuple[list[list[float]], list[list[float]]]:\n    \"\"\"Single-point column crossover of two weight matrices.\n\n    Args:\n        parent1: First parent weight matrix as a list of lists.\n        parent2: Second parent weight matrix with the same shape as *parent1*.\n        cutoff:  Column index at which crossover starts (inclusive).\n\n    Returns:\n        A tuple containing the two children matrices as lists. If the parent\n        matrices do not have the same shape, the function must return ``-1``.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a tuple whose two elements are the children matrices represented as ordinary Python lists.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 294, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Convert Custom Graph to Adjacency Dictionary\n# Description:\nIn many projects graphs are stored in specialised classes or nested containers, but sometimes you have to export them to a plain-Python structure that can easily be serialised or inspected.  \n\nThe custom graph that we want to convert is represented by a **dictionary** `G` with the following fields:\n  * `G['is_directed']` \u2013 a Boolean flag that is **True** when the graph is directed.\n  * `G['_V2I']` \u2013 a dictionary that maps every vertex label to a unique, consecutive integer index starting from **0**.\n  * `G['_G']` \u2013 a list whose *i-th* element stores all outgoing edges of the vertex whose label is the *i-th* key of `G['_V2I']`.  Each edge is a tuple `(source_label, target_label, weight)`.\n\nYour task is to write a function `to_networkx` that converts such a graph into a plain adjacency dictionary `adj` with the following properties:\n  * Every key of `adj` is a vertex label.\n  * `adj[u]` is a list of tuples `(v, w)` describing an edge **u \u2192 v** with weight **w**.\n  * If the input graph is **undirected** every edge must appear **exactly once in each direction** even when the internal storage already contains both copies.\n  * The neighbour lists have to be **sorted alphabetically** by the neighbour label to make the output deterministic.\n  * Vertices without incident edges must still occur in the resulting dictionary with an empty list.\n\nReturn the resulting adjacency dictionary.  An empty dictionary should be returned for a graph with no vertices.\n# Your code should start with:\n```python\nfrom typing import Any, Dict, List, Tuple\nfrom typing import Any, Dict, List, Tuple\n\ndef to_networkx(G: Dict[str, Any]) -> Dict[Any, List[Tuple[Any, float]]]:\n    \"\"\"Convert a custom graph dictionary into a plain adjacency dictionary.\n\n    Parameters\n    ----------\n    G : dict\n        A graph represented as a dictionary with the keys:\n          * 'is_directed' (bool)\n          * '_V2I'        (dict mapping vertex label -> index)\n          * '_G'          (list of adjacency lists, each edge is a\n                           tuple (source_label, target_label, weight))\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a vertex label and the value is a list\n        of `(neighbour, weight)` tuples.  For undirected graphs every edge\n        appears once in each direction and the neighbour lists are sorted\n        alphabetically.\n    \"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nNeighbour lists must be sorted in lexicographical order by the neighbour label.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 296, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Binary Array Validator\n# Description:\nGiven a NumPy array `x`, write a function that verifies whether **all** elements of `x` are binary (only `0` or `1`). The function must return `True` if the array is binary and `False` otherwise. The function has to work for integer, float, or boolean arrays of any shape.\n\nA value is considered *binary* if it is numerically equal to either `0` or `1`. For boolean arrays, both `True` and `False` are acceptable since they correspond to `1` and `0`, respectively.\n# Your code should start with:\n```python\nimport numpy as np\ndef is_binary(x):\n    \"\"\"Check if a NumPy array contains only binary values (0 or 1).\n\n    Parameters\n    ----------\n    x : np.ndarray or array-like\n        Input array to validate.\n\n    Returns\n    -------\n    bool\n        ``True`` if all elements are 0 or 1, otherwise ``False``.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn the Python built-in `bool` value `True` or `False` only.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 298, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Maximum Likelihood N-gram Log-Probability Calculator\n# Description:\nImplement a function that trains a Maximum-Likelihood-Estimation (MLE) N-gram language model on a small corpus and then returns the total log-probability of a query sentence.\n\nGiven\n1. a corpus \u2013 a list whose elements are individual sentences (strings),\n2. a query \u2013 the sentence whose probability you want to evaluate, and\n3. an integer N (\u2265 1) \u2013 the order of the N-gram model to build,\n\nthe function must\n\u2022 split every sentence on white-space to obtain tokens;\n\u2022 for N > 1 pad each token sequence with N\u22121 special tokens \u201c<bol>\u201d at the beginning and one \u201c<eol>\u201d at the end; no padding is used for unigrams;\n\u2022 count N-grams as well as their (N\u22121)-gram prefixes over the whole corpus;\n\u2022 compute the MLE conditional probability\n      P(w_N | w_1\u2026w_{N\u22121}) = count(w_1\u2026w_N) / count(w_1\u2026w_{N\u22121})\n  (for unigrams the denominator is the total number of tokens);\n\u2022 return the sum of natural logarithms of these probabilities for every consecutive N-gram in the *padded* query sentence, rounded to 4 decimal places;\n\u2022 return float('-inf') (negative infinity) if any N-gram needed for the query has zero probability (unseen in the corpus) or if the query is too short for the requested N.\n\nExample\nCorpus: [\"a b a\", \"b a b\"]\nQuery : \"a b\"\nN     : 1\n\nToken counts \u2192 a:3, b:3, total:6\nP(a) = 3/6, P(b) = 3/6\nlog-probability = ln(0.5)+ln(0.5) = \u20131.3863 (rounded)\n\nHence the function returns \u20131.3863.\n# Your code should start with:\n```python\nimport math\nfrom collections import Counter, defaultdict\nfrom typing import List\n\n\ndef ngram_log_prob(corpus: List[str], query: str, N: int) -> float:\n    \"\"\"Return the total log-probability of *query* under an N-gram MLE model.\n\n    Parameters\n    ----------\n    corpus : list[str]\n        A list of training sentences. Each sentence is a plain string; tokens\n        are assumed to be separated by white-spaces.\n    query : str\n        The sentence whose probability should be calculated.\n    N : int\n        The order of the N-gram model (N \u2265 1).\n\n    Returns\n    -------\n    float\n        The sum of natural logarithms of the probabilities of every\n        consecutive N-gram occurring in *query*, rounded to 4 decimal places.\n        If any necessary N-gram is unseen in the corpus the function returns\n        float('-inf').\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nRound the final log-probability to 4 decimal places. If the probability is zero, return float('-inf').\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 302, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Spectral Clustering from Scratch\n# Description:\nImplement the Spectral Clustering algorithm from scratch.  The function receives a set of data points X\u2208\u211d^{n\u00d7d} and the desired number of clusters k.  The algorithm should:\n1. Build a weighted adjacency matrix W where the weight between two points is defined as  w_{ij}=1/(1+\u2016x_i\u2212x_j\u2016_2)  (the diagonal must be 0 so a point is not connected to itself).\n2. Compute the (unnormalised) graph Laplacian  L=D\u2212W,  where  D  is the diagonal degree matrix  D_{ii}=\u2211_j w_{ij}.\n3. Obtain the first k eigenvectors (those associated with the k smallest eigen-values) of L and stack them column-wise into the projection matrix E\u2208\u211d^{n\u00d7k}.\n4. Run k-means on the rows of E using the very first k rows of E as the initial centroids (this keeps the implementation deterministic).  Use Euclidean distance, iterate until the assignments stop changing or a maximum of 100 iterations is reached.  If a cluster becomes empty, re-initialise its centroid with a random row of E.\n5. Return the resulting cluster labels as a Python list (not a NumPy array).\n\nAll intermediate steps must be implemented manually; external libraries such as scikit-learn, TensorFlow, PyTorch, etc. are not allowed.  Only NumPy may be used for numerical operations.\n# Your code should start with:\n```python\nimport numpy as np\ndef spectral_clustering(X: np.ndarray, k: int) -> list[int]:\n    \"\"\"Cluster data using Spectral Clustering.\n\n    Args:\n        X: 2-D NumPy array where each row is a data sample.\n        k: Desired number of clusters.\n\n    Returns:\n        List of length n_samples containing an integer cluster label for each sample.\n    \"\"\"\n```\n# Output Constraints:\nReturn a Python list whose length equals the number of samples in X and whose values are integers in the range [0,k\u22121].\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 303, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Error Message Formatter\n# Description:\nYou are given a list called `params` that stores tuples of the form `(mine, label)` where\n  \u2022 `mine`  \u2013 your program\u2019s current output for this label\n  \u2022 `label` \u2013 a unique identifier for the current test case  \nYou are also given a dictionary called `golds` that maps each `label` to the **expected** (gold-standard) output, an integer `ix` that tells you which element of `params` you want to inspect, and an optional string `warn_str` that can contain an additional warning message.\n\nWrite a function that produces a single, well-formatted, multi-line debugging string with the following exact layout:\n\n```\n------------------------- DEBUG -------------------------   <- 25 dashes on both sides\nMine (prev) [<prev_label>]:\n<prev_mine>\n\nTheirs (prev) [<prev_label>]:\n<golds[prev_label]>\n\nMine [<curr_label>]:\n<curr_mine>\n\nTheirs [<curr_label>]:\n<golds[curr_label]><warn_str>\n----------------------- END DEBUG -----------------------   <- 23 dashes on both sides\n```\n\nwhere\n\u2022 `prev_label`, `prev_mine` refer to the element at index `max(ix-1, 0)` in `params` (so for `ix = 0` the *previous* element is the first element itself).\n\u2022 `curr_label`, `curr_mine` refer to the element at index `ix`.\n\u2022 `warn_str` is appended **exactly as it is passed** (it may start with a new-line or not).\n\nReturn the final string.  No other text, spacing, or line breaks are permitted.\n# Your code should start with:\n```python\ndef err_fmt(params: list[tuple[str, str]], golds: dict[str, str], ix: int, warn_str: str = \"\") -> str:\n    \"\"\"Format a detailed debugging string comparing your output to gold output.\n\n    The function must follow the exact layout described in the task statement.\n\n    Args:\n        params: List of tuples `(mine, label)`.\n        golds:  Dictionary mapping `label` to expected output.\n        ix:     Current index in `params`.\n        warn_str: Optional extra warning string.\n\n    Returns:\n        A single, multi-line string following the required format.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nThe output must match the required format **exactly**, including:\n\u2022 precise number of dashes (25 and 23)\n\u2022 all line breaks (`\\n`)\n\u2022 no extra spaces\n\u2022 `warn_str` must appear exactly as provided.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 304, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Posterior Mean of Bayesian Linear Regression Coefficients\n# Description:\nImplement a function that computes the posterior mean (i.e., the Maximum\u2013A-Posteriori estimate) of the regression coefficients in Bayesian linear regression with an unknown noise variance. \n\nThe model assumes\n\u2022 a normal\u2013inverse-gamma conjugate prior on the coefficients **b** and the noise variance \u03c3\u00b2  \n\u2022 a Gaussian likelihood with identity noise covariance.\n\nPrior\n    \u03c3\u00b2  ~  InverseGamma(\u03b1, \u03b2)  \n    b | \u03c3\u00b2 ~  \ud835\udca9(\u03bc , \u03c3\u00b2 V)\n\nGiven a training design matrix X (with N samples and M features), a target vector y and the hyper-parameters (\u03b1, \u03b2, \u03bc, V), the closed-form posterior parameters are\n    V\u207b\u00b9     =  (V)\u207b\u00b9                      (if V is a scalar or a diagonal list, convert accordingly)\n    \u03a3_b     =  (V\u207b\u00b9 + X\u1d40X)\u207b\u00b9             (posterior covariance up to \u03c3\u00b2)\n    \u03bc_b     =  \u03a3_b ( V\u207b\u00b9 \u03bc + X\u1d40y )        (posterior mean of the coefficients)\n\nBecause \u03c3\u00b2 is unknown, its exact value is not required to obtain \u03bc_b \u2013 it cancels in the MAP estimate.  \nReturn \u03bc_b rounded to 4 decimal places.  \nThe function must optionally add an intercept column to X when `fit_intercept=True` and must work with the following accepted prior specifications:\n\u2022 V omitted \u2192 identity  \n\u2022 V given as scalar \u2192 scalar\u00d7identity  \n\u2022 V given as list/tuple \u2192 treated as a diagonal  \n\u2022 \u03bc given as scalar \u2192 broadcast to a vector of length M (or M+1 when an intercept is fitted)\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef bayesian_posterior_mean(\n    X: np.ndarray,\n    y: np.ndarray,\n    alpha: float = 1.0,\n    beta: float = 1.0,\n    mu = 0.0,\n    V = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Compute the posterior mean (MAP estimate) of the coefficients in\n    Bayesian linear regression with an unknown variance.\n\n    The model places a normal\u2013inverse-gamma prior on (*b*, \u03c3\u00b2), but the MAP\n    estimate of *b* does not depend on \u03c3\u00b2.  See the task description for the\n    closed-form formula used here.\n\n    Args:\n        X: Training design matrix of shape (N, M).\n        y: Target vector of shape (N,).\n        alpha: Shape parameter of the inverse-gamma prior on \u03c3\u00b2 (kept only for\n            API compatibility).\n        beta: Scale parameter of the inverse-gamma prior on \u03c3\u00b2 (unused).\n        mu: Prior mean for *b*. Scalar values are broadcast to the correct\n            length.\n        V: Prior scale for *b*. Accepts None (identity), a scalar (scalar\u00d7I), a\n            1-D sequence (treated as a diagonal), or a full 2-D array.\n        fit_intercept: If True, prepend a bias column of ones to X.\n\n    Returns:\n        A list of floats \u2013 the posterior mean of the coefficients, rounded to\n        4 decimal places.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nRound every returned coefficient to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 308, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Mel Filterbank Matrix Construction\n# Description:\nIn speech and audio processing a spectrogram is usually first converted into so\u2013called Mel\u2013frequency bands.  \nA Mel filterbank is a **non\u2013linear set of triangular filters** that are laid out on the Mel scale \u2013 a perceptual scale that gives high resolution to low frequencies and low resolution to high frequencies.  \nGiven a discrete Fourier transform (DFT) length `N`, the function must create the complete transformation matrix *F* so that a power spectrum vector `P` (length `N//2+1`) can be converted into Mel band energies with a simple matrix multiplication `M = F @ P`.\n\nThe function has to work exactly like the reference implementation below and must obey the following specification:\n\n\u2022 Convert limits expressed in Hertz to the Mel scale and generate `n_filters+2` equally\u2013spaced values on the Mel axis.  \n\u2022 Convert those Mel values back to Hertz \u2013 these are the (n_filters+2) corner frequencies of the triangular filters.  \n\u2022 For every DFT bin `k` (whose centre frequency is `k*fs/N`) and every Mel filter `i` compute the left\u2010hand and right\u2010hand slopes of the triangle and keep the *positive* minimum of both \u2013 this is the weight for filter `i` and bin `k`.  \n\u2022 If `normalize` is true scale every filter by\n$$w_i\\;\\leftarrow\\;\\frac{2}{f_{i+2}-f_{i}}\\;w_i$$\nso that its area in Mel space equals 1.  \n\u2022 Return the complete filterbank as a plain Python list whose shape is `(n_filters,\\;N//2+1)`.\n\nIf `max_freq` is omitted it must default to the Nyquist frequency `fs/2`.\n\nThe helper conversions are\n```text\nmel = 2595 * log10(1 + f/700)            # Hz \u2192 Mel\nf   = 700 * (10**(mel/2595) - 1)          # Mel \u2192 Hz\n```\n\nWhen the job is done you will be able to reproduce the filterbank that packages such as LibROSA compute.\n# Your code should start with:\n```python\nimport numpy as np\ndef mel_filterbank(N: int,\n                   n_filters: int = 20,\n                   fs: int = 44_000,\n                   min_freq: int = 0,\n                   max_freq: int | None = None,\n                   normalize: bool = True) -> list[list[float]]:\n    \"\"\"Build a Mel filterbank transformation matrix.\n\n    The returned matrix has *n_filters* rows and *N//2 + 1* columns.  Each row\n    is a triangular filter defined on the Mel scale.  See the detailed task\n    description for the exact algorithm that has to be implemented.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list (not a NumPy array).  Every element must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 312, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Blackman\u2013Harris Window Generator\n# Description:\nWrite a Python function that generates a Blackman\u2013Harris window of arbitrary length.\n\nA window function is often multiplied with a finite-length signal before an FFT is taken in order to reduce spectral leakage.  The Blackman\u2013Harris window is a popular member of the cosine-sum family (here with K = 3).  Given a desired window length $L$ in samples, its samples are defined by\n\n$$\n\\operatorname{BH}(n)=a_0-a_1\\cos\\left(\\frac{2\\pi n}{N}\\right)\n           +a_2\\cos\\left(\\frac{4\\pi n}{N}\\right)\n           -a_3\\cos\\left(\\frac{6\\pi n}{N}\\right),\\qquad n=0,1,\\dots,L-1,\\quad N=L-1,\n$$\nwhere the fixed coefficients are\n\n$a_0=0.35875\\;,\\;a_1=0.48829\\;,\\;a_2=0.14128\\;,\\;a_3=0.01168.$\n\nTwo slightly different variants are common:\n\u2022 Symmetric\u2003(used for FIR filter design.)  \n\u2022 Periodic\u2003  (used for FFT-based spectral analysis.)\n\nFor the periodic form the window is conceptually generated with length $L+1$ and the last value discarded; this makes the first and last value identical so the window tiles seamlessly when wrapped for an $L$-point FFT.\n\nYour task is to implement a function\n\nblackman_harris(window_len: int, symmetric: bool = False) -> list[float]\n\nthat produces the requested variant and returns the window as a Python list rounded to 4 decimals.\n\nSpecial cases\n\u2022 If window_len \u2264 0\u2003\u2192\u2003return an empty list.  \n\u2022 For window_len == 1\u2003\u2192\u2003return [1.0] whatever the variant.\n# Your code should start with:\n```python\nimport numpy as np\ndef blackman_harris(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Blackman\u2013Harris window.\n\n    Parameters\n    ----------\n    window_len : int\n        Desired number of samples in the returned window.\n    symmetric : bool, optional (default=False)\n        If False, return the *periodic* form suitable for an FFT of length\n        `window_len`.  If True, return the *symmetric* form typically used in\n        filter design.\n\n    Returns\n    -------\n    list[float]\n        Window coefficients rounded to four decimal places.\n    \"\"\"\n    # TODO: implement the function here\n```\n# Output Constraints:\nAll values must be rounded to the nearest 4th decimal place and the result returned as a Python list.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 313, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Expected Logarithm of Dirichlet Components\n# Description:\nIn Bayesian models that use Dirichlet\u2013multinomial components (for example in Latent Dirichlet Allocation) the quantity \ud835\udd3c[log X\u209c] frequently appears, where X follows a Dirichlet distribution with parameters \u03b3 (often noted \"gamma\").  When \u03b3 is represented as a 2-D matrix, the element \u03b3[d,t] corresponds to the *t*-th component of the *d*-th sample (or document).\n\nFor a Dirichlet random variable X \u223c Dir(\u03b3[d,:]) the expectation of the logarithm of its *t*-th component is\n\n    \ud835\udd3c[log X\u209c] = \u03c8(\u03b3[d,t]) \u2212 \u03c8( \u2211\u2096 \u03b3[d,k] ) ,\n\nwhere \u03c8(\u00b7) is the digamma (first derivative of log-Gamma) function.\n\nWrite a function that\n1. accepts\n   \u2022 gamma \u2013 a 2-D list or NumPy array containing the Dirichlet parameters (all positive numbers)\n   \u2022 d     \u2013 the row index (0-based)\n   \u2022 t     \u2013 the column index (0-based)\n2. computes the above expectation using the formula above,\n3. rounds the result to four (4) decimal places and returns it as a Python float.\n\nBecause external scientific libraries are not allowed, you must implement the digamma function yourself.  A simple and accurate strategy is:\n\u2022 Use the recursion \u03c8(x) = \u03c8(x+1) \u2212 1/x to shift small x up to a moderate value (e.g. 6).\n\u2022 Apply the asymptotic expansion\n     \u03c8(x) \u2248 ln x \u2212 1/(2x) \u2212 1/(12x\u00b2) + 1/(120x\u2074) \u2212 1/(252x\u2076)\n  to obtain a good approximation for the remaining (now large) x.\n\nIf the provided indices are outside the matrix dimensions your code may assume no call will be made (no need to handle this explicitly).\n# Your code should start with:\n```python\nimport math\nimport numpy as np\ndef expected_log_dirichlet(gamma, d, t):\n    \"\"\"Compute the expectation of log X_t for a Dirichlet-distributed vector.\n\n    A Dirichlet random vector X with parameters gamma[d,:] satisfies\n        E[log X_t] = \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])),\n    where \u03c8 is the digamma function.  External scientific libraries are not\n    permitted, therefore you must implement digamma yourself (see task\n    description for hints).\n\n    Parameters\n    ----------\n    gamma : list[list[float]] | np.ndarray\n        2-D structure holding the Dirichlet concentration parameters.  All\n        elements are positive.\n    d : int\n        Row index (0-based) identifying which Dirichlet parameter set to use.\n    t : int\n        Column index (0-based) identifying the component whose expected log is\n        requested.\n\n    Returns\n    -------\n    float\n        The value of \u03c8(gamma[d,t]) \u2212 \u03c8(sum(gamma[d,:])) rounded to 4 decimals.\n    \"\"\"\n    # =====  Write your code below this line  =====\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 317, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Formatted Debug-Error Report\n# Description:\nYou are given three inputs that together describe the result of an automatic evaluation of predictions made by a program.\n\n1. `params` \u2013 a list of 2-tuples where each tuple has the form `(prediction, label)`.   \n   \u2022 `prediction` is the text produced by *your* program (\"Mine\").   \n   \u2022 `label` is an identifier that is also present in `golds`.\n2. `golds` \u2013 a dictionary that maps every possible label to the *gold* / *expected* text (\"Theirs\").\n3. `ix` \u2013 an integer index that points to the item in `params` on which you want to build a detailed, readable error report.\n4. `warn_str` \u2013 an **optional** extra message that should be appended to the report (for instance a special warning such as \" (WARNING: mismatch)\").  The default value is the empty string.\n\nThe task is to write a function `err_fmt` that returns a multi-line string with the following exact format:\n\n```\n------------------------- DEBUG -------------------------\nMine (prev) [<prev_label>]:\n<prev_prediction>\n\nTheirs (prev) [<prev_label>]:\n<prev_gold>\n\nMine [<curr_label>]:\n<curr_prediction>\n\nTheirs [<curr_label>]:\n<curr_gold><warn_str>\n----------------------- END DEBUG -----------------------\n```\n\nDetails\n\u2022 The header line consists of 25 dashes, the word **DEBUG** surrounded by single spaces, followed by another 25 dashes and a newline.  \n\u2022 \"prev\" refers to the element situated at index `max(ix \u2212 1, 0)` \u2013 i.e. index 0 if `ix` is already 0.  \n\u2022 After the (prev) block there are two blank lines, then the current block, then `warn_str` (if any), then **one** newline and the footer.  \n\u2022 The footer line consists of 23 dashes, the text **END DEBUG** with surrounding spaces, and another 23 dashes.\n\nReturn the resulting string **exactly** \u2013 including every dash and newline \u2013 so that it can be used directly for logging or debugging.\n\nIf the format is respected, there are no other corner-cases to handle, and no exceptions need to be raised.\n# Your code should start with:\n```python\nfrom typing import List, Tuple, Dict\nfrom typing import List, Tuple, Dict\n\ndef err_fmt(params: List[Tuple[str, str]],\n            golds: Dict[str, str],\n            ix: int,\n            warn_str: str = \"\") -> str:\n    \"\"\"Return a formatted multi-line debug string comparing predictions to golds.\n\n    See the task description for the exact required format.\n\n    Args:\n        params: A list where every element is a tuple (prediction, label).\n        golds:  A dictionary mapping each label to its gold/expected string.\n        ix:     The index inside `params` for which the detailed comparison\n                 is built.\n        warn_str: Optional extra text appended before the footer.\n\n    Returns:\n        A string that follows the specified layout exactly.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nReturn **exactly** the formatted multi-line string (str).  All newlines and dashes must be preserved.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 318, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: AdaBoost From Scratch \u2013 Decision-Stump Ensemble\n# Description:\nImplement the AdaBoost.M1 algorithm **from scratch** using decision stumps (one\u2013level decision trees) as weak learners.  \nThe function must:\n1. Train an AdaBoost classifier on the given training set `(X_train, y_train)` for exactly `n_estimators` boosting rounds.  \n2. Each weak learner is a decision stump that splits the data on a single feature `j` using a threshold `t` and a polarity `p \\in \\{-1,1\\}`:\n   \u2022 prediction $h(x)=p\\;\\text{sign}(x_j-t)$ where `sign(z) = -1` if `z < 0`, `+1` otherwise.  \n3. After training, predict the labels of `X_test` with the final boosted classifier\n   \\[F(x)=\\text{sign}\\Big(\\sum_{m=1}^M\\alpha_m\\,h_m(x)\\Big)\\Big],\\qquad \\alpha_m = \\frac12\\ln\\frac{1-\\epsilon_m}{\\epsilon_m}\\]\n   where $\\epsilon_m$ is the weighted classification error of the *m*-th stump.  \n4. Return the predictions as a **Python list** of `-1` and `1`.\n\nAll training labels are guaranteed to be either `-1` or `1`.  Use only the standard libraries `math` and `numpy` \u2013 **no third-party machine-learning libraries are allowed**.\n# Your code should start with:\n```python\nimport math\nimport numpy as np\ndef adaboost_predict(X_train, y_train, X_test, n_estimators=50):\n    \"\"\"Train AdaBoost on the training set and predict the labels of X_test.\n\n    Parameters\n    ----------\n    X_train : numpy.ndarray\n        2-D array of shape (n_samples, n_features) containing the training data.\n    y_train : numpy.ndarray\n        1-D array of length n_samples containing the training labels. Each label\n        is either -1 or 1.\n    X_test : numpy.ndarray\n        2-D array whose rows are the samples to classify.\n    n_estimators : int, default=50\n        Number of boosting rounds (weak learners) to use.\n\n    Returns\n    -------\n    list[int]\n        Predicted labels (-1 or 1) for each sample in X_test.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a **list** whose elements are either `-1` or `1` (integers).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 329, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Frequency Bins of a DFT\n# Description:\nIn a discrete Fourier transform (DFT) the *k*-th spectrum coefficient corresponds to a sinusoid whose frequency\nis\n    f\u2096 = k \u00b7 (f\u209b / N)  for k = 0,1,\u2026,N\u22121\nwhere N is the number of DFT coefficients and f\u209b is the sampling frequency in Hz.\n\nFor real-valued signals one often needs only the non\u2013negative (\"positive\") part of the spectrum (indices 0 \u2026 \u230aN/2\u230b).\n\nWrite a function that returns the centre frequency (in Hz) of every DFT bin.\nThe function must work in two modes:\n1. **positive_only = True**  \u2013 return the non-negative frequencies (length \u230aN/2\u230b+1)\n2. **positive_only = False** \u2013 return the full list of N bin centres arranged exactly as NumPy\u2019s `fftfreq` does:\n   `[0, 1\u00b7\u0394f, \u2026, (\u2308N/2\u2309\u22121)\u00b7\u0394f, \u2212\u230aN/2\u230b\u00b7\u0394f, \u2026, \u2212\u0394f]`\n\nAll returned numbers have to be rounded to 4 decimal places and handed back as a regular Python list (use NumPy\u2019s\n`tolist()`).\n\nIf either `N` or `fs` is not strictly positive, return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\ndef dft_bins(N: int, fs: int = 44000, positive_only: bool = True) -> list[float]:\n    \"\"\"Compute the centre frequency of each DFT bin.\n\n    Parameters\n    ----------\n    N : int\n        Number of DFT coefficients.\n    fs : int, optional\n        Sampling frequency in Hertz.  Default is 44 kHz.\n    positive_only : bool, optional\n        When *True* return only the non-negative frequency bins; when\n        *False* return all *N* bins.  Default is *True*.\n\n    Returns\n    -------\n    list[float]\n        List of bin frequencies (Hz) rounded to 4 decimals.  An empty\n        list is returned when *N* or *fs* is non-positive.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list with every element rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 331, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Out-of-Bag MSE for Random Forest Regression\n# Description:\nIn a Random Forest each tree is trained on a bootstrap sample of the original data.  For any training sample the trees **not** containing that sample (so-called *out-of-bag* trees) can be used to obtain an unbiased performance estimate.  \n\nWrite a function that computes the *out-of-bag mean squared error* (OOB-MSE) for a Random Forest **regressor**.\n\nThe function receives three arguments:\n1. `y_true` \u2013 the true target values, shape `(n_samples,)`.\n2. `predictions` \u2013 the raw predictions of every tree, shape `(n_samples, n_estimators)`, where element `(i, j)` is the prediction of the *j-th* tree for the *i-th* sample.\n3. `oob_mask` \u2013 a boolean/\u200bbinary matrix of the same shape whose element `(i, j)` is `True` (or `1`) **iff** sample *i* was *out of bag* for tree *j*.\n\nFor every sample that has at least one OOB prediction you must:\n\u2022 average all its OOB predictions,\n\u2022 compute the squared error between this average and the true value.\n\nThe OOB-MSE is the mean of those squared errors taken over **only** the samples that own at least one OOB prediction.  \nIf *no* sample has an OOB prediction, return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef oob_mse(y_true, predictions, oob_mask):\n    \"\"\"Compute the out-of-bag mean squared error for a Random Forest regressor.\n\n    Parameters\n    ----------\n    y_true : list[float] | np.ndarray\n        True target values, shape (n_samples,).\n    predictions : list[list[float]] | np.ndarray\n        Predictions from each tree, shape (n_samples, n_estimators).\n    oob_mask : list[list[bool | int]] | np.ndarray\n        Boolean / binary matrix indicating whether a prediction was obtained\n        from an out-of-bag tree (True/1) or not (False/0), same shape as\n        *predictions*.\n\n    Returns\n    -------\n    float\n        The OOB mean squared error rounded to 4 decimal places, or -1 if the\n        OOB estimate cannot be computed.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a **float** rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 332, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: FP-Tree Construction Without Classes\n# Description:\nGiven a set of transactions and a minimum support threshold, construct the **Frequent-Pattern Tree (FP-Tree)** without using any classes.  \n\nThe tree is represented as a nested dictionary where every node stores two keys:\n1. \"support\"   \u2013 the number of transactions that share the path ending in this node.\n2. \"children\"  \u2013 another dictionary that holds the node\u2019s direct descendants.\n\nThe root node is an empty placeholder with support 0.\n\nBuilding rules\n1. Compute the support (occurrence in distinct transactions) for every item.\n2. Discard the items whose support is smaller than `min_support`.\n3. Create a global ordering of the remaining items \u2013 first by **decreasing support**, then **alphabetically** to break ties.\n4. For every transaction\n   \u2022 remove duplicates, keep only frequent items, and reorder them according to the global ordering;\n   \u2022 walk from the root and update/extend the path, increasing the *support* of every visited node by 1.\n\nReturn the root node of the final FP-Tree.\n\nExample tree format\n{\"support\":0,\n \"children\":{\n     \"a\":{\"support\":3,\n           \"children\":{\n               \"b\":{\"support\":2,\n                     \"children\":{\n                         \"c\":{\"support\":1,\"children\":{}}}},\n               \"c\":{\"support\":1,\"children\":{}}}}}}\n# Your code should start with:\n```python\nfrom collections import Counter\ndef build_fp_tree(transactions: list[list[str]], min_support: int) -> dict:\n    \"\"\"Construct an FP-Tree using only nested dictionaries.\n\n    Parameters\n    ----------\n    transactions : list[list[str]]\n        A list where each element is a list of items representing one transaction.\n    min_support : int\n        Minimum number of transactions an item has to appear in to be kept.\n\n    Returns\n    -------\n    dict\n        The root node of the FP-Tree.  Each node contains two keys:\n        \"support\" and \"children\" (the latter mapping item \u2192 child-node).\n    \"\"\"\n    pass\n```\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 336, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Linear Kernel Matrix\n# Description:\nIn many kernel-based machine-learning algorithms (e.g. Support Vector Machines, Gaussian Processes) the similarity between two input vectors x and y is measured by a kernel function k(x,y).  One of the simplest and most frequently used kernels is the linear kernel\n\n    k(x, y) = x \u00b7 y + c\u2080,\n\nwhere x \u00b7 y is the dot product between the two vectors and c\u2080 is an optional constant that shifts the similarities (for c\u2080 = 0 the kernel is said to be homogeneous).\n\nWrite a function that receives two collections of input vectors X and Y and returns the complete kernel matrix K whose (i,j) entry equals k(X[i], Y[j]).  If Y is omitted (or set to None) the function must assume Y = X and therefore return a square, symmetric matrix.\n\nInput vectors can be passed either as built-in Python lists or as NumPy arrays; your function must treat both in the same way.  All numerical operations must be performed with floating-point precision.\n\nValidation rules\n1. X has shape (N, C) and Y has shape (M, C).  If the number of columns (C) differs, the function must return -1.\n2. An empty X (i.e. N = 0) is allowed and should return an empty list.\n\nReturn value\n\u2022 A list of lists of floats containing the kernel matrix, rounded to 4 decimal places.\n\u2022 Return -1 when the input dimensions are incompatible (rule 1).\n# Your code should start with:\n```python\nimport numpy as np\ndef linear_kernel(X: list[list[int | float]] | \"np.ndarray\", Y: list[list[int | float]] | \"np.ndarray\" | None = None, c0: int | float = 0) -> list[list[float]] | int:\n    \"\"\"Return the linear kernel matrix k(x, y) = x\u00b7y + c0.\n\n    Parameters\n    ----------\n    X : 2-D list or NumPy array with shape (N, C)\n        Collection of N input vectors.\n    Y : 2-D list or NumPy array with shape (M, C) or None, optional\n        Collection of M input vectors.  If None, the function must set\n        ``Y = X``.  Default is None.\n    c0 : int or float, optional\n        Additive constant of the kernel.  Default is 0.\n\n    Returns\n    -------\n    list[list[float]] | int\n        The (N, M) kernel matrix where entry (i, j) equals\n        X[i]\u00b7Y[j] + c0, rounded to 4 decimals.  Return -1 when X and Y have\n        incompatible shapes (different number of columns).\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nAll numbers in the returned kernel matrix must be rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 340, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Single\u2013Step Adam Optimiser\n# Description:\nImplement a single optimisation step of the Adam (Adaptive Moment Estimation) algorithm.  \nThe function receives the current value of a parameter *\u03b8*, its gradient *g*, the time-step *t* (starting from **1**), and the two running moment estimates *m* (first moment / mean) and *v* (second moment / un-centred variance).  \nUsing the standard Adam update rule\n\n    m\u209c   = \u03b2\u2081\u22c5m + (1\u2212\u03b2\u2081)\u22c5g\n    v\u209c   = \u03b2\u2082\u22c5v + (1\u2212\u03b2\u2082)\u22c5g\u00b2\n    m\u0302\u209c  = m\u209c / (1\u2212\u03b2\u2081\u1d57)\n    v\u0302\u209c  = v\u209c / (1\u2212\u03b2\u2082\u1d57)\n    \u03b8\u2032   = \u03b8 \u2212 \u03b1 \u00b7 m\u0302\u209c /(\u221av\u0302\u209c+\u03b5)\n\nreturn the **updated parameter \u03b8\u2032 together with the new moment estimates m\u209c and v\u209c**.  \nThe function must work with multi-dimensional parameters (any NumPy array shape) and should be fully vectorised.\n\nIf the gradient is exactly zero the parameter must stay unchanged and the moment estimates must still be updated according to the above equations.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef adam_update(\n    param: np.ndarray,\n    grad: np.ndarray,\n    t: int,\n    m: np.ndarray,\n    v: np.ndarray,\n    lr: float = 0.001,\n    beta1: float = 0.9,\n    beta2: float = 0.999,\n    eps: float = 1e-8,\n) -> tuple[list, list, list]:\n    \"\"\"Perform **one** Adam optimisation step.\n\n    Args:\n        param: Current value of the parameter \u03b8 (NumPy array).\n        grad:  Current gradient \u2207\u03b8 (same shape as *param*).\n        t:     Time-step counter **starting at 1**.\n        m:     First moment estimate from the previous step (same shape).\n        v:     Second moment estimate from the previous step (same shape).\n        lr:    Learning rate \u03b1 (default 0.001).\n        beta1: Exponential decay rate for the first moment (default 0.9).\n        beta2: Exponential decay rate for the second moment (default 0.999).\n        eps:   Small constant to avoid division by zero (default 1e-8).\n\n    Returns:\n        Tuple containing (updated_parameter, new_moment, new_variance) **as\n        python lists**, each rounded to 8 decimal places.\n    \"\"\"\n\n    # TODO: complete this function\n    pass\n```\n# Output Constraints:\nReturn a tuple of three python lists rounded to **8** decimal places:  \n(updated_param, new_m, new_v)\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 343, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Cross-Entropy Parameter Update\n# Description:\nImplement the core numerical step of the Cross-Entropy Method (CEM) that is often used to search for good policy parameters in Reinforcement Learning.\n\nYou are given\n1. `theta_samples`: an $N\\times D$ NumPy array whose rows are the $N$ different parameter vectors (\\theta) that were evaluated in the current episode.\n2. `rewards`: a one-dimensional array-like object of length $N$ containing the total return obtained with each corresponding parameter vector.\n3. `retain_prcnt`: a float in the open interval $(0,1]$ indicating which fraction of the best\u2010scoring samples should be kept when updating the sampling distribution.\n\nYour task is to write a function that\n\u2022 keeps the top `retain_prcnt` fraction of `theta_samples` according to `rewards`,  \n\u2022 computes the **mean** and the **per-dimension variance** of those retained samples,  \n\u2022 returns the two vectors (mean, variance) as Python lists rounded to four decimal places.\n\nIf `retain_prcnt * N` is not an integer, use `int(retain_prcnt * N)` (the floor of the product) to decide how many samples to retain.  \nThe input is always valid (there will always be at least one sample to retain).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef cross_entropy_update(theta_samples: np.ndarray,\n                          rewards: list[float] | np.ndarray,\n                          retain_prcnt: float) -> tuple[list[float], list[float]]:\n    \"\"\"Compute the updated mean and variance for CEM.\n\n    Parameters\n    ----------\n    theta_samples : np.ndarray\n        2-D array of shape (N, D) containing N sampled parameter vectors.\n    rewards : list | np.ndarray\n        Sequence of length N with the return obtained by each sample.\n    retain_prcnt : float\n        Fraction (0, 1] \u2013 what portion of the best samples to keep.\n\n    Returns\n    -------\n    tuple[list, list]\n        Two Python lists containing the per-dimension mean and variance of the\n        retained (elite) samples, rounded to four decimals.\n    \"\"\"\n    # TODO: implement this function\n    pass\n```\n# Output Constraints:\nAll returned numbers must be rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 353, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Shannon Entropy of Class Labels\n# Description:\nIn a decision\u2013tree learning algorithm the **Shannon entropy** is used to measure the impurity (disorder) of a set of class labels.  \n\nWrite a Python function `entropy` that receives a one-dimensional sequence of class labels (either a Python list, a NumPy array or any iterable) and returns the base-2 Shannon entropy of the label distribution.\n\nDefinition\nEntropy \\(H\\) of a discrete distribution with probabilities \\(p_i\\) is defined as  \n\\[H = -\\sum_i p_i \\log_2 p_i\\]\\\nBy convention the contribution of a label that never occurs (\\(p_i = 0\\)) is taken as \\(0\\), because \\(\\lim_{p \\to 0^+} p\\log p = 0\\).\n\nSpecial cases\n1. If the input is empty, return **0.0**.\n2. If all labels are identical, the entropy is **0.0**.\n\nReturn value\nReturn the entropy rounded to **5 decimal places**.\n# Your code should start with:\n```python\nimport math\nfrom collections import Counter\nimport numpy as np\ndef entropy(labels) -> float:\n    \"\"\"Compute the base-2 Shannon entropy of a 1-D sequence of class labels.\n\n    Args:\n        labels: A one-dimensional iterable (list, NumPy array, etc.) of hashable\n            class labels.\n\n    Returns:\n        The entropy value rounded to five decimal places.\n    \"\"\"\n    # TODO: implement this function\n    pass\n```\n# Output Constraints:\nRound the final entropy to the nearest 5th decimal place using `round(value, 5)` before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 354, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Fast 2-D Convolution via im2col\n# Description:\nImplement a high-level routine that performs the 2-D convolution (technically, cross-correlation) between a batch of images and a bank of kernels by means of the classic *im2col + GEMM* strategy.\n\nThe function must accept\n1. a 4-D NumPy array X of shape `(n_ex, in_rows, in_cols, in_ch)` containing *n_ex* examples, each with *in_ch* input channels,\n2. a 4-D NumPy array W of shape `(kernel_rows, kernel_cols, in_ch, out_ch)` \u2013 one kernel per output channel,\n3. an integer **stride** `s`,\n4. a padding specification **pad** that can be one of the following:\n   \u2022 an integer \u2192 the same number of zero rows/columns is added on every side,\n   \u2022 a 2-tuple `(pr, pc)` \u2192 `pr` rows are added to both the top and bottom and `pc` columns to both the left and right,\n   \u2022 a 4-tuple `(pr1, pr2, pc1, pc2)` \u2192 rows/columns are added individually to the top, bottom, left and right,\n   \u2022 the string `'same'` \u2192 the smallest symmetric padding that makes the spatial output size identical to the input size,\n5. an optional integer **dilation** `d` that specifies how many empty pixels have to be inserted between the kernel elements (`d = 0` \u21d2 normal convolution).\n\nThe routine must return the convolution result as a NumPy array of shape `(n_ex, out_rows, out_cols, out_ch)` **converted to a (deep) Python list via** `tolist()`.\n\nAll computations must be carried out with NumPy only \u2013 **no third-party deep-learning libraries are allowed**.  If the padding specification is invalid the function behaviour is undefined (no need to raise an exception).\n# Your code should start with:\n```python\nimport numpy as np\ndef conv2D(X: \"np.ndarray\", W: \"np.ndarray\", stride: int, pad, dilation: int = 0):\n    \"\"\"Performs a 2-D convolution (cross-correlation).\n\n    Args:\n        X: NumPy array of shape ``(n_ex, in_rows, in_cols, in_ch)`` representing the input batch.\n        W: NumPy array of shape ``(kernel_rows, kernel_cols, in_ch, out_ch)`` containing the kernels.\n        stride: Stride of the convolution.\n        pad: Padding specification \u2013 integer, tuple or the string ``'same'``.\n        dilation: Optional dilation factor. ``0`` corresponds to a normal convolution.\n\n    Returns:\n        The convolution result **as a Python list** obtained through ``numpy.ndarray.tolist()``.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the final NumPy array as a pure Python (nested) list using ndarray.tolist().\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 355, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: PCA with Deterministic Sign Fix\n# Description:\nImplement Principal Component Analysis (PCA) with two possible solvers (``svd`` \u2013 singular value decomposition, and ``eigen`` \u2013 eigen-decomposition of the covariance matrix).  \nYour function must:\n1. Standardise the data by subtracting the feature-wise mean (mean centring).\n2. Depending on the chosen solver, obtain the principal directions (eigen-vectors) \u2013  \n   \u2022 ``svd``  : use *numpy.linalg.svd* on the centred data.  \n   \u2022 ``eigen``: compute the sample covariance matrix *(rowvar=False, ddof=1)* and run *numpy.linalg.eigh* (because the matrix is symmetric) on it.\n3. Sort the directions in descending order of their importance (variance they explain) and keep the first ``n_components`` of them.\n4. Make the sign of every kept direction deterministic: if the first non-zero loading of a direction is negative, multiply the whole direction by \u22121 (and do the same with the corresponding column of the projected data).  \n   This removes the usual PCA sign ambiguity and guarantees identical results on every run \u2013 which is essential for the unit tests.\n5. Project the centred data onto the retained directions (the score matrix).\n6. Return a tuple ``(scores, explained_variance_ratio)`` where  \n   \u2022 *scores* is the projection matrix rounded to 4 decimals and converted to a *list of lists*;  \n   \u2022 *explained_variance_ratio* is a *list* containing the fraction of total variance explained by each selected component, rounded to 4 decimals.\n\nIf *n_components* is larger than the original feature dimension, simply keep all available components.\n\nIn every step round only the final results (not the intermediate calculations).\n# Your code should start with:\n```python\nimport numpy as np\ndef pca_transform(data: np.ndarray, n_components: int, solver: str = \"svd\") -> tuple[list[list[float]], list[float]]:\n    \"\"\"Perform Principal Component Analysis (PCA) on *data*.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array in which rows correspond to samples and columns to features.\n    n_components : int\n        Number of principal components to retain (must be \u22651).\n    solver : str, optional (default=\"svd\")\n        ``\"svd\"`` to use singular value decomposition or ``\"eigen\"`` to use\n        eigen-decomposition of the covariance matrix.\n\n    Returns\n    -------\n    tuple[list[list[float]], list[float]]\n        \u2022 The projected data (scores) as a list of lists \u2013 each inner list is a\n          sample represented in the new sub-space.\n        \u2022 The list of explained variance ratios corresponding to the kept\n          components.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every numeric entry to the nearest 4th decimal before converting to Python built-ins.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 356, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Leaf Node Prediction\n# Description:\nIn many tree-based learning algorithms, every terminal (leaf) node stores the outcome that should be returned once a sample reaches that node.  \n\n\u2022 For a **classification** tree, the leaf usually keeps an array *p* of class\u2013membership probabilities.  The predicted class is the index of the largest probability in *p* (ties are resolved in favour of the smaller index, just as `numpy.argmax` would do).  \n\u2022 For a **regression** tree, the leaf simply stores the scalar mean of the target values that fell into that region of the space.\n\nYour task is to finish the helper function `leaf_predict` that extracts the correct prediction from a `Leaf` instance.\n\nIf `classifier` is `True` you must return the class index (an `int`).  Otherwise return the raw scalar (a `float` or `int`).\n# Your code should start with:\n```python\nclass Leaf:\n    \"\"\"Simple container class for a tree leaf.\n\n    Args:\n        value: Either a list/tuple of class probabilities (for classification)\n               or a scalar representing the regional mean (for regression).\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n\ndef leaf_predict(leaf: \"Leaf\", classifier: bool):\n    \"\"\"Return the prediction stored in a decision-tree leaf.\n\n    Args:\n        leaf: A `Leaf` object whose `value` attribute is either a sequence of\n              class probabilities (classification) or a single number\n              (regression).\n        classifier: When *True* treat the leaf as belonging to a\n                     classification tree; otherwise treat it as regression.\n\n    Returns:\n        int | float: Predicted class index for classification; otherwise the\n                     raw scalar stored in the leaf.\n    \"\"\"\n    # TODO: complete this function\n    pass\n```\n# Output Constraints:\nReturn an `int` when `classifier` is `True`; otherwise return the number stored in the leaf (no rounding necessary).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 357, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: K-Means Clustering\n# Description:\nImplement the K\u2013Means clustering algorithm from scratch.  \nGiven a 2-D (or higher) NumPy array **data** containing *n* samples (rows) and *d* features (columns) together with an integer **K**, partition the samples into **K** clusters so that each sample belongs to the cluster with the nearest (Euclidean) centroid.  \nThe procedure is as follows:\n1. Initialise the *K* centroids with the first **K** samples in the data matrix (this makes the algorithm fully deterministic and easy to test).\n2. Repeat at most **max_iters**=100 times:\n   \u2022 Assign every sample to the closest centroid (use the ordinary Euclidean distance).\n   \u2022 Recompute every centroid as the mean of the samples currently assigned to it. If a centroid loses all its samples, keep it unchanged for that iteration.\n   \u2022 Stop early if none of the centroids changes any more (within a tolerance of 1 \u00d7 10\u207b\u2076).\n3. Sort the final centroids lexicographically (by the first feature, then the second, etc.), round every coordinate to four decimals, and return them as a plain Python *list of lists*.\n\nIf **K** equals 1 the single centroid is simply the mean of the complete data set.  \nThe function must work for any dimensionality \u2265 1.\n\nExample\n=======\nInput\n```\ndata  = np.array([[1, 1],\n                  [1, 2],\n                  [2, 1],\n                  [8, 8],\n                  [9, 8],\n                  [8, 9]])\nK     = 2\n```\n\nOutput\n```\n[[1.3333, 1.3333], [8.3333, 8.3333]]\n```\n\nReasoning\n---------\nThe first two rows `[[1,1],[1,2]]` are used as the initial centroids.  \nAfter two iterations the samples `[1,1]`, `[1,2]`, `[2,1]` form one cluster with centroid `(4/3, 4/3) = (1.3333, 1.3333)` and the samples `[8,8]`, `[9,8]`, `[8,9]` form the other cluster with centroid `(25/3, 25/3) = (8.3333, 8.3333)`.  \nNo centroid moves any further, so the algorithm stops and the centroids (already sorted) are returned.\n# Your code should start with:\n```python\nimport numpy as np\ndef kmeans(data: np.ndarray, K: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Cluster *data* into *K* groups using the K\u2013Means algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features) representing the data to\n        be clustered.\n    K : int\n        The number of clusters (centroids) to find.\n    max_iters : int, optional (default=100)\n        Maximum number of iterations before stopping.\n\n    Returns\n    -------\n    list[list[float]]\n        The sorted list of centroids rounded to four decimals.\n    \"\"\"\n    # Write your code below. Remove the pass statement.\n    pass\n```\n# Output Constraints:\nReturn a Python list of lists.  All coordinates must be rounded to the nearest 4th decimal.  Centroids must be sorted lexicographically.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 362, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Row-wise Stochastic Matrix Normalisation\n# Description:\nIn many probabilistic and numerical applications you need a stochastic matrix \u2013 a matrix whose rows each sum to 1.  \nWrite a Python function that converts an arbitrary 2-D list (or NumPy array) of non\u2013negative numbers into a row-stochastic matrix.  \nThe function must:\n\n1. Accept the data structure and convert it to a float NumPy array.\n2. Compute the sum of every row.\n3. If *any* row sums to 0 (making normalisation impossible) return **-1**.\n4. Otherwise divide every element in a row by that row\u2019s sum.\n5. Round every entry of the resulting matrix to **4 decimal places** and return it as a native Python list using ``tolist()``.\n# Your code should start with:\n```python\nimport numpy as np\ndef row_stochastic_matrix(data: list[list[int | float]] | \"np.ndarray\") -> list[list[float]]:\n    \"\"\"Convert a numeric 2-D structure into a row-stochastic matrix.\n\n    Each row must sum to 1 after transformation.  If a row has a sum of\n    0 the function should return -1.\n\n    Parameters\n    ----------\n    data : list[list[int | float]] | np.ndarray\n        The 2-D input data.\n\n    Returns\n    -------\n    list[list[float]]\n        The row-normalised matrix rounded to 4 decimals or -1 when\n        normalisation is impossible.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nAll values must be rounded to the nearest 4th decimal and the result returned as a Python list of lists (not a NumPy array).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 363, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Detect Continuity of RL Spaces\n# Description:\nIn many reinforcement-learning (RL) libraries (e.g., OpenAI Gym) environments expose two special attributes: `env.action_space` and `env.observation_space`.  \nEach of those *spaces* describes the kind of data the agent will send (actions) or receive (observations).  \nFor the purposes of this task we consider four toy space types that mimic the most common Gym classes:\n\n\u2022 `Box`      \u2013 **continuous** (vector of floats)\n\u2022 `Discrete` \u2013 **non-continuous** (single integer)\n\u2022 `Tuple`    \u2013 an ordered collection of other spaces\n\u2022 `Dict`     \u2013 a mapping whose values are spaces\n\nImplement a helper function `is_continuous` that, given an environment object and two Boolean flags, decides whether each space is continuous:\n\n\u2022 `tuple_action` is `True` when `env.action_space` is known to be a `Tuple` **or** a `Dict`.  In that case the action space is considered continuous *only if **every** sub-space is a `Box`*.\n\u2022 `tuple_obs`   is `True` when `env.observation_space` is known to be a `Tuple` **or** a `Dict`.  In that case the observation space is continuous only if every sub-space is a `Box`.\n\u2022 When the corresponding flag is `False` we simply check whether the space itself is a `Box`.\n\nReturn a pair `(cont_action, cont_obs)` where each element is a Boolean.\n\nA tiny, self-contained imitation of Gym\u2019s space classes (`Box`, `Discrete`, `Tuple`, `Dict`) and a minimal `Env` wrapper are provided in the starter code so **no external libraries are needed**.\n# Your code should start with:\n```python\nfrom typing import Any, Dict, Iterable, Tuple as PyTuple\n\n# ------------------  Minimal imitation of Gym spaces (do not remove)  ------------------\nclass Space:  # abstract base class\n    pass\n\nclass Box(Space):\n    def __init__(self, low: float, high: float, shape: PyTuple[int, ...]):\n        self.low = low\n        self.high = high\n        self.shape = shape\n\nclass Discrete(Space):\n    def __init__(self, n: int):\n        self.n = n\n\nclass Tuple(Space):\n    def __init__(self, spaces: Iterable[Space]):\n        self.spaces = tuple(spaces)\n\nclass Dict(Space):\n    def __init__(self, spaces: Dict[str, Space]):\n        self.spaces = dict(spaces)\n\nclass Env:\n    \"\"\"Tiny environment that only stores two spaces.\"\"\"\n    def __init__(self, action_space: Space, observation_space: Space):\n        self.action_space = action_space\n        self.observation_space = observation_space\n\n# ----------------------------  Complete this function  ----------------------------\ndef is_continuous(env: Env, tuple_action: bool, tuple_obs: bool):\n    \"\"\"Determine whether the given environment's spaces are continuous.\n\n    A space is *continuous* if it is an instance of `Box`. For composite spaces\n    (`Tuple` or `Dict`) the space is continuous only if **all** its sub-spaces\n    are `Box`.\n\n    Args:\n        env:          Environment exposing `action_space` and `observation_space`.\n        tuple_action: Whether the *action* space is composite.\n        tuple_obs:    Whether the *observation* space is composite.\n\n    Returns:\n        A tuple `(cont_action, cont_obs)` of booleans.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nReturn a *tuple* `(cont_action, cont_obs)` consisting of two booleans.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 369, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gradient Boosting Prediction Aggregation\n# Description:\nGradient Boosting models aggregate the outputs of many weak learners (usually decision trees) by adding the learners\u2019 outputs to an ever-improving **running prediction**.  During *inference* the running prediction starts at 0 and each tree\u2019s output is **subtracted** after being scaled by a constant learning rate. \n\nFor **regression** this running prediction is the final numerical output.  \nFor **classification** (multi-class) the running prediction is interpreted as un-normalised log-probabilities (logits).  These logits are first converted to a probability distribution with the soft-max function and then turned into the final class labels via `argmax`.\n\nWrite a function `gradient_boosting_predict` that reproduces this aggregation behaviour.\n\nFunction requirements\n1. `updates` \u2013 *list* of NumPy arrays produced by the individual trees.  All arrays have the same shape:  \n   \u2022 regression\u2003shape = `(n_samples,)`  \n   \u2022 classification\u2003shape = `(n_samples, n_classes)`\n2. `learning_rate` \u2013 positive float that scales every tree\u2019s output.\n3. `regression` \u2013 boolean.  If `True` perform regression, otherwise multi-class classification.\n\nComputation rules\n\u2022 Start with a running prediction filled with zeros having the same shape as a single update array.  \n\u2022 For every tree update `u` do `running_pred -= learning_rate * u`.  \n\u2022 After all updates\n  \u2013 Regression\u2003\u2192\u2003return `running_pred`, rounded to 4 decimals.  \n  \u2013 Classification\u2003\u2192\u2003apply the soft-max row-wise to obtain class probabilities, then return the vector of predicted class indices (`argmax`).\n\nThe function must be fully vectorised (no Python loops over individual samples) and must only rely on NumPy.\n# Your code should start with:\n```python\nimport numpy as np\ndef gradient_boosting_predict(updates: list[np.ndarray], learning_rate: float, regression: bool) -> np.ndarray:\n    \"\"\"Aggregate the outputs of Gradient Boosting trees.\n\n    Parameters\n    ----------\n    updates : list[np.ndarray]\n        Each element is a NumPy array containing the predictions of one tree\n        for **all** samples.  For regression the array shape is\n        ``(n_samples,)``; for classification it is ``(n_samples, n_classes)``.\n    learning_rate : float\n        The learning-rate hyper-parameter used during training.  Every tree\u2019s\n        output is multiplied by this value before aggregation.\n    regression : bool\n        Set ``True`` for regression problems and ``False`` for multi-class\n        classification problems.\n\n    Returns\n    -------\n    np.ndarray\n        \u2022 Regression \u2013 1-D array of floats, rounded to 4 decimals.  \n        \u2022 Classification \u2013 1-D array of integers representing the predicted\n          class labels.\n    \"\"\"\n    pass\n```\n# Output Constraints:\n\u2022 For regression return a 1-D NumPy array of floats rounded to 4 decimal places.\n\u2022 For classification return a 1-D NumPy array (or list) of integers (predicted class labels).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 371, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: 1-D Convolution with Stride, Padding and Dilation\n# Description:\nImplement a 1-D cross-correlation (commonly referred to as a convolution in Deep-Learning literature) between a batch of 1-D, multi-channel signals and a bank of kernels.\n\nThe function has to support\n\u2022 batches of examples\n\u2022 an arbitrary number of input and output channels\n\u2022 strides\n\u2022 zero padding that can be supplied as\n  \u2013 a single integer (add the same amount left and right)\n  \u2013 a 2-tuple (\\(p_{left}, p_{right}\\))\n  \u2013 the string \"same\" that mimics TensorFlow\u2019s **SAME** rule, i.e.\\[1\\]\n      out_len = ceil(l_in / stride)\n      total_pad = max(0, (out_len \u2212 1)\u00b7stride + effective_kernel \u2212 l_in)\n      p_left = \u230atotal_pad / 2\u230b , p_right = total_pad \u2212 p_left\n\u2022 dilation (number of zeros inserted between kernel elements \u2013 give 0 for the usual convolution)\n\nThe operation to perform is a *cross-correlation*, **not** a mathematical convolution, i.e. the kernel is **not** reversed.\n\nReturn the resulting 3-D volume as a regular Python list (use ndarray.tolist()).\n# Your code should start with:\n```python\nimport numpy as np\ndef conv1D(X: np.ndarray, W: np.ndarray, stride: int, pad, dilation: int = 0) -> list:\n    \"\"\"Perform a 1-D cross-correlation between *X* and *W*.\n\n    The function must support batches, multiple input/output channels, padding\n    (integer, tuple or \"same\"), arbitrary stride and dilation.  It should return\n    the output volume as *list* obtained via ``ndarray.tolist()``.\n\n    Args:\n        X: ndarray of shape (n_examples, signal_length, in_channels)\n        W: ndarray of shape (kernel_width, in_channels, out_channels)\n        stride: positive integer, the step size of the sliding window\n        pad:  int, 2-tuple, or the string \"same\" specifying the amount of zero\n              padding to add to the left and right of the signal\n        dilation: non-negative integer, number of points inserted between\n                   neighbouring kernel elements (0 \u21d2 standard convolution)\n\n    Returns:\n        A Python list representing the convolved volume with shape\n        (n_examples, output_length, out_channels).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the result as a (nested) Python list via ndarray.tolist().\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 373, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gini Impurity Calculation\n# Description:\nWrite a Python function that calculates the Gini impurity of a discrete label sequence.  \nThe Gini impurity is a measure used in decision-tree learning to quantify how often a randomly chosen element from the set would be incorrectly labelled if it were randomly labelled according to the distribution of labels in the subset.  \nFor a label vector $y\\,(y_1,\\dots,y_N)$ that contains integer class indices, the Gini impurity is defined as\n$$\nGini = 1-\\sum_{c=0}^{C-1} p_c^{\\,2},\n$$\nwhere $p_c = \\frac{n_c}{N}$ is the relative frequency of class $c$, $n_c$ is the number of samples having class $c$, $C$ is the number of distinct classes, and $N$ is the total number of samples.  \nIf the input sequence is empty, return $0.0$ by convention.\n# Your code should start with:\n```python\nimport numpy as np\ndef gini(y: list[int] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Gini impurity of a sequence of integer labels.\n\n    Args:\n        y: A one-dimensional list or NumPy array containing integer class labels.\n\n    Returns:\n        The Gini impurity of *y*, rounded to 4 decimal places. If *y* is empty\n        an impurity of 0.0 is returned.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 374, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Linear Regression with Batch Gradient Descent\n# Description:\nWrite a Python function that trains a **multiple linear regression** model with *batch gradient descent*.  \nGiven a feature matrix X\u2208\u211d^{m\u00d7n} and a target vector y\u2208\u211d^{m}, the goal is to minimize the mean-squared error\n\n\u2003\u2003MSE(\u03b8,b)=1\u2044m\u2006\u2211_{i=1}^{m}(y\u0302_i\u2212y_i)^2,\n\nwhere y\u0302 = X\u03b8+b\u00b71 and \u03b8 is the weight vector, b is the bias (intercept).\n\nThe function must:\n1. Initialise \u03b8 (n zeros) and b (0).\n2. For *n_iterations* steps perform the gradient descent updates\n\u2003\u2003dw = 2/m \u00b7 X\u1d40\u00b7(y\u0302\u2212y),   db = 2/m \u00b7 \u03a3(y\u0302\u2212y)  \n\u2003\u2003\u03b8  \u2190 \u03b8  \u2212 learning_rate\u00b7dw  \n\u2003\u2003b  \u2190 b  \u2212 learning_rate\u00b7db\n3. Return the learned parameters rounded to 4 decimal places.\n\nIf the input data are inconsistent (different number of samples in X and y) return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef linear_regression_gd(X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01, n_iterations: int = 10000) -> tuple[list[float], float]:\n    \"\"\"Train a multiple linear regression model using batch gradient descent.\n\n    Your task is to implement this function following the specification\n    provided in the problem statement.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a tuple (weights_list, bias).  \nAll returned numbers must be rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 376, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Symmetry Check for Square Matrices\n# Description:\nWrite a Python function that determines whether a given 2-D numeric array is symmetric with respect to its main diagonal.  The array is considered symmetric if it is square and the entry at row i and column j is equal to the entry at row j and column i for every valid index pair (i, j).  Because floating-point numbers can suffer from rounding errors, treat two numbers *a* and *b* as equal if their absolute difference does not exceed 1 \u00d7 10\u207b\u2078 (use NumPy\u2019s `allclose`).\n\nIf the input array is not square the function must return **False**.\n# Your code should start with:\n```python\nimport numpy as np\ndef is_symmetric(X: list[list[int | float]]) -> bool:\n    \"\"\"Check whether the given 2-D array *X* is symmetric.\n\n    A matrix is symmetric if it is square and equal to its own transpose.\n\n    Args:\n        X: Matrix represented as a list of lists containing numbers.\n\n    Returns:\n        True if the matrix is symmetric, otherwise False.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a built-in Python boolean (`True` or `False`).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 377, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Negative Gradient for Logistic Loss\n# Description:\nImplement the negative gradient that Gradient Boosting uses when optimizing the logistic (binomial deviance) loss for binary classification.\n\nFor every sample the true label y\u1d62 is encoded as 0 or 1 while the current model prediction f\u1d62 can be any real number.  Gradient Boosting internally converts the labels to the set {\u22121, 1} using the rule y\u2032 = 2y \u2212 1 and minimises the logistic loss\n\n    L(y\u2032, f) = log(1 + exp(\u22122 y\u2032 f)).\n\nThe **negative** gradient of L with respect to f (the value added to the residuals in the next boosting iteration) is\n\n    g\u1d62 = y\u2032\u1d62 / (1 + exp(y\u2032\u1d62 f\u1d62)).\n\nWrite a function that receives two one-dimensional arrays (or Python lists)\n    \u2022 y \u2013 binary class labels (0 or 1)\n    \u2022 f \u2013 current prediction scores (floats)\n\nand returns the list of negative gradients g rounded to four decimal places.\n\nIf the label array contains values other than 0 or 1, return -1.\n# Your code should start with:\n```python\nimport numpy as np\ndef logistic_negative_gradient(y: list, f: list) -> list:\n    \"\"\"YOUR DOCSTRING HERE\"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list with each value rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 380, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement SELU Activation and Derivatives\n# Description:\nImplement the Scaled Exponential Linear Unit (SELU) activation together with its first and second analytical derivatives.\n\nThe SELU function is defined as\n\nSELU(x) = scale \u00b7 ELU(x, \u03b1)\n\nwhere ELU(x, \u03b1) = x                if x > 0\n                               \u03b1(e\u02e3 \u2013 1)  otherwise\n\nThe recommended constants (from the original paper) are\n\u03b1 = 1.6732632423543772848170429916717\nscale = 1.0507009873554804934193349852946\n\nFor a given numeric input or NumPy array *x* and an integer *order*, your task is to return:\n\u2022 order = 0  \u2192  SELU(x)\n\u2022 order = 1  \u2192  \u2202SELU/\u2202x (first derivative)\n\u2022 order = 2  \u2192  \u2202\u00b2SELU/\u2202x\u00b2 (second derivative)\n\nThe function must work for scalars, 1-D or multi-D arrays and always preserve the input shape.  All results have to be rounded to the nearest 4\u1d57\u02b0 decimal and converted to built-in Python lists via NumPy\u2019s `tolist()` method.\n# Your code should start with:\n```python\nimport numpy as np\ndef selu(x, order: int = 0):\n    \"\"\"Compute the SELU activation or its derivatives.\n\n    Parameters\n    ----------\n    x : float | list | np.ndarray\n        Input value(s). Can be a scalar, 1-D list/array, or multi-D list/array.\n    order : int, default 0\n        0 \u2192 SELU(x)\n        1 \u2192 first derivative d(SELU)/dx\n        2 \u2192 second derivative d\u00b2(SELU)/dx\u00b2\n\n    Returns\n    -------\n    list | float\n        A Python list (or scalar for scalar input) containing the element-wise\n        result, rounded to 4 decimals.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every element to 4 decimal places and return the result via NumPy\u2019s `tolist()` method.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 387, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gradient Boosting with One-Dimensional Stumps\n# Description:\nImplement a very small sized Gradient Boosting Regressor that works on ONE numerical feature only.  \n\nFor every boosting round the algorithm must build a decision **stump** (a depth-1 regression tree): it chooses one split\u2010point on the x\u2013axis that minimises the **sum of squared residuals** on both sides of the split.  The procedure for a data set (x,\u2006y) containing N samples is as follows:\n\n1. Let the current prediction for every sample be the mean of the targets, y\u0302\u207d\u2070\u207e\n2. Repeat `n_estimators` times\n   \u2022 Compute the residuals r\u1d62 = y\u1d62 \u2212 y\u0302\u1d62 (these are the negative gradients of the squared-error loss).\n   \u2022 Sort the samples by their x value and evaluate every possible split that lies halfway between two **different** consecutive x values.  For each candidate split t define the stump prediction\n        r\u0302\u1d62(t) = \\begin{cases} \\bar r_L & \\text{if } x\u1d62 \\le t \\\\ \\bar r_R & \\text{otherwise}\\end{cases}\n     where \\bar r_L and \\bar r_R are the mean residuals on the left and right side of the split.  Choose the t that yields the smallest sum of squared errors.\n   \u2022 If all feature values are identical (no valid split) the stump predicts the **global** mean residual for every point.\n   \u2022 Update the running prediction\n        y\u0302\u1d62 \u2190 y\u0302\u1d62 + learning_rate \u00d7 r\u0302\u1d62(t\\*)\n3. Return the final y\u0302 rounded to four decimals and as a Python list.\n\nYou only need `numpy`; no external ML libraries are allowed.\n# Your code should start with:\n```python\nimport numpy as np\ndef gradient_boosting_1d(x: list[float], y: list[float], n_estimators: int, learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Train a 1-D gradient boosting regressor made of decision stumps.\n\n    Args:\n        x: A list with a single numeric feature for every sample.\n        y: Target values.\n        n_estimators: Number of boosting rounds.\n        learning_rate: Shrinkage applied to every stump (\u03b7).\n\n    Returns:\n        In-sample predictions rounded to 4 decimals and converted to a Python list.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python `list`, rounding every entry to 4 decimal places (use `numpy.round(pred, 4)`).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 394, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Implement RMSprop Optimiser Update Step\n# Description:\nRMSprop is one of the most popular adaptive-learning-rate optimisation algorithms used when training neural networks.  \nIn a single update step the algorithm keeps a running (exponentially decaying) average of the squared gradients and scales the learning rate of every parameter by the inverse square-root of this average.  \n\nGiven the current parameter vector $w$, its gradient $g=\\nabla_w\\mathcal{L}$, the previous running average $E_g$ (which may be `None` if it has not been initialised yet), a learning rate $\\alpha$ and the decay rate $\\rho$, implement one RMSprop update step.\n\nMathematically the update is:\n\n\\[\nE_g^{(t)} = \\rho\\,E_g^{(t-1)} + (1-\\rho)\\,g^{2},\\quad\nw^{(t)} = w^{(t-1)} - \\frac{\\alpha\\,g}{\\sqrt{E_g^{(t)} + \\varepsilon}},\n\\]\n\nwhere $\\varepsilon$ is a small constant (here fixed to $10^{-8}$) added for numerical stability.\n\nYour function must:\n1. Initialise `E_g` with zeros (same shape as the gradient) if it is `None`.\n2. Perform the update exactly as specified above.\n3. Round both the updated parameter vector and the new running average to **4 decimal places** and convert them to regular Python lists before returning.\n\nReturn both the updated parameters **and** the updated running average.\n\nIf the gradient is a multi-dimensional array the operation is applied element-wise.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef rmsprop_update(w: np.ndarray,\n                   grad: np.ndarray,\n                   Eg: np.ndarray | None = None,\n                   learning_rate: float = 0.01,\n                   rho: float = 0.9) -> tuple[list, list]:\n    \"\"\"Perform one update step of the RMSprop optimiser.\n\n    Parameters\n    ----------\n    w : np.ndarray\n        Current parameter values.\n    grad : np.ndarray\n        Gradient of the loss with respect to ``w``.\n    Eg : np.ndarray | None, optional\n        Running average of the squared gradients. If *None* a zero array of the\n        same shape as ``grad`` should be used, by default *None*.\n    learning_rate : float, optional\n        Step size (\u03b1), by default 0.01.\n    rho : float, optional\n        Decay rate (\u03c1) controlling the influence of previous squared gradients,\n        by default 0.9.\n\n    Returns\n    -------\n    tuple[list, list]\n        A tuple ``(w_next, Eg_next)`` where both elements are converted to\n        regular Python lists **and** rounded to four decimal places.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nBoth returned arrays must be rounded to 4 decimal places and converted to regular Python lists.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 398, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: ELU Activation and Gradient\n# Description:\nImplement the Exponential Linear Unit (ELU) activation function and its gradient.\n\nThe Exponential Linear Unit is widely used in deep-learning models because it helps the network converge faster and reduces the vanishing-gradient problem.  For a given input $x$ and hyper-parameter $\\alpha>0$\n\nELU(x, \u03b1) = { x,                            if x \u2265 0  \n             { \u03b1( e\u02e3 \u2212 1 ),                if x < 0\n\nThe element-wise derivative is\n\nELU\u2032(x, \u03b1) = { 1,                    if x \u2265 0  \n              { ELU(x, \u03b1) + \u03b1,       if x < 0\n\nWrite a single Python function that\n1. Accepts a one-dimensional Python list or NumPy array of numeric values `x`, a float `alpha` (default 0.1) and a boolean flag `derivative` (default `False`).\n2. When `derivative` is `False` it returns the ELU activation for every element.\n3. When `derivative` is `True` it returns the element-wise gradient.\n\nReturn the result as a Python list with every value rounded to the 4\u1d57\u02b0 decimal place.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef elu(x, alpha: float = 0.1, derivative: bool = False) -> list[float]:\n    \"\"\"Compute the ELU activation function or its gradient.\n\n    Args:\n        x: 1-D list or NumPy array of numbers.\n        alpha: Positive constant that controls the value for negative saturation. Default 0.1.\n        derivative: If ``True`` return the gradient instead of the activation. Default False.\n\n    Returns:\n        List of floats rounded to 4 decimal places representing ELU(x) or ELU'(x).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every element to 4 decimal places and return the result as a Python list.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 411, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Environment Statistics Summary\n# Description:\nIn many reinforcement\u2013learning tutorials we collect **trajectories** \u2013 sequences of actions that were taken and observations that were produced by the environment.  \nFor a quick sanity check it is often useful to look at simple statistics such as\n\u2022 whether the data are multi-dimensional or not,  \n\u2022 whether the values are discrete (integers only) or continuous (contain real numbers),  \n\u2022 how many different values appear in every dimension, etc.\n\nWrite a function `env_stats` that receives two Python lists \u2013 a list with **actions** and a list with **observations** \u2013 and returns an exhaustive dictionary with the statistics listed below.\n\nEach element in the two input lists may be  \n\u2022 a scalar (e.g. `3` or `0.25`) \u2013 meaning a 1-D space, or  \n\u2022 an iterable of scalars (list / tuple / numpy array) \u2013 meaning a multi-dimensional value.\n\nAssume that all elements belonging to the same list have the same dimensionality.\n\nReturned dictionary keys\n\u2022 `tuple_actions` & `tuple_observations` \u2013 `True` if at least one element of the corresponding list is an iterable (list/tuple/numpy array).  \n\u2022 `multidim_actions` & `multidim_observations` \u2013 `True` when the corresponding values have more than one dimension (i.e. length > 1).  \n\u2022 `continuous_actions` & `continuous_observations` \u2013 `True` when at least one value in the flattened collection is a **non-integer float** (e.g. `1.2`).  \n\u2022 `n_actions_per_dim`, `n_obs_per_dim` \u2013 list with the number of **unique** values that appear in every dimension (the order of dimensions is preserved).  \n\u2022 `action_dim`, `obs_dim` \u2013 dimensionality of the action / observation space.  \n\u2022 `action_ids`, `obs_ids` \u2013 in every dimension the sorted list of unique values.\n\nExample\nInput\nactions = [(0, 1), (1, 0), (1, 1)]  \nobservations = [10.0, 11.5, 12.0]\n\nOutput\n{\n  'tuple_actions': True,\n  'tuple_observations': False,\n  'multidim_actions': True,\n  'multidim_observations': False,\n  'continuous_actions': False,\n  'continuous_observations': True,\n  'n_actions_per_dim': [2, 2],\n  'action_dim': 2,\n  'n_obs_per_dim': [3],\n  'obs_dim': 1,\n  'action_ids': [[0, 1], [0, 1]],\n  'obs_ids': [[10.0, 11.5, 12.0]]\n}\n\nReasoning\n\u2022 Every action is a 2-tuple \u21d2 `tuple_actions=True`, `multidim_actions=True`, `action_dim=2`.  \n\u2022 Each observation is a single float \u21d2 `tuple_observations=False`, `multidim_observations=False`, `obs_dim=1`.  \n\u2022 Action values are integers only \u21d2 `continuous_actions=False`.  \n\u2022 Observations contain non-integer floats \u21d2 `continuous_observations=True`.  \n\u2022 In the 1st as well as the 2nd action dimension the unique values are `{0,1}` \u21d2 two unique values per dimension.  \n\u2022 Observation dimension has the unique values `{10.0, 11.5, 12.0}` \u21d2 3 unique values in that dimension.\n# Your code should start with:\n```python\nimport numpy as np\ndef env_stats(actions: list, observations: list) -> dict:\n    \"\"\"Compute statistics for collections of actions and observations.\n\n    The function inspects *actions* and *observations* and returns a dictionary\n    containing information about dimensionality, data type (discrete or\n    continuous) and the unique values appearing in every dimension.\n\n    Parameters\n    ----------\n    actions : list\n        A list with the actions that were taken.  Each element is either a\n        scalar or an iterable of scalars (for multi-dimensional spaces).\n    observations : list\n        A list with the corresponding observations.  Same structural\n        requirements as *actions*.\n\n    Returns\n    -------\n    dict\n        A dictionary with the keys described in the task description.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nReturn a dictionary **exactly** with the keys listed above \u2013 the order of keys does not matter.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 413, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Simplified Gradient Boosting Regression Trees\n# Description:\nImplement a simplified Gradient Boosting Decision Tree (GBDT) regressor from scratch.  The function must:  \n1. Start with an initial prediction equal to the mean of the training targets.  \n2. For each boosting iteration, compute the residuals (negative gradients of the squared\u2013error loss), fit a CART regression tree of limited depth to those residuals, and update the running prediction by adding the tree\u2019s output multiplied by the learning rate.  \n3. After *n_estimators* iterations, return the final prediction for every sample in *X_test*.  \nThe internal regression trees may be implemented only with NumPy (no external libraries).  For simplicity the tree may be binary-splitting, use mean\u2013squared-error as the split criterion, and stop growing when *max_depth* is reached or no further reduction in error is possible.  All returned numbers must be rounded to 4 decimal places and converted to regular Python lists.\n# Your code should start with:\n```python\nimport numpy as np\ndef gbdt_regression(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 10, learning_rate: float = 0.1, max_depth: int = 3) -> list[float]:\n    \"\"\"Gradient Boosting Decision Tree (GBDT) regressor.\n\n    Parameters\n    ----------\n    X_train : np.ndarray\n        2-D array with shape (n_samples, n_features) containing the training\n        features.\n    y_train : np.ndarray\n        1-D array of length n_samples containing the training targets.\n    X_test : np.ndarray\n        2-D array with shape (m_samples, n_features) containing the test\n        features to predict.\n    n_estimators : int, default=10\n        Number of boosting iterations.\n    learning_rate : float, default=0.1\n        Shrinkage factor applied to each tree\u2019s prediction.\n    max_depth : int, default=3\n        Maximum depth of every individual regression tree.\n\n    Returns\n    -------\n    list[float]\n        Predictions for every sample in *X_test*, rounded to 4 decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every predicted value to the nearest 4th decimal and return a Python list.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 416, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Multivariate Gaussian PDF Implementation\n# Description:\nImplement the probability density function (PDF) of a multivariate Gaussian (Normal) distribution without using any third-party libraries such as SciPy.  \n\nGiven\n\u2022 X \u2013 a NumPy array of shape (n_samples, n_features) containing the data points for which the PDF values must be evaluated;  \n\u2022 mean \u2013 the mean vector of the distribution (length n_features);  \n\u2022 cov \u2013 the covariance matrix of shape (n_features, n_features) which must be positive-definite (invertible),  \n\nyou have to return a Python list whose *i-th* element is the PDF value for *X[i]* rounded to four decimal places.  \n\nMathematically the multivariate Gaussian PDF is defined as  \n\n  \\[ p(x) = \\frac{1}{\\sqrt{(2\\pi)^d\\det(\\Sigma)}}\\;\\exp\\Bigl( -\\tfrac12 (x-\\mu)^\\top\\Sigma^{-1}(x-\\mu) \\Bigr) \\]  \n\nwhere *d* is the dimensionality, \\(\\mu\\) is the mean vector and \\(\\Sigma\\) is the covariance matrix.  \n\nIf *X* is provided as a one-dimensional array it must be treated as (n_samples, 1).\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef multivariate_gaussian_pdf(X, mean, cov):\n    \"\"\"YOUR DOCSTRING HERE\"\"\"\n    # TODO: complete this function\n    pass\n```\n# Output Constraints:\nReturn a Python list with every element rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 419, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Bayesian Linear Regression \u2013 MAP Prediction\n# Description:\nImplement Bayesian linear regression with a conjugate Gaussian prior and **known** observation variance.  \n\nGiven a training set `X \u2208 \u211d^{N\u00d7M}` and targets `y \u2208 \u211d^{N}` you must:  \n1. (Optional) add an intercept column of ones to `X` (**and to every `X_new`**) when `fit_intercept=True`.  \n2. Treat the prior on the parameter vector `\u03b2` as  \n   \u03b2 ~ \ud835\udca9( \u03bc, \u03c3\u00b2 V ) where  \n   \u2022 `\u03bc` can be either a scalar (replicated to every dimension) or a vector of length *M* (\u2006*M + 1* when an intercept is fitted).  \n   \u2022 `V` may be  \n     \u2013 a scalar (interpreted as the multiple of the identity),  \n     \u2013 a 1-D array (its values form the diagonal of `V`), or  \n     \u2013 a full, symmetric, positive-definite square matrix.  \n   If `V` is `None` assume the identity matrix.  \n3. Compute the posterior distribution  \n   \u03a3\u209a = ( V\u207b\u00b9 + X\u1d40 X )\u207b\u00b9  \n   \u03bc\u209a = \u03a3\u209a ( V\u207b\u00b9 \u03bc + X\u1d40 y ).  \n   (The observation variance \u03c3\u00b2 cancels out in the MAP estimate, so `\u03c3` is ignored here.)  \n4. For a new design matrix `X_new` return the **MAP predictive mean**  \n   \u0177 = X_new \u03bc\u209a.  \n\nRound every predicted value to **4 decimal places** and return them as a regular Python `list` (not a NumPy array).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef bayesian_linear_regression_pred(\n    X: np.ndarray,\n    y: np.ndarray,\n    X_new: np.ndarray,\n    mu: float | np.ndarray = 0.0,\n    sigma: float = 1.0,\n    V: float | np.ndarray | None = None,\n    fit_intercept: bool = True,\n) -> list[float]:\n    \"\"\"Bayesian linear regression with known variance.\n\n    The function must return the MAP predictive means for every row in\n    `X_new`.  See the task description for full details.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python `list` where every element is rounded to **4 decimal places**.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 423, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Root Mean Squared Logarithmic Error Calculator\n# Description:\nRoot Mean Squared Logarithmic Error (RMSLE) is a common regression evaluation metric that penalizes the ratio between the predicted and the actual target instead of their absolute difference.  Given two equally-sized sequences of non-negative numbers \u2013 the ground-truth values (`actual`) and the model predictions (`predicted`) \u2013 RMSLE is defined as\n\nRMSLE = \u221a( (1/n) \u00b7 \u03a3 ( log(1 + predicted\u1d62) \u2212 log(1 + actual\u1d62) )\u00b2 ).\n\nWrite a Python function that\n1. validates that both inputs have the same length and contain only non-negative numbers; if not, **return -1**;\n2. computes the RMSLE according to the formula above;  \n3. returns the result rounded to **four decimal places** (use `round(value, 4)`).\n# Your code should start with:\n```python\nimport numpy as np\ndef root_mean_squared_log_error(actual: list[float], predicted: list[float]) -> float:\n    \"\"\"Stub for the RMSLE metric.  Complete the body of the function.\"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 428, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Hann Window Generator\n# Description:\nCreate a Python function that generates a Hann window (also called the Hanning window) of a specified length.  The Hann window is widely used in digital signal-processing tasks such as short-time Fourier transforms and spectral analysis because its end points smoothly reach zero, reducing spectral leakage.\n\nMathematically, the samples of a symmetric Hann window of length $N$ are\n\nhann(n) = 0.5 - 0.5 * cos( 2 * \u03c0 * n / (N-1) ),   0 \u2264 n < N.\n\nWhen a *periodic* window is required (the case typically used before an FFT), one extra symmetric sample is computed and the last sample is discarded, ensuring continuity between successive, adjacent windows.  This behaviour is controlled by the boolean argument *symmetric*:\n\n\u2022 symmetric = True  \u2192  return a strictly symmetric window of length *window_len*.\n\u2022 symmetric = False \u2192  return a periodic window of length *window_len* (produced by building a symmetric window of length *window_len*+1 and dropping its last entry).\n\nSpecial cases:\n\u2022 If *window_len* \u2264 0  \u2192 return an empty list.\n\u2022 If *window_len* = 1  \u2192 return [1.0] for either value of *symmetric*.\n\nAll numbers in the returned list must be rounded to the nearest 4th decimal place.\n# Your code should start with:\n```python\nimport math\nimport numpy as np\ndef hann(window_len: int, symmetric: bool = False) -> list[float]:\n    \"\"\"Generate a Hann (Hanning) window.\n\n    The function returns *window_len* coefficients of the Hann window, an\n    instance of the general cosine-sum windows where the first and last\n    samples smoothly reach zero.  When *symmetric* is ``True`` a classical\n    symmetric window is generated; when ``False`` the function instead\n    builds a periodic window suitable for FFT analysis.\n\n    Args:\n        window_len (int): Number of samples in the returned window.\n        symmetric (bool, optional): ``True`` for a symmetric window,\n            ``False`` for a periodic window. Defaults to ``False``.\n\n    Returns:\n        list[float]: The Hann window coefficients rounded to 4 decimal\n        places.  Returns an empty list when *window_len* \u2264 0.\n    \"\"\"\n    pass  # Write your code here\n```\n# Output Constraints:\nRound every window coefficient to 4 decimal places and convert the final NumPy array to a regular Python list before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 433, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Xavier Fan-in and Fan-out Calculator\n# Description:\nIn many neural-network initialization schemes (e.g. Xavier/Glorot), two quantities called *fan-in* and *fan-out* are required.  \n\u2022 **fan-in** \u2013 the number of input connections that feed into a weight tensor.  \n\u2022 **fan-out** \u2013 the number of output connections produced by that tensor.  \n\nWrite a function `glorot_fan` that receives a weight-tensor shape (a tuple or list of integers, length \u2265 2) and returns `fan_in` and `fan_out` as **float** values.\n\nRules\n1. If the shape has exactly 4 dimensions it is assumed to be a 2-D convolutional kernel with layout `(out_channels, in_channels, kernel_height, kernel_width)`.\n   \u2022 `receptive_field_size = kernel_height \u00d7 kernel_width` (product of the last two dimensions).\n   \u2022 `fan_in  = in_channels  \u00d7 receptive_field_size`.\n   \u2022 `fan_out = out_channels \u00d7 receptive_field_size`.\n2. For every other tensor (dense layer, embedding matrix, higher-dimensional tensor, \u2026) take the **first** two dimensions directly: `fan_in, fan_out = shape[0], shape[1]`.\n\nReturn the two numbers as a tuple `(fan_in, fan_out)` containing floats.\nIf the supplied shape has fewer than two dimensions the program behaviour is undefined (you may assume the tests always obey the rule).\n# Your code should start with:\n```python\nimport numpy as np\ndef glorot_fan(shape: tuple[int, ...] | list[int, ...]) -> tuple[float, float]:\n    \"\"\"Compute fan-in and fan-out for a given weight-tensor shape.\n\n    Your task is to implement this function following the rules described in\n    the problem statement.\n\n    Args:\n        shape: A tuple or list whose first two elements correspond to the input\n            and output dimensions (for 4-D convolutional kernels the layout is\n            `(out_channels, in_channels, kernel_height, kernel_width)`).\n\n    Returns:\n        A tuple `(fan_in, fan_out)` with both values returned as floats.\n    \"\"\"\n    # Write your code below this line\n    pass\n```\n# Output Constraints:\nReturn a tuple containing two floats: (fan_in, fan_out).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 435, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Tiny Gradient Boosting Regressor\n# Description:\nImplement a very small-scale version of the Gradient Boosting Regressor that uses ordinary least\u2013squares (OLS) linear regression as the weak learner and the squared error as the loss function.  \n\nGiven a training matrix X\u2208\u211d^{m\u00d7d} (m samples, d features) and a target vector y\u2208\u211d^{m}, the procedure works as follows:\n1. Convert X and y to NumPy arrays of type float.\n2. Initialise the current prediction \\(\\hat y^{(0)}\\) with the mean of *y*.\n3. Repeat for *t* = 1 \u2026 *n_estimators*:\n   \u2022 Compute the residuals \\(r^{(t)} = y - \\hat y^{(t-1)}\\).\n   \u2022 Fit an OLS linear model (including an intercept) that predicts the residuals from X.\n   \u2022 Obtain the weak-learner prediction \\(h^{(t)}(X)\\).\n   \u2022 Update the overall prediction\n     \\[\\hat y^{(t)} = \\hat y^{(t-1)} + \\text{learning\\_rate}\\; h^{(t)}(X).\\]\n4. Return the final prediction vector rounded to 4 decimal places and converted to a regular Python list.\n\nSpecial cases\n\u2022 If *n_estimators* \u2264 0 or *learning_rate* = 0, simply return a vector filled with the target mean.\n\nThe task is restricted to the Python standard library plus NumPy.  No classes, exception handling or third-party libraries may be used.\n# Your code should start with:\n```python\nimport numpy as np\ndef gradient_boosting_regressor(X: list[list[float]],\n                               y: list[float],\n                               n_estimators: int = 10,\n                               learning_rate: float = 0.1) -> list[float]:\n    \"\"\"Return the training-set predictions of a tiny Gradient Boosting model.\n\n    The model uses linear regression weak learners and squared-error loss. The\n    algorithm proceeds exactly as described in the task description.  Every\n    returned value must be rounded to 4 decimal places and packed into a plain\n    Python list.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every predicted value to the nearest 4th decimal and return a regular Python list.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 437, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Logistic Sigmoid Function & Derivatives\n# Description:\nImplement a single Python function that evaluates the logistic sigmoid activation function and, optionally, its first or second derivative for every element of the supplied input. The function must work with a scalar, a Python list, or a NumPy ``ndarray``.  \n\nGiven an ``order`` parameter:\n\u2022 ``order = 0`` \u2013 return \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\).\n\u2022 ``order = 1`` \u2013 return the first derivative \\(\\sigma(x)(1-\\sigma(x))\\).\n\u2022 ``order = 2`` \u2013 return the second derivative \\(\\sigma(x)(1-\\sigma(x))(1-2\\sigma(x))\\).\n\nIf an ``order`` other than 0, 1, or 2 is supplied the function must return **-1**.\n\nThe result has to keep the original shape, be rounded to **four decimal places**, and be returned as:\n\u2022 a Python ``float`` when the input is a single scalar,\n\u2022 a Python ``list`` (via ``tolist()``) when the input is a list or ``ndarray``.\n# Your code should start with:\n```python\nimport numpy as np\ndef sigmoid(x, order: int = 0):\n    \"\"\"Compute the logistic sigmoid or its derivatives.\n\n    Args:\n        x (float | int | list | np.ndarray): Input data. Can be a scalar, list, or NumPy array.\n        order (int, optional): 0 = function value, 1 = first derivative,\n            2 = second derivative. Defaults to 0.\n\n    Returns:\n        float | list: Result rounded to 4 decimals. Scalar input returns a float;\n            vector/matrix input returns a Python list preserving the shape.\n            If *order* is not 0, 1, or 2, the function returns -1.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every value to the nearest 4th decimal.\nReturn a Python float for scalar input, otherwise return a Python list using ``tolist()``.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 438, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: One-Hot Encoding Helper \u2013 to_categorical\n# Description:\nImplement a simple version of the famous *to_categorical* helper that converts a vector of class labels into a one-hot (dummy/indicator) matrix.\n\nThe function must accept a 1-D list or NumPy array **y** containing non-negative integer class indices and an optional **num_classes** argument.  \n1. If **num_classes** is *None*, determine it automatically as `max(y) + 1`.  \n2. If **num_classes** is provided but smaller than `max(y) + 1`, return **-1** to indicate that one-hot encoding is impossible.  \n3. Otherwise build a 2-D NumPy array whose *i-th* row is all zeros except for a single 1 at the column that corresponds to the *i-th* label in **y**.  \n4. Finally convert the result to a regular Python list of lists (using ``tolist()``) before returning it.\n\nExamples, constraints and test cases below describe the expected behaviour in detail.\n# Your code should start with:\n```python\nimport numpy as np\ndef to_categorical(y: list[int] | np.ndarray, num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert class labels to one-hot encoding.\n\n    Parameters\n    ----------\n    y : list[int] | np.ndarray\n        A 1-D sequence of non-negative integer class indices.\n    num_classes : int | None, optional\n        Total number of classes. If ``None`` the value is inferred.\n\n    Returns\n    -------\n    list[list[int]]\n        A 2-D list of lists containing the one-hot encoded representation of\n        *y*, or ``-1`` when *num_classes* is smaller than required.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nThe returned value must be a Python *list of lists* containing only integers 0 or 1.  Each inner list must sum to 1 unless the function returns -1 to signal an error.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 439, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Apriori Frequent Itemset Mining\n# Description:\nImplement the Apriori algorithm to discover all frequent itemsets in a transactional database.\n\nGiven a list of transactions (each transaction itself being a list of hashable items) and a minimum support threshold `min_sup` (expressed as a fraction in the range `(0, 1]`), write a function that returns **every** itemset whose empirical support is at least `min_sup`.\n\nThe empirical support of an itemset is defined as\n\n```\n#transactions that contain the itemset / total #transactions\n```\n\nThe implementation must follow the classical **Apriori** breadth-first strategy:\n1. Start with all single-item candidates and keep only those that are frequent.\n2. Repeatedly generate size-`k` candidates by self-joining the frequent itemsets of size `k-1` and pruning any candidate that contains an infrequent subset.\n3. Stop when no new frequent itemsets are found.\n\nReturn the resulting collection of frequent itemsets as a list of tuples.  Inside every tuple the items must appear in **ascending (lexicographic) order**, and the list itself must be ordered first by the length of the itemsets (1-item, 2-item, \u2026) and then lexicographically inside each length block.\n\nIn all situations the function must work with any hashable items (integers, strings, etc.).\n# Your code should start with:\n```python\nimport itertools\ndef apriori_frequent_itemsets(transactions: list[list[int]], min_sup: float) -> list[tuple]:\n    \"\"\"Find frequent itemsets with the Apriori algorithm.\n\n    Parameters\n    ----------\n    transactions : list[list[Hashable]]\n        List of transactions; each transaction is itself a list of items.\n    min_sup : float\n        Minimum support threshold expressed as a fraction (>0 and \u22641).\n\n    Returns\n    -------\n    list[tuple]\n        Frequent itemsets ordered by length and then lexicographically.\n    \"\"\"\n    # TODO: implement the algorithm\n    pass\n```\n# Output Constraints:\nReturn frequent itemsets as a list of tuples ordered by length and then lexicographically. Each tuple must be in ascending order.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 440, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Average Ensemble Probabilities\n# Description:\nIn many ensemble learners such as Random Forest classifiers, each tree (estimator) returns a probability distribution over the classes for every sample.  The overall prediction is obtained by averaging these per-tree probability vectors and then taking the class with the highest average probability.\n\nWrite a function that performs this aggregation.\n\nGiven a three-level nested list `predictions` with shape `(n_estimators, n_samples, n_classes)` where each innermost list represents a valid probability distribution (it sums to 1.0), the function must:\n1. Average the probability vectors over all estimators for every sample.\n2. Round every averaged probability to four decimal places.\n3. Return both the averaged probability matrix **and** the final predicted class label (index of the maximal probability) for every sample.\n\nIf two or more classes share the same maximal probability after rounding, break the tie by returning the smallest index (the default behaviour of `numpy.argmax`).\n# Your code should start with:\n```python\nimport numpy as np\ndef aggregate_predictions(predictions: list[list[list[float]]]) -> tuple[list[list[float]], list[int]]:\n    \"\"\"Aggregate per-tree class probability predictions in a random forest.\n\n    Parameters:\n        predictions: A three-level nested list where the first dimension corresponds to\n            estimators (n_estimators), the second to samples (n_samples) and the third\n            to class probabilities (n_classes). Each innermost list should form a valid\n            probability distribution summing to 1.0.\n\n    Returns:\n        A tuple consisting of:\n            1. A 2-D python list of shape (n_samples, n_classes) containing the averaged\n               class probabilities rounded to 4 decimal places.\n            2. A 1-D python list of length n_samples containing the predicted class index\n               for each sample obtained via arg-max on the averaged probabilities.\n    \"\"\"\n    pass\n```\n# Output Constraints:\n1. All probabilities must be rounded to the nearest 4th decimal place.\n2. Return regular python lists (not NumPy arrays).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 444, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Radial Basis Function (RBF) Kernel Matrix\n# Description:\nImplement the Radial Basis Function (RBF) kernel that is frequently used in kernel methods such as Gaussian Processes and Support Vector Machines.\n\nGiven two collections of N-dimensional vectors X (shape N\u00d7C) and Y (shape M\u00d7C), the RBF kernel between two vectors x and y is\n\n    k(x, y) = exp\\{ -0.5 *  \u03a3_j  ((x_j \u2212 y_j)/\u03c3_j)^2 \\}\n\nwhere \u03c3 is a **scale (band-width) parameter**:\n\u2022 If \u03c3 is a single positive float, the same value is used for every feature (isotropic kernel).\n\u2022 If \u03c3 is a list/1-D array of length **C**, each feature j is scaled by its own positive \u03c3_j (anisotropic kernel).\n\u2022 If \u03c3 is None, use the conventional default value  \u221a(C/2).\n\nThe task is to write a function that\n1. Validates the inputs (matching feature dimensions, valid \u03c3).\n2. Computes the full kernel matrix of shape (N, M) (or (N, N) if Y is omitted).\n3. Rounds all entries to **4 decimal places** and returns the result as a (nested) Python list.\n\nReturn **\u22121** in any of the following cases:\n\u2022 \u03c3 is non-positive.\n\u2022 \u03c3 is a list whose length \u2260 number of features.\n\u2022 Feature dimensions of X and Y do not match.\n\nExample:\nInput\n    X = [[1, 0], [0, 1]]\n    Y = [[1, 0], [0, 1]]\n    \u03c3 = 1.0\nOutput\n    [[1.0, 0.3679],\n     [0.3679, 1.0]]\nReasoning\n    The squared Euclidean distance between identical vectors is 0 \u21d2 exp(0)=1.\n    Between (1,0) and (0,1) the squared distance is 2 \u21d2 exp(\u22120.5\u00b72)=exp(\u22121)=0.3679.\n# Your code should start with:\n```python\nimport numpy as np\ndef rbf_kernel(X: list[list[int | float]],\n               Y: list[list[int | float]] | None = None,\n               sigma: float | list[float] | None = None) -> list[list[float]]:\n    \"\"\"Compute the Radial Basis Function (RBF) kernel matrix.\n\n    The function should follow the specifications given in the task\n    description. It must return -1 on invalid input, otherwise a nested list\n    containing the kernel matrix rounded to four decimal places.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nAll numbers must be rounded to the nearest 4th decimal. Return a Python list (not a NumPy array).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 446, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Information-Gain Decision Stump\n# Description:\nYou have been provided with a small utility that is needed when building **decision trees for classification**.  \nYour task is to implement a function `decision_stump` that, given a feature matrix `X` (only continuous numerical features are allowed) and a corresponding 1-D label vector `y`, finds the **best single-level split** (also called a *decision stump*) according to *information gain* (i.e. decrease in entropy).\n\nA split is defined by:\n1. a feature index `j`,\n2. a threshold `t` \u2013 every sample for which `X[i, j] \u2264 t` goes to the left child, the others to the right child.\n\nFor each candidate split you must compute the information gain\n```\nGain = H(parent) \u2212 p_left * H(left) \u2212 p_right * H(right)\n```\nwhere `H(\u00b7)` is the Shannon entropy of the class labels in the corresponding node and `p_left`, `p_right` are the proportions of samples that go to the left and right child.  \nOnly **mid-points between two successive distinct sorted values** in a column are considered as possible thresholds.\n\nThe function has to return a 4-tuple:\n```\n(best_feature_index, best_threshold, left_majority_label, right_majority_label)\n```\n\u2022 `left_majority_label` is the label that occurs most often among the samples sent to the left child; the same definition holds for `right_majority_label`.\n\nTies must be resolved as follows:\n1. If several splits yield the same highest information gain, pick the one with the **smallest feature index**.\n2. If several thresholds of this feature give the same gain, pick the **smallest threshold** among them.\n\nIf **no split can increase the information gain** (this happens when all samples share the same label), return:\n```\n(-1, None, majority_label, majority_label)\n```\nwhere `majority_label` is simply the label that appears most frequently in `y` (if there is still a tie, pick the smallest label value).\n# Your code should start with:\n```python\nimport numpy as np\ndef decision_stump(X: list[list[float]], y: list[int]) -> tuple:\n    \"\"\"Find a decision stump that maximises information gain.\n\n    A *decision stump* is a one-level decision tree: it chooses one feature and\n    one threshold to split the dataset into two parts. This function must find\n    the split that maximises the decrease of entropy (information gain) and\n    return a summarising tuple. Read the detailed task description for exact\n    requirements, tie-breaking rules and the expected return value.\n\n    Args:\n        X: 2-D list (or array-like) of shape (n_samples, n_features) containing\n           only numeric values.\n        y: 1-D list (or array-like) with the class label of every sample.\n\n    Returns:\n        \u2022 (best_feature_index, best_threshold, left_majority_label,\n           right_majority_label)\n        \u2022 If no split can improve information gain, returns\n          (-1, None, majority_label, majority_label).\n    \"\"\"\n    pass  # TODO: implement this function\n```\n# Output Constraints:\n\u2022 `best_threshold` must be rounded to **4 decimal places** (use `round(thr, 4)`).\n\u2022 The returned tuple must follow exactly the described order and types.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 452, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Split Data Set by Feature Threshold\n# Description:\nGiven a data set X (either a Python list of samples or a NumPy 2-D array), write a function that partitions the samples into two subsets according to a single feature and a threshold.\n\nFor a numeric threshold (int or float) the first subset must contain every sample whose value at column feature_i is greater than or equal to the threshold; the second subset must contain all remaining samples.  \nFor a non-numeric (categorical) threshold the first subset must contain every sample whose value at column feature_i is exactly equal to the threshold; the second subset must again contain all remaining samples.\n\nBoth subsets have to be returned in **their original order** and converted to regular Python lists (use ndarray.tolist()).\nIf one of the subsets is empty simply return an empty list for this position.\n\nExample behaviour (numeric split):\nX = np.array([[1, 5], [3, 2], [4, 6], [2, 1]])\nfeature_i = 0, threshold = 3  \u279c  [[ [3, 2], [4, 6] ], [ [1, 5], [2, 1] ]]\n\nExample behaviour (categorical split):\nX = np.array([[1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"C\"]])\nfeature_i = 1, threshold = \"A\"  \u279c  [[ [1, \"A\"], [3, \"A\"] ], [ [2, \"B\"], [4, \"C\"] ]]\n\nReturn a *list* holding the two resulting lists and keep the sample order unchanged.\n# Your code should start with:\n```python\nimport numpy as np\nfrom typing import List\n\nimport numpy as np\n\ndef divide_on_feature(X: np.ndarray | List[list], feature_i: int, threshold) -> List[list]:\n    \"\"\"Split the data set *X* into two subsets using a given feature column and threshold.\n\n    The function must create two disjoint subsets:\n    1. For a numeric threshold (int or float) the first subset contains every\n       sample whose value in column *feature_i* is **greater than or equal** to\n       the threshold.\n    2. For any other type of threshold the first subset contains every sample\n       whose value in column *feature_i* is **exactly equal** to the threshold.\n\n    The second subset always contains the remaining samples.  Both subsets must\n    keep the original order of appearance in *X*.\n\n    Args:\n        X: 2-D iterable (list or ndarray) where each element is a sample.\n        feature_i: Index of the feature column used for the split.\n        threshold: Value that determines how the split is performed.\n\n    Returns:\n        A list of length two.  *result[0]* is the first subset, *result[1]* is\n        the second subset.  Each subset must be converted to a regular Python\n        list via ``ndarray.tolist()`` before returning.\n    \"\"\"\n    # ======  Write your code below this line  ======\n    pass  # Remove this line when implementing your solution.\n```\n# Output Constraints:\nReturn a Python list of length 2 where each element is itself a list produced with ndarray.tolist().  Preserve the order of the original samples.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 453, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Target Q-Value Update for Deep Q-Networks\n# Description:\nIn Deep Q-Networks (DQN) the neural network is trained with targets that depend on the agent\u2019s **current** Q-value estimates and the **next-state** Q-value estimates.  \nGiven\n\u2022 `Q` \u2013 the network\u2019s Q-value predictions for a batch of states (shape **b \u00d7 n_actions**),  \n\u2022 `Q_next` \u2013 the network\u2019s Q-value predictions for the *next* states of the same batch,  \n\u2022 `actions` \u2013 the action actually taken in each state,  \n\u2022 `rewards` \u2013 the immediate reward received after each action,  \n\u2022 `dones` \u2013 boolean flags telling whether the next state is terminal,  \n\u2022 `gamma` \u2013 the discount factor (0 \u2264 \u03b3 \u2264 1),  \nwrite a function that returns the **training targets** `y` used in DQN.\n\nFor every sample `i` in the batch\n```\nif dones[i]:\n    target = rewards[i]\nelse:\n    target = rewards[i] + gamma * max(Q_next[i])\n```\nYou must copy the original `Q[i]`, replace *only* the entry that corresponds to `actions[i]` by `target`, and finally return the whole updated matrix rounded to four decimal places.\n\nIf the input arrays/lists have inconsistent lengths, or if `gamma` is outside the interval [0, 1], return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef update_q_values(\n    Q: \"np.ndarray\",\n    Q_next: \"np.ndarray\",\n    actions: list[int],\n    rewards: list[float],\n    dones: list[bool],\n    gamma: float,\n) -> list[list[float]]:\n    \"\"\"Fill in DQN targets for a training batch.\n\n    Parameters\n    ----------\n    Q : np.ndarray\n        Q-values predicted for the *current* states, shape (batch, n_actions).\n    Q_next : np.ndarray\n        Q-values predicted for the *next* states, same shape as ``Q``.\n    actions : list[int]\n        Action index taken in each state.\n    rewards : list[float]\n        Reward received after each action.\n    dones : list[bool]\n        Whether the next state is terminal for each sample.\n    gamma : float\n        Discount factor in the interval [0, 1].\n\n    Returns\n    -------\n    list[list[float]]\n        Updated Q matrix that can be used as supervised training targets.\n        Round every element to 4 decimals. If inputs are invalid return -1.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every number to the nearest 4th decimal.\nReturn the result as a nested Python list, **not** a NumPy array.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 458, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: One-Hot Encoding of Integer Labels\n# Description:\nIn many machine-learning workflows class labels are represented as integers (e.g. 0, 1, 2 \u2026).  Neural-network libraries, however, usually expect those labels in **one-hot (categorical) form** \u2014 a binary matrix in which every row contains a single *1* at the index that corresponds to the original class label.\n\nWrite a function `to_categorical` that converts a one-dimensional array-like object of non-negative integer labels into a two-dimensional one-hot encoded matrix.\n\nFunction requirements\n1. Parameters\n   \u2022 `labels` \u2013 a Python `list`, `tuple`, or `numpy.ndarray` containing non-negative integers.\n   \u2022 `num_classes` *(optional)* \u2013 the total number of distinct classes.  When omitted (`None`) this value must be inferred as `max(labels) + 1`.\n\n2. Behaviour\n   \u2022 The returned object must be a **Python list of lists** whose shape is `(len(labels), num_classes)`.\n   \u2022 Every row must consist solely of `0`\u2019s except for a single `1` located at the index that matches the original label.\n   \u2022 If `num_classes` is supplied but is **smaller** than `max(labels) + 1`, or if any label is negative, the function must return **-1**.\n\n3. Results must contain integers (`int`, *not* floats or booleans).\n# Your code should start with:\n```python\nimport numpy as np\ndef to_categorical(labels: list | tuple | 'np.ndarray', num_classes: int | None = None) -> list[list[int]]:\n    \"\"\"Convert integer class labels to one-hot encoded format.\n\n    Args:\n        labels: 1-D sequence of non-negative integer labels.\n        num_classes: Total number of classes. If *None*, infer as\n            ``max(labels) + 1``.\n\n    Returns:\n        A list of lists representing the one-hot encoded labels, or -1\n        when the input is invalid (negative label or `num_classes` too\n        small).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list of lists of **ints** (0/1).  Do not return a NumPy array.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 461, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Light-weight Gradient Boosting Regressor (1-D)\n# Description:\nIn the original library snippet a class called ``GradientBoostingRegressor`` is just a very thin wrapper around a generic ``GradientBoosting`` implementation.  \n\nIn this task you will recreate **the essential idea of gradient boosting for one\u2013dimensional regression data, but purely with functions (no classes)**.  \n\nImplement a function that fits an ensemble of *decision stumps* (depth-1 regression trees) to the given training points by **gradient boosting** and then returns the final predictions for the same training inputs.\n\nAlgorithm to implement (Square-loss boosting with stumps)\n1.  Let the current prediction be the mean of the targets \\(\\bar y\\).  \n2.  Repeat **n_estimators** times (or stop early if the residuals become all zeros).  \n   a.  Compute the residuals \\(r_i = y_i-\\hat y_i\\).  \n   b.  For every possible split value *t* chosen from the *unique x values except the greatest one*, split the training set into:\n      \u2022 left:  \\(x_i\\le t\\)  \n      \u2022 right: \\(x_i>t\\)  \n      Ignore a split if either side is empty.  \n   c.  For each split compute the **sum of squared errors (SSE)** obtained by predicting the mean residual of its side.  \n   d.  Pick the split with the smallest SSE (first one in case of ties).  Let \\(v_L\\) and \\(v_R\\) be the mean residuals on the left and right.  \n   e.  The stump predicts\n      \\[\\tilde r_i = \\begin{cases}v_L,&x_i\\le t\\\\v_R,&x_i>t\\end{cases}\\]\n   f.  Update the ensemble prediction:  \\(\\hat y_i \\leftarrow \\hat y_i + \\text{learning\\_rate}\\times\\tilde r_i\\).\n3.  Return the final \\(\\hat y\\) values **rounded to 4 decimal places** as a Python list.\n\nSpecial cases\n\u2022 If *n_estimators* is 0 or negative, simply return the mean target for every sample.  \n\u2022 If no valid split exists, set the stump prediction to the mean residual of the whole data (this keeps the algorithm working when all \\(x\\) are identical).\n\nYou may only use ``numpy`` and the Python standard library.\n# Your code should start with:\n```python\nimport numpy as np\ndef gradient_boosting_regressor(\n        x: list[float],\n        y: list[float],\n        n_estimators: int = 200,\n        learning_rate: float = 0.5) -> list[float]:\n    \"\"\"Gradient Boosting with decision stumps for 1-D regression.\n\n    Parameters\n    ----------\n    x : list[float]\n        Feature values (one-dimensional).\n    y : list[float]\n        Target values.\n    n_estimators : int, default 200\n        Number of boosting iterations.\n    learning_rate : float, default 0.5\n        Shrinkage applied to each weak learner.\n\n    Returns\n    -------\n    list[float]\n        Final predictions for the training data, rounded to four decimals.\n    \"\"\"    \n    # Write your code here\n    pass\n```\n# Output Constraints:\nAll returned numbers must be rounded to the nearest 4th decimal; use ``np.round(arr, 4).tolist()``.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 471, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Expected SARSA TD(0) Q-Table Update\n# Description:\nImplement one step of the on-policy TD(0) Expected\u2013SARSA algorithm for a tabular setting.\n\nYou are given\n1. a Q\u2013table as a (row-major) Python list of lists, where each row corresponds to a state and each column corresponds to an action,\n2. the indices (state, action) of the transition that has just been taken,\n3. the immediate reward obtained from the environment,\n4. the next state\u02bcs index (or ``None`` if the transition terminates the episode), and\n5. the usual Expected\u2013SARSA hyper-parameters \u2013 exploration rate ``epsilon``, learning rate ``lr`` and discount factor ``gamma``.\n\nFor a non-terminal next state ``s'`` the Expected\u2013SARSA TD target is\n\n    target = r + gamma * \ud835\udc38[Q[s', a'] | s']\n\nwhere the expectation is taken w.r.t. the \u03b5-soft policy derived from the current Q\u2013table:\n\n    \u03c0(a|s') = 1 \u2212 \u03b5 + \u03b5/|A|    if a is greedy\n    \u03c0(a|s') = \u03b5/|A|             otherwise\n\n``|A|`` is the number of actions (the length of a row of the Q-table) and *greedy* means the action with the maximum Q-value in ``s'`` (ties are resolved by taking the first such action).\n\nIf ``next_state`` is ``None`` the expectation term is treated as 0.\n\nFinally, update the Q entry ``Q[state][action]`` using\n\n    Q[state][action] += lr * (target \u2212 Q[state][action])\n\nand return the **entire** updated Q-table.  Every number in the returned table must be rounded to four decimal places.\nIn cases where there is no next state (i.e. a terminal transition) treat the expected future value as 0.\n# Your code should start with:\n```python\nimport numpy as np\ndef expected_sarsa_update(q_table: list[list[float]],\n                          state: int,\n                          action: int,\n                          reward: float,\n                          next_state: int | None,\n                          epsilon: float,\n                          lr: float,\n                          gamma: float) -> list[list[float]]:\n    \"\"\"One step Expected\u2013SARSA TD(0) update for a tabular Q function.\n\n    Parameters\n    ----------\n    q_table : list[list[float]]\n        Current Q-table; q_table[s][a] is Q(s,a).\n    state : int\n        Index of the state *s* where the action was taken.\n    action : int\n        Index of the action *a* taken in state *s*.\n    reward : float\n        Immediate reward received after executing the action.\n    next_state : int | None\n        The successor state *s'*.  Use ``None`` if the transition ended the\n        episode.\n    epsilon : float\n        \u03b5 in the \u03b5-soft policy used to compute the expectation.\n    lr : float\n        Learning-rate \u03b7.\n    gamma : float\n        Discount factor \u03b3.\n\n    Returns\n    -------\n    list[list[float]]\n        The updated Q-table (all entries rounded to four decimals).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every entry of the returned Q-table to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 474, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Generate a 2-D Toy Data Set\n# Description:\nYou are given the broken helper _GenerateData that should create a very simple, perfectly separable two\u2013dimensional data set suitable for a binary-classification toy problem.  Each class is arranged in a rectangular cluster:  the first class (label \u20131) lives roughly in the square [1,9]\u00d7[1,9] while the second class (label +1) is shifted upward by the value of the parameter interval (in multiples of 10).  The original code has two problems:\n1.  It hard-codes the parameters and therefore is not reusable.\n2.  It generates **no labels for the validation set** (the second argument passed to the helper is wrong).\n\nWrite a function that fixes these issues and makes the data generator reusable.  The function must\n\u2022 accept the parameters listed below,\n\u2022 optionally take a random seed so that the produced data are reproducible,\n\u2022 round every coordinate to four decimal places,\n\u2022 return four NumPy arrays: `X_train`, `X_val`, `Y_train`, `Y_val`.\n\nPoint generation rule for any class index `i` (starting at 0):\n    x  ~  U([(\u230ai/2\u230b+0.1)\u00b710 , (\u230ai/2\u230b+0.9)\u00b710])\n    y  ~  U([((i mod 2)*interval+0.1)\u00b710 , ((i mod 2)*interval+0.9)\u00b710])\n    label = (i \u2013 0.5)\u00b72   (\u2192 \u20131 for the first class, +1 for the second)\n\nParameters\nm         \u2013 number of classes (\u22652)\nn_train   \u2013 samples per class for the training set\nn_val     \u2013 samples per class for the validation set\ninterval  \u2013 vertical distance (in units of 10) between the two rows of clusters\nseed      \u2013 optional integer; if given, call `random.seed(seed)` before sampling\n\nReturn value (all rounded to 4 decimals)\nX_train : shape (m\u00b7n_train , 2)\nX_val   : shape (m\u00b7n_val   , 2)\nY_train : shape (m\u00b7n_train ,)\nY_val   : shape (m\u00b7n_val   ,)\n# Your code should start with:\n```python\nimport numpy as np\nimport random\nimport numpy as np\nimport random\n\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  interval: float,\n                  seed: int | None = None) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Generate a 2-D toy data set for a binary (or multi-class) classifier.\n\n    Args:\n        m:         Number of distinct classes.\n        n_train:   Number of training samples per class.\n        n_val:     Number of validation samples per class.\n        interval:  Vertical distance (in units of 10) between the two rows\n                    of class clusters.\n        seed:      Optional random seed to make the output deterministic.\n\n    Returns:\n        A tuple (X_train, X_val, Y_train, Y_val) where each element is a\n        NumPy array.  All coordinates must be rounded to 4 decimal places.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nAll coordinates must be rounded to the nearest 4\u1d57\u02b0 decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 475, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Single-step Adam Update\n# Description:\nImplement the core mathematics of the Adam optimiser.  \n\nAdam keeps two moving averages of the gradients \u2013 the first moment (mean) and the second moment (uncentred variance).  After every mini-batch it produces *bias\u2013corrected* versions of those moments and uses them to shift the parameters.\n\nWrite a function that performs **one** Adam update step.\n\nGiven\n\u2022 current parameters `w` (scalar or NumPy array)\n\u2022 current gradient `grad` (same shape as `w`)\n\u2022 previous first moment `m_prev`\n\u2022 previous second moment `v_prev`\n\u2022 time step `t` (an integer that starts at 1 and increases by one each call)\n\u2022 the Adam hyper-parameters\n\nreturn a tuple `(w_new, m_new, v_new)` containing the updated parameters and the new moment estimates.\n\nIf `m_prev` or `v_prev` is **None** treat it as an array of zeros having the same shape as `grad`.\n\nFormulae  \n    m_t = \u03b2\u2081 \u00b7 m_{t\u22121} + (1\u2212\u03b2\u2081) \u00b7 grad  \n    v_t = \u03b2\u2082 \u00b7 v_{t\u22121} + (1\u2212\u03b2\u2082) \u00b7 grad\u00b2  \n    m\u0302_t = m_t / (1\u2212\u03b2\u2081\u1d57)          (bias correction)  \n    v\u0302_t = v_t / (1\u2212\u03b2\u2082\u1d57)          (bias correction)  \n    w_new = w \u2212 \u03b1 \u00b7 m\u0302_t / (\u221av\u0302_t + \u03b5)\n\nwhere \u03b1 is the learning rate.\n\nExample call (with the default hyper-parameters)  \n    >>> w_new, m_new, v_new = adam_update(1.0, 0.1, 0.0, 0.0, 1)  \n    >>> round(w_new, 9)  # \u2248 0.999000001\n\nA correct implementation must work for scalars and arbitrary-shaped NumPy arrays.\n# Your code should start with:\n```python\nimport numpy as np\ndef adam_update(w, grad, m_prev, v_prev, t, learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8):\n    \"\"\"Perform a single Adam optimisation step.\n\n    Parameters\n    ----------\n    w : float | np.ndarray\n        Current value of the parameter(s) to be updated.\n    grad : float | np.ndarray\n        Gradient of the loss with respect to ``w``.\n    m_prev : float | np.ndarray | None\n        Previous estimate of the first moment (mean of gradients).\n    v_prev : float | np.ndarray | None\n        Previous estimate of the second moment (uncentred variance of gradients).\n    t : int\n        Time step (must start at 1 and increase by one on every call).\n    learning_rate : float, default 0.001\n        Step size ``\u03b1``.\n    b1 : float, default 0.9\n        Exponential decay rate for the first moment.\n    b2 : float, default 0.999\n        Exponential decay rate for the second moment.\n    eps : float, default 1e-8\n        Small constant added to the denominator for numerical stability.\n\n    Returns\n    -------\n    tuple\n        ``(w_new, m_new, v_new)`` where:\n        * ``w_new`` \u2013 updated parameters (same shape as ``w``)\n        * ``m_new`` \u2013 updated first moment\n        * ``v_new`` \u2013 updated second moment\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn **three** objects: (updated_w, new_m, new_v).\n\u2022 They must have the same shapes as the corresponding inputs.\n\u2022 Floating results should be accurate to at least 1 \u00d7 10\u207b\u2078.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 479, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Epsilon-Greedy Multi-Armed Bandit Simulation\n# Description:\nImplement a simple \u03b5-greedy algorithm for the stationary multi-armed bandit problem.\n\nYou are given a matrix ``rewards`` where each inner list represents the rewards that **could** be obtained at one time-step if a particular arm were pulled.  The i-th element of the inner list corresponds to the reward for arm *i* at that step.  Your task is to simulate one run of the \u03b5-greedy strategy and return the final estimates of the expected reward for every arm.\n\nAlgorithm\n1.  Let *N* be the number of arms (length of the first inner list).\n2.  Initialise the estimated value of every arm with the constant ``ev_prior`` and set all pull counters to 0.\n3.  For each time-step *t* (row in ``rewards``):\n    \u2022 With probability ``epsilon`` choose an arm uniformly at random.\n    \u2022 Otherwise choose the arm that currently has the largest estimated value (break ties by the smallest index).\n    \u2022 Receive the reward that corresponds to the chosen arm at this time-step.\n    \u2022 Update the chosen arm\u2019s estimate using the incremental sample mean\n      \n      V\u1d62 \u2190 V\u1d62 + (r \u2212 V\u1d62) / C\u1d62\n      \n      where V\u1d62 is the estimate for arm *i*, r is the observed reward and C\u1d62 is the number of times arm *i* has been selected so far (after incrementing it for this pull).\n4.  After the last time-step return the list of arm value estimates rounded to 4 decimal places.\n\nWhen an optional integer ``seed`` is provided, use it to seed NumPy\u2019s random number generator so that results become reproducible.\n\nIf ``epsilon`` is 0 the strategy acts greedily; if it is 1 the strategy acts completely at random.\n# Your code should start with:\n```python\nimport numpy as np\nfrom typing import List, Optional\nimport numpy as np\n\ndef epsilon_greedy_bandit(\n    rewards: List[List[float]],\n    epsilon: float = 0.05,\n    ev_prior: float = 0.5,\n    seed: Optional[int] = None,\n) -> List[float]:\n    \"\"\"Simulate one run of the \u03b5-greedy policy on a stationary multi-armed bandit.\n\n    Parameters\n    ----------\n    rewards : List[List[float]]\n        A matrix where ``rewards[t][i]`` is the reward for arm *i* at time-step\n        *t*.\n    epsilon : float, optional\n        Exploration probability (default 0.05).\n    ev_prior : float, optional\n        Initial expected value for each arm (default 0.5).\n    seed : int | None, optional\n        Random seed for reproducible experiments (default None).\n\n    Returns\n    -------\n    List[float]\n        The final estimated value for every arm rounded to 4 decimals.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturn a Python list where each element is rounded to the nearest 4th decimal place.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 481, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: DBSCAN Clustering From Scratch\n# Description:\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is an unsupervised learning algorithm that groups together points that are closely packed (points with many nearby neighbors) and marks as outliers points that lie alone in low-density regions.  \n\nWrite a Python function that implements DBSCAN **from scratch** (do **not** use `sklearn` or any other external ML library).  \n\nGiven:  \n\u2022 a two-dimensional NumPy array `data` whose rows are samples and columns are features,  \n\u2022 a distance threshold `eps` (all neighbors within this Euclidean radius are considered reachable) and  \n\u2022 an integer `min_samples` (the minimum number of points required to form a dense region),  \nreturn a list of cluster labels for every sample.\n\nLabeling rules\n1. Core points (points that have at least `min_samples` points, **including itself**, within `eps`) start new clusters or expand existing ones.  \n2. Border points (non-core but reachable from a core point) receive the cluster id of that core region.  \n3. Noise points that are not reachable from any core point are labeled **\u22121**.  \n4. Clusters are indexed `0, 1, 2, \u2026` **in the order in which they are discovered while scanning the data from index `0` upward**.  \n\nIf `data` is empty, return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Cluster *data* with the DBSCAN algorithm.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Two-dimensional array where each row is a sample and columns are\n        features.\n    eps : float\n        Maximum radius of the neighborhood to be considered reachable.\n    min_samples : int\n        Minimum number of samples (including the point itself) required to form\n        a dense region.\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for each sample. Noise points are labeled \u20111 and cluster\n        indices start at 0 and increase sequentially in discovery order.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nCluster labels must be integers where noise points are marked exactly with -1 and cluster ids start from 0 and increase consecutively in discovery order.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 482, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: One-Hot Encoding (to_categorical)\n# Description:\nImplement the classical **one-hot encoding** routine that converts a vector of integer class labels into a 2-D array whose rows are the one-hot representations of those labels (also called *categorical* representation).\n\nThe function must support an optional parameter `num_classes`.\n\u2022 If `num_classes` is omitted or set to `None`, treat the number of classes as *max(label) + 1*.\n\u2022 If `num_classes` is provided, create that many columns; the function must raise a `ValueError` if any label is negative or not smaller than `num_classes`.\n\nAll returned values must be integers (0 or 1).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef to_categorical(y, num_classes=None):\n    \"\"\"Convert integer labels to one-hot encoded format.\n\n    This function receives a vector (1-D) of non-negative integer labels and\n    returns a two-dimensional NumPy array where each row corresponds to the\n    one-hot representation of the matching label.\n\n    Args:\n        y (array_like): Sequence of integer labels. Accepted types are list,\n            tuple or NumPy ndarray.\n        num_classes (int | None, optional): Total number of distinct classes.\n            If ``None`` (default), the number is inferred automatically as\n            ``max(y) + 1``.\n\n    Returns:\n        np.ndarray: A matrix of shape ``(len(y), num_classes)`` filled with 0s\n        and 1s (dtype = int), representing the one-hot encoding of the input\n        labels.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturn a NumPy ndarray containing only 0s and 1s (dtype=int).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 485, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Univariate Regression Tree\n# Description:\nImplement a very small version of the CART regression-tree algorithm for *one* numerical input feature.  The function must\n1. build a binary tree by **recursive greedy splitting** on the single feature, selecting the split\u2010point that minimises the **sum of squared errors (SSE)** of the two children,\n2. stop recursing when the current depth equals the user supplied **max_depth** or when a node contains fewer than **two** training samples,\n3. label every leaf with the **mean** of the target values stored in that leaf,\n4. return the predictions for an arbitrary list of test points by traversing the tree.\n\nOnly the feature values and targets are given \u2013 no external libraries such as *scikit-learn* may be used.  The whole task therefore fits in one function: build the tree and immediately use it to predict.\n\nIf the training set is empty the function must return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef regression_tree_predict(X_train: list[float | int],\n                            y_train: list[float | int],\n                            X_test:  list[float | int],\n                            max_depth: int = 3) -> list[float]:\n    \"\"\"Fit a univariate regression tree of depth *max_depth* and predict.\n\n    Parameters\n    ----------\n    X_train : list[float | int]\n        1-D feature values for the training set.\n    y_train : list[float | int]\n        Continuous target values for the training set.\n    X_test : list[float | int]\n        1-D feature values for which predictions are required.\n    max_depth : int, default 3\n        Maximum depth of the binary tree (root has depth 0).\n\n    Returns\n    -------\n    list[float]\n        Predictions for every element in *X_test*.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a python list of floats; **do not** round the values.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 490, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Vector to Diagonal Matrix Converter\n# Description:\nWrite a Python function that converts a one-dimensional vector into a square diagonal matrix. The input can be a Python list, tuple, or one-dimensional NumPy array containing numeric values (int or float). The function must place the elements of the vector on the principal diagonal of the resulting matrix and fill all off-diagonal positions with zeros. If the input vector is empty, return an empty list.\n\nThe function must return the resulting matrix as a list of lists (use NumPy\u2019s `tolist()` method for easy conversion).\n# Your code should start with:\n```python\nimport numpy as np\ndef make_diagonal(x: list | tuple | 'np.ndarray') -> list[list[int | float]]:\n    \"\"\"Convert a one-dimensional vector into a square diagonal matrix.\n\n    Args:\n        x: A one-dimensional structure (list, tuple, or NumPy array) containing\n           numeric values.\n\n    Returns:\n        A list of lists representing the diagonal matrix. If *x* is empty,\n        return an empty list.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list of lists. Do not round or modify input values.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 491, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Frequent Item-set Mining with FP-Growth\n# Description:\nImplement the FP-Growth algorithm to mine **all** frequent item-sets that appear in a collection of transactions at least *min_sup* times.\n\nFP-Growth works in two major stages:\n1. **FP-tree construction** \u2013 Scan the transaction database once to count item frequencies.  Remove items that do not reach the minimum support and order the remaining items in each transaction by descending global frequency.  Insert each ordered transaction into an FP-tree so that identical prefixes share the same path.  Maintain a header table that links every node that contains the same item label.\n2. **Recursive mining** \u2013 Repeatedly generate conditional pattern bases from the header table, build conditional FP-trees, and append discovered single-items to the current prefix to create larger frequent item-sets.  If a conditional tree consists of a single path, enumerate all non-empty combinations of the items on that path and add them to the result in one shot; otherwise, continue mining the conditional tree recursively.\n\nYour function must\n\u2022 take a two-dimensional `list`/`numpy.ndarray` of hashable items (`str`, `int`, \u2026) and an `int` *min_sup* (>0);\n\u2022 return a **sorted** `list` of `tuple`s.  Inside every tuple the items must be given in lexicographically ascending order.  The outer list must be sorted first by tuple length and then lexicographically (this makes grading deterministic).\n\nExample (taken from the original FP-Growth paper):\nTransactions =\n    [ [\"A\",\"B\",\"D\",\"E\"],\n      [\"B\",\"C\",\"E\"],\n      [\"A\",\"B\",\"D\",\"E\"],\n      [\"A\",\"B\",\"C\",\"E\"],\n      [\"A\",\"B\",\"C\",\"D\",\"E\"],\n      [\"B\",\"C\",\"D\"] ]\n\nWith *min_sup* = 3 the algorithm must output\n[('A',), ('B',), ('C',), ('D',), ('E',), ('A','B'), ('A','D'), ('A','E'),\n ('B','C'), ('B','D'), ('B','E'), ('C','E'), ('D','E'),\n ('A','B','D'), ('A','B','E'), ('A','D','E'), ('B','C','E'), ('B','D','E'),\n ('A','B','D','E')]\n\nWhy?  Every set above occurs in at least three transactions, whereas no superset of the largest listed set does.\n# Your code should start with:\n```python\nfrom collections import Counter, defaultdict\nimport itertools\ndef fp_growth(transactions, min_sup):\n    \"\"\"Mine all frequent item-sets using the FP-Growth algorithm.\n\n    Parameters\n    ----------\n    transactions : Iterable[Iterable[Hashable]]\n        A collection of transactions.  Each transaction is an iterable containing hashable items.\n    min_sup : int\n        Minimum number of occurrences an item-set must have to be considered frequent.\n\n    Returns\n    -------\n    list[tuple]\n        All frequent item-sets sorted 1) by length, 2) lexicographically.  Every\n        tuple itself is sorted lexicographically.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a list of tuples sorted 1) by tuple length, 2) lexicographically.  Inside each tuple the items must be in lexicographical order.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 492, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Polynomial Feature Expansion\n# Description:\nIn many machine-learning models we need to enrich the original feature space with non-linear (polynomial) combinations of the existing features.  \nYour task is to implement a function that, for a given data matrix X (shape: n_samples \u00d7 n_features) and an integer degree d \u2265 0, returns a new matrix that contains **all monomials of the original features whose total degree does not exceed d**.\n\nMore formally, for every sample x = [x\u2080, x\u2081, \u2026, x_{m-1}] the resulting row should contain the products\nx\u2080^{k\u2080} x\u2081^{k\u2081} \u2026 x_{m-1}^{k_{m-1}}\nfor all non-negative integer tuples (k\u2080, \u2026, k_{m-1}) such that k\u2080+\u2026+k_{m-1} \u2264 d, arranged in the following order:\n1. Ascending total degree (0, then 1, \u2026, d).\n2. Within the same degree, lexicographic order of the indices as produced by `itertools.combinations_with_replacement`.\n\nThe very first column therefore consists of 1\u2019s (the degree-0 term).\n\nReturn the resulting matrix as a list of lists (use `ndarray.tolist()`).  If *degree* is negative return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X: list[list[int | float]], degree: int) -> list[list[float]]:\n    \"\"\"Generate a full polynomial feature matrix up to a given degree.\n\n    The function should take a 2-D list *X* whose rows correspond to\n    observations and columns correspond to original features.  It must\n    return a new list of lists containing, for every observation, all\n    monomials of the original features whose total degree does not exceed\n    the supplied *degree*.  The very first element in every row should be\n    1 (the degree-0 term).\n\n    If *degree* is negative the function must return -1.\n\n    Args:\n        X: Input data of shape (n_samples, n_features).\n        degree: Maximum total degree of the generated polynomial terms.\n\n    Returns:\n        A 2-D Python list with the enriched feature matrix, or \u20111 when\n        *degree* is negative.\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\nReturn a Python list of lists.  No rounding is required.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 493, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Mean-Squared Error of a Sample\n# Description:\nThe mean-squared-error (MSE) that regression trees (and many other algorithms) use to decide on the best split is simply the average squared distance of every target value from the average of those targets.  \nFormally, given a non-empty one-dimensional sample of real numbers $y=[y_1,\\dots,y_n]$, the MSE is\n$$\n\\text{MSE}(y)=\\frac1n\\sum_{i=1}^n\\bigl(y_i-\\bar y\\bigr)^2,\\qquad\\bar y=\\frac1n\\sum_{i=1}^n y_i.\n$$\nWrite a function that receives the sample either as a Python list or a 1-D NumPy array and returns the MSE rounded to four decimal places.  \nIf the input sequence is empty, return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef mse(y):\n    \"\"\"Compute the mean-squared error (MSE) of a 1-D numeric sample.\n\n    The MSE is the average of squared differences between each element\n    and the sample mean.  If the input sequence is empty the function\n    should return -1.\n\n    Args:\n        y (list[int | float] | numpy.ndarray): 1-D collection of numbers.\n\n    Returns:\n        float: Mean-squared error rounded to 4 decimal places, or -1 when\n            *y* is empty.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a single float rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 496, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Affine Activation and Its Derivatives\n# Description:\nIn neural-network literature an affine (sometimes called **linear**) activation is defined as  \\(f(x)=\\text{slope}\\cdot x+\\text{intercept}\\).  \n\nWrite a function that simultaneously returns\n1. the value of the affine activation applied element-wise to the input vector,\n2. the first derivative evaluated element-wise,\n3. the second derivative evaluated element-wise.\n\nThe function must\n\u2022 accept a 1-D Python list **x** (or a NumPy array) and two optional scalars *slope* (default 1) and *intercept* (default 0);\n\u2022 return a **tuple of three lists** `(y, grad, grad2)` where  \n  \u2013 `y[i]  =  slope * x[i] + intercept`  \n  \u2013 `grad[i]  =  slope`  \n  \u2013 `grad2[i] = 0`  \n\u2022 round every element of the three lists to 4 decimal places.\n\nIf the input is a scalar it should be treated as a length-one vector.\n# Your code should start with:\n```python\nimport numpy as np\ndef apply_affine(x, slope: float = 1.0, intercept: float = 0.0):\n    \"\"\"Apply an affine activation and return its first and second derivatives.\n\n    Args:\n        x: 1-D list or NumPy array containing numeric values.\n        slope: The slope of the affine function. Defaults to 1.0.\n        intercept: The intercept of the affine function. Defaults to 0.0.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing three\n        lists \u2013 the activation output, the first derivative, and the second\n        derivative \u2013 each rounded to 4 decimal places.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nAll returned numbers must be rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 499, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: PCA Dimensionality Reduction\n# Description:\nImplement a Principal Component Analysis (PCA) **dimensionality-reduction** routine from scratch.\n\nGiven a 2-D NumPy array `data` \u2013 where each row is a sample and each column is a feature \u2013 and a positive integer `k`, return the projection of the data onto the first `k` principal components.\n\nThe steps are:\n1. Standardise each feature (zero mean, unit *population* variance).  \n   \u2022  If a feature has zero variance, leave it unchanged (all zeros after centring).\n2. Compute the sample covariance matrix of the standardised data (use Bessel\u2019s correction, i.e. divide by *n \u2212 1*).\n3. Perform an eigen-decomposition of the covariance matrix.\n4. Sort eigenvalues in **descending** order and arrange the corresponding eigenvectors accordingly.\n5. Fix the sign of every eigenvector so that its entry with the largest absolute value is **positive** (this removes the sign ambiguity of eigenvectors and makes the output deterministic).\n6. Project the standardised data on the first `k` eigenvectors.\n\nReturn the projected matrix rounded to four decimal places and converted to a regular (nested-list) Python list.\n\nIf `k` is not in the interval `1 \u2026 n_features`, return **-1**.\n# Your code should start with:\n```python\nimport numpy as np\ndef pca_transform(data: np.ndarray, k: int) -> list[list[float]]:\n    \"\"\"Project *data* onto its first *k* principal components.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        2-D array where each row is a sample and each column is a feature.\n    k : int\n        Number of principal components to retain (1 \u2264 k \u2264 n_features).\n\n    Returns\n    -------\n    list[list[float]]\n        The projected data rounded to 4 decimal places. If *k* is outside the\n        valid range, return \u22121.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nRound every number to 4 decimal places and return a Python list (use NumPy\u2019s `round(..., 4)` followed by `.tolist()`).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 500, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Tiny Gradient Boosting Binary Classifier\n# Description:\nImplement a very small-scale Gradient Boosting **binary** classifier that uses decision stumps (one\u2013dimensional splits) as weak learners and the squared\u2013error loss (regression view of the labels).\\n\\nThe function must:\\n1. Receive a training set `X_train` (list of samples \u2013 each sample is a list of numerical features), the associated binary labels `y_train` (list of 0/1 ints) and a test set `X_test`.\\n2. Build an additive model `F(x)=c+\\sum_{m=1}^{M}\\eta\u00b7h_m(x)` where\\n   \u2022 `c` is the average of the training labels.\\n   \u2022 each `h_m` is a decision stump that predicts a constant value for *left* samples (feature value `<=` threshold) and another constant value for *right* samples.\\n   \u2022 `M=n_estimators` and `\u03b7=learning_rate`.\\n   \u2022 At every stage residuals `r_i=y_i-F(x_i)` are computed and the next stump is fitted to these residuals by minimising the total squared error.\\n3. After the ensemble is built, return the **predicted class labels** (0/1) for every sample in `X_test`, obtained by thresholding the final score `F(x)` at 0.5.\\n\\nAssume the data are perfectly clean (no missing values) and that `y_train` only contains 0 or 1.\\nReturn the predictions as a Python `list` of integers.\n# Your code should start with:\n```python\nimport numpy as np\ndef gradient_boosting_classifier(X_train: list[list[float]], y_train: list[int], X_test: list[list[float]], n_estimators: int = 20, learning_rate: float = 0.1) -> list[int]:\n    \"\"\"Train a tiny Gradient Boosting model using decision stumps and predict labels.\n\n    The ensemble minimises the squared-error on the *binary* targets and turns the\n    final regression score into a class label by thresholding at 0.5.\n\n    Args:\n        X_train: Training samples, each sample being a list of feature values.\n        y_train: Binary labels (0 or 1) for the training samples.\n        X_test: Samples to predict \u2013 identical structure to X_train.\n        n_estimators: Number of boosting stages to perform.\n        learning_rate: Shrinkage factor (\u03b7) applied to each weak learner.\n\n    Returns:\n        A list containing the predicted class (0 or 1) for every sample in\n        X_test.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a list containing only 0 or 1 integers and having exactly the same length as `X_test`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 505, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Contrastive Divergence Update for RBM\n# Description:\nIn this task you will implement a single weight\u2013update step for a binary Restricted Boltzmann Machine (RBM) using the Contrastive Divergence (CD-k) algorithm.  \n\nThe function receives\n1. a mini-batch of visible vectors `X` (shape **m \u00d7 n_v**, `m` samples, `n_v` visible units),\n2. the current weight matrix `W` (shape **n_v \u00d7 n_h**),\n3. visible and hidden bias vectors `vbias` (length `n_v`) and `hbias` (length `n_h`),\n4. the learning rate `learning_rate`,\n5. the number of Gibbs sampling steps `k`.\n\nYou must\n\u2022 compute the positive phase hidden probabilities,\n\u2022 run `k` full Gibbs steps (hidden \u2192 visible \u2192 hidden) **without stochastic sampling \u2013 use the probabilities directly**,  \n\u2022 compute positive and negative gradients\n    \u2022 positive\u2003`pos_grad = X\u1d40 \u00b7 h0_prob`\n    \u2022 negative `neg_grad = v_k_prob\u1d40 \u00b7 h_k_prob`,\n\u2022 update the weight matrix with\n```\nW_new = W + learning_rate \u00b7 (pos_grad \u2212 neg_grad) / m\n```\n\u2022 return the updated weight matrix rounded to 4 decimal places and converted to a plain Python `list[list[float]]`.\n\nIf the mini-batch is empty return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\ndef contrastive_divergence(\n        X: np.ndarray,\n        W: np.ndarray,\n        hbias: np.ndarray,\n        vbias: np.ndarray,\n        learning_rate: float,\n        k: int) -> list[list[float]]:\n    \"\"\"Perform one CD-k weight update for a binary RBM.\n\n    Args:\n        X: A 2-D NumPy array of shape (batch_size, n_visible) containing the\n           mini-batch of visible units. Values are expected to be in [0,1].\n        W: Weight matrix of shape (n_visible, n_hidden).\n        hbias: 1-D NumPy array (length n_hidden) with hidden unit biases.\n        vbias: 1-D NumPy array (length n_visible) with visible unit biases.\n        learning_rate: Scalar learning-rate used for the update.\n        k: Number of Gibbs sampling steps to run (CD-k).\n\n    Returns:\n        The updated weight matrix **rounded to 4 decimal places** and converted\n        to a regular Python list of lists.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn the weight matrix as a list of lists.\nEach value must be rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 509, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Chebyshev Distance Calculator\n# Description:\nWrite a Python function that computes the Chebyshev (also called \\(L_{\\infty}\\) or maximum) distance between two real-valued vectors.  The Chebyshev distance between vectors \\(\\mathbf{x}=(x_{1},x_{2},\\dots ,x_{n})\\) and \\(\\mathbf{y}=(y_{1},y_{2},\\dots ,y_{n})\\) is defined as\n\n\\[\n\\;\\;d(\\mathbf{x},\\mathbf{y})\\;=\\;\\max_{i}\\, |x_{i}-y_{i}| .\\]\n\nThe function must:\n1. Accept the two vectors as Python lists or NumPy 1-D arrays containing integers and/or floats.\n2. Verify that the two vectors have the same length; if not, return **-1**.\n3. Return the distance rounded to **four** decimal places as a standard Python *float* (not a NumPy scalar).\n\nExample\n-------\nInput\n```\nx = [1, 2, 3]\ny = [2, 4, 6]\n```\nOutput\n```\n3.0\n```\nReasoning: The component-wise absolute differences are |1-2|=1, |2-4|=2, |3-6|=3.  The maximum of these values is 3, so the Chebyshev distance is 3.0.\n# Your code should start with:\n```python\nimport numpy as np\ndef chebyshev(x: list[float] | \"np.ndarray\", y: list[float] | \"np.ndarray\") -> float:\n    \"\"\"Compute the Chebyshev (maximum) distance between two real-valued vectors.\n\n    The input vectors *x* and *y* must be one-dimensional and of identical\n    length.  If the lengths differ, the function returns -1.\n\n    Parameters\n    ----------\n    x : list[float] | np.ndarray\n        First input vector.\n    y : list[float] | np.ndarray\n        Second input vector.\n\n    Returns\n    -------\n    float\n        The Chebyshev distance rounded to four decimal places, or -1 if the\n        vector lengths differ.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python float rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 510, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Variational Auto-Encoder Loss\n# Description:\nImplement the Variational Auto-Encoder (VAE) variational lower bound (also called **VAE loss**) for Bernoulli visible units.\n\nFor a mini-batch of reconstructed samples the loss is defined as\n\n    Loss = Reconstruction Loss  +  KL Divergence\n\nwhere\n\n1. Reconstruction Loss is the element-wise binary cross-entropy between the true input $\\mathbf y$ and the reconstruction $\\hat{\\mathbf y}$.\n2. KL Divergence is the analytical Kullback-Leibler divergence between the approximate posterior $q(t\\,|\\,x)=\\mathcal N(\\mu,\\operatorname{diag}(\\sigma^2))$ and the unit Gaussian prior $p(t)=\\mathcal N(0, I)$.  With mean vector $\\mu$ (``t_mean``) and log-variance vector $\\log\\sigma^{2}$ (``t_log_var``) this term is\n\n    KL = -\\tfrac12 \\sum\\bigl(1 + \\log\\sigma^{2} - \\mu^{2} - \\sigma^{2}\\bigr).\n\nFor numerical stability clip each element of *y_pred* into the open interval $(\\varepsilon,1-\\varepsilon)$ with  `\\varepsilon = np.finfo(float).eps` before taking a logarithm.\n\nThe function must return the mini-batch **average** of *Reconstruction Loss + KL Divergence* **rounded to six decimal places**.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef vae_loss(y: np.ndarray,\n             y_pred: np.ndarray,\n             t_mean: np.ndarray,\n             t_log_var: np.ndarray) -> float:\n    \"\"\"Compute the VAE loss for Bernoulli visible units.\n\n    The function must return the mini-batch average of binary cross-entropy\n    plus KL divergence, rounded to 6 decimal places.\n\n    Args:\n        y (np.ndarray): Ground-truth binary data of shape (batch_size, n_features).\n        y_pred (np.ndarray): Reconstructed probabilities with the same shape as *y*.\n        t_mean (np.ndarray): Mean of q(t|x) of shape (batch_size, latent_dim).\n        t_log_var (np.ndarray): Log-variance of q(t|x), same shape as *t_mean*.\n\n    Returns:\n        float: Average VAE loss rounded to 6 decimal places.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nReturn a Python float rounded to exactly 6 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 513, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Factorization Machine Regression \u2013 Prediction\n# Description:\nA Factorization Machine (FM) is a supervised learning model that combines linear regression with pair-wise feature interactions. For a sample **x**\u2208R\u207f the FM prediction in the regression setting is\n\n\u0177 = w\u2080 + \u03a3\u2c7c w\u2c7c x\u2c7c + \u00bd \u03a3_{f=1}^{k} [ (\u03a3\u2c7c V_{j,f} x\u2c7c)\u00b2 \u2212 \u03a3\u2c7c V_{j,f}\u00b2 x\u2c7c\u00b2 ]\n\nwhere\n\u2022 w\u2080 is a scalar bias,\n\u2022 **w** is the vector of linear weights (length n),\n\u2022 **V**\u2208R^{n\u00d7k} contains the latent factors that model pair-wise interactions,\n\u2022 k is the number of latent factors (columns of **V**).\n\nWrite a Python function that implements this formula and returns the predicted values for **all** samples in the design matrix **X**.\n\nRequirements\n1. The function must work for an arbitrary number of samples (rows of **X**), features (columns of **X**) and latent factors (columns of **V**).\n2. The result has to be rounded to 4 decimal places.\n3. Use only ``numpy`` for numerical computations.\n\nIf the input dimensions are inconsistent (e.g. lengths of **w** and **V** do not match the number of columns in **X**) assume the inputs are well-formed \u2013 no explicit error handling is required.\n# Your code should start with:\n```python\nimport numpy as np\ndef fm_predict(X: list[list[int | float]],\n               w0: float,\n               w: list[float],\n               V: list[list[int | float]]) -> list[float]:\n    \"\"\"Calculate Factorization Machine predictions for a batch of samples.\n\n    Args:\n        X: 2-D feature matrix of shape (n_samples, n_features) represented as a\n           list of lists where each inner list is a sample.\n        w0: Global bias term (float).\n        w: List of length n_features containing linear coefficients.\n        V: List of lists with shape (n_features, k) representing latent\n           interaction factors; k is the number of latent dimensions.\n\n    Returns:\n        A list of floats \u2013 one prediction for each sample \u2013 rounded to 4\n        decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nMake sure all results are rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 517, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Sigmoid Activation Function\n# Description:\nThe sigmoid (or logistic) activation function is one of the most widely-used non-linearities in neural networks.  Mathematically it is defined as\n\ng(z) = 1 / (1 + e^(\u2212z))\n\nWrite a Python function named `sigmoid` that:\n1. Accepts a single input `z`, which can be a\n   \u2022 Python scalar (int or float)\n   \u2022 list/tuple of numbers, or\n   \u2022 NumPy `ndarray` of arbitrary shape.\n2. Computes the element-wise sigmoid of the input.\n3. Returns the result rounded to **four decimal places**.\n4. Is numerically stable for very large positive or negative values of *z* (i.e. must not overflow for |z| > 700).\n5. Preserves the input structure:\n   \u2022 If `z` is a scalar, return a float.\n   \u2022 Otherwise return a (nested) Python list produced via NumPy\u2019s `tolist()` method.\n\nIf every step is implemented correctly, calling `sigmoid([-1, 0, 1])` should return `[0.2689, 0.5, 0.7311]`.\n# Your code should start with:\n```python\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"Compute the numerically stable, element-wise sigmoid activation.\n\n    The function must work for a scalar, list/tuple, or NumPy ndarray input.\n\n    Args:\n        z: int, float, list, tuple, or numpy.ndarray \u2013 input values.\n\n    Returns:\n        float if *z* is a scalar, otherwise a Python list obtained with\n        ``numpy.ndarray.tolist()``, where every element is the sigmoid of the\n        corresponding input value and is rounded to four decimal places.\n    \"\"\"\n    # TODO: implement your solution here\n    pass\n```\n# Output Constraints:\nMake sure all results are rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 518, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Automatic One-Hot Decoding Decorator\n# Description:\nIn many machine-learning workflows classification labels are sometimes stored as one-hot encoded matrices (each row contains a single 1 indicating the class).  \nMost metric functions, however, expect the labels as plain one-dimensional integer arrays.  \nWrite a decorator called `unhot` that automatically converts any one-hot encoded **NumPy** array that is passed to the wrapped metric into its corresponding integer label representation before the metric is evaluated.\n\nBehaviour details\n1. The decorator receives a metric function that takes exactly two positional arguments `(actual, predicted)`, both being NumPy arrays of identical length.\n2. Inside the wrapper:\n   \u2022 If `actual` is two-dimensional **and** its second dimension is larger than one, treat it as one-hot and replace it with `actual.argmax(axis=1)`.\n   \u2022 Perform the same check and conversion for `predicted`.\n   \u2022 Arrays that are already one-dimensional (shape `(n,)`) or whose shape is `(n,1)` must stay unchanged.\n3. After the optional conversion the original metric is called with the processed arrays, and its return value is passed back to the caller **unmodified**.\n\nExample usage\n```\nimport numpy as np\n\n@unhot\ndef accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n    \"\"\"Simple accuracy rounded to 4 decimals.\"\"\"\n    return float(np.round(np.mean(actual == predicted), 4))\n\nactual    = np.array([[0,1,0], [1,0,0], [0,0,1]])  # one-hot\npredicted = np.array([[0,1,0], [0,1,0], [0,0,1]])  # one-hot\nprint(accuracy(actual, predicted))  # 0.6667\n```\nHere `accuracy` receives one-hot matrices but internally works with the 1-D label arrays `[1,0,2]` and `[1,1,2]`, giving an accuracy of `2/3 = 0.6667`.\n# Your code should start with:\n```python\nimport numpy as np\nfrom typing import Callable\nimport numpy as np\nfrom typing import Callable\n\ndef unhot(function: Callable) -> Callable:\n    \"\"\"Decorator that converts one-hot encoded label arrays to 1-D class labels.\n\n    If either *actual* or *predicted* is a 2-D array whose second dimension is\n    larger than one, the array is assumed to be one-hot encoded and is replaced\n    by its ``argmax`` along axis 1 before the wrapped *function* is executed.\n\n    Args:\n        function: A metric function accepting two NumPy arrays ``(actual,\n            predicted)`` and returning a value of any type.\n\n    Returns:\n        Callable: A new function that performs the described conversion and then\n        calls *function*.\n    \"\"\"\n    # =======  Write your code below  =======\n\n    # =======  End of your code  =======\n    \n    return wrapper\n```\n# Output Constraints:\nReturn the result exactly as produced by the wrapped metric \u2013 the decorator must not alter it.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 520, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Sigmoid Gradient\n# Description:\nIn neural-network back-propagation we often need the derivative of the sigmoid activation function.  \nGiven the value of the sigmoid function $g(z)=\\dfrac{1}{1+e^{-z}}$ ( **not** the pre\u2013activation $z$ itself), the derivative is\n\\[g'(z)=g(z)\\,[1-g(z)].\\]\nWrite a Python function that returns this gradient for a scalar, 1-D or 2-D input that already contains sigmoid outputs.  \nThe function must:\n\u2022 accept Python scalars or (nested) lists, or NumPy arrays containing floats in the closed interval $[0,1]$;  \n\u2022 compute the element-wise value $x(1-x)$;  \n\u2022 round every result to 4 decimal places;  \n\u2022 return a **Python object of the same rank**: for a scalar input return a float, otherwise return a (nested) list with the same shape as the input.\nIf the input is an empty list, return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\ndef sigmoid_grad(z):\n    \"\"\"Return the derivative of the sigmoid function given its output.\n\n    The input *z* already contains sigmoid values (numbers in [0,1]).  The\n    derivative is computed as z*(1-z) element-wise.\n\n    Args:\n        z: float, list or NumPy ndarray of sigmoid outputs.\n\n    Returns:\n        Same structure as *z* (float or nested list) with each value replaced\n        by its gradient, rounded to 4 decimal places.\n    \"\"\"\n    # TODO: implement this function\n    pass\n```\n# Output Constraints:\nAll numeric results must be rounded to the nearest 4th decimal using numpy.round(x,4).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 528, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Decision Boundary Grid Generation\n# Description:\nIn many machine-learning visualisations we first build a dense, rectangular grid that spans the training data and then ask a classifier to label every grid point.  The resulting matrix of labels can afterwards be used to draw decision boundaries with a contour plot.\n\nWrite a function that builds such a grid for a very simple **linear** classifier working in two dimensions.  The classifier is fully defined by a weight vector `W = [w\u2081 , w\u2082]` and a bias `b`.  A point x = (x\u2081 , x\u2082) is classified by the rule\n\n\u2003\u2003sign( w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b ) ,\n\nwhere `sign(z)` returns **1** when *z* \u2265 0 and **-1** otherwise.\n\nGiven:\n1. `X` \u2013 the original 2-D data set (shape *n\u00d72*) that should determine how wide the grid has to be,\n2. `W` \u2013 the length-2 list or tuple containing the classifier\u2019s weights,\n3. `b` \u2013 the bias term (a single number),\n4. `grid_n` \u2013 the desired resolution of the grid (default 100),\n\nyou must\n\u2022 build two equally spaced 1-D arrays `x1_plot` and `x2_plot`, each of length `grid_n`, that range from the minimum to the maximum value of the corresponding column of `X`,\n\u2022 create a mesh-grid from those arrays,\n\u2022 classify every grid point with the rule above and store the labels (-1 or 1) in a 2-D Python list having shape `grid_n \u00d7 grid_n`.\n\nReturn this list.  Do **not** use any third-party machine-learning libraries; only NumPy is allowed.\n\nIf either component of `W` is 0 the rule still works \u2013 implement it exactly as stated.\n# Your code should start with:\n```python\nimport numpy as np\ndef decision_boundary_grid(X: list[list[int | float]],\n                           W: list[int | float],\n                           b: float,\n                           grid_n: int = 100) -> list[list[int]]:\n    \"\"\"Generate a matrix of predictions for a 2-D linear classifier.\n\n    A point (x\u2081 , x\u2082) is labelled with 1 if w\u2081\u00b7x\u2081 + w\u2082\u00b7x\u2082 + b \u2265 0 and\n    with \u20111 otherwise.  The grid spans the bounding box of *X* and contains\n    *grid_n* points along each axis.\n\n    Args:\n        X: The original data set as a list of [x\u2081 , x\u2082] pairs.\n        W: List or tuple with exactly two weights [w\u2081 , w\u2082].\n        b: Bias term of the linear classifier.\n        grid_n: Number of grid points per axis (default 100).\n\n    Returns:\n        Nested Python list with shape *(grid_n \u00d7 grid_n)* containing only\n        1 and \u20111, the predicted labels of the grid points.\n    \"\"\"\n    # TODO: implement\n    pass\n```\n# Output Constraints:\nThe function must return a Python list of lists consisting solely of the integers 1 and -1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 537, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gaussian Naive Bayes Classifier from Scratch\n# Description:\nImplement a simple Gaussian Naive Bayes classifier from scratch. The function must\\n1. Learn the parameters (mean and standard deviation) of each feature for every class using the training data.\\n2. Compute class priors as the relative frequency of each class in the training set.\\n3. For every sample in `X_test`, calculate the posterior probability of the two classes under the Naive Bayes independence assumption and a Gaussian likelihood model.  \n   The likelihood of observing feature value $x$ given class $c$ is  \n   $$\\mathcal N(x\\mid\\mu_{c},\\sigma_{c}^2)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{c}}\\exp\\Bigl(\\!-\\,\\frac{(x-\\mu_{c})^2}{2\\sigma_{c}^2}\\Bigr).$$  \n4. Predict the label with the larger posterior probability for each test sample.  \n5. To avoid division by zero, add a very small constant $\\varepsilon=10^{-9}$ to every standard deviation.  \nThe function must return a Python list of integers where each element is either 0 or 1.\n# Your code should start with:\n```python\nimport numpy as np\ndef gaussian_naive_bayes(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> list[int]:\n    \"\"\"Gaussian Naive Bayes binary classifier.\n\n    Training phase: estimate mean, standard deviation and class prior for\n    every feature of both classes.\n    Prediction phase: compute the posterior probability for each class\n    using the Gaussian likelihood (with feature independence assumption)\n    and return the most probable class (0 or 1) for every test sample.\n\n    Args:\n        X_train: 2-D NumPy array of shape (n_samples, n_features) containing\n                  the training features.\n        y_train: 1-D NumPy array of length n_samples with binary labels\n                  (each entry is 0 or 1).\n        X_test:  2-D NumPy array of shape (m_samples, n_features) containing\n                  the data to classify.\n\n    Returns:\n        A Python list of length m_samples where each element is the\n        predicted class label (0 or 1).\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a Python list of integers (0 or 1) with the same length as `X_test`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 539, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Binary Cross-Entropy Cost\n# Description:\nIn binary-classification neural networks the last layer usually outputs a vector A\u1d38 of probabilities (values in the open interval (0,1)).  \nGiven the ground-truth label vector Y (0 or 1 for every sample) the **binary cross-entropy** (also called log-loss) is defined as  \n\n               J = \u2212 1\u2044m \u00b7 \u03a3 [ y\u00b7ln(a) + (1\u2212y)\u00b7ln(1\u2212a) ]\n\nwhere m is the number of training examples, a\u2208A\u1d38 and y\u2208Y.  \nWrite a function that implements this formula.\nThe function must:\n\u2022 accept two NumPy arrays AL and Y of identical shape (1,m) or (m,) holding the model probabilities and true labels;\n\u2022 return the scalar cost as a built-in Python float, not as a NumPy array;\n\u2022 internally clip the probabilities to the range [1 \u00d7 10\u207b\u00b9\u2075, 1 \u2212 1 \u00d7 10\u207b\u00b9\u2075] before taking the logarithm to avoid numerical overflow.\n# Your code should start with:\n```python\nimport numpy as np\ndef compute_cost(AL: np.ndarray, Y: np.ndarray) -> float:\n    \"\"\"TODO: Write the implementation.\"\"\"\n```\n# Output Constraints:\nReturn a plain Python float rounded to 4 decimal places when printed (internal precision can be higher).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 552, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Hidden Markov Model \u2013 Posterior State Distribution (\u03b3)\n# Description:\nIn a discrete Hidden Markov Model (HMM) the value  \n\u03b3\u209c(i)=P(q\u209c=s\u1d62 | O, \u03bb)  \nrepresents the posterior probability of being in state s\u1d62 at time step t after the complete observation sequence O has been seen (\u03bb denotes the HMM parameters).  \n\nDevelop a function that, for a given HMM (initial distribution, transition matrix and emission matrix), an observation sequence and a time index t, returns the vector \u03b3\u209c.  \n\nThe function must \u2013\n1. compute the forward probabilities \u03b1 (probability of the partial observation sequence up to t and state i at t),\n2. compute the backward probabilities \u03b2 (probability of the remaining observation sequence from t+1 given state i at t),\n3. combine them to obtain \u03b3\u209c(i)=\u03b1\u209c(i)\u03b2\u209c(i)/\u2211\u2c7c\u03b1\u209c(j)\u03b2\u209c(j),\n4. round every component of \u03b3\u209c to four decimal places and return the result as a Python list.\n\nIf t lies outside the range [0, len(observations) \u2212 1], return an empty list.\n# Your code should start with:\n```python\nimport numpy as np\ndef gamma_probabilities(hmm: dict, observations: list[int], t: int) -> list[float]:\n    \"\"\"Compute the posterior probability \u03b3\u209c for every hidden state in an HMM.\n\n    The implementation must use the forward-backward algorithm described in\n    the task description and return the resulting vector rounded to four\n    decimal places.\n\n    Args:\n        hmm (dict): HMM parameters with keys 'pi', 'A' and 'B'.\n        observations (list[int]): Sequence of observation indices.\n        t (int): Time step for which the posterior distribution is required.\n\n    Returns:\n        list[float]: Posterior probabilities for each state at time t.\n    \"\"\"\n    # Write your code below\n    pass\n```\n# Output Constraints:\nRound every value to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 555, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Density-Based Spatial Clustering (DBSCAN)\n# Description:\nImplement the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm from scratch.  \nThe function must group points that are densely packed together (points with many nearby neighbors) and mark as *noise* the points that lie alone in low-density regions.  \nFor a point to start (or expand) a cluster it has to be a *core* point \u2013 i.e. the number of points (including the point itself) inside an \\(\\varepsilon\\)-radius neighborhood is at least *min_samples*.  \nTwo points are considered to belong to the same cluster when they are density-reachable, meaning a chain of neighboring **core** points exists that links them.  \nYour task is to:\n1. Compute every pairwise Euclidean distance.\n2. Build the neighborhood list for each sample using the supplied \\(\\varepsilon\\).\n3. Identify *core* points (|neighborhood| \u2265 *min_samples*).\n4. Starting with the first unlabeled core point, expand a cluster with a breadth-first search (BFS):\n   \u2022 label the core point,\n   \u2022 put it into a queue,\n   \u2022 pop points from the queue and add every unlabeled neighbor to the current cluster; whenever a neighbor is itself a core point push it into the queue so the cluster can keep growing.\n5. After all core points are visited, every still-unlabeled point is *noise* and must receive the label \u22121.\n\nThe routine must return a list whose *i-th* value is the cluster id of the *i-th* sample (clusters are numbered 1,2,3,\u2026 in the order they are discovered; noise = \u22121).\n# Your code should start with:\n```python\nimport numpy as np\nfrom collections import deque\ndef dbscan(data: np.ndarray, eps: float, min_samples: int) -> list[int]:\n    \"\"\"Density-Based Spatial Clustering (DBSCAN).\n\n    Parameters\n    ----------\n    data : np.ndarray\n        A 2-D array with shape (n_samples, n_features).\n    eps : float\n        Radius that defines the neighbourhood of a point.\n    min_samples : int\n        Minimum number of points required to form a dense region (core point).\n\n    Returns\n    -------\n    list[int]\n        Cluster labels for every sample.  Noise points receive the label \u20131\n        while clusters are numbered 1, 2, 3, \u2026 in the order they are found.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list of integers where cluster ids start from 1; noise points must be labelled -1.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 561, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Gaussian Mixture Model via Expectation\u2013Maximization\n# Description:\nImplement the Expectation\u2013Maximization (EM) algorithm for a Gaussian Mixture Model (GMM).\n\nGiven a 2-D NumPy array containing N samples with D features and an integer K (number of Gaussian components), write a function that:\n1. Randomly initializes the parameters of K Gaussian components (mixture weights, means, full covariance matrices).\n2. Repeatedly performs the Expectation (E) and Maximization (M) steps until either the maximum number of iterations is reached or the change in mixture weights is smaller than a prescribed tolerance.\n3. After convergence assigns every sample to the component with the largest posterior probability (responsibility).\n4. Makes the output deterministic by sorting the components by the first coordinate of their mean in ascending order and **re-labelling** the cluster indices accordingly (left-most component \u21d2 label 0, next \u21d2 1, \u2026).\n\nReturn a Python list of length N containing the final cluster label of each sample.\n\nIf K = 1, all samples belong to the single component and the function must return a list filled with zeros.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef gmm_em(data: np.ndarray, k: int, max_iter: int = 200, epsilon: float = 1e-4) -> list[int]:\n    \"\"\"Your docstring here.\"\"\"\n    pass\n```\n# Output Constraints:\nReturned list must\n\u2022 have the same length as the number of input samples;\n\u2022 contain only the integers 0 \u2026 K\u22121 with no gaps;\n\u2022 be deterministic for identical inputs (the internal component ordering must not influence the external labels).\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 562, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Spectral Clustering\n# Description:\nImplement the Spectral Clustering algorithm without using any third-party machine-learning libraries.  \n\nGiven a set of points in a NumPy array `data` (shape `(N, d)` \u2013 `N` samples, `d` features) and the desired number of clusters `n_cluster`, the function must:  \n1. Build a fully\u2013connected similarity graph using the Gaussian kernel\n   \u2022 pairwise squared distance: $\\|x_i-x_j\\|^2$  \n   \u2022 similarity: $w_{ij}=\\exp(-\\gamma\\,\\|x_i-x_j\\|^2)$  (_`gamma` is a positive float, default 2.0_)  \n2. Construct the un-normalised Laplacian $L=D-W$ where $D$ is the degree diagonal.  \n   If `method=='normalized'`, use the symmetric normalized Laplacian  \n   $L_{sym}=D^{-1/2}LD^{-1/2}$.  \n3. Compute the eigenvectors that correspond to the `n_cluster` smallest eigen-values.  \n   If the normalized variant is chosen, row-normalise the eigenvector matrix.\n4. Run k-means in the eigenvector space to obtain final cluster labels.  \n   \u2022 Use a deterministic k-means that always picks the first `n_cluster` samples as the initial centroids.  \n   \u2022 After convergence, relabel clusters so that the cluster containing the smallest original index gets label 0, the next one 1, etc.  \n5. Return the labels as a Python list of length `N` with integers in `[0, n_cluster-1]`.\n\nIf `n_cluster` is 1 simply return a list of 0s.\n# Your code should start with:\n```python\nimport numpy as np\ndef spectral_clustering(data: np.ndarray, n_cluster: int, gamma: float = 2.0, method: str = 'unnormalized') -> list[int]:\n    \"\"\"Perform spectral clustering on the given dataset.\n\n    Args:\n        data: A NumPy array of shape (N, d) containing N samples with d features.\n        n_cluster: The number of clusters to form.\n        gamma: The gamma parameter of the Gaussian (RBF) kernel used to build the similarity graph.\n        method: Either 'unnormalized' or 'normalized' \u2013 specifies which Laplacian variant to use.\n\n    Returns:\n        A list of length N where the i-th element is an integer label identifying the\n        cluster assignment of the i-th sample (labels are 0-based).\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a standard Python `list` of integers, no additional formatting.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 563, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Dual-form Perceptron Learning\n# Description:\nImplement the Dual-form Perceptron learning algorithm.\n\nThe classical (primal) perceptron updates a weight vector **w** directly.  In the dual formulation the algorithm keeps a coefficient (\"alpha\") for every training example and performs the update in the feature\u2013space inner-product only.  When the algorithm converges the weight vector can be recovered as\n\nw = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62\n\nwhere x\u1d62 and y\u1d62 are the feature vector and class label (\u00b11) of the i-th training sample.\n\nTask\n-----\nWrite a function `perceptron_dual_train` that\n1. takes a 2-D NumPy array `X_data` (shape = *N \u00d7 d*) and a 1-D NumPy array `y_data` (length = *N*, containing only \u22121 or 1) plus an optional learning rate `eta` (default 1.0) and an optional `max_iter` (default 1000),\n2. trains a dual-form perceptron exactly as described below,\n3. returns a tuple consisting of the learned weight vector **w** (*list* of floats) and the bias term *b* (float).\n\nAlgorithm (must be followed literally)\n--------------------------------------\n1. Let `alpha = np.zeros(N)`, `b = 0`.\n2. Build the Gram matrix `G` where `G[i, j] = X_data[i]\u00b7X_data[j]`.\n3. Repeat until either\n   \u2022 an entire pass over the training set produces **no** update, **or**\n   \u2022 the number of complete passes reaches `max_iter`.\n   For every sample i (in the given order 0 \u2026 N\u22121):\n   \u2022 compute  g = \u03a3\u2c7c \u03b1\u2c7c y\u2c7c G[j, i]\n   \u2022 if  y\u1d62 ( g + b ) \u2264 0  then\n       \u2013 \u03b1\u1d62 \u2190 \u03b1\u1d62 + \u03b7\n       \u2013 b   \u2190 b + \u03b7 y\u1d62\n4. After finishing the loop, recover the weight vector through\n   w = \u03a3\u1d62 \u03b1\u1d62 y\u1d62 x\u1d62\n5. Round every component of **w** and *b* to **4 decimal places** and return them as `(w.tolist(), b)`.\n\nIf the algorithm reaches the iteration limit without converging you should still return the weights obtained so far.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef perceptron_dual_train(X_data: np.ndarray,\n                          y_data: np.ndarray,\n                          eta: float = 1.0,\n                          max_iter: int = 1000) -> tuple[list[float], float]:\n    \"\"\"Train a binary (\u00b11) perceptron using the dual formulation.\n\n    Args:\n        X_data: 2-D NumPy array containing N samples (shape = N \u00d7 d).\n        y_data: 1-D NumPy array of length N with labels \u22121 or 1.\n        eta:    Learning rate (default 1.0).\n        max_iter: Maximum number of complete passes over the data.\n\n    Returns:\n        A tuple (w, b) where w is the learned weight vector converted to a\n        Python list (each element rounded to 4 decimals) and b is the rounded\n        bias term.\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nReturn a tuple (w, b) where w is a Python list.  Round every number to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 564, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Decision-Tree Prediction\n# Description:\nYou are given the definition of a very small helper class called `Node` that is typically produced by a decision\u2013tree learning algorithm.  Every `Node` instance can store one of the following pieces of information:\n\u2022 `label` \u2013 an integer index of the feature that has to be inspected in the current sample (internal nodes only).\n\u2022 `x`     \u2013 the value associated with the edge that leads from the parent to this child (classification trees only).\n\u2022 `s`     \u2013 a numerical split-point (regression trees only).\n\u2022 `y`     \u2013 the value kept in the leaf (class label or real number, *already* the prediction).\n\u2022 `child` \u2013 a list that contains all children of the current node (either 2 children for regression, or 1-plus children for classification).\n\nYour task is to write a single function `predict_sample` that, given the **root** of such a tree, a list/tuple of feature values describing one sample, and the string `task_type`, returns the prediction stored in the appropriate leaf.\n\nThe traversal rules are fixed and must be implemented exactly as follows.\n\nClassification tree (`task_type == 'classification'`)\n1. If the current node already stores `y` (i.e., it is a leaf), return that value.\n2. Otherwise, inspect the feature with index `node.label`.\n3. Among the children of the current node choose the first child whose `x` attribute equals the inspected feature value and continue recursively.\n4. If no child matches, **default** to the second child in the list (`node.child[1]`).\n\nRegression tree (`task_type == 'regression'`)\n1. If the current node already stores `y` (leaf), return it.\n2. Otherwise, inspect the feature with index `node.label`.\n3. If the feature value is **less than or equal to** `node.s`, continue with the first child (`node.child[0]`); otherwise continue with the second child (`node.child[1]`).\n\nYou may assume that\n\u2022 the tree is well-formed and every internal node has exactly the number of children required by its type,\n\u2022 `task_type` is either `'classification'` or `'regression'`.\n\nReturn the value found in the reached leaf **unchanged** \u2013 do not perform any rounding or type conversion.\n# Your code should start with:\n```python\ndef predict_sample(root: \"Node\", features: list | tuple, task_type: str):\n    \"\"\"Return the prediction for one sample by traversing the decision tree.\n\n    Args:\n        root (Node): The root node of the decision tree.\n        features (list | tuple): The feature values of the sample.\n        task_type (str): Either 'classification' or 'regression'.\n\n    Returns:\n        The value stored in the reached leaf node.\n    \"\"\"\n```\n# Output Constraints:\nReturn the leaf value exactly as stored in the tree.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 565, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: ID3 Feature Selection \u2013 Choose the Best Feature for Maximum Information Gain\n# Description:\nImplement a utility function used in the ID3 decision-tree learning algorithm.  \nGiven a data matrix X (instances \u00d7 features) whose values are **discrete non-negative integers starting from 0** and a 1-D label vector y (also non-negative integers starting from 0), the task is to select the feature that maximises the **information gain** with respect to the class label.\n\nInformation gain of a feature A is defined as  \nIG(A)=H(y)\u2212H(y|A)  \nwhere H(y) is the entropy of the label distribution and H(y|A) is the conditional entropy obtained after splitting by the values of A.  \nIf two or more features obtain the same (maximal) information gain, the smallest column index must be returned.\n\nThe function must return a tuple `(best_feature_index, max_information_gain)` where the gain is rounded to **6 decimal places**.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef choose_best_feature(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n    \"\"\"Return the feature index that yields maximal information gain.\n\n    Parameters\n    ----------\n    X : np.ndarray\n        2-D array of shape (n_samples, n_features) containing discrete feature\n        values encoded as non-negative integers starting from 0.\n    y : np.ndarray\n        1-D array of shape (n_samples,) containing integer class labels\n        starting from 0.\n\n    Returns\n    -------\n    tuple[int, float]\n        A tuple consisting of the index of the best feature and the maximum\n        information gain rounded to 6 decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a tuple `(int, float)` where the float (information gain) is rounded to 6 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 566, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Mini Isolation Forest for Outlier Detection\n# Description:\nImplement a very small-scale version of the Isolation Forest algorithm for anomaly detection.\n\nGiven a data matrix **data** (NumPy array of shape *(n_samples, n_features)*), build *n_trees* random isolation trees, compute the average path length for every observation and convert it to an anomaly score\n\n             s(x)=2^{ -(\\bar h(x)/\u03c6)}\n\nwhere \\(\\bar h(x)\\) is the mean path length of *x* over all trees and\n\n             \u03c6 = 2\u00b7ln(n\u22121) \u2212 2\u00b7(n\u22121)/n.\n\nAn object is an outlier when its score is among the largest *\u03b5*\u00b7100 % of all scores. The function must return the (zero-based) indices of the detected outliers, **sorted increasingly**.\n\nThe isolation tree that you must use is a *purely random binary tree* built as follows:\n1. Draw a subsample of *sample_size* distinct rows (when *sample_size \u2265 n_samples* use the complete data).\n2. Recursively split the subsample until either\n   \u2022 the current depth reaches *height_limit = \u2308log\u2082(sample_size)\u2309*  \n   \u2022 or the split contains at most one sample.\n3. A split is performed by choosing a **random feature** and a **random threshold** uniformly inside the interval [min, max] of that feature in the current node.\n\nThe path length of an observation is the number of edges it traverses before it reaches a leaf. When a leaf that contains *n* samples is reached, the path length is corrected by *c(n)*, an approximation of the expected path length of unsuccessful searches in a binary search tree:\n\n             c(1)=0,          c(n)=2\u00b7ln(n\u22121)+0.5772156649\u22122\u00b7(n\u22121)/n for n>1.\n\nBecause the algorithm relies on randomness you must set a global seed (``np.random.seed(42)``) so that the function is perfectly reproducible and the tests are deterministic.\n\nIf *\u03b5 = 0* the function must return an empty list because no object is allowed to be classified as an outlier.\n\nYou are NOT allowed to use any third-party implementation of Isolation Forest \u2013 build the trees and compute the scores exactly as specified above.\n# Your code should start with:\n```python\nimport numpy as np\nimport math\ndef detect_outliers(data: np.ndarray, n_trees: int, sample_size: int, epsilon: float) -> list[int]:\n    \"\"\"Tiny Isolation Forest implementation.\n\n    Args:\n        data: 2-D NumPy array where each row is an observation and each column a feature.\n        n_trees: Number of random isolation trees that will be grown.\n        sample_size: Number of samples drawn (without replacement) to grow every tree.\n        epsilon: Fraction (0 \u2264 \u03b5 < 1) of observations that are considered outliers.\n\n    Returns:\n        A list with the zero-based indices of the detected outliers, sorted increasingly.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a Python list sorted in ascending order.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 568, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Maximum Entropy Classifier with Generalised Iterative Scaling\n# Description:\nImplement a **Maximum Entropy (MaxEnt)** classifier that uses the Generalized Iterative Scaling (GIS) algorithm to learn the weight of every (feature-value, label) pair from categorical training data. Your function must\n\n1. build the empirical distributions that GIS needs,\n2. iteratively update the weight vector until the largest absolute update is smaller than `epsilon` or the number of iterations reaches `n_iter`,\n3. return the predicted label (the one with the highest conditional probability) for every sample in the test set.\n\nA feature is treated as *present* when the column takes on a specific value.  The model\u2019s conditional distribution is\n\nP(y|x) = exp( \u03a3_j w_j\u00b7f_j(x,y) ) / Z(x)\n\nwhere every f_j(x,y) is a binary indicator for one concrete tuple *(feature_index, feature_value, y)*.\n\nYour task is to complete the function `maxent_predict`.  The function takes the training samples `X_train`, training labels `y_train`, the samples to classify `X_test`, and two optimiser hyper-parameters (`epsilon` and `n_iter`).  It must return a list containing the predicted label for every row in `X_test`.\n\nThe implementation must **only** use the standard library together with `numpy` and `math`.\n# Your code should start with:\n```python\nimport math\nfrom collections import defaultdict\nimport numpy as np\nimport math\nfrom collections import defaultdict\nimport numpy as np\n\ndef maxent_predict(\n    X_train: np.ndarray,\n    y_train: np.ndarray,\n    X_test: np.ndarray,\n    epsilon: float = 1e-3,\n    n_iter: int = 100,\n) -> list[int]:\n    \"\"\"Fill in here.\"\"\"\n    pass\n```\n# Output Constraints:\nReturn a `list[int]` whose length equals the number of rows in `X_test`.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 569, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Document-Frequency Keyword Statistics\n# Description:\nWrite a Python function that analyses a small collection of text documents and produces two results: (1) a list of all distinct words together with the fraction of documents in which each word appears (document-frequency ratio) ordered from the most common to the least common word, and (2) a set that contains only the *k* most common words (where *k* is supplied by the user through the parameter `cut_off`).\n\nEach document is represented by a tuple `(label, words)` where `label` can be ignored by your function and `words` is an **iterable** (list, set, tuple, etc.) of strings. If `cut_off` is `None` the set must contain **all** words. When two words have the same document-frequency ratio, their relative order in the returned list is not important.\n\nAfter counting the documents, divide every count by the total number of documents so that every ratio falls in the closed interval `[0, 1]`. Finally, round every ratio to 4 decimal places.\n\nIf `cut_off` is larger than the number of distinct words simply return all words in the set; if `cut_off` is `0` return an empty set.\n# Your code should start with:\n```python\nfrom collections import Counter\nfrom typing import Iterable, Tuple, List, Set, Any\n\n\ndef statistic_key_word(\n    data: List[Tuple[Any, Iterable[str]]],\n    cut_off: int | None = None\n) -> Tuple[List[Tuple[str, float]], Set[str]]:\n    \"\"\"Calculate how often each word appears across documents.\n\n    The function returns a list with the document-frequency ratio of every word\n    (sorted from highest to lowest) and a set containing only the *cut_off*\n    most frequent words. When *cut_off* is ``None`` all words are placed in the\n    set. Every ratio must be rounded to four decimal places.\n\n    Args:\n        data: Iterable of documents. Each document is represented by a tuple\n              ``(label, words)`` where *label* is ignored and *words* is an\n              iterable of strings.\n        cut_off: Number of top words to include in the returned set, or\n                  ``None`` to include every word.\n\n    Returns:\n        Tuple with (1) a list of (word, ratio) pairs and (2) a set of the most\n        frequent words as described above.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nAll frequency ratios must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 571, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Compute Linear SVM Parameters from Lagrange Multipliers\n# Description:\nIn the Sequential Minimal Optimization (SMO) algorithm for training a (soft-margin) linear Support Vector Machine, once the optimal Lagrange multipliers $\\alpha\\_i$ have been found, the separating hyper-plane is recovered with the following formulas:\n\n\u2022 Weight vector\u2003$\\displaystyle \\mathbf w = \\sum\\_{i=1}^{n}\\alpha\\_i y\\_i \\mathbf x\\_i = X^\\top(\\boldsymbol\\alpha\\odot\\mathbf y)$\n\n\u2022 Bias\u2003$\\displaystyle b = \\frac{1}{n}\\sum\\_{i=1}^{n}\\bigl(y\\_i-\\mathbf w^{\\top}\\mathbf x\\_i\\bigr)$\n\nwhere $X\\in\\mathbb R^{n\\times d}$ is the training matrix, $\\mathbf y\\in\\{\\!-1,1\\}^n$ the label vector and $\\boldsymbol\\alpha\\in\\mathbb R^{n}$ the multiplier vector.\n\nWrite a function compute_svm_parameters that receives X, y and alpha (all NumPy arrays), computes the weight vector w and the bias term b using the formulas above, rounds every value to 4 decimal places and returns them as a tuple (w_list, b).\n\nIf X contains only one feature, w should still be returned as a one-dimensional Python list.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef compute_svm_parameters(X: np.ndarray, y: np.ndarray, alpha: np.ndarray) -> tuple[list[float], float]:\n    \"\"\"Compute the weight vector and bias term of a linear SVM.\n\n    The function implements the final reconstruction step of the\n    Sequential Minimal Optimization (SMO) algorithm for a hard/soft\n    margin linear Support Vector Machine.\n\n    Args:\n        X: 2-D NumPy array of shape (n_samples, n_features) containing the\n           training samples.\n        y: 1-D NumPy array of shape (n_samples,) with class labels (+1 or\n           \u22121).\n        alpha: 1-D NumPy array of shape (n_samples,) containing the\n           optimized Lagrange multipliers.\n\n    Returns:\n        A tuple (w_list, b) where w_list is a Python list with the weight\n        vector and b is the bias term. Both w and b are rounded to four\n        decimal places.\n    \"\"\"\n    # TODO: implement the function\n    pass\n```\n# Output Constraints:\nRound all elements of the weight vector and the bias term to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 572, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Local Outlier Factor (LOF) Detection\n# Description:\nThe Local Outlier Factor (LOF) algorithm is a popular density\u2013based method used to detect anomalous samples in a data set.  \nA sample is considered an **outlier** if its local density is significantly lower than the density of its neighbours.\n\nYou have to implement the core steps of the algorithm from scratch (do **NOT** rely on `scipy`, `sklearn`, etc.):  \n1. Compute the full pair\u2013wise Euclidean distance matrix.  \n2. For every sample *p* obtain its *k-distance* \u2013 the distance to its *k*-th nearest neighbour \u2013 and the index list of those *k* nearest neighbours *N<sub>k</sub>(p)*.  \n3. Define the reachability distance between two points as  \n   reach-dist<sub>k</sub>(p,q)=max(k-distance(q),\u2006dist(p,q)).  \n4. The Local Reachability Density (LRD) of *p* is  \n   LRD<sub>k</sub>(p)=k / \u03a3<sub>q\u2208N<sub>k</sub>(p)</sub> reach-dist<sub>k</sub>(p,q).  \n5. Finally the Local Outlier Factor is  \n   LOF<sub>k</sub>(p)= (\u03a3<sub>q\u2208N<sub>k</sub>(p)</sub> LRD<sub>k</sub>(q) / k) / LRD<sub>k</sub>(p).  \n\nReturn a tuple consisting of the list of LOF scores rounded to four decimals and the ascending list of indices of all samples whose LOF score is strictly larger than `epsilon`.\n\nConstraints\n\u2022 `1 \u2264 k < n` where `n` is the number of samples.  \n\u2022 Round every LOF score to the **nearest 4th decimal place** using `numpy.round`.\n# Your code should start with:\n```python\nimport numpy as np\ndef local_outlier_factor(data, k, epsilon=1.5):\n    \"\"\"Your task is to implement this function!\"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nReturn a tuple: (list_of_scores, list_of_outlier_indices)\nAll scores must be rounded to 4 decimals.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 574, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Synthetic 2-D Data Generator\n# Description:\nImplement a function that creates a simple 2-D synthetic data-set that is often used for quick experiments or visualisations. \nFor every class label c\u2208{0,\u2026,m\u22121} the function must generate two groups of points:\n \u2022 n_train training points that will be stored in X_train and whose labels (the value c) will be stored in y_train.\n \u2022 n_val validation points that will be stored in X_val and whose labels (again the value c) will be stored in y_val.\n\nThe coordinates of all points for a given class are sampled independently from the continuous uniform distribution on a square that is 8 units wide and centred 10\u00b7\u230ac/2\u230b+5 on both axes, i.e.\n         base = 10\u00b7\u230ac/2\u230b\n         x ~ U(base+1 , base+9)\n         y ~ U(base+1 , base+9)\n\nIf a seed is supplied the function has to call random.seed(seed) so that two successive calls with the same seed return exactly the same arrays.  All coordinates in the returned arrays must be rounded to **4 decimal places** so that the output is compact and deterministic.\n\nThe function returns a tuple of four NumPy arrays:\n(X_train  (m\u00b7n_train, 2),\n X_val    (m\u00b7n_val  , 2),\n y_train  (m\u00b7n_train,),\n y_val    (m\u00b7n_val  ,))\n# Your code should start with:\n```python\nimport numpy as np\nimport random\ndef generate_data(m: int,\n                  n_train: int,\n                  n_val: int,\n                  seed: int | None = None) -> tuple:\n    \"\"\"Generate synthetic 2-D data for *m* classes.\n\n    Parameters\n    ----------\n    m : int\n        Number of classes.\n    n_train : int\n        Points per class in the training set.\n    n_val : int\n        Points per class in the validation set.\n    seed : int | None, optional\n        Random seed for reproducibility.  If *None* the global random\n        state is used as is.\n\n    Returns\n    -------\n    tuple\n        (X_train, X_val, y_train, y_val) as NumPy arrays.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nEvery coordinate must be rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 579, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Deterministic K-Means Clustering\n# Description:\nImplement the classical (Lloyd-style) **K-Means** clustering algorithm from scratch.  \n\nGiven a set of *m* d-dimensional points X and a desired number of clusters *k*, the algorithm must\n\n1. **Initialisation** \u2013 take the **first** *k* points in the order they appear in *X* as the initial cluster centres (this makes the result deterministic and therefore testable).\n2. **Assignment step** \u2013 for every point, compute the Euclidean distance to each centre and assign the point to the nearest one.  In the event of a tie, choose the centre with the smaller index.\n3. **Update step** \u2013 recompute every centre as the arithmetic mean of all points currently assigned to that centre.  If a centre loses all its points, keep it unchanged.\n4. Repeat steps 2-3 until the assignments stop changing or until `max_iters` iterations have been performed.\n\nReturn the final point labels *and* the final cluster centres.\n\nAll coordinates of the returned centres must be rounded to **4 decimal places** so that the results are easily comparable.\n\nYou are **not** allowed to use any implementation that already exists in external libraries such as `scikit-learn`; only base Python and NumPy may be used.\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef kmeans(X: np.ndarray, k: int, max_iters: int = 100) -> tuple[list[int], list[list[float]]]:\n    \"\"\"Cluster the data points in `X` into `k` groups using the K-Means algorithm.\n\n    The first `k` points serve as the initial centres.  Lloyd iterations are\n    performed until the assignments cease to change or the iteration limit is\n    reached.  The function returns the final label of every point as well as the\n    coordinates of the cluster centres (rounded to 4 decimals).\n\n    Args:\n        X: 2-D NumPy array of shape (m, d) containing the data set.\n        k: Number of clusters (1 \u2264 k \u2264 m).\n        max_iters: Upper bound on the number of iterations to execute.\n\n    Returns:\n        A tuple `(labels, centres)` where\n            labels  \u2013 list of length *m* with the cluster index of each point.\n            centres \u2013 list with *k* inner lists, each the rounded coordinates\n                      of a centre.\n    \"\"\"\n    # TODO: write your code here\n    pass\n```\n# Output Constraints:\nEvery coordinate of the returned centres must be rounded to the nearest 4th decimal.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 581, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Elastic-Net Regression from Scratch\n# Description:\nImplement Elastic-Net regularised linear regression trained with batch gradient descent.\n\nGiven\n\u2022 a 2-D NumPy array X of shape (m, n) that stores m training samples and n features,\n\u2022 a 1-D NumPy array y of length m that stores the corresponding target values,\n\u2022 a learning rate \u03b1,\n\u2022 the number of gradient-descent iterations,\n\u2022 two non-negative hyper-parameters \u03bb\u2081 (the L1 penalty) and \u03bb\u2082 (the L2 penalty),\n\nyou must start with all weights w\u2081 \u2026 w\u2099 and the bias term b equal to 0 and perform \u201citerations\u201d rounds of simultaneous parameter updates.\n\nFor every iteration compute the predictions y\u0302 = X\u00b7w + b and the residual r = y \u2013 y\u0302.  The gradients for every weight j and the bias are\n    \u2202L/\u2202w\u2c7c = \u22122\u00b7X[:, j]\u1d40\u00b7r + \u03bb\u2081\u00b7sign(w\u2c7c) + 2\u00b7\u03bb\u2082\u00b7w\u2c7c\n    \u2202L/\u2202b   = \u22122\u00b7\u03a3 r\nwhere sign(0) is defined as \u22121 so that the first update for each weight uses \u2212\u03bb\u2081 (this reproduces the behaviour in the given code).  Divide every gradient by m (the data set size) to obtain the mean gradient and update the parameters with learning rate \u03b1:\n    w\u2c7c \u2190 w\u2c7c \u2212 \u03b1\u00b7(\u2202L/\u2202w\u2c7c) / m\n    b  \u2190 b  \u2212 \u03b1\u00b7(\u2202L/\u2202b)  / m\n\nAfter all iterations finish return the learned weight vector and the bias rounded to four decimal places.\n\nIf either the learning rate is 0 or the number of iterations is 0 simply return the initial parameters ([0.0 \u2026 0.0], 0.0).\n# Your code should start with:\n```python\nimport numpy as np\nimport numpy as np\n\ndef elastic_net_regression(X: np.ndarray,\n                           y: np.ndarray,\n                           learning_rate: float,\n                           iterations: int,\n                           l1_penalty: float,\n                           l2_penalty: float) -> tuple[list[float], float]:\n    \"\"\"Fits a linear model with Elastic-Net regularisation.\n\n    Your task is to complete this function so that it performs batch gradient\n    descent for the given number of iterations and returns the learned weight\n    vector and bias.  All returned values must be rounded to 4 decimal places.\n\n    Args:\n        X: A 2-D NumPy array of shape (m, n) containing the input features.\n        y: A 1-D NumPy array of length m containing the target values.\n        learning_rate: Step size for gradient descent (\u03b1).\n        iterations: Number of optimisation steps to perform.\n        l1_penalty: L1 regularisation strength (\u03bb\u2081).\n        l2_penalty: L2 regularisation strength (\u03bb\u2082).\n\n    Returns:\n        Tuple (weights, bias) where weights is a list of length n and bias is a\n        float.  Every number must be rounded to four decimal places.\n    \"\"\"\n    pass\n```\n# Output Constraints:\nReturn a tuple (weights, bias) where\n\u2022 weights is a Python list of length n,\n\u2022 every number in the tuple is rounded to 4 decimal places.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 591, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: Feed-Forward Neural Network Prediction\n# Description:\nYou are given a fully-connected feed-forward neural network whose parameters (i.e. the weight matrices) are already known. Every hidden layer uses the ReLU activation function and the last layer uses a linear (identity) activation, so the network can be employed for regression.  \n\nThe weight matrices are stored in a nested python list with the following convention:\n1. ``weights[L]`` is the weight matrix of layer ``L`` (``L = 0, \u2026 , n_layers-1``).\n2. Each element of ``weights[L]`` represents one neuron and therefore is itself a list containing that neuron\u2019s weights.\n3. The first weight of every neuron is the **bias weight**; the remaining weights are the connection weights coming from the previous layer.\n4. The size of a neuron\u2019s weight list is therefore ``previous_layer_size + 1``.\n\nFor a single input vector ``x`` (which does **not** contain the bias term) you have to compute the network\u2019s output by successively\n\u2022 adding the bias input ``1`` to the current input,\n\u2022 performing a dot product with the corresponding weight matrix, and\n\u2022 applying ReLU to all layers except the last one (the last layer is linear).\n\nReturn the network\u2019s prediction rounded to four decimals.  \nIf the network has exactly one output neuron, return a single ``float``.  \nIf it has more than one output neuron, return a list of ``float`` s in the same order as the neurons appear in the last layer.\n\nYou may **not** modify the given weights in-place and you may **only** use the standard library together with NumPy.\n# Your code should start with:\n```python\nimport numpy as np\ndef neural_net_predict(weights: list[list[list[float]]], x: list[float]) -> float | list[float]:\n    \"\"\"Perform a forward pass through a fully-connected ReLU neural network.\n\n    The network topology and parameters are encoded in *weights* where:\n      \u2022 *weights[L]* \u2013 weight matrix of layer L,\n      \u2022 each *weights[L][i]* \u2013 list of weights of neuron i of layer L,\n      \u2022 the first weight of every neuron is its bias weight.\n\n    All hidden layers use ReLU, the last layer is linear.\n\n    Args:\n        weights: A three-level nested list containing the network\u2019s weights.\n        x:       Input feature vector (without the bias term).\n\n    Returns:\n        The prediction rounded to four decimals (float if the output layer has\n        one neuron, otherwise list of floats).\n    \"\"\"\n    # Write your code here\n    pass\n```\n# Output Constraints:\nRound the final numeric result(s) to the nearest 4th decimal before returning.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
{"task_id": 595, "prompt": "You are an expert Python programmer, and here is your task:\n# Task: K-Means Clustering \u2013 Compute Centroids Only\n# Description:\nImplement the K-Means clustering algorithm **from scratch** (no third-party ML libraries).  \nThe function receives a 2-D NumPy array `X` (shape: *n_samples \u00d7 n_features*) and an integer `k` \u2013 the number of clusters.  \n\nAlgorithm requirements\n1. Initialise the centroids with the **first** `k` samples in `X` (guarantees deterministic results).\n2. Repeat for at most `max_iters` iterations (default = 100):\n   \u2022 Assign every sample to the nearest centroid using the squared Euclidean distance.\n   \u2022 Update each centroid to the arithmetic mean of the samples currently assigned to it.  \n3. Stop early if all centroids move less than `1e-4` in a full iteration.\n4. If a cluster becomes empty during an update, keep its centroid unchanged.\n5. After convergence, sort the centroids in ascending lexicographical order (first feature, then second, \u2026) and round every coordinate to **4 decimal places**.\n\nReturn the list of sorted, rounded centroids.\n# Your code should start with:\n```python\nimport numpy as np\ndef kmeans_centroids(X: \"np.ndarray\", k: int, max_iters: int = 100) -> list[list[float]]:\n    \"\"\"Your task: implement here. See docstring in the description.\"\"\"  \n    pass\n```\n# Output Constraints:\nReturn a Python **list of lists** (not NumPy array) where each inner list is a centroid rounded to 4 decimals.\n\nPlease think step by step and provide a self-contained Python script that solves the above task in a markdown code block.\n"}
