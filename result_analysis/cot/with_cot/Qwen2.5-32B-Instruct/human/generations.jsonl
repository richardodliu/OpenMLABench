{"task_id": 1, "completion_id": 0, "completion": "To solve the given task, we need to follow these steps:\n\n1. Check if the number of columns in the matrix `a` matches the length of the vector `b`. If not, return `-1`.\n2. If the dimensions are compatible, compute the dot product of each row of the matrix `a` with the vector `b`.\n3. Return the resulting list of dot products.\n\nHere is the Python code that accomplishes this:\n\n```python\ndef matrix_dot_vector(a: list[list[int|float]], b: list[int|float]) -> list[int|float]:\n    # Check if the number of columns in the matrix matches the length of the vector\n    if not a or not b or len(a[0]) != len(b):\n        return -1\n    \n    # Compute the dot product of each row of the matrix with the vector\n    result = []\n    for row in a:\n        dot_product = sum(x * y for x, y in zip(row, b))\n        result.append(dot_product)\n    \n    return result\n\n# Example usage:\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nvector = [1, 0, 1]\nprint(matrix_dot_vector(matrix, vector))  # Output: [4, 10, 16]\n```\n\nThis code defines the function `matrix_dot_vector` which takes a matrix `a` and a vector `b` as input. It first checks if the dimensions are compatible. If they are, it computes the dot product for each row of the matrix with the vector and returns the list of results. If the dimensions are not compatible, it returns `-1`."}
{"task_id": 2, "completion_id": 0, "completion": "To solve the task of computing the transpose of a given matrix, we can follow these steps:\n\n1. **Understand the transpose operation**: The transpose of a matrix is a new matrix whose rows are the columns of the original matrix, and whose columns are the rows of the original matrix. For example, if the original matrix is `a`, then the element at `a[i][j]` will be at `transpose[j][i]` in the transposed matrix.\n\n2. **Initialize the transposed matrix**: We need to create a new matrix with dimensions that are the reverse of the original matrix. If the original matrix has dimensions `m x n`, the transposed matrix will have dimensions `n x m`.\n\n3. **Fill the transposed matrix**: Iterate over the elements of the original matrix and place them in the correct positions in the transposed matrix.\n\nHere is the Python script that implements the above steps:\n\n```python\ndef transpose_matrix(a: list[list[int|float]]) -> list[list[int|float]]:\n    # Get the number of rows and columns in the original matrix\n    rows = len(a)\n    cols = len(a[0]) if a else 0\n    \n    # Initialize the transposed matrix with zeros\n    transposed = [[0 for _ in range(rows)] for _ in range(cols)]\n    \n    # Fill the transposed matrix\n    for i in range(rows):\n        for j in range(cols):\n            transposed[j][i] = a[i][j]\n    \n    return transposed\n\n# Example usage\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\ntransposed_matrix = transpose_matrix(matrix)\nprint(\"Original Matrix:\")\nfor row in matrix:\n    print(row)\nprint(\"\\nTransposed Matrix:\")\nfor row in transposed_matrix:\n    print(row)\n```\n\nThis script defines a function `transpose_matrix` that takes a matrix `a` as input and returns its transpose. The example usage demonstrates how to use this function with a 3x3 matrix."}
{"task_id": 3, "completion_id": 0, "completion": "To solve the task of reshaping a matrix into a specified shape, we can follow these steps:\n\n1. Import the necessary package, `numpy`, which provides a powerful array object and functions to manipulate these arrays.\n2. Define the function `reshape_matrix` that takes a matrix `a` and a new shape `new_shape` as input.\n3. Check if the total number of elements in the original matrix matches the total number of elements in the new shape. If not, return an empty list `[]`.\n4. Use `numpy` to convert the input list to a numpy array, reshape it to the new shape, and then convert it back to a Python list using the `tolist()` method.\n5. Return the reshaped matrix as a Python list.\n\nHere is the complete Python script:\n\n```python\nimport numpy as np\n\ndef reshape_matrix(a: list[list[int|float]], new_shape: tuple[int, int]) -> list[list[int|float]]:\n    # Convert the input list to a numpy array\n    a_np = np.array(a)\n    \n    # Calculate the total number of elements in the original matrix\n    original_size = a_np.size\n    \n    # Calculate the total number of elements in the new shape\n    new_size = new_shape[0] * new_shape[1]\n    \n    # Check if the reshaping is possible\n    if original_size != new_size:\n        return []\n    \n    # Reshape the numpy array to the new shape\n    reshaped_np = a_np.reshape(new_shape)\n    \n    # Convert the reshaped numpy array back to a Python list\n    reshaped_list = reshaped_np.tolist()\n    \n    return reshaped_list\n\n# Example usage:\n# matrix = [[1, 2], [3, 4]]\n# new_shape = (1, 4)\n# print(reshape_matrix(matrix, new_shape))  # Output: [[1, 2, 3, 4]]\n```\n\nThis script defines the function `reshape_matrix` that performs the required operation and includes a comment for example usage. The function checks if the reshaping is possible based on the total number of elements and returns the reshaped matrix as a Python list or an empty list if reshaping is not possible."}
{"task_id": 4, "completion_id": 0, "completion": "To solve this task, we will follow these steps:\n\n1. Import the necessary package to perform mathematical operations, specifically to calculate the mean.\n2. Define the function `calculate_matrix_mean` with the required parameters.\n3. Check the mode to determine whether to calculate the mean by row or by column.\n4. Use a loop to iterate through the matrix and calculate the mean for each row or column.\n5. Return the list of means.\n\nHere is the self-contained Python script that accomplishes the task:\n\n```python\nfrom typing import List\nimport numpy as np\n\ndef calculate_matrix_mean(matrix: List[List[float]], mode: str) -> List[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n    \n    :param matrix: A list of lists of floats representing the matrix.\n    :param mode: A string indicating whether to calculate the mean by 'row' or 'column'.\n    :return: A list of means according to the specified mode.\n    \"\"\"\n    if mode == 'row':\n        # Calculate mean for each row\n        means = [np.mean(row) for row in matrix]\n    elif mode == 'column':\n        # Transpose the matrix to make columns into rows, then calculate mean\n        transposed_matrix = np.array(matrix).T.tolist()\n        means = [np.mean(col) for col in transposed_matrix]\n    else:\n        raise ValueError(\"Mode must be 'row' or 'column'\")\n    \n    return means\n\n# Example usage\nmatrix = [\n    [1.0, 2.0, 3.0],\n    [4.0, 5.0, 6.0],\n    [7.0, 8.0, 9.0]\n]\n\nprint(\"Row means:\", calculate_matrix_mean(matrix, 'row'))\nprint(\"Column means:\", calculate_matrix_mean(matrix, 'column'))\n```\n\nThis script uses `numpy` for its efficient array handling and mean calculation. The `calculate_matrix_mean` function checks the mode and calculates the mean accordingly. If the mode is 'column', it first transposes the matrix to convert columns into rows, making it easier to calculate the mean for each column."}
{"task_id": 5, "completion_id": 0, "completion": "To solve the task of scalar multiplication of a matrix, we need to iterate through each element of the matrix and multiply it by the scalar. The result should be a new matrix with the same dimensions as the original matrix, where each element is the product of the original element and the scalar.\n\nHere's a step-by-step breakdown of the solution:\n1. Define the function `scalar_multiply` that takes a matrix and a scalar as arguments.\n2. Use a nested list comprehension to iterate over each row and each element in the row, multiplying each element by the scalar.\n3. Return the new matrix.\n\nBelow is the self-contained Python script that implements the solution:\n\n```python\ndef scalar_multiply(matrix: list[list[int|float]], scalar: int|float) -> list[list[int|float]]:\n    \"\"\"\n    Multiplies each element of the given matrix by the scalar and returns the resulting matrix.\n    \n    :param matrix: A list of lists representing the matrix.\n    :param scalar: An integer or float representing the scalar value.\n    :return: A new matrix with each element multiplied by the scalar.\n    \"\"\"\n    # Use a nested list comprehension to multiply each element by the scalar\n    return [[element * scalar for element in row] for row in matrix]\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a matrix and a scalar\n    matrix = [\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ]\n    scalar = 2\n    \n    # Call the function and print the result\n    result = scalar_multiply(matrix, scalar)\n    for row in result:\n        print(row)\n```\n\nThis script defines the function `scalar_multiply` and demonstrates its usage with an example matrix and scalar. The output will be a new matrix where each element is twice the value of the corresponding element in the original matrix."}
{"task_id": 6, "completion_id": 0, "completion": "To calculate the eigenvalues of a 2x2 matrix, we can use the characteristic equation of the matrix. For a 2x2 matrix, the characteristic equation is a quadratic equation, and its solutions (the eigenvalues) can be found using the quadratic formula. The general form of a 2x2 matrix is:\n\n\\[ A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\]\n\nThe characteristic equation is given by:\n\n\\[ \\text{det}(A - \\lambda I) = 0 \\]\n\nWhere \\( \\lambda \\) represents the eigenvalues, and \\( I \\) is the identity matrix. For a 2x2 matrix, this equation simplifies to:\n\n\\[ \\lambda^2 - (a + d)\\lambda + (ad - bc) = 0 \\]\n\nThe solutions to this equation are the eigenvalues, which can be found using the quadratic formula:\n\n\\[ \\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\n\nIn this case, \\( a = 1 \\), \\( b = -(a + d) \\), and \\( c = ad - bc \\).\n\nLet's write the Python function to calculate the eigenvalues and sort them from highest to lowest.\n\n```python\nimport math\n\ndef calculate_eigenvalues(matrix: list[list[float|int]]) -> list[float]:\n    \"\"\"\n    Calculate the eigenvalues of a 2x2 matrix and return them sorted from highest to lowest.\n    \n    :param matrix: A 2x2 matrix represented as a list of lists.\n    :return: A list containing the eigenvalues, sorted from highest to lowest.\n    \"\"\"\n    a, b = matrix[0]\n    c, d = matrix[1]\n    \n    # Coefficients for the quadratic equation\n    A = 1\n    B = -(a + d)\n    C = (a * d) - (b * c)\n    \n    # Calculate the discriminant\n    discriminant = B**2 - 4 * A * C\n    \n    # Calculate the two solutions (eigenvalues)\n    eigenvalue1 = (-B + math.sqrt(discriminant)) / (2 * A)\n    eigenvalue2 = (-B - math.sqrt(discriminant)) / (2 * A)\n    \n    # Return the eigenvalues sorted from highest to lowest\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\n\n# Example usage\nmatrix = [[4, 2], [1, 3]]\neigenvalues = calculate_eigenvalues(matrix)\nprint(\"Eigenvalues:\", eigenvalues)\n```\n\nThis script defines the function `calculate_eigenvalues` which calculates the eigenvalues of a given 2x2 matrix and returns them sorted from highest to lowest. The example usage demonstrates how to use this function with a sample matrix."}
{"task_id": 7, "completion_id": 0, "completion": "To solve the given task, we need to follow these steps:\n\n1. **Import necessary libraries**: We need `numpy` for matrix operations and to check for matrix invertibility.\n2. **Check if matrices $T$ and $S$ are invertible**: A matrix is invertible if its determinant is not zero.\n3. **Perform the transformation**: If both matrices are invertible, we compute $T^{-1}AS$.\n4. **Return the result**: The result should be rounded to the nearest 4th decimal and returned as a Python list.\n\nHere is the complete Python script to achieve this:\n\n```python\nimport numpy as np\n\ndef transform_matrix(A: list[list[int|float]], T: list[list[int|float]], S: list[list[int|float]]) -> list[list[int|float]]:\n    # Convert lists to numpy arrays for easier manipulation\n    A = np.array(A)\n    T = np.array(T)\n    S = np.array(S)\n    \n    # Check if T and S are square matrices\n    if T.shape[0] != T.shape[1] or S.shape[0] != S.shape[1]:\n        return -1\n    \n    # Check if T and S are invertible by calculating their determinants\n    det_T = np.linalg.det(T)\n    det_S = np.linalg.det(S)\n    \n    if det_T == 0 or det_S == 0:\n        return -1  # Not invertible\n    \n    # Calculate the inverse of T\n    T_inv = np.linalg.inv(T)\n    \n    # Perform the transformation T^{-1}AS\n    transformed_matrix = np.dot(T_inv, np.dot(A, S))\n    \n    # Round the result to the nearest 4th decimal\n    transformed_matrix = np.round(transformed_matrix, 4)\n    \n    # Convert the result back to a Python list\n    return transformed_matrix.tolist()\n\n# Example usage:\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[1, 0], [0, 1]]\nprint(transform_matrix(A, T, S))\n```\n\n### Explanation:\n- **Matrix Invertibility Check**: We use `np.linalg.det` to check if the determinant of $T$ and $S$ is zero. If it is, the matrix is not invertible, and we return `-1`.\n- **Matrix Transformation**: We use `np.linalg.inv` to compute the inverse of $T$ and then perform the matrix multiplication $T^{-1}AS$ using `np.dot`.\n- **Rounding and Conversion**: The result is rounded to 4 decimal places using `np.round` and then converted to a Python list using `tolist()`.\n\nThis script is self-contained and should work for any valid input matrices $A$, $T$, and $S$."}
{"task_id": 8, "completion_id": 0, "completion": "To solve the task of calculating the inverse of a 2x2 matrix, we need to follow these steps:\n\n1. **Check if the matrix is 2x2**: The function should first ensure that the input matrix is indeed a 2x2 matrix.\n2. **Calculate the determinant**: The determinant of a 2x2 matrix \\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) is calculated as \\(ad - bc\\). If the determinant is zero, the matrix is not invertible, and we should return `None`.\n3. **Calculate the inverse**: If the determinant is not zero, the inverse of the matrix can be calculated using the formula:\n   \\[\n   \\text{Inverse} = \\frac{1}{\\text{determinant}} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\n   \\]\n4. **Return the inverse matrix**: Return the calculated inverse matrix.\n\nHere is the Python script that implements the above steps:\n\n```python\ndef inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    # Step 1: Check if the matrix is 2x2\n    if len(matrix) != 2 or len(matrix[0]) != 2 or len(matrix[1]) != 2:\n        raise ValueError(\"Input matrix must be 2x2\")\n    \n    # Step 2: Calculate the determinant\n    a, b = matrix[0]\n    c, d = matrix[1]\n    determinant = a * d - b * c\n    \n    # Check if the matrix is invertible\n    if determinant == 0:\n        return None\n    \n    # Step 3: Calculate the inverse\n    inverse_matrix = [\n        [d / determinant, -b / determinant],\n        [-c / determinant, a / determinant]\n    ]\n    \n    # Step 4: Return the inverse matrix\n    return inverse_matrix\n\n# Example usage\nmatrix = [[4, 7], [2, 6]]\nprint(inverse_2x2(matrix))  # Output should be the inverse of the matrix if it exists\n```\n\nThis script defines a function `inverse_2x2` that takes a 2x2 matrix as input and returns its inverse if it exists. If the matrix is not invertible (i.e., its determinant is zero), the function returns `None`."}
{"task_id": 9, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that multiplies two matrices. The function should first check if the multiplication is possible, which is the case if the number of columns in the first matrix is equal to the number of rows in the second matrix. If the matrices cannot be multiplied, the function should return `-1`. If they can be multiplied, the function should return the resulting matrix.\n\nHere's a step-by-step breakdown of the solution:\n\n1. **Check if multiplication is possible**: Compare the number of columns in the first matrix with the number of rows in the second matrix.\n2. **Initialize the result matrix**: Create a matrix with the appropriate dimensions (rows of the first matrix by columns of the second matrix).\n3. **Perform matrix multiplication**: Use nested loops to calculate the elements of the result matrix.\n4. **Return the result**: Return the result matrix if multiplication is possible, otherwise return `-1`.\n\nHere is the complete Python script:\n\n```python\ndef matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]] | int:\n    # Check if the number of columns in the first matrix is equal to the number of rows in the second matrix\n    if len(a[0]) != len(b):\n        return -1\n    \n    # Initialize the result matrix with zeros\n    result = [[0 for _ in range(len(b[0]))] for _ in range(len(a))]\n    \n    # Perform matrix multiplication\n    for i in range(len(a)):\n        for j in range(len(b[0])):\n            for k in range(len(b)):\n                result[i][j] += a[i][k] * b[k][j]\n    \n    return result\n\n# Example usage:\n# a = [[1, 2], [3, 4]]\n# b = [[2, 0], [1, 2]]\n# print(matrixmul(a, b))  # Output: [[4, 4], [10, 8]]\n```\n\nThis script defines the `matrixmul` function that performs matrix multiplication if possible and returns `-1` if the matrices cannot be multiplied. The example usage at the end demonstrates how to call the function with two matrices and print the result."}
{"task_id": 10, "completion_id": 0, "completion": "To calculate the covariance matrix for a given set of vectors, we need to follow these steps:\n\n1. Calculate the mean of each vector (feature).\n2. Subtract the mean from each observation in the vector to get the deviations.\n3. Calculate the covariance between each pair of vectors using the deviations.\n4. Return the covariance matrix.\n\nHere is the Python script that implements the above steps:\n\n```python\nfrom typing import List\n\ndef calculate_covariance_matrix(vectors: List[List[float]]) -> List[List[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    :param vectors: A list of lists, where each inner list represents a feature with its observations.\n    :return: A covariance matrix as a list of lists.\n    \"\"\"\n    # Step 1: Calculate the mean of each vector\n    means = [sum(vector) / len(vector) for vector in vectors]\n    \n    # Step 2: Subtract the mean from each observation in the vector to get the deviations\n    deviations = [[x - mean for x in vector] for vector, mean in zip(vectors, means)]\n    \n    # Step 3: Calculate the covariance between each pair of vectors using the deviations\n    n = len(vectors[0])\n    covariance_matrix = [[sum(a * b for a, b in zip(dev1, dev2)) / (n - 1) for dev2 in deviations] for dev1 in deviations]\n    \n    return covariance_matrix\n\n# Test cases\nvectors = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12]\n]\n\n# Expected covariance matrix\n# [[1.6666666666666667, 1.6666666666666667, 1.6666666666666667],\n#  [1.6666666666666667, 1.6666666666666667, 1.6666666666666667],\n#  [1.6666666666666667, 1.6666666666666667, 1.6666666666666667]]\n\ncovariance_matrix = calculate_covariance_matrix(vectors)\nfor row in covariance_matrix:\n    print(row)\n```\n\nThis script defines a function `calculate_covariance_matrix` that calculates the covariance matrix for a given set of vectors. It also includes a test case to verify the correctness of the function. The covariance matrix is printed row by row."}
{"task_id": 11, "completion_id": 0, "completion": "To solve the given task, we need to implement the Jacobi method for solving a system of linear equations. The Jacobi method is an iterative algorithm that can be used to solve a system of linear equations. Here's a step-by-step approach to implementing the Jacobi method in Python:\n\n1. **Initialization**: Start by initializing the solution vector `x` with zeros or any initial guess.\n2. **Iteration**: For each iteration, update the solution vector `x` using the formula derived from the Jacobi method.\n3. **Rounding**: After each iteration, round the solution vector to four decimal places.\n4. **Termination**: Repeat the process for `n` iterations.\n5. **Output**: Return the final solution vector as a list, rounded to four decimal places.\n\nHere's the complete Python script implementing the Jacobi method:\n\n```python\nimport numpy as np\n\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    # Ensure A is a square matrix\n    if A.shape[0] != A.shape[1]:\n        raise ValueError(\"Matrix A must be square.\")\n    \n    # Number of equations\n    N = len(b)\n    \n    # Initialize the solution vector\n    x = np.zeros_like(b, dtype=np.float64)\n    \n    # Iterate n times\n    for _ in range(n):\n        x_new = np.zeros_like(x)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n        \n        # Round the solution vector to 4 decimal places\n        x = np.round(x, decimals=4)\n    \n    # Convert the numpy array to a list and return\n    return x.tolist()\n\n# Example usage\nA = np.array([[4, 1, 2], [3, 5, 1], [1, 1, 3]], dtype=np.float64)\nb = np.array([4, 7, 3], dtype=np.float64)\nn = 25  # Number of iterations\nsolution = solve_jacobi(A, b, n)\nprint(solution)\n```\n\n### Explanation:\n- **Initialization**: The solution vector `x` is initialized to zeros.\n- **Iteration**: For each iteration, a new solution vector `x_new` is computed. Each element of `x_new` is calculated using the Jacobi formula, which involves the current solution vector `x`.\n- **Rounding**: After computing the new solution vector, it is rounded to four decimal places.\n- **Output**: The final solution vector is converted to a list and returned.\n\nThis script provides a self-contained implementation of the Jacobi method for solving a system of linear equations, adhering to the specified constraints."}
{"task_id": 12, "completion_id": 0, "completion": "To solve the task of approximating the Singular Value Decomposition (SVD) of a 2x2 matrix using the Jacobian method, we need to understand the basic principles of SVD and how the Jacobian method can be applied to find the singular values. The SVD of a matrix \\( A \\) is given by \\( A = U \\Sigma V^T \\), where \\( U \\) and \\( V \\) are orthogonal matrices, and \\( \\Sigma \\) is a diagonal matrix containing the singular values of \\( A \\).\n\nFor a 2x2 matrix, the singular values can be found by solving the characteristic equation of \\( A^T A \\) or \\( A A^T \\). The Jacobian method is typically used in optimization problems, but for this specific task, we can use a simpler approach to find the singular values directly.\n\nHere is a self-contained Python script that performs the SVD approximation for a 2x2 matrix:\n\n```python\nimport numpy as np\n\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Approximates the singular values of a 2x2 matrix A using the characteristic equation.\n    \n    Parameters:\n    A (np.ndarray): A 2x2 matrix.\n    \n    Returns:\n    tuple: A tuple containing the two singular values of A, rounded to 4 decimal places.\n    \"\"\"\n    # Calculate A^T * A\n    ATA = np.dot(A.T, A)\n    \n    # Calculate the trace and determinant of ATA\n    trace_ATA = np.trace(ATA)\n    det_ATA = np.linalg.det(ATA)\n    \n    # Calculate the singular values using the characteristic equation\n    # The singular values are the square roots of the eigenvalues of ATA\n    # The eigenvalues are the roots of the characteristic polynomial: lambda^2 - trace_ATA*lambda + det_ATA = 0\n    a = 1\n    b = -trace_ATA\n    c = det_ATA\n    \n    # Solve the quadratic equation\n    discriminant = b**2 - 4*a*c\n    lambda1 = (-b + np.sqrt(discriminant)) / (2*a)\n    lambda2 = (-b - np.sqrt(discriminant)) / (2*a)\n    \n    # The singular values are the square roots of the eigenvalues\n    sigma1 = np.sqrt(lambda1)\n    sigma2 = np.sqrt(lambda2)\n    \n    # Round the singular values to 4 decimal places\n    sigma1 = round(sigma1, 4)\n    sigma2 = round(sigma2, 4)\n    \n    return (sigma1, sigma2)\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nsingular_values = svd_2x2_singular_values(A)\nprint(singular_values)\n```\n\nThis script defines a function `svd_2x2_singular_values` that calculates the singular values of a 2x2 matrix \\( A \\) by finding the eigenvalues of \\( A^T A \\) and taking their square roots. The singular values are then rounded to the nearest 4th decimal place as required."}
{"task_id": 13, "completion_id": 0, "completion": "To solve the task of calculating the determinant of a 4x4 matrix using Laplace's Expansion, we will follow these steps:\n\n1. Define the function `determinant_4x4` that takes a 4x4 matrix as input.\n2. Implement a helper function `determinant_3x3` to calculate the determinant of a 3x3 matrix, as this will be needed for the Laplace expansion.\n3. Use Laplace's Expansion to calculate the determinant of the 4x4 matrix by recursively calling the `determinant_3x3` function for each minor matrix.\n4. Ensure the function handles both integer and floating-point numbers.\n\nHere is the Python script that accomplishes this:\n\n```python\ndef determinant_3x3(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 3x3 matrix.\n    \"\"\"\n    a, b, c = matrix[0]\n    d, e, f = matrix[1]\n    g, h, i = matrix[2]\n    return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\ndef determinant_4x4(matrix: list[list[int|float]]) -> float:\n    \"\"\"\n    Calculate the determinant of a 4x4 matrix using Laplace's Expansion.\n    \"\"\"\n    det = 0\n    for col in range(4):\n        # Calculate the minor matrix by removing the first row and the current column\n        minor = [row[:col] + row[col+1:] for row in matrix[1:]]\n        # Calculate the determinant of the minor matrix\n        minor_det = determinant_3x3(minor)\n        # Add to the determinant with the appropriate sign\n        det += (-1) ** col * matrix[0][col] * minor_det\n    return det\n\n# Example usage:\nmatrix_4x4 = [\n    [1, 2, 3, 4],\n    [5, 6, 7, 8],\n    [9, 10, 11, 12],\n    [13, 14, 15, 16]\n]\n\nprint(determinant_4x4(matrix_4x4))  # Output should be 0\n```\n\nThis script defines two functions: `determinant_3x3` for calculating the determinant of a 3x3 matrix and `determinant_4x4` for calculating the determinant of a 4x4 matrix using Laplace's Expansion. The example usage demonstrates how to use the `determinant_4x4` function with a 4x4 matrix."}
{"task_id": 14, "completion_id": 0, "completion": "To solve the task of performing linear regression using the normal equation, we need to follow these steps:\n\n1. Convert the input lists to NumPy arrays for easier manipulation.\n2. Add a column of ones to the feature matrix X to account for the intercept term in the linear regression model.\n3. Apply the normal equation formula to find the coefficients. The normal equation is given by: \\(\\theta = (X^T X)^{-1} X^T y\\), where \\(X\\) is the feature matrix, \\(y\\) is the target vector, and \\(\\theta\\) are the coefficients.\n4. Round the coefficients to four decimal places.\n5. Return the coefficients as a list.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    # Convert lists to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the intercept term\n    X = np.c_[np.ones(X.shape[0]), X]\n    \n    # Apply the normal equation\n    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    \n    # Round the coefficients to 4 decimal places\n    theta = np.round(theta, 4)\n    \n    # Convert numpy array back to list and return\n    return theta.tolist()\n\n# Example usage:\nX = [[1, 2], [3, 4], [5, 6]]\ny = [2, 4, 6]\nprint(linear_regression_normal_equation(X, y))\n```\n\nThis script defines the function `linear_regression_normal_equation` which takes a list of lists for the features `X` and a list for the target `y`, and returns the coefficients of the linear regression model as a list of floats, rounded to four decimal places."}
{"task_id": 15, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that performs linear regression using gradient descent. The function will take the feature matrix `X`, the target vector `y`, the learning rate `alpha`, and the number of iterations as inputs. The function will return the coefficients of the linear regression model as a list, rounded to four decimal places.\n\nHere's a step-by-step breakdown of the implementation:\n\n1. **Initialize the coefficients**: Start with an array of zeros for the coefficients.\n2. **Gradient Descent**: For each iteration, update the coefficients using the gradient descent formula.\n3. **Return the coefficients**: After completing the iterations, round the coefficients to four decimal places and return them as a list.\n\nLet's write the Python code for this:\n\n```python\nimport numpy as np\n\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    # Initialize coefficients\n    m = len(y)  # Number of training examples\n    n = X.shape[1]  # Number of features\n    theta = np.zeros(n)  # Initialize theta (coefficients) to zeros\n    \n    # Gradient Descent\n    for _ in range(iterations):\n        # Compute the predictions\n        predictions = np.dot(X, theta)\n        # Compute the errors\n        errors = predictions - y\n        # Compute the gradient\n        gradient = np.dot(X.T, errors) / m\n        # Update the coefficients\n        theta -= alpha * gradient\n    \n    # Round the coefficients to 4 decimal places and convert to list\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()\n\n# Example usage:\n# X = np.array([[1, 2], [1, 3], [1, 4]])\n# y = np.array([2, 3, 4])\n# alpha = 0.01\n# iterations = 1000\n# print(linear_regression_gradient_descent(X, y, alpha, iterations))\n```\n\nThis script defines the function `linear_regression_gradient_descent` which performs linear regression using gradient descent. The function initializes the coefficients to zeros, iteratively updates them using the gradient descent formula, and finally returns the coefficients rounded to four decimal places as a list."}
{"task_id": 16, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that performs two types of feature scaling: standardization and min-max normalization. Here's a step-by-step breakdown of the process:\n\n1. **Standardization**: This process involves subtracting the mean of the feature from each value and then dividing by the standard deviation. The formula for standardization is:\n   \\[\n   x' = \\frac{x - \\mu}{\\sigma}\n   \\]\n   where \\(x\\) is the original value, \\(\\mu\\) is the mean of the feature, and \\(\\sigma\\) is the standard deviation of the feature.\n\n2. **Min-Max Normalization**: This process scales the feature values to a range, typically [0, 1]. The formula for min-max normalization is:\n   \\[\n   x' = \\frac{x - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n   \\]\n   where \\(x\\) is the original value, \\(\\text{min}(X)\\) is the minimum value of the feature, and \\(\\text{max}(X)\\) is the maximum value of the feature.\n\n3. **Rounding**: After performing the scaling, we need to round the results to the nearest 4th decimal.\n\n4. **Conversion to List**: Finally, we convert the NumPy arrays to Python lists using the `tolist()` method.\n\nHere is the complete Python script implementing the above steps:\n\n```python\nimport numpy as np\n\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    # Standardization\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    standardized_data = np.round(standardized_data, 4)\n    \n    # Min-Max Normalization\n    min_max_normalized_data = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n    min_max_normalized_data = np.round(min_max_normalized_data, 4)\n    \n    # Convert to list\n    standardized_list = standardized_data.tolist()\n    min_max_normalized_list = min_max_normalized_data.tolist()\n    \n    return standardized_list, min_max_normalized_list\n\n# Example usage\nif __name__ == \"__main__\":\n    # Sample data\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    standardized, min_max_normalized = feature_scaling(data)\n    print(\"Standardized Data:\", standardized)\n    print(\"Min-Max Normalized Data:\", min_max_normalized)\n```\n\nThis script defines the `feature_scaling` function that takes a 2D NumPy array as input and returns two lists: one for standardized data and one for min-max normalized data. The example usage at the end demonstrates how to use this function with a sample dataset."}
{"task_id": 17, "completion_id": 0, "completion": "To implement the k-Means clustering algorithm, we need to follow these steps:\n\n1. Initialize the centroids with the given initial centroids.\n2. For each iteration, assign each point to the nearest centroid.\n3. Recalculate the centroids as the mean of all points assigned to each cluster.\n4. Repeat steps 2 and 3 until the centroids do not change significantly or the maximum number of iterations is reached.\n5. Return the final centroids, rounded to the nearest fourth decimal.\n\nHere is the Python script that implements the k-Means clustering algorithm:\n\n```python\nimport numpy as np\n\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    # Convert points and centroids to numpy arrays for easier manipulation\n    points_array = np.array(points)\n    centroids_array = np.array(initial_centroids)\n    \n    for _ in range(max_iterations):\n        # Assign each point to the nearest centroid\n        distances = np.sqrt(((points_array - centroids_array[:, np.newaxis])**2).sum(axis=2))\n        closest_centroid = np.argmin(distances, axis=0)\n        \n        # Recalculate centroids\n        new_centroids = np.array([points_array[closest_centroid == i].mean(axis=0) for i in range(k)])\n        \n        # Check for convergence\n        if np.all(centroids_array == new_centroids):\n            break\n        \n        centroids_array = new_centroids\n    \n    # Round the final centroids to the nearest fourth decimal\n    final_centroids = [tuple(np.round(centroid, 4)) for centroid in centroids_array]\n    \n    return final_centroids\n\n# Example usage\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (8.0, 8.0)]\nmax_iterations = 100\n\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)\nprint(final_centroids)\n```\n\nThis script defines the `k_means_clustering` function that performs the k-Means clustering algorithm and returns the final centroids rounded to the nearest fourth decimal. The example usage at the end demonstrates how to call the function with a set of points, the number of clusters, initial centroids, and the maximum number of iterations."}
{"task_id": 18, "completion_id": 0, "completion": "To implement K-Fold Cross-Validation, we need to follow these steps:\n\n1. **Shuffle the dataset**: If the `shuffle` parameter is set to `True`, we need to shuffle the dataset to ensure that the data is randomly distributed across the folds. We can use the `random_seed` parameter to ensure reproducibility of the shuffling.\n\n2. **Split the dataset into k folds**: We need to divide the dataset into `k` equal (or nearly equal) parts. Each part will be used as a test set once, while the remaining parts will be used as the training set.\n\n3. **Generate train-test indices**: For each fold, we will generate the indices for the training set and the test set. The training set will consist of all the indices except those in the current fold, and the test set will consist of the indices in the current fold.\n\n4. **Return the list of train-test indices**: Finally, we will return a list of tuples, where each tuple contains the indices for the training set and the test set for each fold.\n\nHere is the Python code that implements the above steps:\n\n```python\nimport numpy as np\n\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    # Set the random seed for reproducibility\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Get the number of samples\n    n_samples = X.shape[0]\n    \n    # Shuffle the indices if required\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    \n    # Split the indices into k folds\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    start = 0\n    folds = []\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        folds.append(indices[start:end])\n        start = end\n    \n    # Generate the train-test splits\n    splits = []\n    for i in range(k):\n        test_indices = folds[i]\n        train_indices = np.concatenate(folds[:i] + folds[i+1:])\n        splits.append((train_indices, test_indices))\n    \n    return splits\n\n# Example usage\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\ny = np.array([0, 1, 0, 1, 0, 1])\nsplits = k_fold_cross_validation(X, y, k=3, shuffle=True, random_seed=42)\nfor train_indices, test_indices in splits:\n    print(f\"Train indices: {train_indices}, Test indices: {test_indices}\")\n```\n\nThis code will output the indices for the training and test sets for each fold, allowing you to perform K-Fold Cross-Validation on your dataset."}
{"task_id": 19, "completion_id": 0, "completion": "To implement PCA from scratch, we need to follow these steps:\n\n1. **Standardize the Data**: PCA is affected by the scale of the data, so it's important to standardize the data to have a mean of 0 and a standard deviation of 1.\n2. **Compute the Covariance Matrix**: The covariance matrix is a square matrix that shows the covariance between each pair of features.\n3. **Calculate Eigenvalues and Eigenvectors**: The eigenvectors and eigenvalues of the covariance matrix are computed. The eigenvectors represent the directions of the new feature space, and the eigenvalues represent the magnitude of the variance in those directions.\n4. **Sort the Eigenvalues and Select the Top k Eigenvectors**: We sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues.\n5. **Return the Principal Components**: The selected eigenvectors are the principal components.\n\nHere is the Python code that implements the above steps:\n\n```python\nimport numpy as np\n\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    # Step 1: Standardize the data\n    standardized_data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n    \n    # Step 2: Compute the covariance matrix\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    \n    # Step 3: Calculate eigenvalues and eigenvectors\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n    \n    # Step 4: Sort eigenvalues and select the top k eigenvectors\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    top_k_eigenvectors = eigenvectors[:, sorted_indices[:k]]\n    \n    # Step 5: Return the principal components\n    principal_components = top_k_eigenvectors.T.tolist()\n    return [[round(value, 4) for value in component] for component in principal_components]\n\n# Example usage:\n# data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n# k = 2\n# print(pca(data, k))\n```\n\nThis script defines a function `pca` that takes a 2D NumPy array `data` and an integer `k` as inputs. It returns a list of lists, where each inner list represents a principal component (eigenvector) rounded to the nearest 4th decimal. The example usage at the end demonstrates how to call the function with a sample dataset and the number of principal components to return."}
{"task_id": 20, "completion_id": 0, "completion": "To implement a decision tree learning algorithm in Python, we need to follow these steps:\n\n1. Calculate the entropy of the dataset.\n2. Calculate the information gain for each attribute.\n3. Choose the attribute with the highest information gain as the splitting criterion.\n4. Recursively split the dataset based on the chosen attribute until all instances in a subset belong to the same class or there are no more attributes to split on.\n\nHere is a self-contained Python script that implements the decision tree learning algorithm:\n\n```python\nimport math\nfrom collections import Counter\n\ndef entropy(examples, target_attr):\n    \"\"\"Calculate the entropy of the dataset.\"\"\"\n    counts = Counter([example[target_attr] for example in examples])\n    total = len(examples)\n    return -sum((count / total) * math.log2(count / total) for count in counts.values())\n\ndef information_gain(examples, attribute, target_attr):\n    \"\"\"Calculate the information gain for a given attribute.\"\"\"\n    total_entropy = entropy(examples, target_attr)\n    attribute_values = set([example[attribute] for example in examples])\n    weighted_entropy = 0\n    for value in attribute_values:\n        subset = [example for example in examples if example[attribute] == value]\n        subset_entropy = entropy(subset, target_attr)\n        weighted_entropy += (len(subset) / len(examples)) * subset_entropy\n    return total_entropy - weighted_entropy\n\ndef best_attribute(examples, attributes, target_attr):\n    \"\"\"Choose the attribute with the highest information gain.\"\"\"\n    gains = {attr: information_gain(examples, attr, target_attr) for attr in attributes}\n    return max(gains, key=gains.get)\n\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n    \"\"\"Learn a decision tree from the given examples and attributes.\"\"\"\n    # If all examples have the same target value, return that value\n    if len(set([example[target_attr] for example in examples])) == 1:\n        return examples[0][target_attr]\n    \n    # If no attributes left, return the most common target value\n    if not attributes:\n        return Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n    \n    # Choose the best attribute to split on\n    best_attr = best_attribute(examples, attributes, target_attr)\n    \n    # Create a new decision tree with the best attribute as the root\n    tree = {best_attr: {}}\n    \n    # Recursively build the tree\n    for value in set([example[best_attr] for example in examples]):\n        subset = [example for example in examples if example[best_attr] == value]\n        if not subset:\n            # If the subset is empty, return the most common target value\n            tree[best_attr][value] = Counter([example[target_attr] for example in examples]).most_common(1)[0][0]\n        else:\n            # Otherwise, continue building the tree\n            remaining_attributes = [attr for attr in attributes if attr != best_attr]\n            tree[best_attr][value] = learn_decision_tree(subset, remaining_attributes, target_attr)\n    \n    return tree\n\n# Example usage\nexamples = [\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'no'},\n    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'wind': 'weak', 'play': 'no'},\n    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'yes'},\n    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'wind': 'weak', 'play': 'yes'},\n    {'outlook': 'rain', 'temperature': 'mild', 'humidity': 'high', 'wind': 'strong', 'play': 'no'}\n]\nattributes = ['outlook', 'temperature', 'humidity', 'wind']\ntarget_attr = 'play'\n\ntree = learn_decision_tree(examples, attributes, target_attr)\nprint(tree)\n```\n\nThis script defines a decision tree learning algorithm that can be used to classify examples based on their attributes. The `learn_decision_tree` function builds a decision tree by recursively splitting the dataset based on the attribute that provides the highest information gain."}
{"task_id": 21, "completion_id": 0, "completion": "To implement the deterministic version of the Pegasos algorithm for a kernel SVM classifier, we need to follow these steps:\n\n1. **Initialize the alpha coefficients and bias**: We start with zero values for both.\n2. **Define the kernel function**: We will implement both linear and RBF kernels.\n3. **Update rule**: For each iteration, we will update the alpha coefficients and bias based on the Pegasos update rule.\n4. **Return the model parameters**: After the iterations, we return the alpha coefficients and bias.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef kernel_matrix(data, kernel='linear', sigma=1.0):\n    n_samples = data.shape[0]\n    kernel_matrix = np.zeros((n_samples, n_samples))\n    \n    if kernel == 'linear':\n        kernel_matrix = np.dot(data, data.T)\n    elif kernel == 'rbf':\n        for i in range(n_samples):\n            for j in range(n_samples):\n                diff = data[i] - data[j]\n                kernel_matrix[i, j] = np.exp(-np.dot(diff, diff) / (2 * sigma**2))\n    return kernel_matrix\n\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    n_samples = data.shape[0]\n    alphas = np.zeros(n_samples)\n    bias = 0.0\n    kernel_matrix = kernel_matrix(data, kernel, sigma)\n    \n    for t in range(1, iterations + 1):\n        step_size = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = np.sum(alphas * labels * kernel_matrix[i]) + bias\n            if labels[i] * prediction < 1:\n                alphas[i] = (1 - step_size * lambda_val) * alphas[i] + step_size * labels[i]\n            else:\n                alphas[i] = (1 - step_size * lambda_val) * alphas[i]\n        bias = bias + step_size * (1 - labels * (np.sum(alphas * labels * kernel_matrix, axis=1) + bias))\n    \n    alphas = np.round(alphas, 4)\n    bias = np.round(bias, 4)\n    return alphas.tolist(), bias.tolist()\n\n# Example usage:\n# data = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n# labels = np.array([1, 1, -1, -1])\n# alphas, bias = pegasos_kernel_svm(data, labels, kernel='linear', lambda_val=0.01, iterations=100)\n# print(\"Alphas:\", alphas)\n# print(\"Bias:\", bias)\n```\n\nThis script defines a function `pegasos_kernel_svm` that implements the deterministic Pegasos algorithm for training a kernel SVM. It supports both linear and RBF kernels. The function returns the alpha coefficients and bias, both rounded to the nearest 4th decimal and converted to lists."}
{"task_id": 22, "completion_id": 0, "completion": "To solve the task of computing the output of the sigmoid activation function, we need to follow these steps:\n\n1. Import the `math` module to use the `exp` function for calculating the exponential of a number.\n2. Define the function `sigmoid(z)` that takes a single argument `z`.\n3. Inside the function, calculate the sigmoid value using the formula `1 / (1 + e^(-z))`, where `e` is the base of the natural logarithm.\n4. Round the result to four decimal places.\n5. Return the rounded result.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport math\n\ndef sigmoid(z: float) -> float:\n    # Calculate the sigmoid value\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    # Round the result to four decimal places\n    return round(sigmoid_value, 4)\n```\n\nThis script defines the `sigmoid` function which computes the sigmoid of the input `z` and returns the result rounded to four decimal places."}
{"task_id": 23, "completion_id": 0, "completion": "To implement the softmax activation function, we need to follow these steps:\n\n1. Calculate the exponential of each score in the input list.\n2. Sum up all the exponentials.\n3. Divide each exponential by the sum of all exponentials to get the probability.\n4. Round each probability to four decimal places.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport math\n\ndef softmax(scores: list[float]) -> list[float]:\n    # Step 1: Calculate the exponential of each score\n    exp_scores = [math.exp(score) for score in scores]\n    \n    # Step 2: Sum up all the exponentials\n    sum_exp_scores = sum(exp_scores)\n    \n    # Step 3: Divide each exponential by the sum of all exponentials\n    # Step 4: Round each probability to four decimal places\n    softmax_values = [round(exp_score / sum_exp_scores, 4) for exp_score in exp_scores]\n    \n    return softmax_values\n\n# Example usage:\nscores = [1.0, 2.0, 3.0]\nprint(softmax(scores))\n```\n\nThis script defines the `softmax` function which takes a list of scores as input and returns a list of softmax values, each rounded to four decimal places. The example usage demonstrates how to call the function with a list of scores."}
{"task_id": 24, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that simulates a single neuron with a sigmoid activation function. The function will take a list of feature vectors, true binary labels, weights, and a bias as input. It will then calculate the predicted probabilities using the sigmoid function and compute the mean squared error between the predicted probabilities and the true labels. The results will be rounded to four decimal places.\n\nHere's a step-by-step breakdown of the implementation:\n1. Define the sigmoid function.\n2. Calculate the weighted sum of features for each input vector.\n3. Apply the sigmoid function to the weighted sum to get the predicted probabilities.\n4. Calculate the mean squared error between the predicted probabilities and the true labels.\n5. Round the results to four decimal places and return them.\n\nBelow is the complete Python script implementing the above steps:\n\n```python\nimport math\nimport numpy as np\n\ndef sigmoid(x: float) -> float:\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\n\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    :param features: List of feature vectors.\n    :param labels: List of true binary labels.\n    :param weights: List of weights for each feature.\n    :param bias: Bias term.\n    :return: A tuple containing the predicted probabilities and the mean squared error.\n    \"\"\"\n    predictions = []\n    for feature_vector in features:\n        # Calculate the weighted sum of features\n        weighted_sum = sum(w * f for w, f in zip(weights, feature_vector)) + bias\n        # Apply the sigmoid function to get the predicted probability\n        prediction = sigmoid(weighted_sum)\n        predictions.append(prediction)\n    \n    # Convert predictions to a numpy array for easier calculation\n    predictions_np = np.array(predictions)\n    labels_np = np.array(labels)\n    \n    # Calculate the mean squared error\n    mse = np.mean((predictions_np - labels_np) ** 2)\n    \n    # Round the results to four decimal places\n    predictions_rounded = np.round(predictions_np, 4).tolist()\n    mse_rounded = np.round(mse, 4)\n    \n    return predictions_rounded, mse_rounded\n\n# Example usage\nfeatures = [[0.5, 0.2], [0.8, 0.1], [0.1, 0.9]]\nlabels = [1, 0, 1]\nweights = [0.4, 0.6]\nbias = 0.1\n\npredictions, mse = single_neuron_model(features, labels, weights, bias)\nprint(\"Predicted Probabilities:\", predictions)\nprint(\"Mean Squared Error:\", mse)\n```\n\nThis script defines the `single_neuron_model` function that performs the required operations and prints the predicted probabilities and mean squared error for a given set of inputs."}
{"task_id": 25, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that simulates a single neuron with a sigmoid activation function. The function will also implement backpropagation to update the neuron's weights and bias using gradient descent based on the Mean Squared Error (MSE) loss. The function will return the updated weights, bias, and a list of MSE values for each epoch, each rounded to four decimal places.\n\nHere's a step-by-step breakdown of the implementation:\n\n1. **Sigmoid Activation Function**: This function will be used to compute the output of the neuron.\n2. **Forward Pass**: Compute the output of the neuron using the sigmoid activation function.\n3. **Loss Calculation**: Calculate the Mean Squared Error (MSE) loss.\n4. **Backward Pass**: Compute the gradients of the loss with respect to the weights and bias.\n5. **Update Weights and Bias**: Use the gradients to update the weights and bias using gradient descent.\n6. **Repeat for Epochs**: Repeat the forward pass, loss calculation, backward pass, and weight updates for the specified number of epochs.\n\nHere is the complete Python script:\n\n```python\nimport numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    weights = initial_weights\n    bias = initial_bias\n    mse_values = []\n    \n    for epoch in range(epochs):\n        # Forward pass\n        predictions = sigmoid(np.dot(features, weights) + bias)\n        \n        # Calculate loss (MSE)\n        mse = np.mean((predictions - labels) ** 2)\n        mse_values.append(round(mse, 4))\n        \n        # Backward pass\n        error = predictions - labels\n        gradient_weights = np.dot(features.T, error) * predictions * (1 - predictions)\n        gradient_bias = np.sum(error * predictions * (1 - predictions))\n        \n        # Update weights and bias\n        weights -= learning_rate * gradient_weights\n        bias -= learning_rate * gradient_bias\n    \n    return weights.tolist(), round(bias, 4), mse_values\n\n# Example usage:\nfeatures = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nlabels = np.array([0, 1, 1, 0])\ninitial_weights = np.array([0.5, -0.5])\ninitial_bias = 0.0\nlearning_rate = 0.1\nepochs = 1000\n\nweights, bias, mse_values = train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs)\nprint(\"Updated Weights:\", weights)\nprint(\"Updated Bias:\", bias)\nprint(\"MSE Values:\", mse_values)\n```\n\nThis script defines the `train_neuron` function that performs the required operations and returns the updated weights, bias, and a list of MSE values for each epoch. The example usage at the end demonstrates how to call the function with a simple XOR problem dataset."}
{"task_id": 26, "completion_id": 0, "completion": "To implement the basic autograd operations, we need to define the `Value` class with methods for addition, multiplication, and ReLU activation. Each of these operations will create a new `Value` object that tracks the operation performed and the operands involved. The `backward` method will be used to compute the gradients through the computational graph.\n\nHere's a step-by-step implementation:\n\n1. **Initialization**: The `__init__` method initializes the data, gradient, and backward function.\n2. **Addition**: The `__add__` method defines the addition operation.\n3. **Multiplication**: The `__mul__` method defines the multiplication operation.\n4. **ReLU Activation**: The `relu` method defines the ReLU activation function.\n5. **Backward Propagation**: The `backward` method computes the gradients through the computational graph.\n\nHere's the complete implementation:\n\n```python\nclass Value:\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __repr__(self):\n        return f\"Value(data={self.data})\"\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n# Example usage:\na = Value(2.0)\nb = Value(-3.0)\nc = Value(10.0)\nd = a * b + c\nd.backward()\n\nprint(f\"a.grad = {a.grad}, b.grad = {b.grad}, c.grad = {c.grad}\")\n```\n\nThis script defines a `Value` class that can perform addition, multiplication, and ReLU activation, and it can compute gradients through backward propagation. The example usage demonstrates how to use the class to compute gradients for a simple expression."}
{"task_id": 27, "completion_id": 0, "completion": "To solve the task of computing the transformation matrix from basis B to C, we need to follow these steps:\n\n1. Convert the list of basis vectors for B and C into numpy arrays.\n2. Compute the inverse of the matrix formed by the basis vectors of C.\n3. Multiply the inverse of C by the matrix formed by the basis vectors of B to get the transformation matrix P.\n4. Round the elements of the transformation matrix to the nearest 4th decimal.\n5. Convert the numpy array back to a Python list and return it.\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to C.\n    \n    Parameters:\n    B (list of list of int): Basis vectors in basis B.\n    C (list of list of int): Basis vectors in basis C.\n    \n    Returns:\n    list of list of float: The transformation matrix P from B to C.\n    \"\"\"\n    # Convert lists to numpy arrays\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    \n    # Compute the inverse of C\n    C_inv = np.linalg.inv(C_matrix)\n    \n    # Compute the transformation matrix P\n    P = np.dot(C_inv, B_matrix)\n    \n    # Round the elements of P to the nearest 4th decimal\n    P_rounded = np.round(P, decimals=4)\n    \n    # Convert numpy array to list and return\n    return P_rounded.tolist()\n\n# Example usage\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 0], [0, 1, 1], [1, 0, 1]]\nP = transform_basis(B, C)\nprint(P)\n```\n\nThis script defines the `transform_basis` function which takes two lists of basis vectors, computes the transformation matrix from B to C, rounds the elements to 4 decimal places, and returns the result as a list of lists. The example usage at the end demonstrates how to call the function with two sets of basis vectors and print the resulting transformation matrix."}
{"task_id": 28, "completion_id": 0, "completion": "To solve the given task, we will follow the steps outlined in the description. We will use NumPy for matrix operations and eigenvalue decomposition. Here is the step-by-step implementation:\n\n1. Compute the matrix $A^T A$.\n2. Perform eigendecomposition on $A^T A$ to get eigenvalues and eigenvectors.\n3. Construct the singular values from the square roots of the eigenvalues.\n4. Use the eigenvectors of $A^T A$ to form the matrix $V$.\n5. Compute the matrix $U$ by normalizing $A v_i$ for each non-zero singular value $\\sigma_i$.\n6. Return the matrices $U$, $S$, and $V$.\n\nHere is the Python script implementing the above steps:\n\n```python\nimport numpy as np\n\ndef svd_2x2(A: np.ndarray) -> tuple:\n    # Step 1: Compute A^T A\n    ATA = np.dot(A.T, A)\n    \n    # Step 2: Perform eigendecomposition on A^T A\n    eigenvalues, eigenvectors = np.linalg.eigh(ATA)\n    \n    # Step 3: Construct the singular values\n    singular_values = np.sqrt(eigenvalues)\n    S = np.diag(singular_values)\n    \n    # Step 4: Use the eigenvectors of A^T A to form V\n    V = eigenvectors\n    \n    # Step 5: Compute U by normalizing A v_i for each non-zero singular value\n    U = np.zeros_like(A)\n    for i in range(2):\n        if singular_values[i] != 0:\n            U[:, i] = np.dot(A, V[:, i]) / singular_values[i]\n        else:\n            # If singular value is zero, we can choose any vector orthogonal to the previous columns of U\n            if i == 0:\n                U[:, i] = np.array([1, 0])\n            else:\n                U[:, i] = np.array([0, 1])\n    \n    # Ensure U is orthogonal\n    U, _ = np.linalg.qr(U)\n    \n    # Step 6: Return the matrices U, S, and V\n    # Round to the nearest 4th decimal and convert to list\n    U = np.round(U, 4).tolist()\n    S = np.round(S, 4).tolist()\n    V = np.round(V, 4).tolist()\n    \n    return U, S, V\n\n# Example usage\nA = np.array([[1, 2], [3, 4]])\nU, S, V = svd_2x2(A)\nprint(\"U:\", U)\nprint(\"S:\", S)\nprint(\"V:\", V)\n```\n\nThis script defines the `svd_2x2` function that computes the SVD of a 2x2 matrix using the specified method. The output matrices $U$, $S$, and $V$ are rounded to the nearest 4th decimal and returned as lists."}
{"task_id": 29, "completion_id": 0, "completion": "To solve the given task, we need to create a function that shuffles two numpy arrays in unison. This means that the elements of `X` and `y` should be shuffled in the same order so that the correspondence between the samples and their labels is maintained. We will use numpy's `random.shuffle` function to shuffle the arrays. Additionally, we will use the `seed` parameter to ensure that the shuffle is reproducible if needed.\n\nHere are the steps to achieve this:\n1. Check if the lengths of `X` and `y` are the same.\n2. Combine the arrays into a single array for shuffling.\n3. Shuffle the combined array.\n4. Separate the shuffled combined array back into `X` and `y`.\n5. Convert the shuffled arrays to lists and return them.\n\nLet's write the code:\n\n```python\nimport numpy as np\n\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays, X and y, while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The label dataset.\n    seed (int, optional): The seed for the random number generator for reproducibility.\n    \n    Returns:\n    tuple: A tuple containing two lists, the shuffled X and y.\n    \"\"\"\n    assert len(X) == len(y), \"X and y must have the same length\"\n    \n    if seed is not None:\n        np.random.seed(seed)\n    \n    # Combine the arrays\n    combined = np.c_[X.reshape(len(X), -1), y.reshape(len(y), -1)]\n    \n    # Shuffle the combined array\n    np.random.shuffle(combined)\n    \n    # Separate the combined array back into X and y\n    shuffled_X = combined[:, :-1]\n    shuffled_y = combined[:, -1]\n    \n    # Convert to lists and return\n    return shuffled_X.tolist(), shuffled_y.tolist()\n\n# Example usage\nX = np.array([[1, 2], [3, 4], [5, 6]])\ny = np.array([7, 8, 9])\nshuffled_X, shuffled_y = shuffle_data(X, y, seed=42)\nprint(\"Shuffled X:\", shuffled_X)\nprint(\"Shuffled y:\", shuffled_y)\n```\n\nThis script defines the `shuffle_data` function that takes two numpy arrays and an optional seed, shuffles them in unison, and returns the shuffled arrays as lists. The example usage demonstrates how to call the function and print the shuffled arrays."}
{"task_id": 30, "completion_id": 0, "completion": "To solve the given task, we need to create a function that can iterate over a dataset (represented as a numpy array `X`) and optionally a target array `y`, and yield batches of a specified size. If `y` is provided, the function should yield pairs of batches from `X` and `y`. If `y` is not provided, the function should only yield batches from `X`. The batches should be returned as Python lists.\n\nHere's a step-by-step approach to implement this:\n\n1. **Import the necessary package**: We need `numpy` for array operations.\n2. **Define the function**: The function `batch_iterator` will take `X`, `y`, and `batch_size` as parameters.\n3. **Calculate the number of batches**: This is done by dividing the length of `X` by `batch_size` and rounding up to ensure all data is included.\n4. **Iterate over the dataset**: Use a loop to iterate over the dataset in steps of `batch_size`.\n5. **Yield batches**: Depending on whether `y` is provided, yield either a single batch from `X` or a pair of batches from `X` and `y`.\n6. **Convert to list**: Use `tolist()` to convert numpy arrays to Python lists before yielding.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of X and optionally y.\n    \n    Parameters:\n    X (np.ndarray): The input data.\n    y (np.ndarray, optional): The target data. Defaults to None.\n    batch_size (int): The size of each batch. Defaults to 64.\n    \n    Yields:\n    list or tuple: A batch of X, or a tuple of batches (X, y) if y is provided.\n    \"\"\"\n    num_samples = X.shape[0]\n    num_batches = int(np.ceil(num_samples / batch_size))\n    \n    for i in range(num_batches):\n        start_idx = i * batch_size\n        end_idx = min(start_idx + batch_size, num_samples)\n        \n        X_batch = X[start_idx:end_idx].tolist()\n        if y is not None:\n            y_batch = y[start_idx:end_idx].tolist()\n            yield X_batch, y_batch\n        else:\n            yield X_batch\n\n# Example usage:\nif __name__ == \"__main__\":\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y = np.array([1, 0, 1, 0, 1])\n    for batch in batch_iterator(X, y, batch_size=2):\n        print(batch)\n```\n\nThis script defines the `batch_iterator` function and demonstrates its usage with an example dataset `X` and target array `y`. The function yields batches of the specified size, converting numpy arrays to Python lists for each batch."}
{"task_id": 31, "completion_id": 0, "completion": "To solve the given task, we need to write a function that takes a dataset `X`, a feature index `feature_i`, and a threshold value. The function will then split the dataset into two subsets based on whether the value of the specified feature is greater than or equal to the threshold. We will use NumPy for efficient array manipulation and to ensure the output is in the form of Python lists.\n\nHere's the step-by-step plan:\n1. Import the necessary package (`numpy`).\n2. Define the function `divide_on_feature` with parameters `X`, `feature_i`, and `threshold`.\n3. Use NumPy boolean indexing to create two subsets: one where the feature value is greater than or equal to the threshold, and another where it is not.\n4. Convert the resulting NumPy arrays to Python lists using the `tolist()` method.\n5. Return the two subsets.\n\nNow, let's implement this in a self-contained Python script:\n\n```python\nimport numpy as np\n\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of the specified feature is\n    greater than or equal to a given threshold.\n\n    Parameters:\n    X (numpy.ndarray): The dataset.\n    feature_i (int): The index of the feature to divide the dataset on.\n    threshold (float): The threshold value to compare the feature values against.\n\n    Returns:\n    tuple: Two lists representing the subsets of the dataset.\n    \"\"\"\n    # Create a boolean mask for the condition\n    condition = X[:, feature_i] >= threshold\n    \n    # Use the mask to divide the dataset into two subsets\n    subset1 = X[condition].tolist()  # Subset where the condition is True\n    subset2 = X[~condition].tolist()  # Subset where the condition is False\n    \n    return subset1, subset2\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample dataset\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n    feature_i = 1  # Index of the feature to divide on\n    threshold = 5  # Threshold value\n    \n    # Divide the dataset\n    subset1, subset2 = divide_on_feature(X, feature_i, threshold)\n    \n    # Print the results\n    print(\"Subset 1 (>= threshold):\", subset1)\n    print(\"Subset 2 (< threshold):\", subset2)\n```\n\nThis script defines the function `divide_on_feature` and demonstrates its usage with a sample dataset. The function correctly divides the dataset into two subsets based on the specified feature and threshold, and the results are printed in the form of Python lists."}
{"task_id": 32, "completion_id": 0, "completion": "To generate polynomial features for a given dataset, we can use combinations with replacement to create all possible combinations of the input features up to the specified degree. Then, we can compute the product of these combinations to form the polynomial features. Here's how we can implement this:\n\n1. Import the necessary packages.\n2. Define the function `polynomial_features` that takes a 2D numpy array `X` and an integer `degree`.\n3. Use `combinations_with_replacement` to generate all possible combinations of the feature indices.\n4. Compute the product of the features for each combination.\n5. Reshape the result and return it as a list using `tolist()`.\n\nHere's the complete code:\n\n```python\nimport numpy as np\nfrom itertools import combinations_with_replacement\n\ndef polynomial_features(X, degree):\n    n_samples, n_features = X.shape\n    # Generate all combinations of feature indices up to the specified degree\n    indices = list(combinations_with_replacement(range(n_features), degree))\n    n_combinations = len(indices)\n    # Initialize the polynomial features array\n    poly_features = np.empty((n_samples, n_combinations))\n    # Compute the product of features for each combination\n    for i, index_combination in enumerate(indices):\n        poly_features[:, i] = np.prod(X[:, index_combination], axis=1)\n    # Include the original features and the constant term\n    for i in range(1, degree):\n        poly_features = np.hstack([poly_features, polynomial_features(X, i)])\n    poly_features = np.hstack([poly_features, np.ones((n_samples, 1))])\n    # Return the result as a list\n    return poly_features.tolist()\n\n# Example usage\nX = np.array([[1, 2], [3, 4]])\ndegree = 2\nprint(polynomial_features(X, degree))\n```\n\nThis script defines the function `polynomial_features` that generates polynomial features up to the specified degree for a given 2D numpy array `X`. The output is returned as a list after reshaping the result."}
{"task_id": 33, "completion_id": 0, "completion": "To solve the given task, we need to create a function that generates random subsets of a dataset. The function will take a 2D numpy array `X` and a 1D numpy array `y` as inputs, along with the number of subsets `n_subsets` to generate and a boolean `replacements` to indicate whether the subsets should be created with or without replacements. The function will return a list of tuples, where each tuple contains a subset of `X` and the corresponding subset of `y`.\n\nHere's a step-by-step approach to achieve this:\n\n1. **Import the necessary package**: We need `numpy` for array manipulation.\n2. **Define the function**: The function will take `X`, `y`, `n_subsets`, `replacements`, and an optional `seed` for reproducibility.\n3. **Generate random indices**: Depending on the value of `replacements`, we will generate random indices with or without replacement.\n4. **Create subsets**: For each subset, we will use the generated indices to slice `X` and `y`.\n5. **Return the list of subsets**: Each subset is a tuple of `(X_subset, y_subset)`.\n\nHere is the complete Python script:\n\n```python\nimport numpy as np\n\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    np.random.seed(seed)  # Ensure reproducibility\n    subsets = []\n    n_samples = X.shape[0]\n    \n    for _ in range(n_subsets):\n        if replacements:\n            indices = np.random.choice(n_samples, n_samples, replace=True)\n        else:\n            indices = np.random.choice(n_samples, n_samples, replace=False)\n        \n        X_subset = X[indices]\n        y_subset = y[indices]\n        \n        # Convert to list for output\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    \n    return subsets\n\n# Example usage\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([0, 1, 0, 1])\nn_subsets = 2\nsubsets = get_random_subsets(X, y, n_subsets, replacements=True)\nfor subset in subsets:\n    print(subset)\n```\n\nThis script defines the function `get_random_subsets` which generates the required random subsets of the dataset. The example usage at the end demonstrates how to call the function and print the generated subsets."}
{"task_id": 34, "completion_id": 0, "completion": "To solve the given task, we need to create a function that converts a 1D numpy array of integer values into a one-hot encoded format. If the number of columns `n_col` is not provided, we will determine it based on the maximum value in the input array `x`. Here's how we can approach this:\n\n1. Import the necessary package `numpy`.\n2. Define the function `to_categorical` with parameters `x` and `n_col`.\n3. If `n_col` is not provided, set it to the maximum value in `x` plus one, as this will be the number of unique categories.\n4. Create a zero matrix of shape `(len(x), n_col)` using `np.zeros`.\n5. Use numpy's advanced indexing to set the appropriate positions to 1 based on the values in `x`.\n6. Convert the resulting numpy array to a list using `tolist()` and return it.\n\nHere's the complete code:\n\n```python\nimport numpy as np\n\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Converts a 1D numpy array of integer values into a one-hot encoded format.\n    \n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array. Defaults to None.\n    \n    Returns:\n    list: A list of lists representing the one-hot encoded array.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1  # Determine the number of columns if not provided\n    one_hot = np.zeros((x.size, n_col), dtype=int)  # Create a zero matrix\n    one_hot[np.arange(x.size), x] = 1  # Set the appropriate positions to 1\n    return one_hot.tolist()  # Convert to list and return\n\n# Example usage:\nx = np.array([0, 1, 2, 1, 0])\nprint(to_categorical(x))\n```\n\nThis function will work for any 1D numpy array of integers and will automatically determine the number of columns needed for one-hot encoding if not specified. The output is a list of lists, which is the one-hot encoded representation of the input array."}
{"task_id": 35, "completion_id": 0, "completion": "To solve this task, we will use the `numpy` library, which provides a convenient function `np.diag` to create a diagonal matrix from a 1D array. After creating the diagonal matrix, we will convert it to a list using the `tolist()` method to meet the output constraints.\n\nHere's the step-by-step plan:\n1. Import the `numpy` library.\n2. Define the function `make_diagonal` that takes a 1D numpy array `x` as input.\n3. Use `np.diag(x)` to create a diagonal matrix.\n4. Convert the resulting 2D numpy array to a list using `tolist()`.\n5. Return the list.\n\nNow, let's write the code:\n\n```python\nimport numpy as np\n\ndef make_diagonal(x):\n    # Create a diagonal matrix from the 1D numpy array x\n    diagonal_matrix = np.diag(x)\n    # Convert the 2D numpy array to a list\n    diagonal_list = diagonal_matrix.tolist()\n    return diagonal_list\n\n# Example usage:\n# x = np.array([1, 2, 3])\n# print(make_diagonal(x))\n# Expected output: [[1, 0, 0], [0, 2, 0], [0, 0, 3]]\n```\n\nThis script defines the function `make_diagonal` that takes a 1D numpy array and returns a diagonal matrix as a list of lists, which matches the output constraints."}
{"task_id": 36, "completion_id": 0, "completion": "To calculate the accuracy score, we need to compare the true labels (`y_true`) with the predicted labels (`y_pred`). The accuracy score is the ratio of the number of correct predictions to the total number of predictions. Here's how we can implement this in Python:\n\n1. Import the necessary package (`numpy`).\n2. Define the function `accuracy_score` that takes two parameters: `y_true` and `y_pred`.\n3. Calculate the number of correct predictions by comparing `y_true` and `y_pred` element-wise and summing the number of `True` values.\n4. Divide the number of correct predictions by the total number of predictions to get the accuracy.\n5. Round the result to the nearest 4th decimal place.\n6. Return the accuracy score.\n\nHere's the complete code:\n\n```python\nimport numpy as np\n\ndef accuracy_score(y_true, y_pred):\n    # Calculate the number of correct predictions\n    correct_predictions = np.sum(y_true == y_pred)\n    # Calculate the total number of predictions\n    total_predictions = len(y_true)\n    # Calculate the accuracy score\n    accuracy = correct_predictions / total_predictions\n    # Round the accuracy to the nearest 4th decimal\n    accuracy_rounded = round(accuracy, 4)\n    return accuracy_rounded\n\n# Example usage:\ny_true = np.array([0, 1, 2, 2, 2])\ny_pred = np.array([0, 0, 2, 2, 1])\nprint(accuracy_score(y_true, y_pred))  # Output should be 0.8\n```\n\nThis script defines the `accuracy_score` function and demonstrates its usage with an example. The output is rounded to the nearest 4th decimal as required."}
{"task_id": 37, "completion_id": 0, "completion": "To solve the given task, we need to calculate the correlation matrix for a given dataset. The correlation matrix is a square matrix that shows the correlation coefficients between variables. If only one dataset (X) is provided, we calculate the correlation matrix of X with itself. If a second dataset (Y) is provided, we calculate the correlation matrix between X and Y.\n\nHere are the steps to achieve this:\n1. Import the necessary libraries.\n2. Define the function `calculate_correlation_matrix` that takes two parameters: X and an optional Y.\n3. Calculate the mean and standard deviation for each column in X and Y (if provided).\n4. Center the data by subtracting the mean from each column.\n5. Calculate the covariance matrix.\n6. Normalize the covariance matrix to get the correlation matrix.\n7. Round the results to the nearest 4th decimal.\n8. Convert the result to a list and return it.\n\nHere is the complete Python script:\n\n```python\nimport numpy as np\n\ndef calculate_correlation_matrix(X, Y=None):\n    # If Y is not provided, set Y to X\n    if Y is None:\n        Y = X\n    \n    # Center the data by subtracting the mean\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    \n    # Calculate the covariance matrix\n    cov_matrix = np.dot(X_centered.T, Y_centered) / (X.shape[0] - 1)\n    \n    # Calculate the standard deviation for each column\n    std_X = np.std(X, axis=0)\n    std_Y = np.std(Y, axis=0)\n    \n    # Normalize the covariance matrix to get the correlation matrix\n    correlation_matrix = cov_matrix / np.outer(std_X, std_Y)\n    \n    # Round the results to the nearest 4th decimal\n    correlation_matrix = np.round(correlation_matrix, 4)\n    \n    # Convert the result to a list and return it\n    return correlation_matrix.tolist()\n\n# Example usage:\nX = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nY = np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]])\nprint(calculate_correlation_matrix(X, Y))\nprint(calculate_correlation_matrix(X))\n```\n\nThis script defines the function `calculate_correlation_matrix` which calculates the correlation matrix as described. It handles both cases where only X is provided and where both X and Y are provided. The results are rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 38, "completion_id": 0, "completion": "To implement the AdaBoost fit method, we need to follow these steps:\n\n1. Initialize the sample weights.\n2. For each classifier, find the best feature and threshold to split the data.\n3. Calculate the error and the weight of the classifier.\n4. Update the sample weights.\n5. Repeat steps 2-4 for the specified number of classifiers.\n6. Return the list of classifiers with their parameters.\n\nHere is the Python code that implements the AdaBoost fit method:\n\n```python\nimport numpy as np\n\ndef adaboost_fit(X, y, n_clf):\n    n_samples, n_features = X.shape\n    sample_weights = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    \n    for _ in range(n_clf):\n        best_error = float('inf')\n        best_clf = None\n        \n        for feature in range(n_features):\n            feature_values = X[:, feature]\n            thresholds = np.unique(feature_values)\n            \n            for threshold in thresholds:\n                pred = np.where(feature_values <= threshold, -1, 1)\n                error = np.sum(sample_weights * (pred != y))\n                \n                if error < best_error:\n                    best_error = error\n                    best_clf = {'feature': feature, 'threshold': threshold, 'prediction': pred}\n        \n        # Calculate classifier weight\n        clf_weight = 0.5 * np.log((1 - best_error) / best_error)\n        \n        # Update sample weights\n        sample_weights *= np.exp(-clf_weight * y * best_clf['prediction'])\n        sample_weights /= np.sum(sample_weights)\n        \n        # Store classifier\n        best_clf['weight'] = clf_weight\n        classifiers.append(best_clf)\n    \n    return classifiers\n\n# Example usage\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([1, 1, -1, -1])\nn_clf = 2\nclassifiers = adaboost_fit(X, y, n_clf)\nprint(classifiers)\n```\n\nThis code defines the `adaboost_fit` function that implements the AdaBoost algorithm. It iterates over the number of classifiers specified by `n_clf`, finds the best feature and threshold to split the data, calculates the error and classifier weight, updates the sample weights, and stores the classifier parameters. The classifiers are returned as a list of dictionaries, each containing the feature index, threshold, prediction, and weight."}
{"task_id": 39, "completion_id": 0, "completion": "To implement the log-softmax function, we need to follow these steps:\n\n1. Convert the input list to a numpy array for easier manipulation.\n2. Compute the softmax of the scores. The softmax function is defined as:\n   \\[\n   \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n   \\]\n   where \\(x_i\\) is the score for the \\(i\\)-th class.\n3. Compute the log-softmax by taking the natural logarithm of the softmax values.\n4. Round the results to the nearest 4th decimal.\n5. Convert the numpy array back to a Python list using the `tolist()` method.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef log_softmax(scores: list):\n    # Convert the list to a numpy array\n    scores_array = np.array(scores)\n    \n    # Compute the softmax\n    exp_scores = np.exp(scores_array - np.max(scores_array))  # Subtracting max for numerical stability\n    softmax_scores = exp_scores / np.sum(exp_scores)\n    \n    # Compute the log-softmax\n    log_softmax_scores = np.log(softmax_scores)\n    \n    # Round to the nearest 4th decimal\n    log_softmax_scores_rounded = np.round(log_softmax_scores, 4)\n    \n    # Convert numpy array to list\n    return log_softmax_scores_rounded.tolist()\n\n# Example usage\nscores = [1.0, 2.0, 3.0]\nprint(log_softmax(scores))\n```\n\n### Explanation:\n- **Numerical Stability**: We subtract the maximum value in the scores array from each score before exponentiating. This step is crucial for numerical stability, as it prevents overflow when the scores are large.\n- **Softmax Calculation**: The softmax function is computed by exponentiating each score, then dividing by the sum of all exponentiated scores.\n- **Logarithm**: The natural logarithm of the softmax values gives us the log-softmax values.\n- **Rounding**: The `np.round` function is used to round the results to 4 decimal places.\n- **Conversion to List**: Finally, the numpy array is converted to a Python list using `tolist()` for the output.\n\nThis script is self-contained and should work for any 1D list of scores provided as input."}
{"task_id": 40, "completion_id": 0, "completion": "```python\nimport numpy as np\nimport math\n\nclass Layer(object):\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\n\nclass Dense(Layer):\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.optimizer_W = None\n        self.optimizer_w0 = None\n\n    def initialize(self, optimizer):\n        # Initialize weights and biases\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        # Initialize optimizers\n        self.optimizer_W = copy.deepcopy(optimizer)\n        self.optimizer_w0 = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        # Total number of parameters\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training):\n        # Store input for backpropagation\n        self.layer_input = X\n        # Compute output\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        # Compute gradient with respect to input\n        w_grad = np.dot(self.layer_input.T, accum_grad)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        # Update weights and biases if trainable\n        if self.trainable:\n            self.W = self.optimizer_W.update(self.W, w_grad)\n            self.w0 = self.optimizer_w0.update(self.w0, w0_grad)\n        # Return gradient with respect to the input\n        return np.dot(accum_grad, self.W.T)\n\n    def output_shape(self):\n        # Output shape is the number of units\n        return (self.n_units,)\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define a simple optimizer\n    class SimpleOptimizer:\n        def __init__(self, learning_rate=0.01):\n            self.learning_rate = learning_rate\n\n        def update(self, param, grad):\n            return param - self.learning_rate * grad\n\n    # Create a Dense layer\n    dense_layer = Dense(n_units=3, input_shape=(2,))\n    dense_layer.initialize(SimpleOptimizer(learning_rate=0.01))\n\n    # Forward pass\n    input_data = np.array([[1, 2], [3, 4]])\n    output = dense_layer.forward_pass(input_data, training=True)\n    print(\"Output:\", output.round(4).tolist())\n\n    # Backward pass\n    accum_grad = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    grad_input = dense_layer.backward_pass(accum_grad)\n    print(\"Gradient with respect to input:\", grad_input.round(4).tolist())\n```"}
{"task_id": 41, "completion_id": 0, "completion": "To implement a simple 2D convolutional layer, we need to follow these steps:\n\n1. **Padding the Input Matrix**: If padding is specified, we need to add zeros around the input matrix.\n2. **Convolution Operation**: We will slide the kernel over the input matrix (or the padded input matrix) with the specified stride and compute the dot product between the kernel and the part of the input it overlaps with.\n3. **Output Calculation**: The result of each dot product operation will be placed in the output matrix at the appropriate position.\n4. **Rounding and Returning the Result**: Finally, we will round the results to the nearest 4th decimal and return the output as a Python list.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    # Step 1: Padding the input matrix\n    padded_input = np.pad(input_matrix, padding, mode='constant')\n    \n    # Step 2: Convolution operation\n    kernel_height, kernel_width = kernel.shape\n    input_height, input_width = padded_input.shape\n    output_height = (input_height - kernel_height) // stride + 1\n    output_width = (input_width - kernel_width) // stride + 1\n    \n    output = np.zeros((output_height, output_width))\n    \n    for i in range(0, input_height - kernel_height + 1, stride):\n        for j in range(0, input_width - kernel_width + 1, stride):\n            output[i // stride, j // stride] = np.sum(padded_input[i:i+kernel_height, j:j+kernel_width] * kernel)\n    \n    # Step 3: Rounding the results\n    output = np.round(output, decimals=4)\n    \n    # Step 4: Returning the result as a Python list\n    return output.tolist()\n\n# Example usage:\ninput_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nkernel = np.array([[1, 0], [0, 1]])\npadding = 1\nstride = 1\n\nresult = simple_conv2d(input_matrix, kernel, padding, stride)\nprint(result)\n```\n\nThis script defines the `simple_conv2d` function that takes an input matrix, a kernel, padding, and stride as parameters. It applies the convolution operation and returns the result as a Python list, rounded to the nearest 4th decimal."}
{"task_id": 42, "completion_id": 0, "completion": "To implement the ReLU activation function, we need to define a function that takes a single floating-point number as input and returns the same number if it is positive. If the number is negative or zero, the function should return zero. This can be achieved using a simple `if` statement to check the condition and return the appropriate value.\n\nHere is the step-by-step implementation:\n\n1. Define the function `relu` that takes a single argument `z` of type `float`.\n2. Use an `if` statement to check if `z` is greater than 0.\n3. If `z` is greater than 0, return `z`.\n4. If `z` is not greater than 0, return 0.\n\nBelow is the Python code implementing the ReLU function:\n\n```python\ndef relu(z: float) -> float:\n    if z > 0:\n        return z\n    else:\n        return 0\n```\n\nThis function directly applies the definition of the ReLU activation function, which is a simple yet effective non-linear function used in neural networks to introduce non-linearity while preserving the sparsity of the data."}
{"task_id": 43, "completion_id": 0, "completion": "To implement the Ridge Regression loss function, we need to calculate the Mean Squared Error (MSE) between the predicted values and the true labels, and then add a regularization term to it. The regularization term in Ridge Regression is the sum of the squares of the coefficients, multiplied by the regularization parameter `alpha`.\n\nHere's how we can break down the implementation:\n\n1. Calculate the predicted values `y_pred` using the dot product of `X` and `w`.\n2. Compute the Mean Squared Error (MSE) between `y_true` and `y_pred`.\n3. Calculate the regularization term as the sum of the squares of the coefficients `w`, multiplied by `alpha`.\n4. Add the MSE and the regularization term to get the Ridge loss.\n5. Round the final result to the nearest 4th decimal.\n\nLet's write the Python code for this:\n\n```python\nimport numpy as np\n\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    # Calculate the predicted values\n    y_pred = np.dot(X, w)\n    \n    # Calculate the Mean Squared Error (MSE)\n    mse = np.mean((y_true - y_pred) ** 2)\n    \n    # Calculate the regularization term\n    reg_term = alpha * np.sum(w ** 2)\n    \n    # Calculate the Ridge loss\n    ridge_loss = mse + reg_term\n    \n    # Round the result to the nearest 4th decimal\n    return round(ridge_loss, 4)\n\n# Example usage:\n# X = np.array([[1, 2], [3, 4], [5, 6]])\n# w = np.array([0.5, 1.0])\n# y_true = np.array([1, 2, 3])\n# alpha = 0.1\n# print(ridge_loss(X, w, y_true, alpha))\n```\n\nThis function `ridge_loss` takes the feature matrix `X`, the coefficient vector `w`, the true labels `y_true`, and the regularization parameter `alpha` as inputs, and returns the Ridge loss rounded to the nearest 4th decimal."}
{"task_id": 44, "completion_id": 0, "completion": "To implement the Leaky ReLU activation function, we need to consider the following:\n\n1. The Leaky ReLU function is defined as:\n   - `f(z) = z` if `z > 0`\n   - `f(z) = alpha * z` if `z <= 0`\n2. The function should take two parameters: `z` (the input value) and `alpha` (the slope for negative inputs, with a default value of 0.01).\n3. The function should return a float or integer based on the input and the value of `alpha`.\n\nHere is the Python code that implements the Leaky ReLU function:\n\n```python\ndef leaky_relu(z: float, alpha: float = 0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n    \n    Returns:\n    float: The output of the Leaky ReLU function.\n    \"\"\"\n    if z > 0:\n        return z\n    else:\n        return alpha * z\n\n# Example usage:\n# print(leaky_relu(2))  # Expected output: 2\n# print(leaky_relu(-2))  # Expected output: -0.02\n# print(leaky_relu(-2, alpha=0.1))  # Expected output: -0.2\n```\n\nThis code defines the `leaky_relu` function, which checks if the input `z` is greater than 0. If it is, it returns `z`. If `z` is less than or equal to 0, it returns `alpha * z`. The function is designed to be flexible with the `alpha` parameter, allowing for customization of the slope for negative inputs."}
{"task_id": 45, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that calculates the linear kernel, which is essentially the dot product of two vectors. We will use the `numpy` library to perform the dot product operation efficiently. Here are the steps to achieve this:\n\n1. Import the `numpy` library.\n2. Define the function `kernel_function` that takes two parameters, `x1` and `x2`.\n3. Use the `numpy` function `np.dot` to compute the dot product of `x1` and `x2`.\n4. Return the result of the dot product.\n\nBelow is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.ndarray): First input vector.\n    x2 (np.ndarray): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    # Compute the dot product of x1 and x2\n    kernel_value = np.dot(x1, x2)\n    return kernel_value\n\n# Example usage\nx1 = np.array([1, 2, 3])\nx2 = np.array([4, 5, 6])\nprint(\"Linear Kernel Value:\", kernel_function(x1, x2))\n```\n\nThis script defines the `kernel_function` that computes the linear kernel as the dot product of two input vectors `x1` and `x2`. The example usage demonstrates how to call this function with two sample vectors and prints the result."}
{"task_id": 46, "completion_id": 0, "completion": "To implement the precision metric, we need to calculate the true positives (TP) and the false positives (FP). The precision is then calculated as TP / (TP + FP). Here's how we can do it step by step:\n\n1. Calculate the true positives (TP) by finding the elements where both `y_true` and `y_pred` are 1.\n2. Calculate the false positives (FP) by finding the elements where `y_true` is 0 and `y_pred` is 1.\n3. Compute the precision as TP / (TP + FP). If TP + FP is 0, precision should be 0 to avoid division by zero.\n\nHere's the Python code implementing the above steps:\n\n```python\nimport numpy as np\n\ndef precision(y_true, y_pred):\n    # Ensure the input arrays are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Positives (FP)\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    \n    # Calculate Precision\n    if TP + FP == 0:\n        return 0\n    else:\n        return TP / (TP + FP)\n\n# Example usage:\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nprint(f\"Precision: {precision(y_true, y_pred)}\")\n```\n\nThis code snippet defines the `precision` function, which takes two numpy arrays as input and returns the precision score. The example usage at the end demonstrates how to use this function with sample data."}
{"task_id": 47, "completion_id": 0, "completion": "To implement the gradient descent variants (Stochastic, Batch, and Mini-Batch) with Mean Squared Error (MSE) as the loss function, we need to follow these steps:\n\n1. **Initialize Parameters**: The function will take the input features `X`, target values `y`, initial weights `weights`, learning rate `learning_rate`, number of iterations `n_iterations`, batch size `batch_size`, and the method `method` (which can be 'stochastic', 'batch', or 'mini-batch').\n\n2. **Define MSE Loss and Gradient**: We need to define the MSE loss function and its gradient with respect to the weights.\n\n3. **Gradient Descent Variants**:\n   - **Stochastic Gradient Descent (SGD)**: Update weights using a single sample at each iteration.\n   - **Batch Gradient Descent**: Update weights using the entire dataset at each iteration.\n   - **Mini-Batch Gradient Descent**: Update weights using a subset of the dataset (mini-batch) at each iteration.\n\n4. **Return the Final Weights**: After performing the specified number of iterations, return the final weights.\n\nHere is the Python script implementing the above steps:\n\n```python\nimport numpy as np\n\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    # Ensure X and y are numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    \n    # Add a column of ones to X for the bias term\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    \n    # Initialize weights with the provided initial values\n    weights = np.array(weights)\n    \n    # Number of samples\n    n_samples = X.shape[0]\n    \n    for iteration in range(n_iterations):\n        if method == 'stochastic':\n            # Stochastic Gradient Descent\n            for i in range(n_samples):\n                # Compute the gradient for the current sample\n                gradient = 2 * X[i] * (X[i] @ weights - y[i])\n                # Update the weights\n                weights -= learning_rate * gradient\n        elif method == 'batch':\n            # Batch Gradient Descent\n            # Compute the gradient for the entire dataset\n            gradient = 2 * X.T @ (X @ weights - y) / n_samples\n            # Update the weights\n            weights -= learning_rate * gradient\n        elif method == 'mini-batch':\n            # Mini-Batch Gradient Descent\n            for i in range(0, n_samples, batch_size):\n                # Compute the gradient for the current mini-batch\n                X_batch = X[i:i+batch_size]\n                y_batch = y[i:i+batch_size]\n                gradient = 2 * X_batch.T @ (X_batch @ weights - y_batch) / batch_size\n                # Update the weights\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose 'stochastic', 'batch', or 'mini-batch'.\")\n    \n    # Round the final weights to the nearest 4th decimal\n    weights = np.round(weights, 4)\n    \n    # Return the final weights as a list\n    return weights.tolist()\n\n# Example usage:\n# X = [[1, 2], [3, 4], [5, 6]]\n# y = [2, 4, 6]\n# weights = [0, 0, 0]\n# learning_rate = 0.01\n# n_iterations = 1000\n# batch_size = 2\n# method = 'mini-batch'\n# print(gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size, method))\n```\n\nThis script defines the `gradient_descent` function that can perform SGD, Batch GD, and Mini-Batch GD based on the specified method. The function returns the final weights after the specified number of iterations, rounded to the nearest 4th decimal."}
{"task_id": 48, "completion_id": 0, "completion": "To implement the Reduced Row Echelon Form (RREF) function, we will follow these steps:\n\n1. Convert the input matrix into a numpy array for easier manipulation.\n2. Iterate through each row of the matrix to find the pivot (leading 1) for that row.\n3. For each pivot, scale the row so that the pivot becomes 1.\n4. Use the pivot to eliminate all other entries in its column.\n5. Continue this process until the matrix is in RREF.\n6. Convert the numpy array back to a list before returning it.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef rref(matrix):\n    # Convert the input matrix to a numpy array\n    matrix = np.array(matrix, dtype=float)\n    rows, cols = matrix.shape\n    row = 0\n    \n    for col in range(cols):\n        if row >= rows:\n            break\n        \n        # Find the pivot row for the current column\n        pivot_row = row\n        while pivot_row < rows and matrix[pivot_row, col] == 0:\n            pivot_row += 1\n        \n        if pivot_row == rows:\n            continue\n        \n        # Swap the current row with the pivot row\n        matrix[[row, pivot_row]] = matrix[[pivot_row, row]]\n        \n        # Scale the pivot row so that the pivot is 1\n        pivot = matrix[row, col]\n        matrix[row] /= pivot\n        \n        # Eliminate all other entries in the current column\n        for other_row in range(rows):\n            if other_row != row:\n                factor = matrix[other_row, col]\n                matrix[other_row] -= factor * matrix[row]\n        \n        row += 1\n    \n    # Convert the numpy array back to a list\n    return matrix.tolist()\n\n# Example usage:\nmatrix = [\n    [1, 2, -1, -4],\n    [2, 3, -1, -11],\n    [-2, 0, -3, 22]\n]\nprint(rref(matrix))\n```\n\nThis script defines the `rref` function that takes a matrix as input and returns its RREF. The function uses numpy for matrix operations and ensures that the output is a list of lists, as required."}
{"task_id": 49, "completion_id": 0, "completion": "To implement the Adam optimization algorithm, we need to follow these steps:\n\n1. Initialize the parameters `x0` and set the initial values for the first and second moment estimates (`m` and `v`) to zero.\n2. For each iteration, compute the gradient of the objective function `f` at the current parameters.\n3. Update the first and second moment estimates using the gradient.\n4. Correct the bias in the first and second moment estimates.\n5. Update the parameters using the corrected moment estimates.\n6. Repeat the process for the specified number of iterations.\n7. Return the optimized parameters.\n\nHere is the Python code implementing the Adam optimization algorithm:\n\n```python\nimport numpy as np\n\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=1000):\n    # Initialize parameters\n    m = np.zeros_like(x0)  # First moment vector\n    v = np.zeros_like(x0)  # Second moment vector\n    t = 0  # Time step\n    \n    for _ in range(num_iterations):\n        t += 1\n        # Compute the gradient\n        g = grad(x0)\n        \n        # Update biased first and second moment estimates\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g ** 2)\n        \n        # Correct bias in first and second moment estimates\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        \n        # Update parameters\n        x0 = x0 - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Return the optimized parameters, rounded to 4 decimal places and converted to list\n    return np.round(x0, 4).tolist()\n\n# Example usage:\n# Define a simple quadratic function and its gradient\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# Initial parameters\nx0 = np.array([1.0, 1.0])\n\n# Optimize\noptimized_params = adam_optimizer(f, grad, x0)\nprint(optimized_params)\n```\n\nThis script defines the `adam_optimizer` function that implements the Adam optimization algorithm. It also includes an example usage with a simple quadratic function and its gradient. The optimized parameters are returned as a list, rounded to the nearest 4th decimal place."}
{"task_id": 50, "completion_id": 0, "completion": "To implement Lasso Regression using Gradient Descent, we need to follow these steps:\n\n1. Initialize the weights and bias.\n2. Define the loss function with L1 regularization.\n3. Compute the gradients of the loss function with respect to the weights and bias.\n4. Update the weights and bias using the gradients and the learning rate.\n5. Repeat the process until the maximum number of iterations is reached or the change in the loss function is less than a tolerance level.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float = 0.1, learning_rate: float = 0.01, max_iter: int = 1000, tol: float = 1e-4) -> tuple:\n    # Add a column of ones to X for the bias term\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    n_samples, n_features = X.shape\n    \n    # Initialize weights and bias\n    weights = np.zeros(n_features)\n    \n    # Initialize previous loss to a large number\n    prev_loss = float('inf')\n    \n    for iteration in range(max_iter):\n        # Predicted values\n        y_pred = X.dot(weights)\n        \n        # Compute the loss with L1 regularization\n        loss = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2) + alpha * np.sum(np.abs(weights))\n        \n        # Check for convergence\n        if np.abs(prev_loss - loss) < tol:\n            break\n        prev_loss = loss\n        \n        # Compute gradients\n        gradients = (1 / n_samples) * X.T.dot(y_pred - y) + alpha * np.sign(weights)\n        \n        # Update weights\n        weights -= learning_rate * gradients\n    \n    # Separate bias and weights\n    bias = weights[0]\n    weights = weights[1:]\n    \n    # Round to 4 decimal places and convert to list\n    bias = round(float(bias), 4)\n    weights = np.round(weights, 4).tolist()\n    \n    return weights, bias\n\n# Example usage\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([2, 3, 4, 5])\nweights, bias = l1_regularization_gradient_descent(X, y)\nprint(\"Weights:\", weights)\nprint(\"Bias:\", bias)\n```\n\nThis script defines the `l1_regularization_gradient_descent` function that takes the feature matrix `X`, the target vector `y`, and optional parameters for the regularization strength `alpha`, learning rate, maximum number of iterations, and tolerance for convergence. The function returns the optimized weights and bias as a tuple, with the weights as a list and the bias as a float, both rounded to the nearest 4th decimal."}
{"task_id": 51, "completion_id": 0, "completion": "To solve the problem of calculating the Optimal String Alignment (OSA) distance between two strings, we can use dynamic programming. The idea is to build a matrix where each cell [i][j] represents the OSA distance between the first i characters of the source string and the first j characters of the target string. We will then fill this matrix based on the minimum cost of operations (insert, delete, substitute, transpose) to transform one string into the other.\n\nHere's a step-by-step breakdown of the approach:\n1. Initialize a 2D array `dp` of size (len(source)+1) x (len(target)+1) with zeros. This array will store the OSA distances.\n2. Fill the first row and the first column of the `dp` array to represent the cost of transforming an empty string into the first i or j characters of the source or target string, respectively.\n3. Iterate through each character of the source and target strings, and for each pair of characters, calculate the cost of operations (insert, delete, substitute) and the special case of transposition.\n4. Update the `dp` array with the minimum cost of these operations.\n5. The value in the bottom-right cell of the `dp` array will be the OSA distance between the two strings.\n\nHere is the Python script implementing the above logic:\n\n```python\nimport numpy as np\n\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    \n    # Initialize the DP table\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    \n    # Fill the first row and column\n    for i in range(len_source + 1):\n        dp[i][0] = i\n    for j in range(len_target + 1):\n        dp[0][j] = j\n    \n    # Fill the DP table\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            # Cost of substitution\n            cost = 0 if source[i-1] == target[j-1] else 1\n            dp[i][j] = min(\n                dp[i-1][j] + 1,        # Deletion\n                dp[i][j-1] + 1,        # Insertion\n                dp[i-1][j-1] + cost    # Substitution\n            )\n            # Transposition\n            if i > 1 and j > 1 and source[i-1] == target[j-2] and source[i-2] == target[j-1]:\n                dp[i][j] = min(dp[i][j], dp[i-2][j-2] + 1)\n    \n    return dp[len_source][len_target]\n\n# Example usage\nsource = \"caper\"\ntarget = \"acer\"\nprint(OSA(source, target))  # Output: 2\n```\n\nThis script defines the `OSA` function that calculates the Optimal String Alignment distance between two strings using dynamic programming. The example usage at the end demonstrates how to call the function and print the result."}
{"task_id": 52, "completion_id": 0, "completion": "To implement the recall metric for binary classification, we need to calculate the number of true positives (TP) and false negatives (FN). Recall is defined as TP / (TP + FN). Here's how we can break down the implementation:\n\n1. **Identify True Positives (TP):** These are the cases where the true label (`y_true`) and the predicted label (`y_pred`) are both 1.\n2. **Identify False Negatives (FN):** These are the cases where the true label (`y_true`) is 1, but the predicted label (`y_pred`) is 0.\n3. **Calculate Recall:** Use the formula TP / (TP + FN). If TP + FN is zero, return 0.0 to avoid division by zero.\n\nLet's write the code:\n\n```python\nimport numpy as np\n\ndef recall(y_true, y_pred):\n    # Convert lists to numpy arrays for easier manipulation\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate True Positives (TP)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    \n    # Calculate False Negatives (FN)\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    \n    # Calculate Recall\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)\n\n# Example usage\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 1, 0, 0, 1]\nprint(recall(y_true, y_pred))  # Output should be the recall value\n```\n\nThis script defines the `recall` function which calculates the recall metric for binary classification. It uses NumPy for efficient array operations to compute TP and FN, and then calculates the recall value. The result is rounded to three decimal places as required."}
{"task_id": 53, "completion_id": 0, "completion": "To implement the self-attention mechanism, we need to follow these steps:\n\n1. **Input Transformation**: Transform the input sequence `X` into query (`Q`), key (`K`), and value (`V`) matrices using the weight matrices `W_q`, `W_k`, and `W_v` respectively.\n2. **Attention Scores Calculation**: Calculate the attention scores using the dot product of the query and key matrices, scaled by the square root of the key's dimension.\n3. **Softmax**: Apply the softmax function to the attention scores to get the attention weights.\n4. **Weighted Sum**: Compute the weighted sum of the value matrix using the attention weights.\n5. **Output**: Return the output as a numpy array, rounded to the nearest 4th decimal and converted to a list.\n\nHere is the Python script implementing the self-attention mechanism:\n\n```python\nimport numpy as np\n\ndef self_attention(X, W_q, W_k, W_v):\n    # Step 1: Transform the input sequence into Q, K, V matrices\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    \n    # Step 2: Calculate the attention scores\n    attention_scores = np.dot(Q, K.T) / np.sqrt(K.shape[1])\n    \n    # Step 3: Apply softmax to get the attention weights\n    attention_weights = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=-1, keepdims=True)\n    \n    # Step 4: Compute the weighted sum of the value matrix\n    output = np.dot(attention_weights, V)\n    \n    # Step 5: Round the output to the nearest 4th decimal and convert to list\n    output_rounded = np.round(output, decimals=4).tolist()\n    \n    return output_rounded\n\n# Example usage\nX = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\nW_q = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nW_k = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\nW_v = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 1.0]])\n\noutput = self_attention(X, W_q, W_k, W_v)\nprint(output)\n```\n\nThis script defines the `self_attention` function that takes an input sequence `X` and weight matrices `W_q`, `W_k`, and `W_v` for the query, key, and value transformations, respectively. It then calculates the self-attention output and returns it as a rounded list."}
{"task_id": 54, "completion_id": 0, "completion": "To implement a simple RNN cell in Python, we need to follow these steps:\n\n1. **Initialize the hidden state**: Start with the initial hidden state provided.\n2. **Iterate over the input sequence**: For each input vector in the sequence, update the hidden state using the RNN formula.\n3. **Update the hidden state**: The new hidden state is calculated using the tanh activation function applied to the sum of the weighted input and the weighted previous hidden state, plus a bias.\n4. **Return the final hidden state**: After processing the entire sequence, return the final hidden state, rounded to four decimal places.\n\nHere is the Python code that implements the above steps:\n\n```python\nimport numpy as np\n\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    # Convert lists to numpy arrays for easier manipulation\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    \n    # Initialize the hidden state\n    hidden_state = initial_hidden_state\n    \n    # Iterate over the input sequence\n    for x in input_sequence:\n        # Update the hidden state using the RNN formula\n        hidden_state = np.tanh(np.dot(x, Wx) + np.dot(hidden_state, Wh) + b)\n    \n    # Round the final hidden state to 4 decimal places and convert back to list\n    final_hidden_state = np.round(hidden_state, 4).tolist()\n    \n    return final_hidden_state\n\n# Example usage\ninput_sequence = [[1.0, 2.0], [3.0, 4.0]]\ninitial_hidden_state = [0.0, 0.0]\nWx = [[0.1, 0.2], [0.3, 0.4]]\nWh = [[0.5, 0.6], [0.7, 0.8]]\nb = [0.1, 0.1]\n\nfinal_state = rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b)\nprint(final_state)\n```\n\nThis script defines the `rnn_forward` function that takes the input sequence, initial hidden state, weight matrices, and bias vector as inputs, and returns the final hidden state after processing the entire sequence. The function uses numpy for matrix operations and ensures the final state is rounded to four decimal places before returning it as a list."}
{"task_id": 55, "completion_id": 0, "completion": "To implement the `translate_object` function, we need to follow these steps:\n\n1. Convert the list of points into a NumPy array for easier manipulation.\n2. Create a translation matrix that will be used to translate each point.\n3. Apply the translation matrix to each point.\n4. Convert the result back to a list and return it.\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Translates a list of 2D points by a specified distance in the x and y directions.\n    \n    Parameters:\n    points (list of [x, y]): List of points to be translated.\n    tx (float): Translation distance in the x direction.\n    ty (float): Translation distance in the y direction.\n    \n    Returns:\n    list: List of translated points.\n    \"\"\"\n    # Convert the list of points to a NumPy array\n    points_array = np.array(points)\n    \n    # Create the translation matrix\n    translation_matrix = np.array([[1, 0, tx],\n                                   [0, 1, ty],\n                                   [0, 0, 1]])\n    \n    # Add a column of ones to the points array to make it compatible with the translation matrix\n    ones_column = np.ones((points_array.shape[0], 1))\n    points_with_ones = np.hstack((points_array, ones_column))\n    \n    # Apply the translation matrix to each point\n    translated_points = np.dot(points_with_ones, translation_matrix.T)\n    \n    # Remove the last column (which is now all ones) and convert back to a list\n    translated_points_list = translated_points[:, :2].tolist()\n    \n    return translated_points_list\n\n# Example usage\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)\nprint(translated_points)\n```\n\nThis script defines the `translate_object` function, which takes a list of 2D points and translation distances `tx` and `ty`, and returns a new list of points after applying the translation. The example usage at the end demonstrates how to call the function and print the translated points."}
{"task_id": 56, "completion_id": 0, "completion": "To calculate the KL divergence between two normal distributions, we can use the formula for the KL divergence between two univariate Gaussian distributions. The formula is as follows:\n\n\\[ D_{KL}(P \\parallel Q) = \\log \\left( \\frac{\\sigma_Q}{\\sigma_P} \\right) + \\frac{\\sigma_P^2 + (\\mu_P - \\mu_Q)^2}{2 \\sigma_Q^2} - \\frac{1}{2} \\]\n\nWhere:\n- \\( \\mu_P \\) and \\( \\sigma_P \\) are the mean and standard deviation of distribution P.\n- \\( \\mu_Q \\) and \\( \\sigma_Q \\) are the mean and standard deviation of distribution Q.\n\nLet's implement this in Python:\n\n```python\nimport numpy as np\n\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Computes the Kullback-Leibler divergence between two normal distributions.\n    \n    Parameters:\n    - mu_p: Mean of distribution P.\n    - sigma_p: Standard deviation of distribution P.\n    - mu_q: Mean of distribution Q.\n    - sigma_q: Standard deviation of distribution Q.\n    \n    Returns:\n    - The KL divergence as a float.\n    \"\"\"\n    if sigma_p <= 0 or sigma_q <= 0:\n        raise ValueError(\"Standard deviations must be positive.\")\n    \n    term1 = np.log(sigma_q / sigma_p)\n    term2 = (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2)\n    kl_div = term1 + term2 - 0.5\n    \n    return kl_div\n\n# Example usage:\nmu_p = 0\nsigma_p = 1\nmu_q = 1\nsigma_q = 2\n\nkl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\nprint(f\"KL Divergence: {kl_div}\")\n```\n\nThis script defines the function `kl_divergence_normal` which calculates the KL divergence between two normal distributions given their means and standard deviations. It also includes a simple example of how to use this function."}
{"task_id": 57, "completion_id": 0, "completion": "To implement the Gauss-Seidel method for solving a system of linear equations, we need to follow these steps:\n\n1. Initialize the solution vector `x` with the initial guess `x_ini` or zeros if `x_ini` is not provided.\n2. For each iteration, update each element of `x` using the most recent values of the other elements.\n3. Ensure the result is rounded to the nearest 4th decimal and returned as a Python list.\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef gauss_seidel(A, b, n, x_ini=None):\n    # Ensure A is a numpy array\n    A = np.array(A)\n    # Ensure b is a numpy array\n    b = np.array(b)\n    # Get the number of equations\n    n_eq = A.shape[0]\n    \n    # Initialize the solution vector x\n    if x_ini is None:\n        x = np.zeros(n_eq)\n    else:\n        x = np.array(x_ini)\n    \n    # Perform the Gauss-Seidel iterations\n    for _ in range(n):\n        for i in range(n_eq):\n            # Calculate the new value for x[i]\n            x[i] = (b[i] - np.dot(A[i, :i], x[:i]) - np.dot(A[i, i+1:], x[i+1:])) / A[i, i]\n    \n    # Round the solution to the nearest 4th decimal\n    x = np.round(x, 4)\n    \n    # Convert the solution to a Python list\n    return x.tolist()\n\n# Example usage:\nA = [[4, 1, 2], [3, 5, 1], [1, 1, 3]]\nb = [4, 7, 3]\nn = 100\nx_ini = [0, 0, 0]\n\nsolution = gauss_seidel(A, b, n, x_ini)\nprint(solution)\n```\n\nThis script defines the `gauss_seidel` function which takes a matrix `A`, a vector `b`, the number of iterations `n`, and an optional initial guess `x_ini`. It then iteratively updates the solution vector `x` using the Gauss-Seidel method and returns the result as a Python list, rounded to the nearest 4th decimal."}
{"task_id": 58, "completion_id": 0, "completion": "To solve the given task, we will implement the Gaussian Elimination method with partial pivoting. This method involves transforming the system of linear equations into an upper triangular matrix, which can then be solved using backward substitution. Here's a step-by-step breakdown of the process:\n\n1. **Partial Pivoting**: Before eliminating the elements below the pivot, we swap rows to ensure the pivot element is the largest in its column to improve numerical stability.\n2. **Forward Elimination**: We use the pivot element to eliminate the elements below it in the same column.\n3. **Backward Substitution**: Once the matrix is in upper triangular form, we solve for the variables starting from the last equation and moving upwards.\n\nLet's implement this in Python:\n\n```python\nimport numpy as np\n\ndef gaussian_elimination(A, b):\n    n = len(b)\n    # Augment the matrix A with the vector b\n    Ab = np.hstack([A, b.reshape(-1, 1)])\n    \n    # Partial pivoting and forward elimination\n    for i in range(n):\n        # Find the row with the largest absolute value in the current column\n        max_row = i + np.argmax(np.abs(Ab[i:, i]))\n        # Swap the current row with the row having the largest element\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        \n        # Eliminate the elements below the pivot\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    \n    # Backward substitution\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, :-1], x)) / Ab[i, i]\n    \n    # Round the results to the nearest 4th decimal and convert to list\n    return np.round(x, 4).tolist()\n\n# Example usage\nA = np.array([[2, 1, -1], [-3, -1, 2], [-2, 1, 2]], dtype=float)\nb = np.array([8, -11, -3], dtype=float)\nsolution = gaussian_elimination(A, b)\nprint(solution)\n```\n\nThis script defines the `gaussian_elimination` function that takes a matrix `A` and a vector `b` as inputs, performs Gaussian Elimination with partial pivoting, and returns the solution vector `x` rounded to the nearest 4th decimal place. The example usage demonstrates how to use this function with a sample system of linear equations."}
{"task_id": 59, "completion_id": 0, "completion": "To implement the LSTM network, we need to follow the standard LSTM equations for the forget gate, input gate, candidate cell state, and output gate. We will also need to update the cell state and hidden state at each time step. Here is the complete implementation:\n\n```python\nimport numpy as np\n\nclass LSTM:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # Initialize weights and biases\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def tanh(self, x):\n        return np.tanh(x)\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, final cell state.\n        \"\"\"\n        # Initialize hidden and cell states\n        h = initial_hidden_state\n        c = initial_cell_state\n        hidden_states = []\n\n        # Process each time step\n        for t in range(x.shape[1]):\n            # Concatenate input and previous hidden state\n            concat = np.concatenate((x[:, t].reshape(-1, 1), h), axis=0)\n\n            # Compute gates\n            f = self.sigmoid(self.Wf @ concat + self.bf)\n            i = self.sigmoid(self.Wi @ concat + self.bi)\n            c_tilde = self.tanh(self.Wc @ concat + self.bc)\n            o = self.sigmoid(self.Wo @ concat + self.bo)\n\n            # Update cell state and hidden state\n            c = f * c + i * c_tilde\n            h = o * self.tanh(c)\n\n            # Store hidden state\n            hidden_states.append(h)\n\n        # Convert hidden states to list and round to 4 decimal places\n        hidden_states = np.array(hidden_states).round(4).tolist()\n        final_hidden_state = h.round(4).tolist()\n        final_cell_state = c.round(4).tolist()\n\n        return hidden_states, final_hidden_state, final_cell_state\n\n# Example usage\nif __name__ == \"__main__\":\n    input_size = 3\n    hidden_size = 2\n    lstm = LSTM(input_size, hidden_size)\n\n    # Example input\n    x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    initial_hidden_state = np.zeros((hidden_size, 1))\n    initial_cell_state = np.zeros((hidden_size, 1))\n\n    hidden_states, final_hidden_state, final_cell_state = lstm.forward(x, initial_hidden_state, initial_cell_state)\n    print(\"Hidden States:\", hidden_states)\n    print(\"Final Hidden State:\", final_hidden_state)\n    print(\"Final Cell State:\", final_cell_state)\n```\n\nThis script defines an LSTM class with the necessary methods to perform the forward pass. The `forward` method processes a sequence of inputs and returns the hidden states at each time step, as well as the final hidden state and cell state. The example usage at the bottom demonstrates how to use the LSTM class with a simple input sequence."}
{"task_id": 60, "completion_id": 0, "completion": "To implement the TF-IDF calculation, we need to follow these steps:\n\n1. Calculate the Term Frequency (TF) for each term in each document.\n2. Calculate the Inverse Document Frequency (IDF) for each term in the corpus.\n3. Compute the TF-IDF score for each term in the query for each document.\n4. Handle edge cases and ensure the output is rounded to the nearest 4th decimal.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError(\"Corpus cannot be empty.\")\n    \n    # Step 1: Calculate Term Frequency (TF)\n    def term_frequency(doc):\n        doc_count = Counter(doc)\n        return {term: count / len(doc) for term, count in doc_count.items()}\n    \n    # Step 2: Calculate Inverse Document Frequency (IDF)\n    def inverse_document_frequency(corpus, term):\n        df = sum(1 for doc in corpus if term in doc)\n        return np.log((1 + len(corpus)) / (1 + df)) + 1\n    \n    # Step 3: Compute TF-IDF scores\n    tf_idf_scores = []\n    for doc in corpus:\n        tf = term_frequency(doc)\n        doc_scores = []\n        for term in query:\n            tf_idf = tf.get(term, 0) * inverse_document_frequency(corpus, term)\n            doc_scores.append(round(tf_idf, 4))\n        tf_idf_scores.append(doc_scores)\n    \n    return np.array(tf_idf_scores).tolist()\n\n# Example usage:\ncorpus = [\n    [\"the\", \"quick\", \"brown\", \"fox\"],\n    [\"the\", \"lazy\", \"dog\"],\n    [\"the\", \"quick\", \"dog\"]\n]\nquery = [\"the\", \"quick\", \"dog\"]\nprint(compute_tf_idf(corpus, query))\n```\n\nThis script defines the `compute_tf_idf` function which calculates the TF-IDF scores for a given query against a corpus of documents. It handles edge cases such as an empty corpus and ensures the output is rounded to the nearest 4th decimal."}
{"task_id": 61, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    # Calculate true positives, false positives, and false negatives\n    tp = np.sum((y_pred == 1) & (y_true == 1))\n    fp = np.sum((y_pred == 1) & (y_true == 0))\n    fn = np.sum((y_pred == 0) & (y_true == 1))\n    \n    # Calculate Precision and Recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F-Score\n    if precision + recall == 0:\n        f_beta = 0\n    else:\n        f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n    \n    return round(f_beta, 3)\n\n# Example usage\ny_true = np.array([1, 0, 1, 1, 0, 1])\ny_pred = np.array([1, 1, 1, 0, 0, 1])\nbeta = 1.0\nprint(f_score(y_true, y_pred, beta))  # Output should be the F-Score\n```\n\nThis code snippet defines a function `f_score` that calculates the F-Score for a binary classification task based on the provided true labels (`y_true`), predicted labels (`y_pred`), and a `beta` value that adjusts the importance of precision and recall in the calculation. The function returns the F-Score rounded to three decimal places."}
{"task_id": 62, "completion_id": 0, "completion": "```python\nimport numpy as np\n\nclass SimpleRNN:\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \"\"\"\n        T = len(input_sequence)\n        hidden_states = [np.zeros((self.hidden_size, 1))]\n        outputs = []\n        last_inputs = []\n\n        for t in range(T):\n            x = np.array(input_sequence[t]).reshape(-1, 1)\n            last_inputs.append(x)\n            h_prev = hidden_states[-1]\n            h = np.tanh(self.W_xh @ x + self.W_hh @ h_prev + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            hidden_states.append(h)\n            outputs.append(y)\n\n        return outputs, last_inputs, hidden_states\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate):\n        \"\"\"\n        Backward pass through the RNN for a given sequence of inputs and expected outputs.\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(hidden_states[0])\n\n        for t in reversed(range(T)):\n            x = last_inputs[t]\n            h = hidden_states[t+1]\n            h_prev = hidden_states[t]\n            y = outputs[t]\n            dy = y - np.array(expected_output[t]).reshape(-1, 1)\n\n            dW_hy += dy @ h.T\n            db_y += dy\n\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h**2) * dh\n\n            dW_xh += dh_raw @ x.T\n            dW_hh += dh_raw @ h_prev.T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n\n        for dparam in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n            np.clip(dparam, -5, 5, out=dparam)\n\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def train(self, input_sequence, expected_output, learning_rate, epochs):\n        for epoch in range(epochs):\n            outputs, last_inputs, hidden_states = self.forward(input_sequence)\n            self.backward(input_sequence, expected_output, outputs, last_inputs, hidden_states, learning_rate)\n            loss = 0.5 * np.sum((np.array(expected_output) - np.array(outputs)) ** 2)\n            print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n\n# Example usage\ninput_size = 2\nhidden_size = 3\noutput_size = 2\nrnn = SimpleRNN(input_size, hidden_size, output_size)\ninput_sequence = [[1, 2], [2, 3], [3, 4]]\nexpected_output = [[2, 3], [3, 4], [4, 5]]\nlearning_rate = 0.01\nepochs = 1000\nrnn.train(input_sequence, expected_output, learning_rate, epochs)\n```\n\nThis script defines a simple RNN with the ability to perform forward and backward passes. The `train` method is used to train the RNN on a given sequence of inputs and expected outputs over a specified number of epochs. The example usage at the end demonstrates how to initialize the RNN, define an input sequence and expected output, and train the network."}
{"task_id": 63, "completion_id": 0, "completion": "To implement the Conjugate Gradient (CG) method, we need to follow the algorithm's steps closely. The CG method is particularly useful for solving large, sparse, symmetric, positive-definite linear systems. Here's a step-by-step breakdown of the implementation:\n\n1. **Initialization**: If no initial guess `x0` is provided, we start with a zero vector. We also initialize the residual `r` as `b - Ax`, the search direction `p` as `r`, and set the initial error norm `r_norm` as the norm of `r`.\n\n2. **Iteration**: For each iteration, we:\n   - Compute `Ap` (the matrix-vector product of `A` and `p`).\n   - Calculate the step size `alpha` as the ratio of the dot product of `r` and `r` to the dot product of `p` and `Ap`.\n   - Update the solution `x` by adding `alpha * p` to it.\n   - Update the residual `r` by subtracting `alpha * Ap` from it.\n   - Calculate the new error norm `r_norm_new` as the norm of the updated `r`.\n   - Check if the new error norm is below the tolerance `tol`. If so, we can stop early.\n   - Update the search direction `p` using the Polak-Ribiere formula.\n   - Update `r_norm` to `r_norm_new`.\n\n3. **Return the solution**: After completing the iterations or meeting the tolerance, we return the solution vector `x`, rounded to 8 decimal places and converted to a list.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-8):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x0 = np.zeros_like(b)\n    x = x0\n    r = b - np.dot(A, x)\n    p = r\n    r_norm = np.linalg.norm(r)\n    \n    for _ in range(n):\n        Ap = np.dot(A, p)\n        alpha = np.dot(r, r) / np.dot(p, Ap)\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_norm_new = np.linalg.norm(r)\n        if r_norm_new < tol:\n            break\n        beta = r_norm_new**2 / r_norm**2\n        p = r + beta * p\n        r_norm = r_norm_new\n    \n    return np.round(x, 8).tolist()\n```\n\nThis function will solve the linear system `Ax = b` using the Conjugate Gradient method, with the specified parameters and constraints."}
{"task_id": 64, "completion_id": 0, "completion": "To calculate the Gini Impurity for a set of classes, we need to follow these steps:\n\n1. Count the occurrences of each class in the list.\n2. Calculate the probability of each class.\n3. Use the formula for Gini Impurity: \\( G = 1 - \\sum_{i=1}^{n} p_i^2 \\), where \\( p_i \\) is the probability of class \\( i \\).\n4. Return the Gini Impurity rounded to three decimal places.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    # Count the occurrences of each class\n    class_counts = Counter(y)\n    total_samples = len(y)\n    \n    # Calculate the Gini Impurity\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    \n    # Return the Gini Impurity rounded to three decimal places\n    return round(gini, 3)\n\n# Example usage\nif __name__ == \"__main__\":\n    y = [1, 1, 2, 2, 3, 3, 3, 3]\n    print(f\"Gini Impurity: {gini_impurity(y)}\")\n```\n\nThis script defines a function `gini_impurity` that calculates the Gini Impurity for a given list of class labels. It uses the `Counter` class from the `collections` module to count the occurrences of each class, then calculates the Gini Impurity using the formula provided. The result is rounded to three decimal places before being returned."}
{"task_id": 65, "completion_id": 0, "completion": "To implement the `compressed_row_sparse_matrix` function, we need to follow these steps:\n\n1. Traverse the dense matrix to collect all non-zero elements, their column indices, and the cumulative count of non-zero elements per row.\n2. Initialize the values array, column indices array, and row pointer array.\n3. Populate the values array and column indices array with non-zero elements and their column indices.\n4. Populate the row pointer array with the cumulative count of non-zero elements per row. The first element of the row pointer array should be 0, and the last element should be the total number of non-zero elements in the matrix.\n\nHere is the Python script that accomplishes this:\n\n```python\ndef compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    col_indices = []\n    row_pointers = [0]\n    \n    for row in dense_matrix:\n        non_zero_count = 0\n        for col_index, value in enumerate(row):\n            if value != 0:\n                values.append(value)\n                col_indices.append(col_index)\n                non_zero_count += 1\n        row_pointers.append(row_pointers[-1] + non_zero_count)\n    \n    return (values, col_indices, row_pointers)\n\n# Example usage:\ndense_matrix = [\n    [1, 0, 0, 2],\n    [0, 3, 0, 0],\n    [4, 0, 5, 0]\n]\n\nvalues, col_indices, row_pointers = compressed_row_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Column Indices:\", col_indices)\nprint(\"Row Pointers:\", row_pointers)\n```\n\nThis script defines the function and demonstrates its usage with an example dense matrix. The output will be the CSR representation of the matrix, which includes the values array, column indices array, and row pointer array."}
{"task_id": 66, "completion_id": 0, "completion": "To solve this task, we need to implement the mathematical formula for the orthogonal projection of a vector `v` onto another vector `L`. The formula for the projection of `v` onto `L` is given by:\n\n\\[ \\text{proj}_L(v) = \\left( \\frac{v \\cdot L}{L \\cdot L} \\right) L \\]\n\nWhere:\n- \\( v \\cdot L \\) is the dot product of `v` and `L`.\n- \\( L \\cdot L \\) is the dot product of `L` with itself, which is also the squared magnitude of `L`.\n\nLet's implement this in Python, ensuring that the result is rounded to three decimal places.\n\n```python\ndef orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected\n    :param L: The line vector defining the direction of projection\n    :return: List representing the projection of v onto L\n    \"\"\"\n    # Calculate the dot product of v and L\n    dot_product_vL = sum(x*y for x, y in zip(v, L))\n    # Calculate the dot product of L with itself\n    dot_product_LL = sum(x*x for x in L)\n    # Calculate the scalar projection factor\n    scalar_projection = dot_product_vL / dot_product_LL\n    # Calculate the projection vector\n    projection = [scalar_projection * x for x in L]\n    # Round the projection vector to three decimal places\n    projection = [round(x, 3) for x in projection]\n    return projection\n\n# Example usage:\n# v = [1, 2, 3]\n# L = [4, 5, 6]\n# print(orthogonal_projection(v, L))  # Output should be the projection of v onto L\n```\n\nThis script defines the `orthogonal_projection` function that computes the projection of vector `v` onto vector `L` and returns the result rounded to three decimal places."}
{"task_id": 67, "completion_id": 0, "completion": "To implement the `compressed_col_sparse_matrix` function, we need to follow these steps:\n\n1. Initialize the `values`, `row_indices`, and `column_pointer` lists.\n2. Iterate over the columns of the dense matrix in column-major order.\n3. For each non-zero element, append its value to `values` and its row index to `row_indices`.\n4. After processing each column, append the current length of `values` to `column_pointer` to indicate the starting index of the next column.\n5. Return the `values`, `row_indices`, and `column_pointer` lists as a tuple.\n\nHere is the Python script that implements the above steps:\n\n```python\ndef compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    # Initialize the lists\n    values = []\n    row_indices = []\n    column_pointer = [0]  # The first column starts at index 0\n    \n    # Get the number of rows and columns in the dense matrix\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0]) if num_rows > 0 else 0\n    \n    # Iterate over the columns in column-major order\n    for col in range(num_cols):\n        for row in range(num_rows):\n            value = dense_matrix[row][col]\n            if value != 0:\n                values.append(value)\n                row_indices.append(row)\n        # Append the current length of values to column_pointer\n        column_pointer.append(len(values))\n    \n    return (values, row_indices, column_pointer)\n\n# Example usage\ndense_matrix = [\n    [0, 2, 0],\n    [3, 0, 0],\n    [0, 0, 4],\n    [5, 0, 0]\n]\n\nvalues, row_indices, column_pointer = compressed_col_sparse_matrix(dense_matrix)\nprint(\"Values:\", values)\nprint(\"Row Indices:\", row_indices)\nprint(\"Column Pointer:\", column_pointer)\n```\n\nThis script defines the `compressed_col_sparse_matrix` function and demonstrates its usage with an example dense matrix. The output will show the `values`, `row_indices`, and `column_pointer` lists that represent the matrix in CSC format."}
{"task_id": 68, "completion_id": 0, "completion": "To solve this task, we need to follow these steps:\n\n1. Convert the input matrix `A` into row echelon form (REF). This can be done using Gaussian elimination.\n2. Identify the pivot columns in the REF. These columns correspond to the linearly independent columns in the original matrix.\n3. Extract the columns from the original matrix that correspond to the pivot columns in the REF.\n4. Return these columns as the basis vectors for the column space of `A`.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef matrix_image(A):\n    # Convert the matrix to row echelon form\n    A_ref = np.array(A, dtype=float)\n    m, n = A_ref.shape\n    pivot_cols = []\n    \n    for col in range(n):\n        # Find the first non-zero row\n        row = col\n        while row < m and A_ref[row, col] == 0:\n            row += 1\n        if row == m:\n            continue  # All remaining rows are zero, so skip this column\n        \n        # Swap rows to bring the first non-zero element to the top\n        A_ref[[row, col]] = A_ref[[col, row]]\n        \n        # Make the pivot element 1\n        pivot = A_ref[col, col]\n        A_ref[col] /= pivot\n        \n        # Eliminate the entries below the pivot\n        for row in range(col + 1, m):\n            factor = A_ref[row, col]\n            A_ref[row] -= factor * A_ref[col]\n        \n        pivot_cols.append(col)\n    \n    # Extract the corresponding columns from the original matrix\n    basis_vectors = A[:, pivot_cols]\n    \n    # Round to 8 decimal places and convert to list\n    return np.round(basis_vectors, 8).tolist()\n\n# Example usage\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(matrix_image(A))\n```\n\nThis script defines the `matrix_image` function that performs Gaussian elimination to convert the matrix into row echelon form, identifies the pivot columns, and extracts the corresponding columns from the original matrix to form the basis of the column space. The result is rounded to 8 decimal places and returned as a list of lists."}
{"task_id": 69, "completion_id": 0, "completion": "To calculate the R-squared value, we need to follow these steps:\n\n1. Calculate the mean of the true values (`y_true`).\n2. Calculate the total sum of squares (TSS), which is the sum of the squared differences between each true value and the mean of the true values.\n3. Calculate the residual sum of squares (RSS), which is the sum of the squared differences between each true value and the corresponding predicted value (`y_pred`).\n4. Calculate the R-squared value using the formula: \\( R^2 = 1 - \\frac{RSS}{TSS} \\).\n5. Return the R-squared value rounded to three decimal places.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef r_squared(y_true, y_pred):\n    # Step 1: Calculate the mean of the true values\n    y_true_mean = np.mean(y_true)\n    \n    # Step 2: Calculate the total sum of squares (TSS)\n    tss = np.sum((y_true - y_true_mean) ** 2)\n    \n    # Step 3: Calculate the residual sum of squares (RSS)\n    rss = np.sum((y_true - y_pred) ** 2)\n    \n    # Step 4: Calculate the R-squared value\n    r2 = 1 - (rss / tss)\n    \n    # Step 5: Return the R-squared value rounded to three decimal places\n    return round(r2, 3)\n\n# Example usage:\ny_true = np.array([3, -0.5, 2, 7])\ny_pred = np.array([2.5, 0.0, 2, 8])\nprint(r_squared(y_true, y_pred))  # Output should be the R-squared value\n```\n\nThis script defines the `r_squared` function that takes two numpy arrays as input: `y_true` for the true values and `y_pred` for the predicted values. It then calculates the R-squared value according to the steps outlined above and returns it rounded to three decimal places."}
{"task_id": 70, "completion_id": 0, "completion": "To solve the given task, we need to follow these steps:\n\n1. Check if the image matrix is empty.\n2. Check if all rows in the matrix have the same length.\n3. Check if all pixel values are within the valid range (0-255).\n4. Calculate the average brightness by summing all pixel values and dividing by the total number of pixels.\n5. Return the average brightness rounded to two decimal places.\n\nHere is the Python script that implements the `calculate_brightness` function:\n\n```python\ndef calculate_brightness(img):\n    # Check if the image matrix is empty\n    if not img or not img[0]:\n        return -1\n    \n    # Check if all rows have the same length\n    row_length = len(img[0])\n    if any(len(row) != row_length for row in img):\n        return -1\n    \n    # Initialize total brightness and pixel count\n    total_brightness = 0\n    pixel_count = 0\n    \n    # Iterate through each pixel in the image\n    for row in img:\n        for pixel in row:\n            # Check if the pixel value is within the valid range\n            if pixel < 0 or pixel > 255:\n                return -1\n            # Add the pixel value to the total brightness\n            total_brightness += pixel\n            pixel_count += 1\n    \n    # Calculate the average brightness\n    if pixel_count == 0:\n        return -1\n    average_brightness = total_brightness / pixel_count\n    \n    # Return the average brightness rounded to two decimal places\n    return round(average_brightness, 2)\n\n# Example usage:\nimage = [\n    [100, 150, 200],\n    [120, 130, 140],\n    [110, 160, 170]\n]\nprint(calculate_brightness(image))  # Output: 136.67\n```\n\nThis script includes the function `calculate_brightness` that performs the necessary checks and calculations as described in the task. It also includes an example usage of the function with a sample image matrix."}
{"task_id": 71, "completion_id": 0, "completion": "To implement the `rmse` function, we need to follow these steps:\n\n1. Import the necessary package (`numpy`).\n2. Define the function `rmse(y_true, y_pred)`.\n3. Check for edge cases such as mismatched array shapes, empty arrays, and invalid input types.\n4. Calculate the RMSE using the formula provided.\n5. Return the RMSE value rounded to three decimal places.\n\nHere is the self-contained Python script that accomplishes the task:\n\n```python\nimport numpy as np\n\ndef rmse(y_true, y_pred):\n    # Check if inputs are numpy arrays\n    if not (isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray)):\n        raise ValueError(\"Inputs must be numpy arrays.\")\n    \n    # Check if arrays are empty\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError(\"Input arrays cannot be empty.\")\n    \n    # Check if arrays have the same shape\n    if y_true.shape != y_pred.shape:\n        raise ValueError(\"Input arrays must have the same shape.\")\n    \n    # Calculate the squared differences\n    squared_diff = (y_true - y_pred) ** 2\n    \n    # Calculate the mean of the squared differences\n    mean_squared_diff = np.mean(squared_diff)\n    \n    # Calculate the square root of the mean squared differences\n    rmse_value = np.sqrt(mean_squared_diff)\n    \n    # Return the RMSE value rounded to three decimal places\n    return round(rmse_value, 3)\n\n# Example usage:\n# y_true = np.array([3, -0.5, 2, 7])\n# y_pred = np.array([2.5, 0.0, 2, 8])\n# print(rmse(y_true, y_pred))  # Output: 0.612\n```\n\nThis script includes the necessary checks for input types, array emptiness, and shape mismatches. It then calculates the RMSE using the provided formula and returns the result rounded to three decimal places."}
{"task_id": 72, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that calculates the Jaccard Index between two binary arrays. The Jaccard Index is a measure of similarity between two sets, and in this case, the sets are represented by binary arrays of true and predicted labels. The function will calculate the intersection and union of the two sets and then compute the Jaccard Index as the ratio of the size of the intersection to the size of the union.\n\nHere's a step-by-step plan:\n1. Convert the input lists or arrays to numpy arrays if they are not already.\n2. Calculate the intersection and union of the two arrays.\n3. Compute the Jaccard Index using the formula provided.\n4. Handle edge cases where the union might be zero to avoid division by zero.\n5. Return the Jaccard Index rounded to three decimal places.\n\nNow, let's implement this in Python:\n\n```python\nimport numpy as np\n\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    # Ensure inputs are numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate intersection and union\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    \n    # Handle edge case where union is zero\n    if union == 0:\n        return 0.0\n    \n    # Calculate Jaccard Index\n    jaccard = intersection / union\n    \n    # Return Jaccard Index rounded to three decimal places\n    return round(jaccard, 3)\n\n# Example usage\ny_true = np.array([1, 1, 0, 0])\ny_pred = np.array([1, 0, 1, 0])\nprint(jaccard_index(y_true, y_pred))  # Output should be 0.333\n```\n\nThis script defines the `jaccard_index` function that calculates the Jaccard Index between two binary arrays, handling edge cases and rounding the result to three decimal places as required."}
{"task_id": 73, "completion_id": 0, "completion": "To implement the `dice_score` function, we need to follow these steps:\n\n1. Convert the input arrays to NumPy arrays to ensure compatibility and to leverage NumPy's efficient operations.\n2. Calculate the intersection of `y_true` and `y_pred` by performing an element-wise AND operation.\n3. Count the number of true positives (elements that are 1 in both `y_true` and `y_pred`).\n4. Count the number of elements in `y_true` and `y_pred` that are 1.\n5. Use the formula to calculate the Dice Score.\n6. Handle the edge case where both the true and predicted labels have no positive elements to avoid division by zero.\n7. Return the Dice Score rounded to 3 decimal places.\n\nHere is the Python code implementing the above steps:\n\n```python\nimport numpy as np\n\ndef dice_score(y_true, y_pred):\n    # Convert inputs to numpy arrays\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    # Calculate the intersection\n    intersection = np.sum(y_true & y_pred)\n    \n    # Calculate the number of elements in y_true and y_pred\n    num_true = np.sum(y_true)\n    num_pred = np.sum(y_pred)\n    \n    # Calculate the Dice Score\n    if num_true == 0 and num_pred == 0:\n        # Handle the edge case where there are no true or predicted positives\n        dice = 1.0\n    else:\n        dice = (2.0 * intersection) / (num_true + num_pred)\n    \n    # Return the Dice Score rounded to 3 decimal places\n    return round(dice, 3)\n\n# Example usage:\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 1, 1, 0, 0, 1]\nprint(dice_score(y_true, y_pred))  # Output should be the Dice Score\n```\n\nThis code snippet is self-contained and includes the necessary import statement for NumPy. It defines the `dice_score` function and provides an example usage to demonstrate how to calculate the Dice Score for given true and predicted labels."}
{"task_id": 74, "completion_id": 0, "completion": "To solve the given task, we need to follow these steps:\n\n1. **Initialize the Hypervectors**: For each feature in the row, we will generate a hypervector for the feature name and its value. The hypervector for the value will be generated using a seed from the `random_seeds` dictionary to ensure reproducibility.\n\n2. **Binding Hypervectors**: We will bind the hypervector of the feature name with the hypervector of the feature value. The binding operation can be a simple element-wise multiplication or any other operation that suits the context of HDC.\n\n3. **Bundling Hypervectors**: After binding the hypervectors for each feature, we will bundle them together to form a composite hypervector for the entire row. The bundling operation can be a simple element-wise addition or another operation that suits the context of HDC.\n\n4. **Return the Composite Hypervector**: Finally, we will convert the composite hypervector to a list and return it.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef create_row_hv(row, dim, random_seeds):\n    # Initialize the composite hypervector with zeros\n    composite_hv = np.zeros(dim)\n    \n    # Loop through each feature in the row\n    for feature, value in row.items():\n        # Generate hypervector for the feature name\n        feature_hv = np.random.RandomState(seed=random_seeds[feature]).randint(2, size=dim) * 2 - 1\n        \n        # Generate hypervector for the feature value\n        value_hv = np.random.RandomState(seed=random_seeds[feature]).randint(2, size=dim) * 2 - 1\n        \n        # Bind the feature name hypervector with the feature value hypervector\n        bound_hv = feature_hv * value_hv\n        \n        # Bundle the bound hypervector into the composite hypervector\n        composite_hv += bound_hv\n    \n    # Convert the composite hypervector to a list and return\n    return composite_hv.tolist()\n\n# Example usage\nrow = {'feature1': 10, 'feature2': 20}\ndim = 100\nrandom_seeds = {'feature1': 42, 'feature2': 24}\ncomposite_hv = create_row_hv(row, dim, random_seeds)\nprint(composite_hv)\n```\n\nIn this script, we use `np.random.RandomState` to generate hypervectors with a specified seed for reproducibility. The hypervectors are binary (values of -1 or 1) for simplicity, but this can be adjusted based on the specific requirements of HDC. The binding operation is a simple element-wise multiplication, and the bundling operation is an element-wise addition."}
{"task_id": 75, "completion_id": 0, "completion": "To generate a confusion matrix for binary classification, we need to count the occurrences of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These can be summarized as follows:\n\n- **True Positives (TP):** The number of times the model correctly predicted the positive class.\n- **True Negatives (TN):** The number of times the model correctly predicted the negative class.\n- **False Positives (FP):** The number of times the model incorrectly predicted the positive class.\n- **False Negatives (FN):** The number of times the model incorrectly predicted the negative class.\n\nGiven the input data structure, we can use a `Counter` to tally these occurrences. The confusion matrix will be a 2x2 matrix where the first row represents the actual negatives and positives, and the second row represents the predicted negatives and positives.\n\nHere is the Python script to accomplish this:\n\n```python\nfrom collections import Counter\n\ndef confusion_matrix(data):\n    # Initialize a counter to count the occurrences of each combination\n    counts = Counter()\n    \n    # Iterate over each pair in the data\n    for y_true, y_pred in data:\n        # Update the counter with the pair\n        counts[(y_true, y_pred)] += 1\n    \n    # Extract the counts for each combination\n    tp = counts[(1, 1)]  # True Positives\n    tn = counts[(0, 0)]  # True Negatives\n    fp = counts[(0, 1)]  # False Positives\n    fn = counts[(1, 0)]  # False Negatives\n    \n    # Construct the confusion matrix\n    matrix = [[tn, fp],\n              [fn, tp]]\n    \n    return matrix\n\n# Example usage\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1]]\nprint(confusion_matrix(data))\n```\n\nThis script defines the `confusion_matrix` function, which takes a list of `[y_true, y_pred]` pairs and returns a 2x2 confusion matrix. The `Counter` from the `collections` module is used to count the occurrences of each pair, and these counts are then used to construct the confusion matrix."}
{"task_id": 76, "completion_id": 0, "completion": "To calculate the cosine similarity between two vectors, we need to follow these steps:\n\n1. Ensure that the vectors have the same shape.\n2. Calculate the dot product of the two vectors.\n3. Calculate the magnitude (or norm) of each vector.\n4. Divide the dot product by the product of the magnitudes to get the cosine of the angle between the vectors.\n5. Round the result to three decimal places.\n\nHere is the Python script that implements the `cosine_similarity` function:\n\n```python\nimport numpy as np\n\ndef cosine_similarity(v1, v2):\n    # Check if the vectors have the same shape\n    if v1.shape != v2.shape:\n        raise ValueError(\"Vectors must have the same shape\")\n    \n    # Check if the vectors are not empty and have non-zero magnitude\n    if v1.size == 0 or v2.size == 0 or np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError(\"Vectors cannot be empty or have zero magnitude\")\n    \n    # Calculate the dot product of the vectors\n    dot_product = np.dot(v1, v2)\n    \n    # Calculate the magnitudes of the vectors\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    \n    # Calculate the cosine similarity\n    cos_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    \n    # Return the cosine similarity rounded to three decimal places\n    return round(cos_sim, 3)\n\n# Example usage\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\nprint(cosine_similarity(v1, v2))\n```\n\nThis script defines the `cosine_similarity` function that takes two numpy arrays as input and returns the cosine similarity between them, rounded to three decimal places. It also includes checks to ensure that the vectors have the same shape and are not empty or have zero magnitude."}
{"task_id": 77, "completion_id": 0, "completion": "```python\nfrom collections import Counter\nfrom typing import List, Tuple\n\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    # Validate input\n    if len(actual) != len(predicted):\n        raise ValueError(\"The length of actual and predicted lists must be the same.\")\n    if not all(x in [0, 1] for x in actual + predicted):\n        raise ValueError(\"All elements in actual and predicted lists must be either 0 or 1.\")\n    \n    # Calculate confusion matrix\n    confusion_matrix = [[0, 0], [0, 0]]\n    for a, p in zip(actual, predicted):\n        confusion_matrix[a][p] += 1\n    \n    # Extract values from confusion matrix\n    tp, fp = confusion_matrix[1][1], confusion_matrix[0][1]\n    fn, tn = confusion_matrix[1][0], confusion_matrix[0][0]\n    \n    # Calculate metrics\n    accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1_score = round(2 * (precision * recall) / (precision + recall), 3) if (precision + recall) > 0 else 0\n    specificity = round(tn / (tn + fp), 3) if (tn + fp) > 0 else 0\n    negative_predictive_value = round(tn / (tn + fn), 3) if (tn + fn) > 0 else 0\n    \n    return confusion_matrix, accuracy, f1_score, specificity, negative_predictive_value\n\n# Example usage\nactual = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\npredicted = [1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\nprint(performance_metrics(actual, predicted))\n```\n\nThis script defines a function `performance_metrics` that calculates the confusion matrix, accuracy, F1 score, specificity, and negative predictive value for a binary classification problem. It validates the input lists, calculates the confusion matrix, and then computes the required metrics based on the values from the confusion matrix. The metrics are rounded to three decimal places as required."}
{"task_id": 78, "completion_id": 0, "completion": "To solve the given task, we will follow these steps:\n\n1. Import the necessary libraries: `numpy` for numerical operations and `scipy.stats` for calculating the mode.\n2. Define the function `descriptive_statistics` that takes a list or NumPy array of numerical values.\n3. Calculate the mean, median, mode, variance, and standard deviation using NumPy functions.\n4. Calculate the 25th, 50th, and 75th percentiles using NumPy's `percentile` function.\n5. Calculate the interquartile range (IQR) as the difference between the 75th and 25th percentiles.\n6. Return a dictionary containing all the calculated statistics, rounding all values to four decimal places except for the mode.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\nfrom scipy import stats\n\ndef descriptive_statistics(data):\n    # Convert data to a NumPy array if it's not already\n    data = np.array(data)\n    \n    # Calculate statistics\n    mean = np.mean(data)\n    median = np.median(data)\n    mode = stats.mode(data)[0][0]  # scipy.stats.mode returns a ModeResult object\n    variance = np.var(data, ddof=1)  # ddof=1 for sample variance\n    standard_deviation = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n    percentile_25 = np.percentile(data, 25)\n    percentile_50 = np.percentile(data, 50)\n    percentile_75 = np.percentile(data, 75)\n    interquartile_range = percentile_75 - percentile_25\n    \n    # Prepare the output dictionary\n    output = {\n        'mean': round(mean, 4),\n        'median': round(median, 4),\n        'mode': mode,\n        'variance': round(variance, 4),\n        'standard_deviation': round(standard_deviation, 4),\n        '25th_percentile': round(percentile_25, 4),\n        '50th_percentile': round(percentile_50, 4),\n        '75th_percentile': round(percentile_75, 4),\n        'interquartile_range': round(interquartile_range, 4)\n    }\n    \n    return output\n\n# Example usage\ndata = [1, 2, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(descriptive_statistics(data))\n```\n\nThis script defines the `descriptive_statistics` function that calculates the required statistics and returns them in a dictionary. The `stats.mode` function from `scipy.stats` is used to calculate the mode, as NumPy does not have a built-in mode function. The `ddof=1` parameter in `np.var` and `np.std` is used to calculate the sample variance and standard deviation, respectively, which is the common practice when dealing with a sample of a population."}
{"task_id": 79, "completion_id": 0, "completion": "To solve the given task, we need to implement the binomial probability formula. The formula for the binomial distribution is:\n\n\\[ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} \\]\n\nWhere:\n- \\( \\binom{n}{k} \\) is the binomial coefficient, which can be calculated as \\( \\frac{n!}{k!(n-k)!} \\)\n- \\( p \\) is the probability of success on a single trial\n- \\( n \\) is the total number of trials\n- \\( k \\) is the number of successful trials\n\nWe will use the `math` module for factorial calculations and to round the final result to five decimal places.\n\nHere is the Python script that implements the solution:\n\n```python\nimport math\n\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    # Calculate the binomial coefficient\n    binom_coeff = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))\n    # Calculate the probability\n    probability = binom_coeff * (p ** k) * ((1 - p) ** (n - k))\n    # Return the probability rounded to five decimal places\n    return round(probability, 5)\n\n# Example usage\nn = 10  # Total number of trials\nk = 3   # Number of successes\np = 0.5 # Probability of success on each trial\nprint(binomial_probability(n, k, p))\n```\n\nThis script defines the `binomial_probability` function that calculates the probability of achieving exactly `k` successes in `n` independent Bernoulli trials, each with a probability `p` of success. The result is rounded to five decimal places as required."}
{"task_id": 80, "completion_id": 0, "completion": "To solve the given task, we need to implement the `normal_pdf` function using the formula for the probability density function of the normal distribution. The formula is:\n\n\\[ f(x | \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2} \\]\n\nWhere:\n- \\( x \\) is the value at which the PDF is evaluated.\n- \\( \\mu \\) is the mean of the distribution.\n- \\( \\sigma \\) is the standard deviation of the distribution.\n\nHere's how we can implement this in Python:\n\n1. Import the `math` module to use mathematical functions like `exp` and `sqrt`.\n2. Define the function `normal_pdf` with parameters `x`, `mean`, and `std_dev`.\n3. Calculate the exponent part of the formula.\n4. Calculate the denominator part of the formula.\n5. Compute the PDF value using the formula.\n6. Round the result to 5 decimal places and return it.\n\nHere is the complete Python script:\n\n```python\nimport math\n\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -((x - mean) ** 2) / (2 * std_dev ** 2)\n    denominator = std_dev * math.sqrt(2 * math.pi)\n    pdf_value = (math.exp(exponent) / denominator)\n    return round(pdf_value, 5)\n\n# Example usage:\n# print(normal_pdf(0, 0, 1))  # Should return the PDF value for a standard normal distribution at x=0\n```\n\nThis script defines the `normal_pdf` function that calculates the PDF of a normal distribution for given parameters and rounds the result to 5 decimal places."}
{"task_id": 81, "completion_id": 0, "completion": "To solve the task, we need to implement the Poisson distribution formula in the `poisson_probability` function. The Poisson distribution formula is given by:\n\n\\[ P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\]\n\nWhere:\n- \\( k \\) is the number of events we want to calculate the probability for.\n- \\( \\lambda \\) (lam) is the average rate of occurrences.\n- \\( e \\) is the base of the natural logarithm, approximately equal to 2.71828.\n\nHere are the steps to implement the function:\n1. Calculate \\( \\lambda^k \\).\n2. Calculate \\( e^{-\\lambda} \\).\n3. Calculate \\( k! \\) (k factorial).\n4. Compute the probability using the Poisson distribution formula.\n5. Round the result to 5 decimal places.\n\nLet's write the Python code for this:\n\n```python\nimport math\n\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    # Calculate the components of the Poisson distribution formula\n    lambda_k = lam ** k\n    e_neg_lambda = math.exp(-lam)\n    k_factorial = math.factorial(k)\n    \n    # Calculate the probability\n    probability = (lambda_k * e_neg_lambda) / k_factorial\n    \n    # Return the probability rounded to 5 decimal places\n    return round(probability, 5)\n\n# Example usage\nk = 3\nlam = 2.5\nprint(f\"The probability of observing exactly {k} events with a mean rate of {lam} is {poisson_probability(k, lam)}\")\n```\n\nThis script defines the `poisson_probability` function, which calculates the probability of observing exactly `k` events given the mean rate of occurrences `lam`. The result is rounded to 5 decimal places as required."}
{"task_id": 82, "completion_id": 0, "completion": "To calculate the contrast of a grayscale image, we need to find the difference between the maximum and minimum pixel values in the image. The contrast is a measure of the difference in intensity between the lightest and darkest parts of the image. Here's how we can achieve this:\n\n1. Import the necessary package (`numpy`).\n2. Define the function `calculate_contrast` that takes a 2D numpy array as input.\n3. Use numpy functions to find the maximum and minimum pixel values in the array.\n4. Calculate the contrast by subtracting the minimum value from the maximum value.\n5. Return the calculated contrast.\n\nHere's the complete Python script:\n\n```python\nimport numpy as np\n\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    Returns:\n        int: The contrast of the image, defined as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    # Find the maximum and minimum pixel values in the image\n    max_pixel_value = np.max(img)\n    min_pixel_value = np.min(img)\n    \n    # Calculate the contrast\n    contrast = max_pixel_value - min_pixel_value\n    \n    return contrast\n\n# Example usage:\n# Create a sample grayscale image as a 2D numpy array\nsample_image = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 255]])\n\n# Calculate the contrast of the sample image\nimage_contrast = calculate_contrast(sample_image)\nprint(\"Contrast of the image:\", image_contrast)\n```\n\nThis script defines the function `calculate_contrast` which calculates the contrast of a given grayscale image represented as a 2D numpy array. The example usage demonstrates how to use this function with a sample image."}
{"task_id": 83, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that calculates the dot product of two vectors using NumPy. The dot product of two vectors is calculated by multiplying corresponding entries and then summing those products. NumPy provides a convenient function `np.dot()` for this purpose.\n\nHere are the steps to implement the function:\n1. Import the NumPy library.\n2. Define the function `calculate_dot_product` that takes two 1D NumPy arrays as arguments.\n3. Use `np.dot()` to calculate the dot product of the two vectors.\n4. Return the result.\n\nBelow is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two vectors.\n    \"\"\"\n    # Calculate the dot product using np.dot\n    dot_product = np.dot(vec1, vec2)\n    return dot_product\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create two example vectors\n    vector1 = np.array([1, 2, 3])\n    vector2 = np.array([4, 5, 6])\n    \n    # Calculate the dot product\n    result = calculate_dot_product(vector1, vector2)\n    \n    # Print the result\n    print(\"The dot product is:\", result)\n```\n\nThis script includes an example usage section that demonstrates how to use the `calculate_dot_product` function with two example vectors. The expected output for the example vectors `[1, 2, 3]` and `[4, 5, 6]` is `32`, which is the sum of `1*4 + 2*5 + 3*6`."}
{"task_id": 84, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n    \"\"\"\n    if degree < 0:\n        return []\n    \n    # Convert the list to a numpy array for easier manipulation\n    data_array = np.array(data)\n    \n    # Initialize an empty list to store the transformed features\n    transformed_features = []\n    \n    # Generate polynomial features for each data point\n    for value in data_array:\n        # Start with a list containing 1 for the constant term\n        feature_vector = [1]\n        for d in range(1, degree + 1):\n            feature_vector.append(value ** d)\n        transformed_features.append(feature_vector)\n    \n    # Round the output to 8 decimal places\n    transformed_features = np.round(transformed_features, 8).tolist()\n    \n    return transformed_features\n\n# Example usage:\ndata = [1.0, 2.0, 3.0]\ndegree = 3\nprint(phi_transform(data, degree))\n```\n\nThis code snippet defines a function `phi_transform` that takes a list of numerical data and a degree as inputs. It then generates polynomial features for each data point up to the specified degree and returns the transformed features as a list of lists, with each inner list representing the transformed features of a data point. The output is rounded to 8 decimal places to ensure precision and readability."}
{"task_id": 85, "completion_id": 0, "completion": "To implement the Positional Encoding layer for Transformers, we need to follow these steps:\n\n1. Check if `position` is 0 or if `d_model` is less than or equal to 0. If either condition is true, return -1.\n2. Create a position encoding array of shape `(position, d_model)` filled with zeros.\n3. Use sine and cosine functions to calculate the positional encodings. The formula for the positional encoding at position `pos` and dimension `i` is:\n   - If `i` is even: `pos / (10000 ** (2 * i / d_model))` and apply sine function.\n   - If `i` is odd: `pos / (10000 ** (2 * (i - 1) / d_model))` and apply cosine function.\n4. Convert the array to `np.float16` and return it as a list.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    \n    # Initialize the position encoding array\n    pe = np.zeros((position, d_model), dtype=np.float16)\n    \n    # Calculate the positional encodings\n    for pos in range(position):\n        for i in range(d_model):\n            if i % 2 == 0:\n                pe[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))\n            else:\n                pe[pos, i] = np.cos(pos / (10000 ** (2 * (i - 1) / d_model)))\n    \n    # Convert the array to a list and return\n    return pe.tolist()\n\n# Example usage\nposition = 5\nd_model = 4\nprint(pos_encoding(position, d_model))\n```\n\nThis script defines the `pos_encoding` function that calculates the positional encodings for a given sequence length and model dimensionality, adhering to the constraints and requirements specified in the task."}
{"task_id": 86, "completion_id": 0, "completion": "To solve the given task, we will follow these steps:\n\n1. Define the function `model_fit_quality` with two parameters: `training_accuracy` and `test_accuracy`.\n2. Check if the model is overfitting by comparing the difference between `training_accuracy` and `test_accuracy` to see if it's greater than 0.2.\n3. Check if the model is underfitting by verifying if both `training_accuracy` and `test_accuracy` are below 0.7.\n4. If neither of the above conditions is met, the model is considered to have a good fit.\n5. Return 1 for overfitting, -1 for underfitting, and 0 for a good fit.\n\nHere is the Python script implementing the above logic:\n\n```python\ndef model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    # Check for overfitting\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    # Check for underfitting\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    # If neither overfitting nor underfitting, it's a good fit\n    else:\n        return 0\n\n# Example usage\nprint(model_fit_quality(0.9, 0.6))  # Overfitting, should return 1\nprint(model_fit_quality(0.6, 0.5))  # Underfitting, should return -1\nprint(model_fit_quality(0.8, 0.75)) # Good fit, should return 0\n```\n\nThis script defines the function as required and includes example usages to demonstrate how the function works under different conditions."}
{"task_id": 87, "completion_id": 0, "completion": "```python\nimport numpy as np\n\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    # Update biased first moment estimate\n    m = beta1 * m + (1 - beta1) * grad\n    # Update biased second moment estimate\n    v = beta2 * v + (1 - beta2) * (grad ** 2)\n    # Apply bias correction\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    # Update parameter\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    \n    # Ensure the output is in the same format as the input (list or numpy array)\n    if isinstance(parameter, np.ndarray):\n        return parameter.round(5).tolist(), m.round(5).tolist(), v.round(5).tolist()\n    else:\n        return round(parameter, 5), round(m, 5), round(v, 5)\n\n# Example usage:\nparameter = np.array([0.1, 0.2, 0.3])\ngrad = np.array([0.01, 0.02, 0.03])\nm = np.array([0.0, 0.0, 0.0])\nv = np.array([0.0, 0.0, 0.0])\nt = 1\n\nupdated_parameter, updated_m, updated_v = adam_optimizer(parameter, grad, m, v, t)\nprint(updated_parameter, updated_m, updated_v)\n```\n\nThis script defines the `adam_optimizer` function that implements the Adam optimization algorithm update step. It takes the current parameter value, gradient, and moving averages as inputs, and returns the updated parameter value and new moving averages, with bias correction applied. The function handles both scalar and array inputs and ensures the output is rounded to 5 decimal places and converted to a list if the input was a numpy array."}
{"task_id": 88, "completion_id": 0, "completion": "To implement a simplified GPT-2-like text generation function, we will follow these steps:\n\n1. **Load the necessary components**: We will use the `load_encoder_hparams_and_params` function to get the encoder, hyperparameters, and parameters.\n2. **Tokenize the prompt**: Convert the input prompt into token IDs using the encoder.\n3. **Embedding**: Combine token embeddings and positional embeddings.\n4. **Multi-head Attention**: Implement a simplified version of multi-head attention.\n5. **Feed-Forward Network**: Process the attention output.\n6. **Layer Normalization**: Apply layer normalization.\n7. **Generate Text**: Use the model to generate new tokens and decode them back into text.\n\nHere is the complete Python script:\n\n```python\nimport numpy as np\n\ndef load_encoder_hparams_and_params(model_size: str = \"124M\", models_dir: str = \"models\"):\n    class DummyBPE:\n        def __init__(self):\n            self.encoder_dict = {\"hello\": 1, \"world\": 2, \"<UNK>\": 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict[\"<UNK>\"]) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for k, v in self.encoder_dict.items()}\n            return \" \".join([reversed_dict.get(tok_id, \"<UNK>\") for tok_id in token_ids])\n\n    hparams = {\n        \"n_ctx\": 1024,\n        \"n_head\": 12\n    }\n\n    params = {\n        \"wte\": np.random.rand(3, 10),\n        \"wpe\": np.random.rand(1024, 10),\n        \"blocks\": [],\n        \"ln_f\": {\n            \"g\": np.ones(10),\n            \"b\": np.zeros(10),\n        }\n    }\n\n    encoder = DummyBPE()\n    return encoder, hparams, params\n\ndef gen_text(prompt: str, n_tokens_to_generate: int = 40):\n    encoder, hparams, params = load_encoder_hparams_and_params()\n    token_ids = encoder.encode(prompt)\n    token_embeddings = params[\"wte\"][token_ids]\n    pos_embeddings = params[\"wpe\"][:len(token_ids)]\n    embeddings = token_embeddings + pos_embeddings\n\n    # Simplified multi-head attention\n    def multi_head_attention(x):\n        return x  # Placeholder for actual attention mechanism\n\n    # Simplified feed-forward network\n    def feed_forward(x):\n        return x  # Placeholder for actual feed-forward network\n\n    # Layer normalization\n    def layer_norm(x, g, b, eps=1e-5):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        std = np.std(x, axis=-1, keepdims=True)\n        return g * (x - mean) / (std + eps) + b\n\n    # Generate tokens\n    for _ in range(n_tokens_to_generate):\n        x = multi_head_attention(embeddings)\n        x = feed_forward(x)\n        x = layer_norm(x, params[\"ln_f\"][\"g\"], params[\"ln_f\"][\"b\"])\n        # For simplicity, we assume the last token is the next generated token\n        next_token_id = np.argmax(x[-1])\n        token_ids.append(next_token_id)\n        embeddings = np.concatenate([embeddings, params[\"wte\"][next_token_id:next_token_id+1] + pos_embeddings[len(token_ids)-1:len(token_ids)]], axis=0)\n\n    return encoder.decode(token_ids)\n\n# Example usage\nprint(gen_text(\"hello world\", 5))\n```\n\nThis script provides a simplified version of the GPT-2 text generation process. The placeholders for the attention mechanism and feed-forward network should be replaced with actual implementations for a complete model."}
{"task_id": 89, "completion_id": 0, "completion": "To solve the task, we need to implement a simplified version of the self-attention mechanism. The self-attention mechanism is a key component in many modern neural network architectures, such as transformers, which allows each element in a sequence to attend to all other elements. In this case, we will use the crystal values to calculate attention scores and derive the final weighted pattern for each crystal.\n\nHere's a step-by-step breakdown of the solution:\n1. Implement the softmax function to normalize the attention scores.\n2. Calculate the attention scores between each pair of crystals.\n3. Apply the softmax function to the attention scores to get the attention weights.\n4. Compute the weighted sum of the crystal values using the attention weights.\n5. Return the final weighted patterns for each crystal, rounded to the 4th decimal place.\n\nLet's implement this in Python:\n\n```python\nimport numpy as np\n\ndef pattern_weaver(n, crystal_values, dimension):\n    def softmax(values):\n        # Implement the softmax function first\n        e_x = np.exp(values - np.max(values))\n        return e_x / e_x.sum(axis=0)\n    \n    # Convert the list of crystal values to a numpy array\n    crystal_values = np.array(crystal_values)\n    \n    # Initialize the attention scores matrix\n    attention_scores = np.zeros((n, n))\n    \n    # Calculate the attention scores between each pair of crystals\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                # Here we use a simple dot product as the attention score\n                attention_scores[i, j] = np.dot(crystal_values[i], crystal_values[j])\n    \n    # Apply softmax to the attention scores to get the attention weights\n    attention_weights = softmax(attention_scores)\n    \n    # Compute the weighted sum of the crystal values using the attention weights\n    weighted_patterns = np.dot(attention_weights, crystal_values)\n    \n    # Return the final weighted patterns for each crystal, rounded to the 4th decimal place\n    return [round(pattern, 4) for pattern in weighted_patterns]\n\n# Example usage:\nn = 3\ncrystal_values = [[1, 2], [3, 4], [5, 6]]\ndimension = 2\nprint(pattern_weaver(n, crystal_values, dimension))\n```\n\nThis script defines the `pattern_weaver` function that takes the number of crystals `n`, a list of `crystal_values`, and the `dimension` of each crystal value. It then calculates the attention scores, applies the softmax function to get the attention weights, and computes the final weighted patterns for each crystal. The output is a list of floats, each rounded to the 4th decimal place."}
{"task_id": 90, "completion_id": 0, "completion": "To implement the BM25 ranking function, we need to follow these steps:\n\n1. Calculate the average document length in the corpus.\n2. Calculate the term frequency (TF) for each term in the query for each document.\n3. Calculate the inverse document frequency (IDF) for each term in the query.\n4. Apply the BM25 formula to calculate the score for each document.\n5. Return the scores rounded to three decimal places.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    # Step 1: Calculate the average document length\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    \n    # Step 2: Calculate term frequency (TF) for each term in the query for each document\n    query_terms = set(query.split())\n    tf = []\n    for doc in corpus:\n        doc_words = doc.split()\n        doc_tf = {term: Counter(doc_words)[term] for term in query_terms}\n        tf.append(doc_tf)\n    \n    # Step 3: Calculate the inverse document frequency (IDF) for each term in the query\n    idf = {}\n    for term in query_terms:\n        df = sum(1 for doc in corpus if term in doc)\n        idf[term] = np.log((len(corpus) - df + 0.5) / (df + 0.5))\n    \n    # Step 4: Apply the BM25 formula to calculate the score for each document\n    scores = []\n    for i, doc in enumerate(corpus):\n        doc_words = doc.split()\n        dl = len(doc_words)\n        score = 0\n        for term in query_terms:\n            if term in doc:\n                tfi = tf[i][term]\n                score += idf[term] * (tfi * (k1 + 1)) / (tfi + k1 * (1 - b + b * dl / avgdl))\n        scores.append(score)\n    \n    # Step 5: Return the scores rounded to three decimal places\n    return [round(score, 3) for score in scores]\n\n# Example usage:\ncorpus = [\n    \"the cat in the hat\",\n    \"a cat and a hat\",\n    \"the quick brown fox jumps over the lazy dog\"\n]\nquery = \"cat hat\"\nscores = calculate_bm25_scores(corpus, query)\nprint(scores)\n```\n\nThis script defines the `calculate_bm25_scores` function that takes a corpus (a list of documents), a query, and optional parameters `k1` and `b` for the BM25 formula. It calculates the BM25 scores for each document in the corpus based on the query and returns a list of scores rounded to three decimal places."}
{"task_id": 91, "completion_id": 0, "completion": "To calculate the F1 score, we need to first calculate the precision and recall. The F1 score is the harmonic mean of precision and recall. Here's how we can break down the task:\n\n1. Calculate the number of true positives (TP), false positives (FP), and false negatives (FN).\n2. Use these values to calculate precision and recall.\n3. Calculate the F1 score using the formula: \\( F1 = 2 \\times \\frac{precision \\times recall}{precision + recall} \\).\n4. Round the F1 score to three decimal places.\n\nHere's the Python code to accomplish this:\n\n```python\ndef calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    # Initialize counters\n    tp = 0  # True Positives\n    fp = 0  # False Positives\n    fn = 0  # False Negatives\n    \n    # Calculate TP, FP, FN\n    for true, pred in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1 score\n    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Return F1 score rounded to 3 decimal places\n    return round(f1_score, 3)\n```\n\nThis function takes two lists, `y_true` and `y_pred`, as input and returns the F1 score rounded to three decimal places. It handles the edge cases where precision or recall might be undefined (i.e., when there are no true positives or no predictions were made)."}
{"task_id": 92, "completion_id": 0, "completion": "To solve the given task, we need to follow the steps outlined in the description. We will start by subtracting the daily fluctuation from the consumption data to get the detrended data. Then, we will fit a linear regression model to this detrended data to predict the base consumption for day 15. After that, we will add back the fluctuation for day 15 and apply a 5% safety margin. Finally, we will round the result to the nearest integer.\n\nHere is the self-contained Python script to achieve this:\n\n```python\nimport math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nPI = 3.14159\n\ndef power_grid_forecast(consumption_data):\n    # Step 1: Subtract the daily fluctuation from each data point\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    fluctuation = 10 * np.sin(2 * PI * days / 10)\n    detrended_data = consumption_data - fluctuation.flatten()\n\n    # Step 2: Perform linear regression on the detrended data\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n\n    # Step 3: Predict day 15's base consumption\n    day_15 = np.array([15]).reshape(-1, 1)\n    base_consumption_day_15 = model.predict(day_15)\n\n    # Step 4: Add the day 15 fluctuation back\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n\n    # Step 5: Round, then add a 5% safety margin (rounded up)\n    final_consumption = math.ceil(predicted_consumption_day_15 * 1.05)\n\n    return int(final_consumption)\n\n# Example usage:\nconsumption_data = np.array([100, 105, 110, 115, 120, 125, 130, 135, 140, 145])\nprint(power_grid_forecast(consumption_data))\n```\n\nThis script uses the `numpy` library for numerical operations and the `LinearRegression` model from `sklearn` to perform the linear regression. The `math` library is used for mathematical functions like `sin` and `ceil`. The `power_grid_forecast` function takes the consumption data as input, processes it according to the steps outlined, and returns the final integer value for day 15's consumption with a 5% safety margin."}
{"task_id": 93, "completion_id": 0, "completion": "To calculate the Mean Absolute Error (MAE) between two arrays of actual and predicted values, we can follow these steps:\n\n1. Calculate the absolute differences between each pair of true and predicted values.\n2. Compute the mean of these absolute differences.\n3. Round the result to 3 decimal places.\n\nHere is the self-contained Python script that implements the `mae` function:\n\n```python\nimport numpy as np\n\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    # Calculate the absolute differences\n    absolute_differences = np.abs(y_true - y_pred)\n    \n    # Compute the mean of the absolute differences\n    mean_absolute_error = np.mean(absolute_differences)\n    \n    # Round the result to 3 decimal places\n    return round(mean_absolute_error, 3)\n\n# Example usage:\nif __name__ == \"__main__\":\n    y_true = np.array([3, -0.5, 2, 7])\n    y_pred = np.array([2.5, 0.0, 2, 8])\n    print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n```\n\nThis script defines the `mae` function and includes an example usage section to demonstrate how to use the function with sample data."}
{"task_id": 94, "completion_id": 0, "completion": "To implement the multi-head attention mechanism, we need to break down the task into three main functions: `compute_qkv`, `self_attention`, and `multi_head_attention`. The `compute_qkv` function will compute the query, key, and value matrices. The `self_attention` function will perform the self-attention mechanism on a single head. Finally, the `multi_head_attention` function will combine the results from multiple heads.\n\nHere's the step-by-step implementation:\n\n1. **Compute Q, K, V**: This function will take the input matrix `X` and the weight matrices `W_q`, `W_k`, and `W_v` to compute the query, key, and value matrices.\n2. **Self-Attention**: This function will take the query, key, and value matrices and compute the attention scores and weighted values for a single head.\n3. **Multi-Head Attention**: This function will split the input into multiple heads, apply the self-attention mechanism to each head, and then concatenate the results.\n\nLet's implement this in Python:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    attention = np.dot(np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True), V)\n    return attention\n\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    batch_size, seq_len, d_model = X.shape\n    d_k = d_model // n_heads\n    \n    # Compute Q, K, V for each head\n    Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n    \n    # Split into heads\n    Q_heads = np.split(Q, n_heads, axis=-1)\n    K_heads = np.split(K, n_heads, axis=-1)\n    V_heads = np.split(V, n_heads, axis=-1)\n    \n    # Apply self-attention to each head\n    heads = [self_attention(Q_head, K_head, V_head) for Q_head, K_head, V_head in zip(Q_heads, K_heads, V_heads)]\n    \n    # Concatenate the heads\n    concatenated = np.concatenate(heads, axis=-1)\n    \n    # Round to the nearest 4th decimal\n    concatenated = np.round(concatenated, 4)\n    \n    # Convert to list\n    return concatenated.tolist()\n\n# Example usage\nX = np.random.rand(2, 4, 8)  # Batch size 2, sequence length 4, model dimension 8\nW_q = np.random.rand(8, 8)  # Query weight matrix\nW_k = np.random.rand(8, 8)  # Key weight matrix\nW_v = np.random.rand(8, 8)  # Value weight matrix\nn_heads = 2  # Number of heads\n\nresult = multi_head_attention(X, W_q, W_k, W_v, n_heads)\nprint(result)\n```\n\nThis script defines the multi-head attention mechanism and demonstrates its usage with random input data. The `multi_head_attention` function returns the result as a list after rounding to the nearest 4th decimal."}
{"task_id": 95, "completion_id": 0, "completion": "To calculate the Phi coefficient between two binary variables, we need to follow these steps:\n\n1. Calculate the contingency table (a 2x2 table) for the two binary variables.\n2. Use the contingency table to calculate the Phi coefficient using the formula:\n   \\[\n   \\phi = \\frac{n_{11}n_{00} - n_{10}n_{01}}{\\sqrt{(n_{1\\cdot}n_{0\\cdot}n_{\\cdot1}n_{\\cdot0})}}\n   \\]\n   where:\n   - \\(n_{11}\\) is the count of both variables being 1,\n   - \\(n_{00}\\) is the count of both variables being 0,\n   - \\(n_{10}\\) is the count of the first variable being 1 and the second being 0,\n   - \\(n_{01}\\) is the count of the first variable being 0 and the second being 1,\n   - \\(n_{1\\cdot}\\) is the total count of the first variable being 1,\n   - \\(n_{0\\cdot}\\) is the total count of the first variable being 0,\n   - \\(n_{\\cdot1}\\) is the total count of the second variable being 1,\n   - \\(n_{\\cdot0}\\) is the total count of the second variable being 0.\n\n3. Return the Phi coefficient rounded to 4 decimal places.\n\nHere is the Python script that implements the above steps:\n\n```python\nfrom typing import List\n\ndef phi_corr(x: List[int], y: List[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (List[int]): A list of binary values (0 or 1).\n    y (List[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    n11 = sum(a == 1 and b == 1 for a, b in zip(x, y))\n    n00 = sum(a == 0 and b == 0 for a, b in zip(x, y))\n    n10 = sum(a == 1 and b == 0 for a, b in zip(x, y))\n    n01 = sum(a == 0 and b == 1 for a, b in zip(x, y))\n    \n    n1_dot = n11 + n10\n    n0_dot = n00 + n01\n    n_dot1 = n11 + n01\n    n_dot0 = n00 + n10\n    \n    numerator = n11 * n00 - n10 * n01\n    denominator = (n1_dot * n0_dot * n_dot1 * n_dot0) ** 0.5\n    \n    if denominator == 0:\n        return 0.0  # Avoid division by zero\n    \n    phi = numerator / denominator\n    return round(phi, 4)\n\n# Example usage:\nx = [1, 0, 1, 0, 1, 1, 0, 0]\ny = [1, 0, 0, 1, 1, 1, 0, 0]\nprint(phi_corr(x, y))  # Output should be the Phi coefficient rounded to 4 decimal places\n```\n\nThis script defines the `phi_corr` function that calculates the Phi coefficient for two binary variables and returns the result rounded to 4 decimal places."}
{"task_id": 96, "completion_id": 0, "completion": "To implement the Hard Sigmoid activation function, we need to follow its mathematical definition. The Hard Sigmoid function is defined as follows:\n\n\\[ \\text{HardSigmoid}(x) = \\max(0, \\min(1, \\frac{x + 1}{2})) \\]\n\nThis function is a piecewise linear approximation of the sigmoid function, which makes it computationally less expensive. It maps any real-valued number to the range [0, 1]. The function behaves as follows:\n- If \\( x \\) is less than -1, the output is 0.\n- If \\( x \\) is between -1 and 1, the output is linearly interpolated between 0 and 1.\n- If \\( x \\) is greater than 1, the output is 1.\n\nLet's implement this in Python:\n\n```python\ndef hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    # Calculate the intermediate value\n    intermediate_value = (x + 1) / 2\n    \n    # Apply the min and max functions to ensure the output is between 0 and 1\n    return max(0.0, min(1.0, intermediate_value))\n\n# Example usage\nif __name__ == \"__main__\":\n    test_values = [-2.0, -1.0, 0.0, 1.0, 2.0]\n    for value in test_values:\n        print(f\"hard_sigmoid({value}) = {hard_sigmoid(value)}\")\n```\n\nThis script defines the `hard_sigmoid` function and includes a simple test to demonstrate its behavior with a few input values."}
{"task_id": 97, "completion_id": 0, "completion": "To implement the ELU activation function, we need to follow the mathematical definition of ELU. The ELU function is defined as:\n\n\\[ \\text{ELU}(x) = \\begin{cases} \nx & \\text{if } x > 0 \\\\\n\\alpha \\cdot (\\exp(x) - 1) & \\text{if } x \\leq 0 \n\\end{cases} \\]\n\nWhere:\n- \\( x \\) is the input value.\n- \\( \\alpha \\) is a positive scalar that controls the value to which an ELU neuron's output is clamped for negative net inputs.\n\nGiven this definition, we can implement the function as follows:\n\n1. Check if the input \\( x \\) is greater than 0.\n2. If \\( x > 0 \\), return \\( x \\).\n3. If \\( x \\leq 0 \\), compute \\( \\alpha \\cdot (\\exp(x) - 1) \\) and return the result.\n4. Round the result to the 4th decimal place before returning.\n\nHere is the Python code implementing the ELU activation function:\n\n```python\nimport math\n\ndef elu(x: float, alpha: float = 1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        return round(x, 4)\n    else:\n        return round(alpha * (math.exp(x) - 1), 4)\n```\n\nThis function checks the condition for \\( x \\) and applies the appropriate formula. The result is rounded to the 4th decimal place to meet the output constraints."}
{"task_id": 98, "completion_id": 0, "completion": "To implement the PReLU activation function, we need to consider the following:\n\n1. If the input `x` is positive, the output is the same as the input `x`.\n2. If the input `x` is negative, the output is `alpha * x`, where `alpha` is a learnable parameter that we set to 0.25 by default.\n\nHere is the Python code that implements the PReLU activation function:\n\n```python\ndef prelu(x: float, alpha: float = 0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Test the function with a positive and a negative input\n    print(prelu(1.0))  # Expected output: 1.0\n    print(prelu(-1.0))  # Expected output: -0.25\n```\n\nThis code defines the `prelu` function that takes an input `x` and an optional parameter `alpha`. It then checks if `x` is positive or negative and returns the appropriate value based on the PReLU formula. The example usage at the bottom demonstrates how to call the function with both positive and negative inputs."}
{"task_id": 99, "completion_id": 0, "completion": "To implement the Softplus activation function, we need to compute the value of the function `log(1 + e^x)`. However, we need to handle edge cases to prevent numerical overflow or underflow. Specifically, when `x` is very large, `e^x` can cause overflow, and when `x` is very small, `e^x` can underflow to zero, leading to a loss of precision.\n\nTo handle these cases, we can use the following approach:\n1. For large `x`, we can use the approximation `log(1 + e^x) \u2248 x` because `e^x` will dominate the `1` in the expression.\n2. For small `x`, we can use the approximation `log(1 + e^x) \u2248 e^x` because `e^x` will be very small compared to `1`.\n\nHere is the Python code implementing the Softplus function with these considerations:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 18:  # Prevent overflow by using the approximation for large x\n        return x\n    elif x < -37:  # Prevent underflow by using the approximation for small x\n        return math.exp(x)\n    else:\n        return math.log(1 + math.exp(x))\n    \n    # Return the result rounded to the 4th decimal place\n    return round(result, 4)\n```\n\nNote: The thresholds `18` and `-37` are chosen based on the limits of floating-point precision in Python. For `x > 18`, `e^x` will be so large that adding `1` to it will not change the value due to precision limits. For `x < -37`, `e^x` will be so small that it will underflow to `0`, and thus `log(1 + e^x)` will be approximately `e^x`.\n\nHowever, the final return statement should be adjusted to ensure the result is rounded to the 4th decimal place. Here is the corrected version:\n\n```python\nimport math\n\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x)\n    \"\"\"\n    if x > 18:  # Prevent overflow by using the approximation for large x\n        result = x\n    elif x < -37:  # Prevent underflow by using the approximation for small x\n        result = math.exp(x)\n    else:\n        result = math.log(1 + math.exp(x))\n    \n    # Return the result rounded to the 4th decimal place\n    return round(result, 4)\n```"}
{"task_id": 100, "completion_id": 0, "completion": "To implement the Softsign activation function, we need to follow the mathematical formula for the Softsign function, which is defined as:\n\n\\[ \\text{Softsign}(x) = \\frac{x}{1 + |x|} \\]\n\nThis function is smooth and its output is bounded between -1 and 1, making it suitable for use in neural networks. The function takes a single floating-point number as input and returns a floating-point number as output.\n\nHere's how we can implement this in Python:\n\n1. Define the function `softsign` that takes a single argument `x`.\n2. Compute the absolute value of `x`.\n3. Use the formula to compute the Softsign value.\n4. Return the result, rounded to the 4th decimal place.\n\nLet's write the code:\n\n```python\ndef softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input\n    \"\"\"\n    # Compute the absolute value of x\n    abs_x = abs(x)\n    # Compute the Softsign value\n    result = x / (1 + abs_x)\n    # Return the result rounded to the 4th decimal place\n    return round(result, 4)\n```\n\nThis function will correctly compute the Softsign activation for any given floating-point input and return the result rounded to the 4th decimal place, as required."}
{"task_id": 101, "completion_id": 0, "completion": "To implement the GRPO (Group Relative Policy Optimization) objective function, we need to follow these steps:\n\n1. Calculate the clipped likelihood ratios.\n2. Compute the GRPO objective using the clipped likelihood ratios, advantage estimates, and the KL divergence penalty.\n3. Return the computed GRPO objective value, rounded to the nearest 6th decimal.\n\nHere is the Python code that accomplishes this:\n\n```python\nimport numpy as np\n\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    # Convert lists to numpy arrays for vectorized operations\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    \n    # Clip the likelihood ratios\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    \n    # Compute the GRPO objective\n    grpo_obj = np.mean(np.minimum(rhos * A, clipped_rhos * A))\n    \n    # Compute the KL divergence between the old and reference policies\n    kl_divergence = np.mean(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    \n    # Apply the KL divergence penalty\n    grpo_obj -= beta * kl_divergence\n    \n    # Return the GRPO objective value rounded to the nearest 6th decimal\n    return round(grpo_obj, 6)\n\n# Example usage:\nrhos = [1.2, 0.8, 1.5, 0.5]\nA = [0.3, -0.2, 0.4, -0.1]\npi_theta_old = [0.7, 0.3, 0.6, 0.4]\npi_theta_ref = [0.6, 0.4, 0.5, 0.5]\nprint(grpo_objective(rhos, A, pi_theta_old, pi_theta_ref))\n```\n\nThis code defines the `grpo_objective` function that computes the GRPO objective value given the likelihood ratios, advantage estimates, old policy probabilities, and reference policy probabilities. It applies the clipping mechanism and the KL divergence penalty to ensure training stability. The result is rounded to the nearest 6th decimal as required."}
{"task_id": 102, "completion_id": 0, "completion": "To implement the Swish activation function, we need to follow the mathematical formula for Swish, which is defined as:\n\n\\[ \\text{Swish}(x) = \\frac{x}{1 + e^{-x}} \\]\n\nThis function is a smooth approximation of the ReLU function and has been shown to improve the performance of deep neural networks. The function is self-gated, meaning it uses the input value to control the activation level.\n\nHere's how we can implement this in Python:\n\n1. Import the necessary modules.\n2. Define the `swish` function that takes a single float as input.\n3. Inside the function, compute the Swish value using the formula.\n4. Round the result to the nearest 4th decimal place.\n5. Return the computed value.\n\nLet's write the code:\n\n```python\nimport math\n\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value\n    \"\"\"\n    # Compute the Swish value\n    swish_value = x / (1 + math.exp(-x))\n    # Round the result to the nearest 4th decimal place\n    return round(swish_value, 4)\n```\n\nThis code snippet defines the `swish` function that computes the Swish activation value for a given input `x` and returns the result rounded to the nearest 4th decimal place."}
{"task_id": 103, "completion_id": 0, "completion": "To implement the SELU activation function, we need to follow the mathematical definition of SELU. The SELU function is defined as:\n\n\\[ \\text{SELU}(x) = \\lambda \\cdot x \\quad \\text{if} \\quad x > 0 \\]\n\\[ \\text{SELU}(x) = \\lambda \\cdot \\alpha \\cdot (\\exp(x) - 1) \\quad \\text{if} \\quad x \\leq 0 \\]\n\nwhere:\n- \\(\\lambda\\) is a scaling parameter, which is approximately 1.0507009873554804.\n- \\(\\alpha\\) is a scale factor for the negative part, which is approximately 1.6732632423543772.\n\nGiven the task, we will implement this function in Python, ensuring that the output is rounded to the nearest 4th decimal place. Here is the complete implementation:\n\n```python\nimport math\nimport numpy as np\n\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554804\n    \n    if x > 0:\n        return round(scale * x, 4)\n    else:\n        return round(scale * alpha * (math.exp(x) - 1), 4)\n\n# Example usage\nprint(selu(-1.0))  # Example input\n```\n\nThis script defines the `selu` function, which takes a single float `x` as input and returns the SELU activation value of `x`, rounded to the nearest 4th decimal place. The function checks if `x` is greater than 0 and applies the appropriate formula based on the condition."}
{"task_id": 104, "completion_id": 0, "completion": "To implement the prediction function for binary classification using Logistic Regression, we need to follow these steps:\n\n1. Compute the linear combination of the input features and the model weights, adding the bias term.\n2. Apply the sigmoid function to the linear combination to get the probability estimates.\n3. Convert the probability estimates to binary predictions by comparing them to a threshold (0.5 in this case).\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N x D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    # Step 1: Compute the linear combination\n    linear_combination = np.dot(X, weights) + bias\n    \n    # Step 2: Apply the sigmoid function\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    \n    # Step 3: Convert probabilities to binary predictions\n    predictions = (probabilities > 0.5).astype(int)\n    \n    # Return the predictions as a list\n    return predictions.tolist()\n\n# Example usage:\n# Define a sample input matrix X, weights, and bias\nX_sample = np.array([[1, 2], [2, 3], [3, 4]])\nweights_sample = np.array([0.5, -0.5])\nbias_sample = 0.1\n\n# Call the function and print the result\npredictions = predict_logistic(X_sample, weights_sample, bias_sample)\nprint(predictions)\n```\n\nThis script defines the `predict_logistic` function that takes an input feature matrix `X`, a weight vector `weights`, and a scalar `bias`. It computes the linear combination of `X` and `weights`, adds the `bias`, applies the sigmoid function to get the probability estimates, and then converts these probabilities to binary predictions. The predictions are returned as a list."}
{"task_id": 105, "completion_id": 0, "completion": "To implement the gradient descent-based training algorithm for Softmax regression, we need to follow these steps:\n\n1. Initialize the parameters (weights) randomly.\n2. Define the Softmax function to convert raw scores into probabilities.\n3. Define the Cross Entropy loss function to measure the error between predicted probabilities and actual labels.\n4. Compute the gradients of the loss with respect to the parameters.\n5. Update the parameters using the gradients and the learning rate.\n6. Repeat steps 2-5 for a specified number of iterations.\n7. Collect the loss values at each iteration to monitor the training process.\n\nHere is the Python code that implements the above steps:\n\n```python\nimport numpy as np\n\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in z.\"\"\"\n    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n    return e_z / e_z.sum(axis=1, keepdims=True)\n\ndef cross_entropy_loss(y_true, y_pred):\n    \"\"\"Compute the cross entropy loss.\"\"\"\n    return -np.sum(y_true * np.log(y_pred + 1e-10)) / y_true.shape[0]\n\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    n_samples, n_features = X.shape\n    n_classes = len(np.unique(y))\n    y = np.eye(n_classes)[y]  # One-hot encoding of y\n    B = np.random.randn(n_features, n_classes)  # Initialize parameters\n    losses = []\n\n    for _ in range(iterations):\n        # Forward pass\n        scores = X @ B\n        probabilities = softmax(scores)\n        \n        # Compute loss\n        loss = cross_entropy_loss(y, probabilities)\n        losses.append(round(loss, 4))\n        \n        # Compute gradients\n        gradients = X.T @ (probabilities - y) / n_samples\n        \n        # Update parameters\n        B -= learning_rate * gradients\n    \n    B = np.round(B, 4).tolist()\n    losses = [round(loss, 4) for loss in losses]\n    return B, losses\n```\n\nThis script defines the `train_softmaxreg` function that takes the input features `X`, the target labels `y`, a learning rate, and the number of iterations as input. It initializes the parameters randomly, computes the Softmax probabilities, calculates the Cross Entropy loss, computes the gradients, and updates the parameters using gradient descent. The function returns the optimized parameters and the collected loss values over iterations, both rounded to the nearest 4th decimal."}
{"task_id": 106, "completion_id": 0, "completion": "To implement a gradient descent-based training algorithm for logistic regression, we need to follow these steps:\n\n1. Initialize the weights (coefficients) of the logistic regression model.\n2. Define the sigmoid function, which is used to map predicted values to probabilities.\n3. Define the Binary Cross Entropy loss function.\n4. Implement the gradient descent algorithm to update the weights iteratively.\n5. Collect the loss values over iterations and return the optimized coefficients and the loss values.\n\nHere is the self-contained Python script that accomplishes the task:\n\n```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"\n    Sigmoid function to map predicted values to probabilities.\n    \"\"\"\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy(y_true, y_pred):\n    \"\"\"\n    Binary Cross Entropy loss function.\n    \"\"\"\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    # Add a column of ones to X for the bias term\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    # Initialize weights\n    weights = np.zeros(X.shape[1])\n    # Initialize list to store loss values\n    loss_values = []\n    \n    for i in range(iterations):\n        # Compute predictions\n        z = np.dot(X, weights)\n        y_pred = sigmoid(z)\n        \n        # Compute gradient\n        gradient = np.dot(X.T, (y_pred - y)) / y.size\n        \n        # Update weights\n        weights -= learning_rate * gradient\n        \n        # Compute and store loss\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n    \n    # Return optimized weights and loss values\n    return weights.tolist(), loss_values\n\n# Example usage\nX = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\ny = np.array([0, 0, 1, 1])\nlearning_rate = 0.1\niterations = 1000\nweights, losses = train_logreg(X, y, learning_rate, iterations)\nprint(\"Optimized Weights:\", weights)\nprint(\"Losses:\", losses)\n```\n\nThis script defines the necessary functions and implements the gradient descent algorithm to optimize the logistic regression model's parameters. The `train_logreg` function takes the input features `X`, the target values `y`, the learning rate, and the number of iterations as input and returns the optimized weights and the loss values over iterations."}
{"task_id": 107, "completion_id": 0, "completion": "To implement masked self-attention, we need to follow these steps:\n\n1. Compute the Query (Q), Key (K), and Value (V) matrices using the provided `compute_qkv` function.\n2. Calculate the attention scores using the dot product of Q and K, scaled by the square root of the key dimension.\n3. Apply the mask to the attention scores to prevent attention to certain positions.\n4. Apply a softmax function to the masked attention scores to get the attention weights.\n5. Compute the weighted sum of the value vectors using the attention weights.\n6. Return the result as a Python list using `tolist()`.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n    \"\"\"\n    Compute Query (Q), Key (K), and Value (V) matrices.\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    return Q, K, V\n\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    # Calculate the dot product of Q and K, and scale by the square root of the key dimension\n    d_k = K.shape[-1]\n    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n    \n    # Apply the mask to the scores\n    scores = np.where(mask, scores, -np.inf)\n    \n    # Apply softmax to get the attention weights\n    weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    \n    # Compute the weighted sum of the value vectors\n    output = np.dot(weights, V)\n    \n    # Return the result as a Python list\n    return output.tolist()\n\n# Example usage\nX = np.array([[1, 2, 3], [4, 5, 6]])\nW_q = np.array([[1, 0], [0, 1], [1, 1]])\nW_k = np.array([[1, 0], [0, 1], [1, 1]])\nW_v = np.array([[1, 0], [0, 1], [1, 1]])\nmask = np.array([[True, False], [True, True]])\n\nQ, K, V = compute_qkv(X, W_q, W_k, W_v)\noutput = masked_attention(Q, K, V, mask)\nprint(output)\n```\n\nThis script defines the necessary functions and provides an example usage. The `masked_attention` function applies the mask to the attention scores, ensuring that positions where the mask is `False` do not contribute to the attention weights."}
{"task_id": 108, "completion_id": 0, "completion": "To solve this task, we can use the concept of entropy from information theory to measure the disorder. Entropy is a measure of uncertainty or randomness in a set of data. In this case, the entropy of the colors of the apples will be higher if the colors are more evenly distributed, and lower if the colors are more concentrated in a few types.\n\nHere's a step-by-step approach to implement the function:\n\n1. Count the frequency of each color in the list.\n2. Calculate the probability of each color.\n3. Use the formula for entropy: \\( H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2(p(x_i)) \\), where \\( p(x_i) \\) is the probability of color \\( x_i \\).\n4. Return the entropy value, rounded to the nearest 4th decimal.\n\nLet's implement this in Python:\n\n```python\nimport math\nfrom collections import Counter\n\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    # Count the frequency of each color\n    color_counts = Counter(apples)\n    \n    # Calculate the total number of apples\n    total_apples = len(apples)\n    \n    # Calculate the entropy\n    entropy = 0.0\n    for count in color_counts.values():\n        probability = count / total_apples\n        entropy -= probability * math.log2(probability)\n    \n    # Return the entropy rounded to the nearest 4th decimal\n    return round(entropy, 4)\n\n# Test cases\nprint(disorder([0,0,0,0]))  # Expected output: 0.0\nprint(disorder([1,1,0,0]))  # Expected output: 1.0\nprint(disorder([0,1,2,3]))  # Expected output: 2.0\nprint(disorder([0,0,1,1,2,2,3,3]))  # Expected output: 2.0\nprint(disorder([0,0,0,0,0,1,2,3]))  # Expected output: 1.4591\n```\n\nThis script defines the `disorder` function that calculates the entropy of the apple colors and returns it rounded to the nearest 4th decimal. The test cases demonstrate the function's behavior with different inputs, satisfying the conditions specified in the task."}
{"task_id": 109, "completion_id": 0, "completion": "To implement the Layer Normalization for sequence data, we need to follow these steps:\n\n1. Calculate the mean and variance of the input tensor `X` across the feature dimension for each sequence.\n2. Normalize the input tensor using the calculated mean and variance.\n3. Apply scaling and shifting using the `gamma` and `beta` parameters.\n4. Round the result to 5 decimal places and convert it to a list.\n\nHere is the self-contained Python script that performs the task:\n\n```python\nimport numpy as np\n\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor X.\n    \n    Parameters:\n    X (np.ndarray): Input tensor of shape (batch_size, seq_length, feature_dim).\n    gamma (np.ndarray): Scaling parameter of shape (feature_dim,).\n    beta (np.ndarray): Shifting parameter of shape (feature_dim,).\n    epsilon (float): Small constant to avoid division by zero.\n    \n    Returns:\n    list: Normalized tensor rounded to 5 decimal places.\n    \"\"\"\n    # Calculate the mean and variance across the feature dimension\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    \n    # Normalize the input tensor\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scaling and shifting\n    X_scaled = X_norm * gamma + beta\n    \n    # Round to 5 decimal places and convert to list\n    return np.round(X_scaled, decimals=5).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define the input tensor X, scaling parameter gamma, and shifting parameter beta\n    X = np.random.rand(2, 3, 4)  # Example input tensor with shape (batch_size, seq_length, feature_dim)\n    gamma = np.random.rand(4)    # Scaling parameter with shape (feature_dim,)\n    beta = np.random.rand(4)     # Shifting parameter with shape (feature_dim,)\n    \n    # Perform layer normalization\n    normalized_X = layer_normalization(X, gamma, beta)\n    \n    # Print the normalized tensor\n    print(normalized_X)\n```\n\nThis script defines a function `layer_normalization` that takes an input tensor `X`, scaling parameter `gamma`, shifting parameter `beta`, and a small constant `epsilon` to avoid division by zero. The function calculates the mean and variance across the feature dimension, normalizes the input tensor, applies scaling and shifting, and finally rounds the result to 5 decimal places and converts it to a list. The example usage demonstrates how to use the function with random data."}
{"task_id": 110, "completion_id": 0, "completion": "To calculate the METEOR score, we need to follow these steps:\n\n1. **Tokenize** the reference and candidate sentences.\n2. **Count** the number of matching unigrams between the reference and candidate.\n3. Calculate **precision** and **recall** based on the number of matches.\n4. Compute the **F-mean** using the precision and recall.\n5. Calculate the **fragmentation penalty** based on the number of matching n-grams.\n6. Combine the F-mean and the fragmentation penalty to get the final METEOR score.\n\nHere is the Python script that implements the METEOR score calculation:\n\n```python\nimport numpy as np\nfrom collections import Counter\n\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    # Tokenize the reference and candidate sentences\n    ref_tokens = reference.split()\n    cand_tokens = candidate.split()\n    \n    # Count the number of matching unigrams\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    matches = sum((ref_counter & cand_counter).values())\n    \n    # Calculate precision and recall\n    precision = matches / len(cand_tokens) if len(cand_tokens) > 0 else 0\n    recall = matches / len(ref_tokens) if len(ref_tokens) > 0 else 0\n    \n    # Compute the F-mean\n    f_mean = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall) if precision + recall > 0 else 0\n    \n    # Calculate the fragmentation penalty\n    # This is a simplified version of the fragmentation penalty calculation\n    # In a full implementation, you would calculate the number of matching n-grams\n    # and use that to compute the penalty. Here, we assume a penalty based on the\n    # difference in lengths of the reference and candidate sentences.\n    penalty = max(0, 1 - gamma * abs(len(ref_tokens) - len(cand_tokens)) / (len(ref_tokens) + len(cand_tokens)))\n    \n    # Combine the F-mean and the fragmentation penalty\n    meteor = alpha * f_mean + (1 - alpha) * penalty\n    \n    # Return the METEOR score rounded to 3 decimal places\n    return round(meteor, 3)\n\n# Example usage\nreference = \"the cat sat on the mat\"\ncandidate = \"the cat is sitting on the mat\"\nprint(meteor_score(reference, candidate))\n```\n\nThis script provides a simplified version of the METEOR score calculation. The fragmentation penalty is approximated based on the difference in lengths of the reference and candidate sentences, which is a simplification of the actual METEOR algorithm. For a full implementation, you would need to calculate the number of matching n-grams and use that to compute the fragmentation penalty."}
{"task_id": 111, "completion_id": 0, "completion": "To compute the Pointwise Mutual Information (PMI), we need to use the formula:\n\n\\[ PMI(x, y) = \\log\\left(\\frac{P(x, y)}{P(x)P(y)}\\right) \\]\n\nWhere:\n- \\( P(x, y) \\) is the joint probability of events \\( x \\) and \\( y \\) occurring together.\n- \\( P(x) \\) and \\( P(y) \\) are the individual probabilities of events \\( x \\) and \\( y \\) occurring, respectively.\n\nGiven the joint occurrence count of two events, their individual counts, and the total number of samples, we can calculate the probabilities as follows:\n- \\( P(x, y) = \\frac{\\text{joint\\_counts}}{\\text{total\\_samples}} \\)\n- \\( P(x) = \\frac{\\text{total\\_counts\\_x}}{\\text{total\\_samples}} \\)\n- \\( P(y) = \\frac{\\text{total\\_counts\\_y}}{\\text{total\\_samples}} \\)\n\nLet's implement this in Python:\n\n```python\nimport numpy as np\n\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Computes the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events x and y occur together.\n    - total_counts_x: int, the total number of occurrences of event x.\n    - total_counts_y: int, the total number of occurrences of event y.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    # Calculate probabilities\n    p_x_y = joint_counts / total_samples\n    p_x = total_counts_x / total_samples\n    p_y = total_counts_y / total_samples\n    \n    # Calculate PMI\n    pmi = np.log2(p_x_y / (p_x * p_y))\n    \n    # Return PMI rounded to 3 decimal places\n    return round(pmi, 3)\n\n# Example usage\njoint_counts = 100\ntotal_counts_x = 200\ntotal_counts_y = 300\ntotal_samples = 1000\nprint(compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples))\n```\n\nThis script defines a function `compute_pmi` that calculates the PMI based on the provided counts and total samples, and returns the result rounded to three decimal places."}
{"task_id": 112, "completion_id": 0, "completion": "To implement the Min-Max Normalization function, we need to follow these steps:\n\n1. Find the minimum and maximum values in the list.\n2. For each value in the list, apply the Min-Max Normalization formula: \n   \\[ \\text{normalized_value} = \\frac{x - \\text{min}}{\\text{max} - \\text{min}} \\]\n3. Round the normalized values to 4 decimal places.\n4. Return the list of normalized values.\n\nHere is the Python script that accomplishes this:\n\n```python\ndef min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    \n    min_val = min(x)\n    max_val = max(x)\n    \n    if min_val == max_val:  # Avoid division by zero if all elements are the same\n        return [0.0] * len(x)\n    \n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]\n\n# Example usage:\nfeatures = [10, 20, 15, 30, 25]\nnormalized_features = min_max(features)\nprint(normalized_features)\n```\n\nThis script defines the `min_max` function which takes a list of integers as input and returns a list of floats representing the normalized values. The function handles edge cases such as an empty list and a list where all elements are the same."}
{"task_id": 113, "completion_id": 0, "completion": "To implement a simple residual block with a shortcut connection using NumPy, we need to follow these steps:\n\n1. Define the function `residual_block` that takes an input array `x` and two weight matrices `w1` and `w2`.\n2. Apply the first weight matrix `w1` to the input `x` using matrix multiplication.\n3. Apply the ReLU activation function to the result of the first matrix multiplication.\n4. Apply the second weight matrix `w2` to the result of the first ReLU activation.\n5. Add the original input `x` to the result of the second matrix multiplication (this is the shortcut connection).\n6. Apply the ReLU activation function to the result of the addition.\n7. Round the final output to 4 decimal places and convert it to a list before returning.\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    \"\"\"\n    Implements a simple residual block with a shortcut connection.\n    \n    Parameters:\n    x (np.ndarray): The input array.\n    w1 (np.ndarray): The first weight matrix.\n    w2 (np.ndarray): The second weight matrix.\n    \n    Returns:\n    list: The output of the residual block, rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Step 2: Apply the first weight matrix\n    y = np.dot(x, w1)\n    \n    # Step 3: Apply ReLU activation\n    y = np.maximum(y, 0)\n    \n    # Step 4: Apply the second weight matrix\n    y = np.dot(y, w2)\n    \n    # Step 5: Add the original input (shortcut connection)\n    y = y + x\n    \n    # Step 6: Apply ReLU activation\n    y = np.maximum(y, 0)\n    \n    # Step 7: Round to 4 decimal places and convert to list\n    y = np.round(y, 4).tolist()\n    \n    return y\n\n# Example usage\nx = np.array([1, 2, 3])\nw1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\nw2 = np.array([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4], [0.3, 0.2, 0.1]])\n\noutput = residual_block(x, w1, w2)\nprint(output)\n```\n\nThis script defines the `residual_block` function and demonstrates its usage with an example input array `x` and two weight matrices `w1` and `w2`. The output is printed after rounding to 4 decimal places and converting to a list."}
{"task_id": 114, "completion_id": 0, "completion": "To implement global average pooling, we need to average the values across the height and width dimensions of each channel in the input array. Here's how we can achieve this:\n\n1. Iterate over each channel in the input array.\n2. For each channel, calculate the average of all its values.\n3. Collect these averages into a 1D array and return it.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform global average pooling on a 3D NumPy array.\n    \n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: A 1D array of shape (channels,) containing the average of each channel.\n    \"\"\"\n    # Calculate the average across the height and width dimensions for each channel\n    avg_values = np.mean(x, axis=(0, 1))\n    return avg_values\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create a sample 3D array with shape (4, 4, 3) representing 3 feature maps\n    np.random.seed(0)  # For reproducibility\n    sample_input = np.random.rand(4, 4, 3)\n    print(\"Input shape:\", sample_input.shape)\n    \n    # Perform global average pooling\n    pooled_output = global_avg_pool(sample_input)\n    print(\"Pooled output shape:\", pooled_output.shape)\n    print(\"Pooled output:\", pooled_output)\n```\n\nThis script defines the `global_avg_pool` function which takes a 3D NumPy array as input and returns a 1D array containing the average value of each channel. The example usage demonstrates how to use this function with a randomly generated 3D array."}
{"task_id": 115, "completion_id": 0, "completion": "To implement batch normalization for a 4D NumPy array in BCHW format, we need to follow these steps:\n\n1. Calculate the mean and variance for each channel across the batch and spatial dimensions.\n2. Normalize the input using these means and variances.\n3. Apply the scale (gamma) and shift (beta) parameters to the normalized input.\n4. Ensure numerical stability by adding a small epsilon value to the variance before taking the square root.\n5. Round the final output to 4 decimal places and convert it to a list.\n\nHere is the self-contained Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> list:\n    \"\"\"\n    Performs batch normalization on a 4D NumPy array in BCHW format.\n    \n    Parameters:\n    X (np.ndarray): Input array of shape (batch, channels, height, width).\n    gamma (np.ndarray): Scale parameter of shape (channels,).\n    beta (np.ndarray): Shift parameter of shape (channels,).\n    epsilon (float): Small value to ensure numerical stability.\n    \n    Returns:\n    list: Normalized array rounded to 4 decimal places and converted to a list.\n    \"\"\"\n    # Calculate the mean and variance for each channel\n    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n    \n    # Normalize the input\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Apply scale and shift parameters\n    X_normalized = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    \n    # Round to 4 decimal places and convert to list\n    return np.round(X_normalized, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    X = np.random.rand(2, 3, 4, 4)  # Batch size 2, 3 channels, 4x4 feature maps\n    gamma = np.array([1.0, 2.0, 3.0])  # Scale parameters for each channel\n    beta = np.array([0.5, -0.5, 0.0])  # Shift parameters for each channel\n    \n    # Perform batch normalization\n    normalized_X = batch_normalization(X, gamma, beta)\n    print(normalized_X)\n```\n\nThis script defines a function `batch_normalization` that takes a 4D NumPy array `X` in BCHW format, along with scale (`gamma`) and shift (`beta`) parameters, and performs batch normalization. The function calculates the mean and variance for each channel across the batch and spatial dimensions, normalizes the input, applies the scale and shift parameters, and ensures numerical stability by adding a small epsilon value. The final output is rounded to 4 decimal places and converted to a list."}
{"task_id": 116, "completion_id": 0, "completion": "To solve the given task, we need to implement a function that calculates the derivative of a polynomial term at a specific point. The derivative of a polynomial term `c * x^n` is given by the power rule: `n * c * x^(n-1)`. After calculating the derivative, we will round the result to 4 decimal places as required.\n\nHere is the step-by-step implementation:\n\n1. Define the function `poly_term_derivative` with parameters `c`, `x`, and `n`.\n2. Apply the power rule to calculate the derivative.\n3. Round the result to 4 decimal places.\n4. Return the rounded result.\n\nBelow is the Python code implementing the above steps:\n\n```python\ndef poly_term_derivative(c: float, x: float, n: float) -> float:\n    # Calculate the derivative using the power rule\n    derivative = n * c * (x ** (n - 1))\n    # Round the result to 4 decimal places\n    return round(derivative, 4)\n\n# Example usage:\n# Calculate the derivative of 3 * x^2 at x = 2\nprint(poly_term_derivative(3, 2, 2))  # Output should be 12.0\n```\n\nThis function will correctly compute the derivative of a polynomial term at any given point `x`, with the specified coefficient `c` and exponent `n`, and return the result rounded to 4 decimal places."}
{"task_id": 117, "completion_id": 0, "completion": "To solve this task, we will follow these steps:\n\n1. Import the necessary `numpy` package.\n2. Define the function `orthonormal_basis` that takes a list of 2D vectors and a tolerance value.\n3. Convert the list of vectors into a numpy array for easier manipulation.\n4. Implement the Gram-Schmidt process to orthogonalize the vectors.\n5. Normalize the orthogonal vectors to make them orthonormal.\n6. Round the resulting vectors to 4 decimal places and convert them to lists before returning.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef orthonormal_basis(vectors: list[list[float]], tol: float = 1e-10) -> list[list[float]]:\n    \"\"\"\n    Computes an orthonormal basis for the subspace spanned by a list of 2D vectors using the Gram-Schmidt process.\n    \n    Parameters:\n    vectors (list of list of float): A list of 2D vectors.\n    tol (float): Tolerance value to determine linear independence.\n    \n    Returns:\n    list of list of float: A list of orthonormal vectors.\n    \"\"\"\n    # Convert the list of vectors to a numpy array\n    vectors = np.array(vectors)\n    \n    # Initialize an empty list to store the orthonormal basis\n    basis = []\n    \n    for v in vectors:\n        # Subtract the projection of v onto each vector in the current basis\n        for b in basis:\n            v -= np.dot(v, b) * b\n        \n        # Check if the vector is linearly independent (i.e., not the zero vector)\n        if np.linalg.norm(v) > tol:\n            # Normalize the vector to make it a unit vector\n            v = v / np.linalg.norm(v)\n            # Append the orthonormal vector to the basis\n            basis.append(v)\n    \n    # Convert the basis vectors to lists and round to 4 decimal places\n    basis = [np.round(np.array(b), 4).tolist() for b in basis]\n    \n    return basis\n\n# Example usage\nvectors = [[1, 2], [2, 4], [1, 0]]\nprint(orthonormal_basis(vectors))\n```\n\nThis script defines the function `orthonormal_basis` which takes a list of 2D vectors and a tolerance value. It then applies the Gram-Schmidt process to compute an orthonormal basis for the subspace spanned by the input vectors. The resulting orthonormal vectors are rounded to 4 decimal places and returned as a list of lists."}
{"task_id": 118, "completion_id": 0, "completion": "To solve the given task, we will use the `numpy` library, which provides a convenient function `np.cross` to compute the cross product of two vectors. We will define a function `cross_product` that takes two 3-dimensional vectors as input, computes their cross product, rounds the result to 4 decimal places, and then converts the result to a list before returning it.\n\nHere is the step-by-step plan:\n1. Import the `numpy` library.\n2. Define the function `cross_product` that takes two vectors `a` and `b` as input.\n3. Use `np.cross` to compute the cross product of `a` and `b`.\n4. Round the result to 4 decimal places using `np.round`.\n5. Convert the result to a list using `tolist`.\n6. Return the list.\n\nNow, let's implement this in a Python script:\n\n```python\nimport numpy as np\n\ndef cross_product(a, b):\n    # Compute the cross product of vectors a and b\n    cross = np.cross(a, b)\n    # Round the result to 4 decimal places\n    rounded_cross = np.round(cross, decimals=4)\n    # Convert the result to a list\n    return rounded_cross.tolist()\n\n# Example usage:\n# Define two 3D vectors\nvector_a = np.array([1.0, 2.0, 3.0])\nvector_b = np.array([4.0, 5.0, 6.0])\n# Compute the cross product\nresult = cross_product(vector_a, vector_b)\n# Print the result\nprint(result)\n```\n\nThis script defines the function `cross_product` and demonstrates its usage with an example. The output will be the cross product of the two vectors, rounded to 4 decimal places and in list form."}
{"task_id": 119, "completion_id": 0, "completion": "To solve the system of linear equations using Cramer's Rule, we need to follow these steps:\n\n1. Calculate the determinant of the coefficient matrix $A$.\n2. If the determinant is zero, the system does not have a unique solution, so we return -1.\n3. For each variable in the system, replace the corresponding column of $A$ with the constant vector $b$ to form a new matrix.\n4. Calculate the determinant of each new matrix.\n5. The solution for each variable is the determinant of the new matrix divided by the determinant of $A$.\n6. Round the results to the nearest 4th decimal and return the solution as a list.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef cramers_rule(A, b):\n    # Convert inputs to numpy arrays for easier manipulation\n    A = np.array(A)\n    b = np.array(b)\n    \n    # Calculate the determinant of A\n    det_A = np.linalg.det(A)\n    \n    # Check if the determinant is zero\n    if det_A == 0:\n        return -1\n    \n    # Initialize the solution vector\n    x = []\n    \n    # Iterate over each column of A\n    for i in range(A.shape[1]):\n        # Create a copy of A and replace the i-th column with b\n        A_i = A.copy()\n        A_i[:, i] = b\n        \n        # Calculate the determinant of the new matrix\n        det_A_i = np.linalg.det(A_i)\n        \n        # Calculate the solution for the i-th variable\n        x_i = det_A_i / det_A\n        \n        # Round the result to the nearest 4th decimal\n        x.append(round(x_i, 4))\n    \n    # Return the solution as a list\n    return x\n\n# Example usage\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]\nprint(cramers_rule(A, b))\n```\n\nThis script defines the `cramers_rule` function that takes a square matrix `A` and a vector `b` as inputs and returns the solution vector `x` using Cramer's Rule. If the determinant of `A` is zero, it returns -1 indicating no unique solution. The solution vector is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 120, "completion_id": 0, "completion": "To calculate the Bhattacharyya distance between two probability distributions, we need to follow these steps:\n\n1. Check if the input lists `p` and `q` are of the same length and are not empty. If they are not, return `0.0`.\n2. Calculate the Bhattacharyya coefficient, which is the sum of the square roots of the product of the corresponding probabilities from `p` and `q`.\n3. The Bhattacharyya distance is then calculated as `-ln(BC)`, where `BC` is the Bhattacharyya coefficient.\n4. Return the Bhattacharyya distance rounded to 4 decimal places.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    # Check if the inputs are valid\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    \n    # Calculate the Bhattacharyya coefficient\n    bc = sum(np.sqrt(pi * qi) for pi, qi in zip(p, q))\n    \n    # Calculate the Bhattacharyya distance\n    bd = -np.log(bc)\n    \n    # Return the distance rounded to 4 decimal places\n    return round(bd, 4)\n\n# Example usage\np = [0.1, 0.2, 0.3, 0.4]\nq = [0.4, 0.3, 0.2, 0.1]\nprint(bhattacharyya_distance(p, q))  # Output should be the Bhattacharyya distance between p and q\n```\n\nThis script defines the function `bhattacharyya_distance` which takes two lists of floats as input and returns the Bhattacharyya distance between the two distributions. The example usage at the end demonstrates how to call the function with two example distributions `p` and `q`."}
{"task_id": 121, "completion_id": 0, "completion": "To solve the given task, we need to follow these steps:\n\n1. Check if the two input vectors have the same length. If they do not, return `-1` as the operation is not valid.\n2. If the vectors have the same length, create a new list that will store the element-wise sum of the two vectors.\n3. Iterate through the elements of the vectors, summing corresponding elements and storing the result in the new list.\n4. Return the new list containing the element-wise sum.\n\nHere is the Python script that implements the above steps:\n\n```python\ndef vector_sum(a: list[int|float], b: list[int|float]) -> list[int|float] | int:\n    # Check if the vectors have the same length\n    if len(a) != len(b):\n        return -1\n    \n    # Initialize the result vector\n    result = []\n    \n    # Compute the element-wise sum\n    for i in range(len(a)):\n        result.append(a[i] + b[i])\n    \n    return result\n\n# Example usage:\n# vector1 = [1, 2, 3]\n# vector2 = [4, 5, 6]\n# print(vector_sum(vector1, vector2))  # Output: [5, 7, 9]\n\n# vector3 = [1, 2]\n# vector4 = [3, 4, 5]\n# print(vector_sum(vector3, vector4))  # Output: -1\n```\n\nThis script defines the `vector_sum` function that takes two lists (`a` and `b`) as input and returns either a new list with the element-wise sum of the input vectors or `-1` if the vectors have different lengths. The example usage section demonstrates how to use the function with two pairs of vectors."}
{"task_id": 122, "completion_id": 0, "completion": "To implement the REINFORCE algorithm for policy gradient estimation, we need to follow these steps:\n\n1. **Initialize the Policy**: The policy is parameterized by a 2D NumPy array `theta` of shape `(num_states, num_actions)`. The policy for each state is computed via softmax over `theta[s, :]`.\n\n2. **Compute the Policy**: For each state, compute the policy using the softmax function. The softmax function converts raw scores into probabilities.\n\n3. **Compute the Log-Policy**: Calculate the log of the policy for each action taken in each state.\n\n4. **Compute the Return**: For each episode, calculate the return (cumulative reward) for each time step.\n\n5. **Compute the Gradient**: For each episode, compute the gradient of the log-policy multiplied by the return at each time step.\n\n6. **Average the Gradients**: Average the gradients across all episodes.\n\n7. **Return the Result**: Return the average gradient, rounded to the nearest 4th decimal and converted to a Python list.\n\nHere is the Python script implementing the above steps:\n\n```python\nimport numpy as np\n\ndef softmax(x):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    num_states, num_actions = theta.shape\n    gradient = np.zeros_like(theta)\n    \n    for episode in episodes:\n        returns = np.zeros(len(episode))\n        G = 0\n        # Compute returns for each step in the episode\n        for t in reversed(range(len(episode))):\n            state, action, reward = episode[t]\n            G = reward + G\n            returns[t] = G\n        \n        for t, (state, action, _) in enumerate(episode):\n            # Compute policy for the current state\n            policy = softmax(theta[state, :])\n            # Compute log-policy for the action taken\n            log_policy = np.log(policy[action])\n            # Update the gradient\n            gradient[state, action] += log_policy * returns[t]\n    \n    # Average the gradient across all episodes\n    gradient /= len(episodes)\n    # Round to the nearest 4th decimal\n    gradient = np.round(gradient, 4)\n    # Convert to list\n    return gradient.tolist()\n\n# Example usage\ntheta = np.array([[1, 2, 3], [4, 5, 6]])\nepisodes = [\n    [(0, 1, 1.0), (1, 2, 1.0)],\n    [(0, 2, 1.0), (1, 1, 1.0)]\n]\nprint(compute_policy_gradient(theta, episodes))\n```\n\nThis script defines a function `compute_policy_gradient` that takes a policy parameter `theta` and a list of episodes, each containing tuples of (state, action, reward). It computes the average policy gradient using the REINFORCE algorithm and returns the result as a Python list."}
{"task_id": 123, "completion_id": 0, "completion": "To calculate the computational cost savings of a Mixture-of-Experts (MoE) layer compared to a dense layer, we need to compute the number of floating-point operations (FLOPs) for both types of layers. The FLOPs for a dense layer can be calculated as the product of the input dimension, output dimension, and the number of experts (since each expert in the MoE layer would have the same dimensions as a dense layer). For the MoE layer, we only consider the active experts, which reduces the computational cost.\n\nHere's the step-by-step approach to solve the task:\n\n1. Calculate the FLOPs for the dense layer.\n2. Calculate the FLOPs for the MoE layer.\n3. Compute the savings by comparing the FLOPs of the MoE layer to the dense layer.\n4. Round the savings percentage to the nearest 1th decimal.\n\nLet's implement this in Python:\n\n```python\ndef compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational cost savings of an MoE layer compared to a dense layer.\n    \n    :param n_experts: Total number of experts in the MoE layer.\n    :param k_active: Number of active experts in the MoE layer.\n    :param d_in: Input dimension.\n    :param d_out: Output dimension.\n    :return: Savings percentage rounded to the nearest 1th decimal.\n    \"\"\"\n    # FLOPs for a dense layer\n    flops_dense = n_experts * d_in * d_out\n    \n    # FLOPs for the MoE layer\n    flops_moe = k_active * d_in * d_out\n    \n    # Calculate savings\n    savings = (flops_dense - flops_moe) / flops_dense * 100\n    \n    # Round to the nearest 1th decimal\n    savings_rounded = round(savings, 1)\n    \n    return savings_rounded\n\n# Example usage\nn_experts = 10\nk_active = 2\nd_in = 512\nd_out = 512\nprint(compute_efficiency(n_experts, k_active, d_in, d_out))\n```\n\nThis script defines a function `compute_efficiency` that calculates the computational cost savings of an MoE layer compared to a dense layer, given the number of experts, the number of active experts, and the input/output dimensions. The savings percentage is then rounded to the nearest 1th decimal as required."}
{"task_id": 124, "completion_id": 0, "completion": "To implement the Noisy Top-K gating mechanism, we need to follow these steps:\n\n1. Compute the logits by multiplying the input matrix `X` with the gating weight matrix `W_g`.\n2. Add the pre-sampled noise `N` to the logits, scaled by the noise weight matrix `W_noise`.\n3. Apply the softmax function to the noisy logits to get the probabilities.\n4. Select the top-k probabilities for each input and normalize them to ensure they sum to 1.\n5. Return the final probabilities matrix as a Python list, rounded to the nearest 4th decimal.\n\nHere is the Python script that accomplishes this:\n\n```python\nimport numpy as np\n\ndef noisy_topk_gating(\n    X: np.ndarray,\n    W_g: np.ndarray,\n    W_noise: np.ndarray,\n    N: np.ndarray,\n    k: int\n) -> list:\n    # Step 1: Compute the logits\n    logits = X @ W_g\n    \n    # Step 2: Add the pre-sampled noise to the logits\n    noisy_logits = logits + (W_noise * N)\n    \n    # Step 3: Apply the softmax function\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    probabilities = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    \n    # Step 4: Select the top-k probabilities and normalize\n    topk_indices = np.argsort(probabilities, axis=1)[:, -k:]\n    topk_probabilities = np.zeros_like(probabilities)\n    for i, indices in enumerate(topk_indices):\n        topk_probabilities[i, indices] = probabilities[i, indices]\n    topk_probabilities /= np.sum(topk_probabilities, axis=1, keepdims=True)\n    \n    # Step 5: Return the final probabilities matrix as a Python list, rounded to the nearest 4th decimal\n    return np.round(topk_probabilities, decimals=4).tolist()\n\n# Example usage:\n# X = np.random.rand(10, 5)  # Example input matrix\n# W_g = np.random.rand(5, 3)  # Example gating weight matrix\n# W_noise = np.random.rand(3)  # Example noise weight vector\n# N = np.random.rand(10, 3)  # Example pre-sampled noise matrix\n# k = 2  # Example sparsity constraint\n# probabilities = noisy_topk_gating(X, W_g, W_noise, N, k)\n# print(probabilities)\n```\n\nThis script defines the `noisy_topk_gating` function that implements the Noisy Top-K gating mechanism as described. The example usage at the bottom demonstrates how to call the function with example data. Note that the example data should be replaced with actual data when using the function."}
{"task_id": 125, "completion_id": 0, "completion": "To implement the Mixture-of-Experts (MoE) layer as described, we need to follow these steps:\n\n1. **Gating Mechanism**: Compute the gating probabilities using the gating weight matrix `Wg`. This will determine the influence of each expert on the input.\n2. **Top-k Routing**: Select the top-k experts for each input token based on the gating probabilities.\n3. **Expert Transformations**: Apply the transformation of the selected experts to the input.\n4. **Aggregation**: Aggregate the transformed outputs weighted by the gating probabilities.\n\nHere is the Python script implementing the MoE layer:\n\n```python\nimport numpy as np\n\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    # Step 1: Compute gating probabilities\n    gating_scores = x @ Wg\n    gating_probs = np.exp(gating_scores) / np.sum(np.exp(gating_scores), axis=-1, keepdims=True)\n    \n    # Step 2: Select top-k experts\n    top_k_indices = np.argsort(gating_probs, axis=-1)[:, :, -top_k:]\n    top_k_probs = np.take_along_axis(gating_probs, top_k_indices, axis=-1)\n    \n    # Step 3: Apply expert transformations\n    # Reshape x to match the dimensions for multiplication with We\n    x_expanded = np.expand_dims(x, axis=-2)\n    x_expanded = np.repeat(x_expanded, n_experts, axis=-2)\n    expert_outputs = x_expanded @ We\n    \n    # Select the top-k expert outputs\n    top_k_expert_outputs = np.take_along_axis(expert_outputs, np.expand_dims(top_k_indices, axis=-1), axis=-2)\n    \n    # Step 4: Aggregate the outputs\n    aggregated_output = np.sum(top_k_probs[..., np.newaxis] * top_k_expert_outputs, axis=-2)\n    \n    # Round to the nearest 4th decimal and return as a list\n    return np.round(aggregated_output, decimals=4).tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input\n    x = np.array([[1, 2], [3, 4]])\n    We = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[1, 1], [1, 1]], [[-1, -1], [-1, -1]]])\n    Wg = np.array([[1, 0], [0, 1]])\n    n_experts = 4\n    top_k = 2\n    \n    # Call the function\n    output = moe(x, We, Wg, n_experts, top_k)\n    print(output)\n```\n\nThis script defines the `moe` function that implements the MoE layer with the specified parameters. It computes the gating probabilities, selects the top-k experts, applies the expert transformations, and aggregates the results. The final output is rounded to the nearest 4th decimal and returned as a list."}
{"task_id": 126, "completion_id": 0, "completion": "To implement Group Normalization, we need to follow these steps:\n\n1. **Reshape the Input Tensor**: Split the input tensor into groups along the channel dimension.\n2. **Calculate Mean and Variance**: For each group, calculate the mean and variance.\n3. **Normalize**: Normalize the input tensor using the calculated mean and variance.\n4. **Scale and Shift**: Apply the learned scale (gamma) and shift (beta) parameters.\n5. **Reshape Back**: Reshape the tensor back to its original shape.\n\nHere is the Python script that implements the above steps:\n\n```python\nimport numpy as np\n\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float = 1e-5):\n    # Step 1: Reshape the input tensor\n    B, C, H, W = X.shape\n    if C % num_groups != 0:\n        raise ValueError(\"Number of channels must be divisible by num_groups\")\n    G = num_groups\n    X = X.reshape(B, G, C // G, H, W)\n    \n    # Step 2: Calculate mean and variance\n    mean = X.mean(axis=(2, 3, 4), keepdims=True)\n    var = X.var(axis=(2, 3, 4), keepdims=True)\n    \n    # Step 3: Normalize\n    X_norm = (X - mean) / np.sqrt(var + epsilon)\n    \n    # Step 4: Scale and shift\n    X_norm = X_norm.reshape(B, C, H, W)\n    gamma = gamma.reshape(1, C, 1, 1)\n    beta = beta.reshape(1, C, 1, 1)\n    X_out = X_norm * gamma + beta\n    \n    # Step 5: Reshape back to original shape\n    X_out = X_out.round(4)\n    return X_out.tolist()\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example input tensor\n    X = np.random.randn(2, 8, 4, 4)\n    # Example gamma and beta parameters\n    gamma = np.random.randn(8)\n    beta = np.random.randn(8)\n    # Number of groups\n    num_groups = 2\n    \n    # Perform group normalization\n    normalized_output = group_normalization(X, gamma, beta, num_groups)\n    print(normalized_output)\n```\n\nThis script defines a function `group_normalization` that takes a 4D input tensor `X`, scale `gamma`, shift `beta`, number of groups `num_groups`, and an optional `epsilon` for numerical stability. The function reshapes the input tensor to split it into groups, calculates the mean and variance for each group, normalizes the tensor, applies the scale and shift, and finally reshapes the tensor back to its original shape. The output is rounded to the nearest 4th decimal and returned as a Python list."}
{"task_id": 127, "completion_id": 0, "completion": "To solve the task of finding the minimum of the function $f(x) = x^4 - 3x^3 + 2$, we can use a gradient descent approach. The idea is to iteratively adjust the value of $x$ in the direction of the negative gradient (the derivative of the function) until we reach a point where the gradient is close to zero, indicating a local minimum. Given the function, we can calculate its derivative analytically, which is $f'(x) = 4x^3 - 9x^2$.\n\nHere's how we can implement this in Python:\n\n1. Define the function $f(x)$ and its derivative $f'(x)$.\n2. Implement the gradient descent algorithm to find the minimum.\n3. Ensure the algorithm stops when the change in $x$ is less than a specified tolerance or when the maximum number of iterations is reached.\n4. Round the final value of $x$ to the nearest 4th decimal.\n\nLet's write the code:\n\n```python\nimport numpy as np\n\ndef find_treasure(start_x: float, learning_rate: float = 0.1, tolerance: float = 1e-6, max_iters: int = 10000) -> float:\n    # Define the function and its derivative\n    def f(x):\n        return x**4 - 3*x**3 + 2\n    \n    def df(x):\n        return 4*x**3 - 9*x**2\n    \n    # Initialize the starting point\n    x = start_x\n    \n    # Gradient descent loop\n    for _ in range(max_iters):\n        # Calculate the gradient\n        gradient = df(x)\n        \n        # Update x\n        x_new = x - learning_rate * gradient\n        \n        # Check for convergence\n        if abs(x_new - x) < tolerance:\n            break\n        \n        # Update x for the next iteration\n        x = x_new\n    \n    # Return the result rounded to 4 decimal places\n    return round(x, 4)\n\n# Example usage\nstart_x = 1.0\ntreasure_location = find_treasure(start_x)\nprint(f\"The treasure is located at x = {treasure_location}\")\n```\n\nThis script defines the function and its derivative, implements the gradient descent algorithm to find the minimum of the function, and ensures the result is rounded to the nearest 4th decimal. The example usage demonstrates how to call the function with a starting point of `1.0`."}
{"task_id": 128, "completion_id": 0, "completion": "To implement the Dynamic Tanh (DyT) function, we need to follow these steps:\n\n1. **Understand the DyT Function**: The DyT function is a transformation that aims to replace layer normalization in Transformer architectures. It is inspired by the Tanh function but includes additional parameters to control its behavior. The function can be defined as:\n   \\[\n   \\text{DyT}(x) = \\gamma \\cdot \\tanh(\\alpha \\cdot x + \\beta)\n   \\]\n   where:\n   - \\( \\alpha \\) controls the slope of the function.\n   - \\( \\beta \\) controls the shift along the x-axis.\n   - \\( \\gamma \\) scales the output of the Tanh function.\n\n2. **Implement the Function**: We will implement the DyT function using NumPy for efficient array operations. The function will take an input array `x` and three parameters `alpha`, `beta`, and `gamma`.\n\n3. **Apply the Function**: Apply the DyT function to each element of the input array `x`.\n\n4. **Round the Results**: Round the results to the nearest 4th decimal place.\n\n5. **Return the Result**: Convert the NumPy array to a Python list using the `tolist()` method.\n\nHere is the complete implementation:\n\n```python\nimport numpy as np\n\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    # Apply the DyT transformation\n    dyt_result = gamma * np.tanh(alpha * x + beta)\n    \n    # Round the results to the nearest 4th decimal place\n    dyt_rounded = np.round(dyt_result, 4)\n    \n    # Convert the NumPy array to a Python list\n    return dyt_rounded.tolist()\n\n# Example usage\nx = np.array([0.1, 0.5, 1.0, 1.5, 2.0])\nalpha = 1.0\ngamma = 1.0\nbeta = 0.0\nresult = dynamic_tanh(x, alpha, gamma, beta)\nprint(result)\n```\n\nThis script defines the `dynamic_tanh` function, applies it to an example input array `x`, and prints the result. The parameters `alpha`, `gamma`, and `beta` can be adjusted to control the behavior of the DyT function."}
