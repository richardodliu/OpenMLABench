{"task_id": 1, "completion_id": 0, "solution": "def matrix_dot_vector(a: list[list[int | float]], b: list[int | float]) -> list[int | float]:\n    if not a or not b:\n        return -1\n    num_columns_in_matrix = len(a[0])\n    length_of_vector = len(b)\n    if num_columns_in_matrix != length_of_vector:\n        return -1\n    result_vector = []\n    for row in a:\n        dot_product = sum((element * b[i] for (i, element) in enumerate(row)))\n        result_vector.append(dot_product)\n    return result_vector"}
{"task_id": 2, "completion_id": 0, "solution": "def transpose_matrix(a: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not a[0]:\n        return []\n    transposed = [[a[row][col] for row in range(len(a))] for col in range(len(a[0]))]\n    return transposed"}
{"task_id": 3, "completion_id": 0, "solution": "import numpy as np\ndef reshape_matrix(a: list[list[int | float]], new_shape: tuple[int, int]) -> list[list[int | float]]:\n    array = np.array(a)\n    if array.size != new_shape[0] * new_shape[1]:\n        return []\n    reshaped_array = array.reshape(new_shape)\n    return reshaped_array.tolist()\nnew_shape = (3, 2)"}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Parameters:\n    matrix (list[list[float]]): A 2D list representing the matrix.\n    mode (str): The mode of calculation, either 'row' or 'column'.\n\n    Returns:\n    list[float]: A list of means according to the specified mode.\n    \"\"\"\n    if mode not in ['row', 'column']:\n        raise ValueError(\"Mode must be either 'row' or 'column'.\")\n    if mode == 'row':\n        means = [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        num_columns = len(matrix[0])\n        means = [sum((matrix[row][col] for row in range(len(matrix)))) / len(matrix) for col in range(num_columns)]\n    return means\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies a matrix by a scalar and returns the result.\n\n    :param matrix: A 2D list of integers or floats representing the matrix.\n    :param scalar: An integer or float representing the scalar to multiply the matrix by.\n    :return: A 2D list of integers or floats representing the resulting matrix.\n    \"\"\"\n    result = [[0] * len(matrix[0]) for _ in range(len(matrix))]\n    for i in range(len(matrix)):\n        for j in range(len(matrix[i])):\n            result[i][j] = matrix[i][j] * scalar\n    return result"}
{"task_id": 6, "completion_id": 0, "solution": "import math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    eigenvalue1 = (trace + math.sqrt(discriminant)) / 2\n    eigenvalue2 = (trace - math.sqrt(discriminant)) / 2\n    return sorted([eigenvalue1, eigenvalue2], reverse=True)\nmatrix = [[2, 1], [1, 2]]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[int | float]]:\n    A_np = np.array(A, dtype=float)\n    T_np = np.array(T, dtype=float)\n    S_np = np.array(S, dtype=float)\n    if T_np.shape[0] != T_np.shape[1] or S_np.shape[0] != S_np.shape[1]:\n        return -1\n    det_T = np.linalg.det(T_np)\n    det_S = np.linalg.det(S_np)\n    if det_T == 0 or det_S == 0:\n        return -1\n    T_inv = np.linalg.inv(T_np)\n    S_inv = np.linalg.inv(S_np)\n    transformed_matrix = T_inv @ A_np @ S_np\n    transformed_matrix_rounded = np.round(transformed_matrix, 4)\n    result = transformed_matrix_rounded.tolist()\n    return result\nA = [[1, 2], [3, 4]]\nT = [[1, 0], [0, 1]]\nS = [[2, 0], [0, 2]]\nresult = transform_matrix(A, T, S)"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = matrix[0]\n    (c, d) = matrix[1]\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inverse = [[d / determinant, -b / determinant], [-c / determinant, a / determinant]]\n    return inverse\nmatrix = [[1, 2], [3, 4]]\ninverse = inverse_2x2(matrix)"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    (rows_a, cols_a) = (len(a), len(a[0]))\n    (rows_b, cols_b) = (len(b), len(b[0]))\n    if cols_a != rows_b:\n        return -1\n    result = [[0 for _ in range(cols_b)] for _ in range(rows_a)]\n    for i in range(rows_a):\n        for j in range(cols_b):\n            for k in range(cols_a):\n                result[i][j] += a[i][k] * b[k][j]\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    \"\"\"\n    Calculate the covariance matrix for a given set of vectors.\n    \n    Parameters:\n    vectors (list[list[float]]): A list of lists where each inner list represents a feature with its observations.\n    \n    Returns:\n    list[list[float]]: The covariance matrix as a list of lists.\n    \"\"\"\n    import numpy as np\n    data = np.array(vectors)\n    means = np.mean(data, axis=1)\n    centered_data = data - means[:, np.newaxis]\n    num_observations = data.shape[1]\n    covariance_matrix = centered_data @ centered_data.T / (num_observations - 1)\n    return covariance_matrix.tolist()"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    N = len(b)\n    x = np.zeros(N)\n    for _ in range(n):\n        x_new = np.zeros(N)\n        for i in range(N):\n            s = sum((A[i][j] * x[j] for j in range(N) if j != i))\n            x_new[i] = (b[i] - s) / A[i][i]\n        x = np.round(x_new, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    ATA = np.dot(A.T, A)\n\n    def jacobian_eigenvalue_2x2(M, tol=1e-10, max_iter=1000):\n        v = np.array([1.0, 0.0])\n        v = v / np.linalg.norm(v)\n        for _ in range(max_iter):\n            v_new = np.dot(M, v)\n            v_new = v_new / np.linalg.norm(v_new)\n            if np.linalg.norm(v_new - v) < tol:\n                break\n            v = v_new\n        eigenvalue = np.dot(v.T, np.dot(M, v))\n        return (eigenvalue, v)\n    (eigenvalue1, v1) = jacobian_eigenvalue_2x2(ATA)\n    (eigenvalue2, v2) = jacobian_eigenvalue_2x2(ATA - eigenvalue1 * np.outer(v1, v1))\n    sigma1 = np.sqrt(eigenvalue1)\n    sigma2 = np.sqrt(eigenvalue2)\n    Sigma = np.array([[sigma1, 0], [0, sigma2]])\n    V = np.column_stack((v1, v2))\n    U = np.dot(A, np.dot(V, np.linalg.inv(Sigma)))\n    U = np.round(U, 4)\n    Sigma = np.round(Sigma, 4)\n    V = np.round(V, 4)\n    return (U, Sigma, V)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(minor: list[list[int | float]]) -> float:\n        (a, b, c) = minor[0]\n        (d, e, f) = minor[1]\n        (g, h, i) = minor[2]\n        return a * (e * i - f * h) - b * (d * i - f * g) + c * (d * h - e * g)\n\n    def minor_matrix(matrix: list[list[int | float]], row: int, col: int) -> list[list[int | float]]:\n        return [row[:col] + row[col + 1:] for row in matrix[:row] + matrix[row + 1:]]\n    det = 0\n    for c in range(4):\n        sign = (-1) ** c\n        minor = minor_matrix(matrix, 0, c)\n        det += sign * matrix[0][c] * determinant_3x3(minor)\n    return det\nmatrix = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    X_b = np.c_[np.ones((X_np.shape[0], 1)), X_np]\n    theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_np)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> list:\n    theta = np.zeros(X.shape[1])\n    m = len(y)\n    for _ in range(iterations):\n        predictions = X.dot(theta)\n        errors = predictions - y\n        theta -= alpha / m * X.T.dot(errors)\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    standardized_data = (data - mean) / std\n    min_val = np.min(data, axis=0)\n    max_val = np.max(data, axis=0)\n    min_max_normalized_data = (data - min_val) / (max_val - min_val)\n    standardized_data_rounded = np.round(standardized_data, 4)\n    min_max_normalized_data_rounded = np.round(min_max_normalized_data, 4)\n    standardized_data_list = standardized_data_rounded.tolist()\n    min_max_normalized_data_list = min_max_normalized_data_rounded.tolist()\n    return (standardized_data_list, min_max_normalized_data_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points = np.array(points)\n    centroids = np.array(initial_centroids)\n    for _ in range(max_iterations):\n        distances = np.linalg.norm(points[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        new_centroids = np.array([points[labels == i].mean(axis=0) for i in range(k)])\n        if np.all(centroids == new_centroids):\n            break\n        centroids = new_centroids\n    final_centroids = np.round(centroids, 4)\n    return [tuple(centroid) for centroid in final_centroids]\npoints = [(1.0, 2.0), (1.5, 1.8), (5.0, 8.0), (8.0, 8.0), (1.0, 0.6), (9.0, 11.0)]\nk = 2\ninitial_centroids = [(1.0, 2.0), (5.0, 8.0)]\nmax_iterations = 100\nfinal_centroids = k_means_clustering(points, k, initial_centroids, max_iterations)"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test splits for K-Fold Cross-Validation.\n\n    Parameters:\n    X (np.ndarray): The feature dataset.\n    y (np.ndarray): The target dataset.\n    k (int): The number of folds.\n    shuffle (bool): Whether to shuffle the data before splitting.\n    random_seed (int): Random seed for reproducibility if shuffling is enabled.\n\n    Returns:\n    list: A list of tuples, where each tuple contains the train and test indices for a fold.\n    \"\"\"\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    if k < 2:\n        raise ValueError('Number of folds k must be at least 2.')\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    splits = []\n    start = 0\n    for i in range(k):\n        end = start + fold_size + (1 if i < remainder else 0)\n        test_indices = indices[start:end]\n        train_indices = np.concatenate((indices[:start], indices[end:]))\n        splits.append((train_indices, test_indices))\n        start = end\n    return splits"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    mean = np.mean(data, axis=0)\n    std_dev = np.std(data, axis=0)\n    standardized_data = (data - mean) / std_dev\n    covariance_matrix = np.cov(standardized_data, rowvar=False)\n    (eigenvalues, eigenvectors) = np.linalg.eig(covariance_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    principal_components_rounded = np.round(principal_components, 4)\n    principal_components_list = principal_components_rounded.tolist()\n    return principal_components_list"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(class_counts: Counter) -> float:\n        total = sum(class_counts.values())\n        return -sum((count / total * math.log2(count / total) for count in class_counts.values() if count > 0))\n\n    def information_gain(examples: list[dict], attribute: str, target_attr: str) -> float:\n        class_counts = Counter((example[target_attr] for example in examples))\n        total_entropy = entropy(class_counts)\n        value_counts = Counter((example[attribute] for example in examples))\n        weighted_entropy = 0.0\n        for value in value_counts:\n            subset = [example for example in examples if example[attribute] == value]\n            subset_entropy = entropy(Counter((example[target_attr] for example in subset)))\n            weighted_entropy += value_counts[value] / len(examples) * subset_entropy\n        return total_entropy - weighted_entropy\n\n    def choose_best_attribute(examples: list[dict], attributes: list[str], target_attr: str) -> str:\n        return max(attributes, key=lambda attr: information_gain(examples, attr, target_attr))\n\n    def majority_value(examples: list[dict], target_attr: str) -> str:\n        return Counter((example[target_attr] for example in examples)).most_common(1)[0][0]\n    if not examples:\n        return None\n    class_counts = Counter((example[target_attr] for example in examples))\n    if len(class_counts) == 1:\n        return next(iter(class_counts))\n    if not attributes:\n        return majority_value(examples, target_attr)\n    best_attr = choose_best_attribute(examples, attributes, target_attr)\n    tree = {best_attr: {}}\n    for value in set((example[best_attr] for example in examples)):\n        subset = [example for example in examples if example[best_attr] == value]\n        remaining_attributes = [attr for attr in attributes if attr != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def rbf_kernel(x1, x2, sigma):\n        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n\n    def compute_kernel_matrix(data, kernel, sigma):\n        n_samples = data.shape[0]\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if kernel == 'linear':\n                    K[i, j] = linear_kernel(data[i], data[j])\n                elif kernel == 'rbf':\n                    K[i, j] = rbf_kernel(data[i], data[j], sigma)\n        return K\n    (n_samples, n_features) = data.shape\n    alpha = np.zeros(n_samples)\n    bias = 0.0\n    K = compute_kernel_matrix(data, kernel, sigma)\n    for t in range(1, iterations + 1):\n        learning_rate = 1.0 / (lambda_val * t)\n        for i in range(n_samples):\n            prediction = np.sum(alpha * labels * K[:, i]) + bias\n            if labels[i] * prediction < 1:\n                alpha[i] = alpha[i] + learning_rate * labels[i]\n                bias = bias + learning_rate * labels[i]\n        alpha = alpha * min(1, 1.0 / (lambda_val * np.sqrt(t)) * (1.0 / np.sqrt(np.sum(alpha * labels * K.dot(alpha * labels)))))\n    return (alpha.round(4).tolist(), round(bias, 4))"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    Parameters:\n    z (float): The input value to the sigmoid function.\n    \n    Returns:\n    float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    sigmoid_value = 1 / (1 + math.exp(-z))\n    return round(sigmoid_value, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    \"\"\"\n    Compute the softmax activation for a given list of scores.\n    \n    Parameters:\n    scores (list[float]): A list of scores (logits) for which to compute the softmax.\n    \n    Returns:\n    list[float]: A list of softmax values, each rounded to four decimal places.\n    \"\"\"\n    max_score = max(scores)\n    exp_scores = [math.exp(score - max_score) for score in scores]\n    sum_exp_scores = sum(exp_scores)\n    softmax_values = [exp_score / sum_exp_scores for exp_score in exp_scores]\n    softmax_values_rounded = [round(value, 4) for value in softmax_values]\n    return softmax_values_rounded"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef sigmoid(x: float) -> float:\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + math.exp(-x))\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    \"\"\"\n    Simulates a single neuron with a sigmoid activation function for binary classification.\n    \n    Parameters:\n    - features: List of feature vectors (each vector representing multiple features for an example).\n    - labels: Associated true binary labels.\n    - weights: Neuron's weights (one for each feature).\n    - bias: Neuron's bias.\n    \n    Returns:\n    - Predicted probabilities after sigmoid activation, rounded to four decimal places.\n    - Mean squared error between the predicted probabilities and the true labels, rounded to four decimal places.\n    \"\"\"\n    predicted_probabilities = []\n    errors = []\n    for (feature_vector, label) in zip(features, labels):\n        weighted_sum = sum((w * x for (w, x) in zip(weights, feature_vector))) + bias\n        predicted_probability = sigmoid(weighted_sum)\n        predicted_probabilities.append(predicted_probability)\n        error = (predicted_probability - label) ** 2\n        errors.append(error)\n    mean_squared_error = sum(errors) / len(errors)\n    predicted_probabilities_rounded = [round(p, 4) for p in predicted_probabilities]\n    mean_squared_error_rounded = round(mean_squared_error, 4)\n    predicted_probabilities_rounded = np.array(predicted_probabilities_rounded).tolist()\n    return (predicted_probabilities_rounded, mean_squared_error_rounded)\nfeatures = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\nlabels = [0, 1, 0]\nweights = [0.5, -0.5]\nbias = 0.1"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(x: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid activation function.\"\"\"\n    return 1 / (1 + np.exp(-x))\ndef sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n    \"\"\"Derivative of the sigmoid activation function.\"\"\"\n    return x * (1 - x)\ndef mean_squared_error(predictions: np.ndarray, labels: np.ndarray) -> float:\n    \"\"\"Mean Squared Error loss function.\"\"\"\n    return np.mean((predictions - labels) ** 2)\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    \"\"\"\n    Train a single neuron with sigmoid activation using backpropagation.\n    \n    Parameters:\n    - features: np.ndarray, input feature vectors.\n    - labels: np.ndarray, true binary labels.\n    - initial_weights: np.ndarray, initial weights of the neuron.\n    - initial_bias: float, initial bias of the neuron.\n    - learning_rate: float, learning rate for gradient descent.\n    - epochs: int, number of epochs to train.\n    \n    Returns:\n    - Updated weights, bias, and a list of MSE values for each epoch.\n    \"\"\"\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_values = []\n    for epoch in range(epochs):\n        weighted_sum = np.dot(features, weights) + bias\n        predictions = sigmoid(weighted_sum)\n        mse = mean_squared_error(predictions, labels)\n        mse_values.append(round(mse, 4))\n        error = predictions - labels\n        d_loss_d_pred = 2 * error / len(labels)\n        d_pred_d_z = sigmoid_derivative(predictions)\n        d_loss_d_z = d_loss_d_pred * d_pred_d_z\n        d_loss_d_weights = np.dot(features.T, d_loss_d_z)\n        d_loss_d_bias = np.sum(d_loss_d_z)\n        weights -= learning_rate * d_loss_d_weights\n        bias -= learning_rate * d_loss_d_bias\n    return (weights.tolist(), round(bias, 4), mse_values)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, (self, other), '+')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, (self, other), '*')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f'Value(data={self.data}, grad={self.grad})'"}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    B_matrix = np.array(B)\n    C_matrix = np.array(C)\n    B_inv = np.linalg.inv(B_matrix)\n    P = np.dot(C_matrix, B_inv)\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()\nB = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\nC = [[1, 1, 1], [1, 0, 1], [0, 1, 1]]\nP = transform_basis(B, C)"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    A_transpose_A = np.dot(A.T, A)\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_transpose_A)\n    singular_values = np.sqrt(eigenvalues)\n    sorted_indices = np.argsort(singular_values)[::-1]\n    singular_values = singular_values[sorted_indices]\n    V = eigenvectors[:, sorted_indices]\n    S = np.diag(singular_values)\n    U = np.zeros((2, 2))\n    for i in range(2):\n        if singular_values[i] != 0:\n            u_i = np.dot(A, V[:, i]) / singular_values[i]\n            U[:, i] = u_i\n        else:\n            U[:, i] = np.zeros(2)\n    U = np.round(U, 4)\n    S = np.round(S, 4)\n    V = np.round(V, 4)\n    U_list = U.tolist()\n    S_list = S.tolist()\n    V_list = V.tolist()\n    return (U_list, S_list, V_list)\nA = np.array([[1, 2], [3, 4]])"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    \"\"\"\n    Shuffles the samples in two numpy arrays X and y while maintaining the corresponding order between them.\n    \n    Parameters:\n    X (numpy.ndarray): The feature dataset.\n    y (numpy.ndarray): The target dataset.\n    seed (int, optional): Seed for random number generator for reproducibility.\n    \n    Returns:\n    list: A list of lists containing the shuffled data.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    num_samples = X.shape[0]\n    indices = np.random.permutation(num_samples)\n    X_shuffled = X[indices]\n    y_shuffled = y[indices]\n    shuffled_data = [X_shuffled[i].tolist() + [y_shuffled[i]] for i in range(num_samples)]\n    return shuffled_data"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    Yields batches of specified size from the numpy arrays X and y.\n    \n    Parameters:\n    - X: numpy array, the input data.\n    - y: numpy array, optional, the target data.\n    - batch_size: int, the size of each batch.\n    \n    Yields:\n    - If y is provided, yields tuples of (X_batch, y_batch) as lists.\n    - If y is not provided, yields X_batch as a list.\n    \"\"\"\n    num_samples = X.shape[0]\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    for start_idx in range(0, num_samples, batch_size):\n        end_idx = min(start_idx + batch_size, num_samples)\n        X_batch = X[indices[start_idx:end_idx]]\n        if y is not None:\n            y_batch = y[indices[start_idx:end_idx]]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divide the dataset X into two subsets based on the feature at index feature_i\n    and the given threshold.\n\n    Parameters:\n    X (np.ndarray): The dataset to be divided.\n    feature_i (int): The index of the feature to use for division.\n    threshold (float): The threshold value to compare the feature against.\n\n    Returns:\n    tuple: Two lists, the first containing samples where the feature value is\n           greater than or equal to the threshold, and the second containing\n           samples where the feature value is less than the threshold.\n    \"\"\"\n    X = np.array(X)\n    mask_greater_equal = X[:, feature_i] >= threshold\n    mask_less = X[:, feature_i] < threshold\n    subset_greater_equal = X[mask_greater_equal]\n    subset_less = X[mask_less]\n    subset_greater_equal_list = subset_greater_equal.tolist()\n    subset_less_list = subset_less.tolist()\n    return (subset_greater_equal_list, subset_less_list)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    (n_samples, n_features) = X.shape\n    combinations = combinations_with_replacement(range(n_features), degree)\n    polynomial_feature_list = []\n    for combo in combinations:\n        feature = np.prod(X[:, combo], axis=1)\n        polynomial_feature_list.append(feature)\n    polynomial_features_array = np.column_stack(polynomial_feature_list)\n    bias_term = np.ones((n_samples, 1))\n    polynomial_features_array = np.hstack((bias_term, polynomial_features_array))\n    return polynomial_features_array.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a given dataset.\n\n    Parameters:\n    - X: 2D numpy array, the feature dataset.\n    - y: 1D numpy array, the target dataset.\n    - n_subsets: int, number of random subsets to generate.\n    - replacements: bool, whether to sample with replacements.\n    - seed: int, random seed for reproducibility.\n\n    Returns:\n    - List of tuples, each containing a subset of X and y.\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    \"\"\"\n    Perform one-hot encoding of nominal values.\n\n    Parameters:\n    x (numpy.ndarray): 1D numpy array of integer values.\n    n_col (int, optional): Number of columns for the one-hot encoded array.\n                           If not provided, it is determined from the input array.\n\n    Returns:\n    list: One-hot encoded array as a Python list.\n    \"\"\"\n    if n_col is None:\n        n_col = np.max(x) + 1\n    one_hot_matrix = np.zeros((len(x), n_col), dtype=int)\n    one_hot_matrix[np.arange(len(x)), x] = 1\n    return one_hot_matrix.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    diagonal_matrix = np.diag(x)\n    diagonal_matrix_list = diagonal_matrix.tolist()\n    return diagonal_matrix_list"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy score of a model's predictions.\n    \n    Parameters:\n    y_true (numpy.ndarray): 1D array containing the true labels.\n    y_pred (numpy.ndarray): 1D array containing the predicted labels.\n    \n    Returns:\n    float: Accuracy score rounded to the nearest 4th decimal.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if y_true.shape != y_pred.shape:\n        raise ValueError('The input arrays y_true and y_pred must have the same length.')\n    correct_predictions = np.sum(y_true == y_pred)\n    accuracy = correct_predictions / len(y_true)\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix for a given dataset.\n    \n    Parameters:\n    X (numpy.ndarray): A 2D numpy array representing the first dataset.\n    Y (numpy.ndarray, optional): A 2D numpy array representing the second dataset.\n    \n    Returns:\n    list: The correlation matrix as a Python list.\n    \"\"\"\n    if Y is None:\n        Y = X\n    correlation_matrix = np.corrcoef(X, Y, rowvar=False)\n    correlation_matrix_rounded = np.round(correlation_matrix, 4)\n    correlation_matrix_list = correlation_matrix_rounded.tolist()\n    return correlation_matrix_list"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    (n_samples, n_features) = X.shape\n    sample_weights = np.ones(n_samples) / n_samples\n    classifiers = []\n    for _ in range(n_clf):\n        best_clf = {}\n        min_error = float('inf')\n        for feature_i in range(n_features):\n            feature_values = X[:, feature_i]\n            unique_values = np.unique(feature_values)\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            for threshold in thresholds:\n                p = 1\n                predictions = np.ones(n_samples)\n                predictions[X[:, feature_i] < threshold] = -1\n                weighted_error = np.sum(sample_weights[predictions != y])\n                if weighted_error > 0.5:\n                    weighted_error = 1 - weighted_error\n                    p = -1\n                if weighted_error < min_error:\n                    min_error = weighted_error\n                    best_clf = {'feature_index': feature_i, 'threshold': threshold, 'polarity': p}\n        clf_weight = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n        predictions = np.ones(n_samples)\n        feature_i = best_clf['feature_index']\n        threshold = best_clf['threshold']\n        polarity = best_clf['polarity']\n        predictions[X[:, feature_i] < threshold] = -1\n        predictions *= polarity\n        sample_weights *= np.exp(-clf_weight * y * predictions)\n        sample_weights /= np.sum(sample_weights)\n        best_clf['alpha'] = clf_weight\n        classifiers.append(best_clf)\n    return [np.round(list(clf.values()), 4) for clf in classifiers]"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_array = np.array(scores)\n    max_score = np.max(scores_array)\n    stable_scores = scores_array - max_score\n    softmax_values = np.exp(stable_scores) / np.sum(np.exp(stable_scores))\n    log_softmax_values = np.log(softmax_values)\n    rounded_log_softmax_values = np.round(log_softmax_values, 4)\n    result_list = rounded_log_softmax_values.tolist()\n    return result_list\nscores = [1.0, 2.0, 3.0]"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros((1, self.n_units))\n        self.W_opt = copy.deepcopy(optimizer)\n        self.w0_opt = copy.deepcopy(optimizer)\n\n    def parameters(self):\n        return np.prod(self.W.shape) + np.prod(self.w0.shape)\n\n    def forward_pass(self, X, training=True):\n        self.layer_input = X\n        return np.dot(X, self.W) + self.w0\n\n    def backward_pass(self, accum_grad):\n        W_grad = np.dot(self.layer_input.T, accum_grad)\n        w0_grad = np.sum(accum_grad, axis=0, keepdims=True)\n        input_grad = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            self.W = self.W_opt.update(self.W, W_grad)\n            self.w0 = self.w0_opt.update(self.w0, w0_grad)\n        return input_grad\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_height, input_width) = input_matrix.shape\n    (kernel_height, kernel_width) = kernel.shape\n    output_height = (input_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (input_width + 2 * padding - kernel_width) // stride + 1\n    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant', constant_values=0)\n    output = np.zeros((output_height, output_width))\n    for i in range(output_height):\n        for j in range(output_width):\n            region = padded_input[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_width]\n            output[i, j] = np.sum(region * kernel)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": "def relu(z: float) -> float:\n    \"\"\"\n    Implements the Rectified Linear Unit (ReLU) activation function.\n    \n    Parameters:\n    z (float): The input value to the ReLU function.\n    \n    Returns:\n    float: The output value after applying the ReLU function.\n    \"\"\"\n    return z if z > 0 else 0"}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Calculate the Ridge Regression loss.\n\n    Parameters:\n    X (np.ndarray): 2D feature matrix of shape (n_samples, n_features).\n    w (np.ndarray): 1D array of coefficients of shape (n_features,).\n    y_true (np.ndarray): 1D array of true labels of shape (n_samples,).\n    alpha (float): Regularization parameter.\n\n    Returns:\n    float: Ridge loss rounded to the nearest 4th decimal.\n    \"\"\"\n    y_pred = X @ w\n    mse = np.mean((y_true - y_pred) ** 2)\n    regularization = alpha * np.sum(w ** 2)\n    ridge_loss_value = mse + regularization\n    return round(ridge_loss_value, 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float:\n    \"\"\"\n    Implements the Leaky Rectified Linear Unit (Leaky ReLU) activation function.\n\n    Parameters:\n    z (float): The input value to the activation function.\n    alpha (float): The slope for negative inputs. Default is 0.01.\n\n    Returns:\n    float: The output value after applying the Leaky ReLU function.\n    \"\"\"\n    if z >= 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Computes the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (np.array): First input vector.\n    x2 (np.array): Second input vector.\n    \n    Returns:\n    float: The linear kernel value, which is the dot product of x1 and x2.\n    \"\"\"\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric given true and predicted binary labels.\n    \n    Parameters:\n    y_true (numpy array): True binary labels.\n    y_pred (numpy array): Predicted binary labels.\n    \n    Returns:\n    float: Precision score.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    if TP + FP == 0:\n        return 0.0\n    precision_score = TP / (TP + FP)\n    return precision_score"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    X = np.array(X)\n    y = np.array(y).reshape(-1, 1)\n    weights = np.array(weights).reshape(-1, 1)\n    (n_samples, n_features) = X.shape\n    for _ in range(n_iterations):\n        if method == 'batch':\n            y_pred = X.dot(weights)\n            error = y_pred - y\n            gradient = 2 / n_samples * X.T.dot(error)\n            weights -= learning_rate * gradient\n        elif method == 'stochastic':\n            for i in range(n_samples):\n                xi = X[i:i + 1]\n                yi = y[i:i + 1]\n                y_pred = xi.dot(weights)\n                error = y_pred - yi\n                gradient = 2 * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        elif method == 'mini_batch':\n            for i in range(0, n_samples, batch_size):\n                xi = X[i:i + batch_size]\n                yi = y[i:i + batch_size]\n                y_pred = xi.dot(weights)\n                error = y_pred - yi\n                gradient = 2 / batch_size * xi.T.dot(error)\n                weights -= learning_rate * gradient\n        else:\n            raise ValueError(\"Invalid method. Choose 'batch', 'stochastic', or 'mini_batch'.\")\n    return weights.round(4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    A = np.array(matrix, dtype=float)\n    (rows, cols) = A.shape\n    row = 0\n    for col in range(cols):\n        if row >= rows:\n            break\n        pivot_row = row\n        while pivot_row < rows and A[pivot_row, col] == 0:\n            pivot_row += 1\n        if pivot_row < rows:\n            A[[row, pivot_row]] = A[[pivot_row, row]]\n            pivot = A[row, col]\n            A[row] /= pivot\n            for other_row in range(rows):\n                if other_row != row:\n                    factor = A[other_row, col]\n                    A[other_row] -= factor * A[row]\n            row += 1\n    return A.tolist()\nmatrix = [[1, 2, -1, -4], [2, 3, -1, -11], [-2, 0, -3, 22]]"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=1000):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    t = 0\n    for t in range(1, num_iterations + 1):\n        g = grad(x)\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * g ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    return x.round(4).tolist()\ndef grad(x):\n    return 2 * (x - 3)\nx0 = [0.0]"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    (n_samples, n_features) = X.shape\n    weights = np.zeros(n_features)\n    bias = 0.0\n    y = np.array(y)\n    for iteration in range(max_iter):\n        y_pred = np.dot(X, weights) + bias\n        error = y_pred - y\n        gradient_weights = 1 / n_samples * np.dot(X.T, error)\n        gradient_bias = 1 / n_samples * np.sum(error)\n        weights -= learning_rate * (gradient_weights + alpha * np.sign(weights))\n        bias -= learning_rate * gradient_bias\n        if np.linalg.norm(gradient_weights) < tol:\n            break\n    return (weights.round(4).tolist(), round(bias, 4))"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    len_source = len(source)\n    len_target = len(target)\n    dp = np.zeros((len_source + 1, len_target + 1), dtype=int)\n    for i in range(1, len_source + 1):\n        dp[i][0] = i\n    for j in range(1, len_target + 1):\n        dp[0][j] = j\n    for i in range(1, len_source + 1):\n        for j in range(1, len_target + 1):\n            if source[i - 1] == target[j - 1]:\n                cost = 0\n            else:\n                cost = 1\n            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n            if i > 1 and j > 1 and (source[i - 1] == target[j - 2]) and (source[i - 2] == target[j - 1]):\n                dp[i][j] = min(dp[i][j], dp[i - 2][j - 2] + 1)\n    return dp[len_source][len_target]\nsource = 'caper'\ntarget = 'acer'"}
{"task_id": 52, "completion_id": 0, "solution": "import numpy as np\ndef recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    y_true (list): A list of true binary labels (0 or 1).\n    y_pred (list): A list of predicted binary labels (0 or 1).\n\n    Returns:\n    float: The recall value rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FN == 0:\n        return 0.0\n    else:\n        recall_value = TP / (TP + FN)\n        return round(recall_value, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Implement the self-attention mechanism.\n\n    Parameters:\n    X (numpy.ndarray): Input data of shape (batch_size, seq_length, embed_dim)\n    W_q (numpy.ndarray): Query weight matrix of shape (embed_dim, hidden_dim)\n    W_k (numpy.ndarray): Key weight matrix of shape (embed_dim, hidden_dim)\n    W_v (numpy.ndarray): Value weight matrix of shape (embed_dim, hidden_dim)\n\n    Returns:\n    list: Self-attention output as a Python list after rounding to 4 decimal places\n    \"\"\"\n    Q = np.dot(X, W_q)\n    K = np.dot(X, W_k)\n    V = np.dot(X, W_v)\n    attention_scores = np.dot(Q, K.T) / np.sqrt(W_k.shape[1])\n    attention_weights = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.dot(attention_weights, V)\n    output_rounded = np.round(output, 4)\n    output_list = output_rounded.tolist()\n    return output_list"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    input_sequence = np.array(input_sequence)\n    initial_hidden_state = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    h = initial_hidden_state\n    for x in input_sequence:\n        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)\n    return h.round(4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    points_array = np.array(points)\n    translation_matrix = np.array([[1, 0, tx], [0, 1, ty], [0, 0, 1]])\n    homogeneous_points = np.hstack((points_array, np.ones((points_array.shape[0], 1))))\n    translated_homogeneous_points = np.dot(homogeneous_points, translation_matrix.T)\n    translated_points = translated_homogeneous_points[:, :2] / translated_homogeneous_points[:, 2:]\n    return translated_points.tolist()\npoints = [[1, 2], [3, 4], [5, 6]]\ntx = 2\nty = 3\ntranslated_points = translate_object(points, tx, ty)"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Calculate the Kullback-Leibler (KL) divergence between two normal distributions.\n    \n    Parameters:\n    mu_p (float): Mean of the first normal distribution P.\n    sigma_p (float): Standard deviation of the first normal distribution P.\n    mu_q (float): Mean of the second normal distribution Q.\n    sigma_q (float): Standard deviation of the second normal distribution Q.\n    \n    Returns:\n    float: KL divergence from Q to P.\n    \"\"\"\n    kl_divergence = np.log(sigma_q / sigma_p) + (sigma_p ** 2 + (mu_p - mu_q) ** 2) / (2 * sigma_q ** 2) - 0.5\n    return kl_divergence"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    N = len(b)\n    if x_ini is None:\n        x = np.zeros(N, dtype=float)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        x_new = np.zeros(N, dtype=float)\n        for i in range(N):\n            s1 = np.dot(A[i, :i], x_new[:i])\n            s2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x_new[i] = (b[i] - s1 - s2) / A[i, i]\n        x = x_new\n    x_rounded = np.round(x, 4).tolist()\n    return x_rounded"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(b)\n    Ab = np.hstack((A, b.reshape(-1, 1)))\n    for i in range(n):\n        max_row = np.argmax(np.abs(Ab[i:, i])) + i\n        Ab[[i, max_row]] = Ab[[max_row, i]]\n        for j in range(i + 1, n):\n            factor = Ab[j, i] / Ab[i, i]\n            Ab[j, i:] -= factor * Ab[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        x[i] = (Ab[i, -1] - np.dot(Ab[i, i + 1:n], x[i + 1:n])) / Ab[i, i]\n    x = np.round(x, 4)\n    return x.tolist()\nA = [[3, 2, -1], [2, -2, 4], [-1, 0.5, -1]]\nb = [1, -2, 0]"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states, final hidden state, and final cell state.\n        \"\"\"\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n        for x_t in x:\n            x_t = x_t.reshape(-1, 1)\n            combined = np.vstack((x_t, h_prev))\n            f_t = self.sigmoid(np.dot(self.Wf, combined) + self.bf)\n            i_t = self.sigmoid(np.dot(self.Wi, combined) + self.bi)\n            c_t_hat = np.tanh(np.dot(self.Wc, combined) + self.bc)\n            o_t = self.sigmoid(np.dot(self.Wo, combined) + self.bo)\n            c_t = f_t * c_prev + i_t * c_t_hat\n            h_t = o_t * np.tanh(c_t)\n            hidden_states.append(h_t)\n            h_prev = h_t\n            c_prev = c_t\n        hidden_states_rounded = [h.round(4).tolist() for h in hidden_states]\n        final_hidden_state_rounded = h_prev.round(4).tolist()\n        final_cell_state_rounded = c_prev.round(4).tolist()\n        return (hidden_states_rounded, final_hidden_state_rounded, final_cell_state_rounded)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\nfrom collections import defaultdict\ndef compute_tf_idf(corpus, query):\n    if not corpus:\n        raise ValueError('The corpus is empty. Please provide a non-empty list of documents.')\n\n    def compute_tf(document):\n        tf = defaultdict(float)\n        word_count = len(document)\n        for word in document:\n            tf[word] += 1\n        for word in tf:\n            tf[word] /= word_count\n        return tf\n\n    def compute_idf(corpus):\n        idf = defaultdict(float)\n        num_docs = len(corpus)\n        for document in corpus:\n            unique_words = set(document)\n            for word in unique_words:\n                idf[word] += 1\n        for word in idf:\n            idf[word] = np.log((num_docs + 1) / (idf[word] + 1)) + 1\n        return idf\n\n    def compute_tf_idf_scores(tf, idf, query):\n        tf_idf_scores = []\n        for word in query:\n            tf_idf_scores.append(tf.get(word, 0) * idf.get(word, 0))\n        return tf_idf_scores\n    tfs = [compute_tf(doc) for doc in corpus]\n    idf = compute_idf(corpus)\n    tf_idf_results = [compute_tf_idf_scores(tf, idf, query) for tf in tfs]\n    tf_idf_results = np.array(tf_idf_results).round(4).tolist()\n    return tf_idf_results\ncorpus = [['the', 'quick', 'brown', 'fox'], ['jumps', 'over', 'the', 'lazy', 'dog'], ['the', 'fox', 'is', 'quick']]\nquery = ['the', 'fox', 'jumps']\ntf_idf_scores = compute_tf_idf(corpus, query)"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    :param y_true: Numpy array of true labels\n    :param y_pred: Numpy array of predicted labels\n    :param beta: The weight of precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    precision = TP / (TP + FP) if TP + FP > 0 else 0\n    recall = TP / (TP + FN) if TP + FN > 0 else 0\n    if precision + recall == 0:\n        f_score_value = 0\n    else:\n        f_score_value = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n    return round(f_score_value, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Forward pass through the RNN for a given sequence of inputs.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1).\n        \n        Returns:\n        - outputs: A list of output vectors, each of shape (output_size, 1).\n        - last_inputs: A list of input vectors used in the forward pass.\n        - last_hiddens: A list of hidden states used in the forward pass.\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        outputs = []\n        last_inputs = []\n        last_hiddens = []\n        for x in input_sequence:\n            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n            y = np.dot(self.W_hy, h) + self.b_y\n            outputs.append(y)\n            last_inputs.append(x)\n            last_hiddens.append(h)\n            h_prev = h\n        return (outputs, last_inputs, last_hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Backpropagation through time (BPTT) to adjust the weights based on the loss.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1).\n        - expected_output: A list of expected output vectors, each of shape (output_size, 1).\n        - outputs: A list of output vectors from the forward pass, each of shape (output_size, 1).\n        - last_inputs: A list of input vectors used in the forward pass.\n        - last_hiddens: A list of hidden states used in the forward pass.\n        - learning_rate: The learning rate for weight updates.\n        \"\"\"\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(len(input_sequence))):\n            dy = outputs[t] - expected_output[t]\n            dW_hy += np.dot(dy, last_hiddens[t].T)\n            db_y += dy\n            dh = np.dot(self.W_hy.T, dy) + dh_next\n            dh_raw = (1 - last_hiddens[t] * last_hiddens[t]) * dh\n            db_h += dh_raw\n            dW_xh += np.dot(dh_raw, last_inputs[t].T)\n            dW_hh += np.dot(dh_raw, last_hiddens[t - 1].T) if t > 0 else np.zeros_like(dW_hh)\n            dh_next = np.dot(self.W_hh.T, dh_raw)\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y\n\n    def compute_loss(self, outputs, expected_output):\n        \"\"\"\n        Computes the loss using 1/2 * Mean Squared Error (MSE).\n        \n        Parameters:\n        - outputs: A list of output vectors from the forward pass, each of shape (output_size, 1).\n        - expected_output: A list of expected output vectors, each of shape (output_size, 1).\n        \n        Returns:\n        - loss: The computed loss.\n        \"\"\"\n        loss = 0\n        for (y, y_hat) in zip(outputs, expected_output):\n            loss += 0.5 * np.sum((y - y_hat) ** 2)\n        return loss"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix\n    :param b: Right-hand side vector\n    :param n: Maximum number of iterations\n    :param x0: Initial guess for solution (default is zero vector)\n    :param tol: Convergence tolerance\n    :return: Solution vector x\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0\n    r = b - A @ x\n    p = r\n    r_norm_squared = r @ r\n    for i in range(n):\n        Ap = A @ p\n        alpha = r_norm_squared / (p @ Ap)\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        r_new_norm_squared = r_new @ r_new\n        if np.sqrt(r_new_norm_squared) < tol:\n            break\n        beta = r_new_norm_squared / r_norm_squared\n        p = r_new + beta * p\n        r_norm_squared = r_new_norm_squared\n    return x.round(8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    if not y:\n        return 0.0\n    class_counts = Counter(y)\n    total_samples = len(y)\n    gini = 1.0\n    for count in class_counts.values():\n        probability = count / total_samples\n        gini -= probability ** 2\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_pointer = [0]\n    for row in dense_matrix:\n        current_row_non_zeros = 0\n        for (col_index, value) in enumerate(row):\n            if value != 0:\n                values.append(value)\n                column_indices.append(col_index)\n                current_row_non_zeros += 1\n        row_pointer.append(row_pointer[-1] + current_row_non_zeros)\n    return (values, column_indices, row_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 5], [0, 6, 0, 0], [7, 0, 8, 0]]"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    :param v: The vector to be projected (list of numbers)\n    :param L: The line vector defining the direction of projection (list of numbers)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    import numpy as np\n    v = np.array(v)\n    L = np.array(L)\n    dot_product_vL = np.dot(v, L)\n    dot_product_LL = np.dot(L, L)\n    scalar_projection = dot_product_vL / dot_product_LL\n    projection = scalar_projection * L\n    return [round(coord, 3) for coord in projection]\nv = [3, 4]\nL = [1, 2]\nprojection = orthogonal_projection(v, L)"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    if not dense_matrix or not dense_matrix[0]:\n        return ([], [], [])\n    num_rows = len(dense_matrix)\n    num_cols = len(dense_matrix[0])\n    values = []\n    row_indices = []\n    column_pointer = [0] * (num_cols + 1)\n    for col in range(num_cols):\n        for row in range(num_rows):\n            if dense_matrix[row][col] != 0:\n                values.append(dense_matrix[row][col])\n                row_indices.append(row)\n        column_pointer[col + 1] = len(values)\n    return (values, row_indices, column_pointer)\ndense_matrix = [[0, 0, 3, 0], [4, 0, 0, 0], [0, 5, 0, 6], [0, 0, 0, 0]]"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\nfrom sympy import Matrix\ndef matrix_image(A):\n    A_sympy = Matrix(A)\n    (A_rref, pivot_columns) = A_sympy.rref()\n    basis_vectors = [A[:, col] for col in pivot_columns]\n    basis_matrix = np.column_stack(basis_vectors)\n    basis_matrix_rounded = np.round(basis_matrix, 8).tolist()\n    return basis_matrix_rounded\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    tss = np.sum((y_true - y_mean) ** 2)\n    rss = np.sum((y_true - y_pred) ** 2)\n    r2 = 1 - rss / tss\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    if not img or not img[0]:\n        return -1\n    total_brightness = 0\n    pixel_count = 0\n    for row in img:\n        if len(row) != len(img[0]):\n            return -1\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total_brightness += pixel\n            pixel_count += 1\n    average_brightness = round(total_brightness / pixel_count, 2)\n    return average_brightness"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    if not isinstance(y_true, np.ndarray) or not isinstance(y_pred, np.ndarray):\n        raise ValueError('Both y_true and y_pred must be numpy arrays.')\n    if y_true.size == 0 or y_pred.size == 0:\n        raise ValueError('y_true and y_pred must not be empty arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('y_true and y_pred must have the same shape.')\n    n = y_true.size\n    residuals = y_true - y_pred\n    mse = np.mean(residuals ** 2)\n    rmse_value = np.sqrt(mse)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index for binary classification.\n    \n    Parameters:\n    y_true (array-like): True binary labels.\n    y_pred (array-like): Predicted binary labels.\n    \n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    if len(y_true) != len(y_pred):\n        raise ValueError('y_true and y_pred must be of the same length.')\n    intersection = np.sum(np.logical_and(y_true, y_pred))\n    union = np.sum(np.logical_or(y_true, y_pred))\n    if union == 0:\n        return 1.0\n    jaccard_idx = intersection / union\n    return round(jaccard_idx, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score for binary classification.\n    \n    Parameters:\n    y_true (np.array): True binary labels.\n    y_pred (np.array): Predicted binary labels.\n    \n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    elif sum_true == 0 or sum_pred == 0:\n        return 0.0\n    else:\n        dice = 2.0 * intersection / (sum_true + sum_pred)\n        return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef generate_hypervector(seed, dim):\n    \"\"\"Generate a random hypervector with a given seed and dimension.\"\"\"\n    np.random.seed(seed)\n    return np.random.choice([-1, 1], size=dim)\ndef bind_hypervectors(hv1, hv2):\n    \"\"\"Bind two hypervectors using element-wise multiplication.\"\"\"\n    return hv1 * hv2\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Create a composite hypervector for a given dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row: A dictionary representing a dataset row, where keys are feature names and values are their corresponding values.\n    - dim: The dimensionality of the hypervectors.\n    - random_seeds: A dictionary where keys are feature names and values are seeds to ensure reproducibility of hypervectors.\n    \n    Returns:\n    - A composite hypervector representing the entire row as a list.\n    \"\"\"\n    composite_hv = np.ones(dim)\n    for (feature_name, feature_value) in row.items():\n        feature_name_seed = hash(feature_name)\n        feature_name_hv = generate_hypervector(feature_name_seed, dim)\n        feature_value_seed = random_seeds.get(feature_name, None)\n        if feature_value_seed is None:\n            raise ValueError(f\"Seed for feature '{feature_name}' not provided in random_seeds.\")\n        feature_value_hv = generate_hypervector(feature_value_seed + hash(feature_value), dim)\n        bound_hv = bind_hypervectors(feature_name_hv, feature_value_hv)\n        composite_hv = bind_hypervectors(composite_hv, bound_hv)\n    return composite_hv.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    cm = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        if y_true == 0 and y_pred == 0:\n            cm[0][0] += 1\n        elif y_true == 0 and y_pred == 1:\n            cm[0][1] += 1\n        elif y_true == 1 and y_pred == 0:\n            cm[1][0] += 1\n        elif y_true == 1 and y_pred == 1:\n            cm[1][1] += 1\n    return cm\ndata = [[0, 0], [0, 1], [1, 0], [1, 1], [0, 0], [1, 1], [0, 1], [1, 0]]"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    \"\"\"\n    Calculate the cosine similarity between two vectors.\n    \n    Parameters:\n    - v1: Numpy array representing the first vector.\n    - v2: Numpy array representing the second vector.\n    \n    Returns:\n    - A float representing the cosine similarity, rounded to three decimal places.\n    \"\"\"\n    if v1.shape != v2.shape:\n        raise ValueError('Both input vectors must have the same shape.')\n    if v1.size == 0 or v2.size == 0:\n        raise ValueError('Input vectors cannot be empty.')\n    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:\n        raise ValueError('Input vectors cannot have zero magnitude.')\n    dot_product = np.dot(v1, v2)\n    magnitude_v1 = np.linalg.norm(v1)\n    magnitude_v2 = np.linalg.norm(v2)\n    cosine_sim = dot_product / (magnitude_v1 * magnitude_v2)\n    return round(cosine_sim, 3)\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\nfrom typing import List, Tuple\ndef performance_metrics(actual: List[int], predicted: List[int]) -> Tuple[List[List[int]], float, float, float, float]:\n    if len(actual) != len(predicted):\n        raise ValueError(\"The 'actual' and 'predicted' lists must have the same length.\")\n    if not all((x in [0, 1] for x in actual + predicted)):\n        raise ValueError(\"All elements in 'actual' and 'predicted' lists must be either 0 or 1.\")\n    true_positives = 0\n    true_negatives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            true_positives += 1\n        elif a == 0 and p == 0:\n            true_negatives += 1\n        elif a == 0 and p == 1:\n            false_positives += 1\n        elif a == 1 and p == 0:\n            false_negatives += 1\n    confusion_matrix = [[true_negatives, false_positives], [false_negatives, true_positives]]\n    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n    if true_positives + false_positives == 0 or true_positives + false_negatives == 0:\n        f1_score = 0.0\n    else:\n        precision = true_positives / (true_positives + false_positives)\n        recall = true_positives / (true_positives + false_negatives)\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    if true_negatives + false_positives == 0:\n        specificity = 0.0\n    else:\n        specificity = true_negatives / (true_negatives + false_positives)\n    if true_negatives + false_negatives == 0:\n        negative_predictive_value = 0.0\n    else:\n        negative_predictive_value = true_negatives / (true_negatives + false_negatives)\n    return (confusion_matrix, round(accuracy, 3), round(f1_score, 3), round(specificity, 3), round(negative_predictive_value, 3))"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\nfrom scipy import stats\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean = np.mean(data)\n    median = np.median(data)\n    mode_result = stats.mode(data)\n    mode = mode_result.mode[0] if mode_result.count[0] > 1 else np.nan\n    variance = np.var(data, ddof=1)\n    standard_deviation = np.std(data, ddof=1)\n    percentiles = np.percentile(data, [25, 50, 75])\n    percentile_25 = percentiles[0]\n    percentile_50 = percentiles[1]\n    percentile_75 = percentiles[2]\n    iqr = percentile_75 - percentile_25\n    results = {'mean': round(mean, 4), 'median': round(median, 4), 'mode': round(mode, 4) if not np.isnan(mode) else np.nan, 'variance': round(variance, 4), 'standard_deviation': round(standard_deviation, 4), '25th_percentile': round(percentile_25, 4), '50th_percentile': round(percentile_50, 4), '75th_percentile': round(percentile_75, 4), 'interquartile_range': round(iqr, 4)}\n    return results\ndata = [1, 2, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9]"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    binom_coeff = math.comb(n, k)\n    probability = binom_coeff * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    :return: The PDF value rounded to 5 decimal places.\n    \"\"\"\n    if std_dev <= 0:\n        raise ValueError('Standard deviation must be greater than 0.')\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    exponent = -0.5 * ((x - mean) / std_dev) ** 2\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)\nx = 10\nmean = 10\nstd_dev = 2"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    :return: Probability of observing exactly k events, rounded to 5 decimal places\n    \"\"\"\n    if k < 0 or not isinstance(k, int):\n        raise ValueError('k must be a non-negative integer.')\n    if lam <= 0:\n        raise ValueError('lam must be a positive number.')\n    probability = math.exp(-lam) * lam ** k / math.factorial(k)\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \n    Returns:\n        int: The contrast of the image, calculated as the difference between the maximum and minimum pixel values.\n    \"\"\"\n    if not isinstance(img, np.ndarray) or img.ndim != 2:\n        raise ValueError('Input must be a 2D numpy array representing a grayscale image.')\n    if img.min() < 0 or img.max() > 255:\n        raise ValueError('Pixel values must be between 0 and 255.')\n    max_pixel = img.max()\n    min_pixel = img.min()\n    contrast = max_pixel - min_pixel\n    return contrast"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n        \n    Returns:\n        float: The dot product of the two vectors.\n        \n    Raises:\n        ValueError: If the input vectors are not 1D or do not have the same length.\n    \"\"\"\n    if vec1.ndim != 1 or vec2.ndim != 1:\n        raise ValueError('Both inputs must be 1D arrays.')\n    if vec1.shape[0] != vec2.shape[0]:\n        raise ValueError('Both vectors must have the same length.')\n    dot_product = np.dot(vec1, vec2)\n    return dot_product"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int) -> list[list[float]]:\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A list of lists, where each inner list contains the polynomial features of the corresponding data point.\n    \"\"\"\n    if degree < 0:\n        return []\n    data_array = np.array(data)\n    transformed_data = []\n    for x in data_array:\n        features = [x ** i for i in range(degree + 1)]\n        transformed_data.append(features)\n    transformed_data = np.round(transformed_data, 8).tolist()\n    return transformed_data\ndata = [1.0, 2.0, 3.0]\ndegree = 3"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    pos_enc = np.zeros((position, d_model), dtype=np.float16)\n    for pos in range(position):\n        for i in range(0, d_model, 2):\n            pos_enc[pos, i] = np.sin(pos / 10000 ** (2 * i / d_model))\n            if i + 1 < d_model:\n                pos_enc[pos, i + 1] = np.cos(pos / 10000 ** (2 * (i + 1) / d_model))\n    return pos_enc.tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of '1', '-1', or '0'.\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    :param parameter: Current parameter value\n    :param grad: Current gradient\n    :param m: First moment estimate\n    :param v: Second moment estimate\n    :param t: Current timestep\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n    \"\"\"\n    parameter = np.array(parameter)\n    grad = np.array(grad)\n    m = np.array(m)\n    v = np.array(v)\n    m = beta1 * m + (1 - beta1) * grad\n    v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = m / (1 - beta1 ** t)\n    v_hat = v / (1 - beta2 ** t)\n    parameter = parameter - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n    updated_parameter = parameter.round(5).tolist()\n    updated_m = m.round(5).tolist()\n    updated_v = v.round(5).tolist()\n    return (updated_parameter, updated_m, updated_v)"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef layer_norm(x, g, b, eps=1e-05):\n    mean = np.mean(x, axis=-1, keepdims=True)\n    variance = np.var(x, axis=-1, keepdims=True)\n    return g * (x - mean) / np.sqrt(variance + eps) + b\ndef multi_head_attention(q, k, v, n_head):\n    (batch_size, seq_len, d_model) = q.shape\n    d_k = d_model // n_head\n    q = q.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    k = k.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    v = v.reshape(batch_size, seq_len, n_head, d_k).transpose(0, 2, 1, 3)\n    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, v).transpose(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n    return output\ndef feed_forward(x, d_model, d_ff):\n    w1 = np.random.rand(d_model, d_ff)\n    b1 = np.random.rand(d_ff)\n    w2 = np.random.rand(d_ff, d_model)\n    b2 = np.random.rand(d_model)\n    return np.matmul(np.maximum(0, np.matmul(x, w1) + b1), w2) + b2\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_ctx = hparams['n_ctx']\n    n_head = hparams['n_head']\n    d_model = params['wte'].shape[1]\n    input_ids = encoder.encode(prompt)\n    input_ids = input_ids[-n_ctx:]\n    for _ in range(n_tokens_to_generate):\n        context = np.array(input_ids[-n_ctx:]).reshape(1, -1)\n        token_embeddings = params['wte'][context]\n        positions = np.arange(context.shape[1]).reshape(1, -1)\n        positional_embeddings = params['wpe'][positions]\n        x = token_embeddings + positional_embeddings\n        q = k = v = x\n        x = multi_head_attention(q, k, v, n_head)\n        x = layer_norm(x + token_embeddings, params['ln_f']['g'], params['ln_f']['b'])\n        x = feed_forward(x, d_model, d_model * 4)\n        x = layer_norm(x + x, params['ln_f']['g'], params['ln_f']['b'])\n        logits = np.matmul(x, params['wte'].T)\n        next_token = np.argmax(logits[:, -1, :], axis=-1)[0]\n        input_ids.append(next_token)\n    return encoder.decode(input_ids)\nprompt = 'hello world'"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        exp_values = np.exp(values - np.max(values))\n        return exp_values / np.sum(exp_values)\n    crystal_values = np.array(crystal_values)\n    attention_scores = np.zeros((n, n))\n    weight_matrix = np.random.rand(dimension, n)\n    crystal_values_expanded = np.repeat(crystal_values[:, np.newaxis], dimension, axis=1)\n    raw_scores = np.dot(crystal_values_expanded, weight_matrix)\n    for i in range(n):\n        attention_scores[i] = softmax(raw_scores[i])\n    weighted_patterns = np.dot(attention_scores, crystal_values)\n    return [round(pattern, 4) for pattern in weighted_patterns]\nn = 5\ncrystal_values = [10, 20, 30, 40, 50]\ndimension = 3"}
{"task_id": 90, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    tokenized_corpus = [doc.lower().split() for doc in corpus]\n    tokenized_query = query.lower().split()\n    doc_lengths = [len(doc) for doc in tokenized_corpus]\n    avg_doc_length = np.mean(doc_lengths)\n    N = len(corpus)\n    df = {}\n    for term in set(tokenized_query):\n        df[term] = sum((1 for doc in tokenized_corpus if term in doc))\n    idf = {}\n    for term in df:\n        idf[term] = np.log((N - df[term] + 0.5) / (df[term] + 0.5))\n    scores = []\n    for (i, doc) in enumerate(tokenized_corpus):\n        score = 0\n        doc_length = doc_lengths[i]\n        term_freq = Counter(doc)\n        for term in tokenized_query:\n            if term in term_freq:\n                f_qi_D = term_freq[term]\n                score += idf[term] * (f_qi_D * (k1 + 1)) / (f_qi_D + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n        scores.append(round(score, 3))\n    return scores\ncorpus = ['the quick brown fox jumps over the lazy dog', 'never jump over the lazy dog quickly', 'a quick brown dog outpaces a quick fox']"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        raise ValueError('The length of y_true and y_pred must be the same.')\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            true_positives += 1\n        elif true == 0 and pred == 1:\n            false_positives += 1\n        elif true == 1 and pred == 0:\n            false_negatives += 1\n    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n    return round(f1_score, 3)\ny_true = [1, 0, 1, 1, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1]"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    detrended_data = []\n    for (i, consumption) in enumerate(consumption_data, start=1):\n        fluctuation = 10 * math.sin(2 * PI * i / 10)\n        detrended_consumption = consumption - fluctuation\n        detrended_data.append(detrended_consumption)\n    days = np.array(range(1, len(consumption_data) + 1)).reshape(-1, 1)\n    model = LinearRegression()\n    model.fit(days, detrended_data)\n    day_15 = np.array([[15]])\n    base_consumption_day_15 = model.predict(day_15)[0]\n    fluctuation_day_15 = 10 * math.sin(2 * PI * 15 / 10)\n    predicted_consumption_day_15 = base_consumption_day_15 + fluctuation_day_15\n    final_consumption = math.ceil(predicted_consumption_day_15 * 1.05)\n    return final_consumption\nconsumption_data = [120, 135, 150, 165, 180, 195, 210, 225, 240, 255]"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_absolute_error = np.mean(absolute_errors)\n    return round(mean_absolute_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the self-attention mechanism.\n    \n    Parameters:\n    - Q: Query matrix of shape (batch_size, seq_len, d_k)\n    - K: Key matrix of shape (batch_size, seq_len, d_k)\n    - V: Value matrix of shape (batch_size, seq_len, d_v)\n    \n    Returns:\n    - Z: Output of self-attention mechanism of shape (batch_size, seq_len, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n    scores = np.dot(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n    attention_weights = np.softmax(scores, axis=-1)\n    Z = np.dot(attention_weights, V)\n    return Z\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Compute the multi-head attention mechanism.\n    \n    Parameters:\n    - X: Input data of shape (batch_size, seq_len, d_model)\n    - W_q: Query weight matrix of shape (n_heads, d_model, d_k)\n    - W_k: Key weight matrix of shape (n_heads, d_model, d_k)\n    - W_v: Value weight matrix of shape (n_heads, d_model, d_v)\n    - n_heads: Number of attention heads\n    \n    Returns:\n    - output: Output of multi-head attention mechanism as a Python list\n    \"\"\"\n    (batch_size, seq_len, d_model) = X.shape\n    d_k = W_q.shape[-1]\n    d_v = W_v.shape[-1]\n    X_heads = X.reshape(batch_size, seq_len, n_heads, d_k).transpose(0, 2, 1, 3)\n    Q_heads = np.einsum('bhld,hdk->bhkd', X_heads, W_q)\n    K_heads = np.einsum('bhld,hdk->bhkd', X_heads, W_k)\n    V_heads = np.einsum('bhld,hdv->bhkv', X_heads, W_v)\n    Z_heads = self_attention(Q_heads, K_heads, V_heads)\n    Z_concat = Z_heads.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, n_heads * d_v)\n    output = np.round(Z_concat, 4).tolist()\n    return output"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    if len(x) != len(y):\n        raise ValueError('The input lists must have the same length.')\n    n11 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 1))\n    n10 = sum((1 for (xi, yi) in zip(x, y) if xi == 1 and yi == 0))\n    n01 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 1))\n    n00 = sum((1 for (xi, yi) in zip(x, y) if xi == 0 and yi == 0))\n    n = len(x)\n    numerator = n11 * n00 - n10 * n01\n    denominator = ((n11 + n10) * (n11 + n01) * (n00 + n10) * (n00 + n01)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    phi = numerator / denominator\n    return round(phi, 4)\nx = [1, 0, 1, 1, 0, 1, 0, 0]\ny = [1, 1, 0, 1, 0, 0, 0, 1]"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    linear_transformation = 0.2 * x + 0.5\n    return max(0, min(1, linear_transformation))"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value rounded to 4 decimal places\n    \"\"\"\n    if x >= 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The softplus value: log(1 + e^x), rounded to 4 decimal places\n    \"\"\"\n    if x > 20:\n        return round(x, 4)\n    elif x < -20:\n        return round(math.exp(x), 4)\n    else:\n        return round(math.log(1 + math.exp(x)), 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    result = x / (1 + abs(x))\n    return round(result, 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos = np.array(rhos)\n    A = np.array(A)\n    pi_theta_old = np.array(pi_theta_old)\n    pi_theta_ref = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos, 1 - epsilon, 1 + epsilon)\n    term1 = rhos * A\n    term2 = clipped_rhos * A\n    policy_gradient_term = np.minimum(term1, term2)\n    kl_divergence = np.sum(pi_theta_old * np.log(pi_theta_old / pi_theta_ref))\n    grpo_obj = np.mean(policy_gradient_term) - beta * kl_divergence\n    return round(grpo_obj, 6)\nrhos = [1.1, 0.9, 1.2, 0.8]\nA = [0.5, -0.3, 0.2, -0.1]\npi_theta_old = [0.25, 0.25, 0.25, 0.25]\npi_theta_ref = [0.3, 0.2, 0.25, 0.25]"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to the nearest 4th decimal\n    \"\"\"\n    swish_value = x * math.sigmoid(x)\n    return round(swish_value, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        SELU activation value rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        selu_value = scale * x\n    else:\n        selu_value = scale * alpha * (math.exp(x) - 1)\n    return round(selu_value, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1)\n    \"\"\"\n    linear_combination = np.dot(X, weights) + bias\n    probabilities = 1 / (1 + np.exp(-linear_combination))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], ...]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list[float], CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    C = len(np.unique(y))\n    M = X.shape[1]\n    B = np.random.randn(C, M)\n    y_one_hot = np.eye(C)[y]\n    losses = []\n    for _ in range(iterations):\n        scores = np.dot(X, B.T)\n        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        loss = -np.sum(y_one_hot * np.log(probs)) / len(y)\n        losses.append(round(loss, 4))\n        dscores = probs - y_one_hot\n        dW = np.dot(dscores.T, X) / len(y)\n        B -= learning_rate * dW\n    return (B.round(4).tolist(), losses)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef sigmoid(z: np.ndarray) -> np.ndarray:\n    \"\"\"Sigmoid function to map predictions to probabilities.\"\"\"\n    return 1 / (1 + np.exp(-z))\ndef binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Compute the Binary Cross Entropy loss.\"\"\"\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n    \"\"\"\n    n_features = X.shape[1]\n    weights = np.zeros(n_features)\n    bias = 0.0\n    loss_values = []\n    for _ in range(iterations):\n        linear_model = np.dot(X, weights) + bias\n        y_pred = sigmoid(linear_model)\n        dw = 1 / X.shape[0] * np.dot(X.T, y_pred - y)\n        db = 1 / X.shape[0] * np.sum(y_pred - y)\n        weights -= learning_rate * dw\n        bias -= learning_rate * db\n        loss = binary_cross_entropy(y, y_pred)\n        loss_values.append(round(loss, 4))\n    return (weights.tolist(), bias, loss_values)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \"\"\"\n    scores = np.dot(Q, K.T) / np.sqrt(K.shape[-1])\n    scores = scores + mask\n    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n    softmax_scores = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n    output = np.dot(softmax_scores, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    if not apples:\n        return 0.0\n    color_counts = Counter(apples)\n    total_apples = len(apples)\n    probabilities = [count / total_apples for count in color_counts.values()]\n    entropy = -sum((p * math.log2(p) for p in probabilities if p > 0))\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on the input tensor X.\n    \n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) for scaling\n    - beta: np.ndarray of shape (feature_dim,) for shifting\n    - epsilon: small constant for numerical stability\n    \n    Returns:\n    - Normalized X as a list rounded to 5 decimal places\n    \"\"\"\n    if gamma.shape != beta.shape or gamma.shape != (X.shape[-1],):\n        raise ValueError('gamma and beta must have the same shape as the feature dimension of X')\n    mean = np.mean(X, axis=-1, keepdims=True)\n    variance = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(variance + epsilon)\n    X_normalized = gamma * X_normalized + beta\n    return X_normalized.round(5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\nimport re\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n\n    def tokenize(text):\n        return re.findall('\\\\w+', text.lower())\n    ref_tokens = tokenize(reference)\n    cand_tokens = tokenize(candidate)\n    ref_counter = Counter(ref_tokens)\n    cand_counter = Counter(cand_tokens)\n    matches = sum((ref_counter & cand_counter).values())\n    precision = matches / len(cand_tokens) if cand_tokens else 0\n    recall = matches / len(ref_tokens) if ref_tokens else 0\n    if precision + recall == 0:\n        f_mean = 0\n    else:\n        f_mean = (1 + beta ** 2) * (precision * recall) / (beta ** 2 * precision + recall)\n\n    def get_chunks(tokens):\n        chunks = []\n        chunk = []\n        for token in tokens:\n            if token in ref_counter:\n                chunk.append(token)\n            elif chunk:\n                chunks.append(chunk)\n                chunk = []\n        if chunk:\n            chunks.append(chunk)\n        return chunks\n    ref_chunks = get_chunks(ref_tokens)\n    cand_chunks = get_chunks(cand_tokens)\n    chunk_matches = 0\n    for ref_chunk in ref_chunks:\n        for cand_chunk in cand_chunks:\n            if ref_chunk == cand_chunk:\n                chunk_matches += 1\n                break\n    chunk_precision = chunk_matches / len(cand_chunks) if cand_chunks else 0\n    chunk_recall = chunk_matches / len(ref_chunks) if ref_chunks else 0\n    if chunk_precision + chunk_recall == 0:\n        frag_penalty = 0\n    else:\n        frag_penalty = gamma * chunk_precision * chunk_recall / (gamma * chunk_precision + chunk_recall)\n    meteor = (1 - alpha) * f_mean + alpha * frag_penalty\n    return round(meteor, 3)\nreference = 'the cat sat on the mat'\ncandidate = 'the cat lay on the mat'"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) given the joint occurrence count of two events,\n    their individual counts, and the total number of samples.\n    \n    Parameters:\n    - joint_counts: int, the number of times both events X and Y occur together.\n    - total_counts_x: int, the number of times event X occurs.\n    - total_counts_y: int, the number of times event Y occurs.\n    - total_samples: int, the total number of samples.\n    \n    Returns:\n    - float, the PMI value rounded to 3 decimal places.\n    \"\"\"\n    expected_joint_prob = total_counts_x / total_samples * (total_counts_y / total_samples)\n    actual_joint_prob = joint_counts / total_samples\n    if expected_joint_prob == 0:\n        pmi = -np.inf\n    else:\n        pmi = np.log2(actual_joint_prob / expected_joint_prob)\n    return round(pmi, 3)\njoint_counts = 10\ntotal_counts_x = 100\ntotal_counts_y = 50\ntotal_samples = 1000"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    if not x:\n        return []\n    min_val = min(x)\n    max_val = max(x)\n    if min_val == max_val:\n        return [0.0] * len(x)\n    normalized = [(val - min_val) / (max_val - min_val) for val in x]\n    return [round(val, 4) for val in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> list:\n    assert x.shape[1] == w1.shape[0], \"Input dimension must match the first weight matrix's rows\"\n    assert w1.shape[1] == w2.shape[0], \"First weight matrix's columns must match the second weight matrix's rows\"\n    z1 = np.dot(x, w1)\n    a1 = np.maximum(0, z1)\n    z2 = np.dot(a1, w2)\n    a2 = np.maximum(0, z2)\n    z3 = x + a2\n    output = np.maximum(0, z3)\n    return output.round(4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array representing feature maps.\n    \n    Parameters:\n    x (np.ndarray): Input array of shape (height, width, channels).\n    \n    Returns:\n    np.ndarray: Output array of shape (channels,), where each element is the average\n                of all values in the corresponding feature map.\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError('Input must be a 3D array of shape (height, width, channels).')\n    avg_pooled = np.mean(x, axis=(0, 1))\n    return avg_pooled"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Batch Normalization on a 4D NumPy array in BCHW format.\n\n    Parameters:\n    - X: np.ndarray of shape (batch_size, channels, height, width)\n    - gamma: np.ndarray of shape (channels,)\n    - beta: np.ndarray of shape (channels,)\n    - epsilon: float, a small value for numerical stability\n\n    Returns:\n    - normalized_X: list, the normalized and scaled input rounded to 4 decimal places\n    \"\"\"\n    (batch_size, channels, height, width) = X.shape\n    assert gamma.shape == (channels,)\n    assert beta.shape == (channels,)\n    X_reshaped = X.reshape(batch_size, channels, -1)\n    mean = np.mean(X_reshaped, axis=(0, 2), keepdims=True)\n    var = np.var(X_reshaped, axis=(0, 2), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(batch_size, channels, height, width)\n    normalized_X = gamma[np.newaxis, :, np.newaxis, np.newaxis] * X_normalized + beta[np.newaxis, :, np.newaxis, np.newaxis]\n    normalized_X = normalized_X.round(4).tolist()\n    return normalized_X"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    derivative = c * n * x ** (n - 1)\n    return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    vectors = np.array(vectors, dtype=float)\n    orthonormal_basis = []\n    for v in vectors:\n        u = v\n        for basis_vector in orthonormal_basis:\n            projection = np.dot(u, basis_vector) * basis_vector\n            u -= projection\n        if np.linalg.norm(u) > tol:\n            u = u / np.linalg.norm(u)\n            orthonormal_basis.append(u)\n    orthonormal_basis = [vec.round(4).tolist() for vec in orthonormal_basis]\n    return orthonormal_basis\nvectors = [[1, 0], [1, 1], [2, 2]]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    a = np.array(a)\n    b = np.array(b)\n    result = np.cross(a, b)\n    result_rounded = np.round(result, 4)\n    result_list = result_rounded.tolist()\n    return result_list"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Coefficient matrix A must be square.')\n    det_A = np.linalg.det(A)\n    if det_A == 0:\n        return -1\n    n = len(b)\n    x = np.zeros(n)\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x[i] = det_A_i / det_A\n    x_rounded = np.round(x, 4)\n    x_list = x_rounded.tolist()\n    return x_list\nA = [[2, 1, -1], [-3, -1, 2], [-2, 1, 2]]\nb = [8, -11, -3]"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    if len(p) != len(q) or not p or (not q):\n        return 0.0\n    p = np.array(p)\n    q = np.array(q)\n    bc = np.sum(np.sqrt(p * q))\n    bd = -np.log(bc) if bc > 0 else float('inf')\n    return round(bd, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "def vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    result = [x + y for (x, y) in zip(a, b)]\n    return result\nresult = vector_sum(vector3, vector4)\nvector3 = [1, 2]\nvector4 = [3, 4, 5]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    gradient = np.zeros_like(theta, dtype=float)\n    for episode in episodes:\n        G = 0\n        returns = []\n        for (_, _, reward) in reversed(episode):\n            G += reward\n            returns.append(G)\n        returns.reverse()\n        for ((state, action, _), G) in zip(episode, returns):\n            probs = np.exp(theta[state, :]) / np.sum(np.exp(theta[state, :]))\n            gradient[state, action] += G * (1 - probs[action])\n            for a in range(num_actions):\n                if a != action:\n                    gradient[state, a] -= G * probs[a]\n    gradient /= len(episodes)\n    gradient = np.round(gradient, 4)\n    return gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    flops_dense = d_in * d_out\n    flops_moe = k_active * d_in * d_out\n    savings_percentage = (flops_dense - flops_moe) / flops_dense * 100\n    flops_dense_rounded = round(flops_dense, 1)\n    flops_moe_rounded = round(flops_moe, 1)\n    savings_percentage_rounded = round(savings_percentage, 1)\n    return (flops_dense_rounded, flops_moe_rounded, savings_percentage_rounded)\nk_active = 4\nd_in = 512\nd_out = 512"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    logits = np.dot(X, W_g)\n    noisy_logits = logits + np.dot(N, W_noise)\n    exp_noisy_logits = np.exp(noisy_logits - np.max(noisy_logits, axis=1, keepdims=True))\n    gating_probs = exp_noisy_logits / np.sum(exp_noisy_logits, axis=1, keepdims=True)\n    topk_indices = np.argsort(gating_probs, axis=1)[:, -k:]\n    topk_mask = np.zeros_like(gating_probs, dtype=bool)\n    np.put_along_axis(topk_mask, topk_indices[:, :, np.newaxis], True, axis=1)\n    gating_probs[~topk_mask] = 0\n    gating_probs = gating_probs / np.sum(gating_probs, axis=1, keepdims=True)\n    gating_probs_rounded = np.round(gating_probs, 4)\n    gating_probs_list = gating_probs_rounded.tolist()\n    return gating_probs_list"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    gating_scores = x @ Wg\n    gating_probs = np.exp(gating_scores - np.max(gating_scores, axis=1, keepdims=True))\n    gating_probs /= np.sum(gating_probs, axis=1, keepdims=True)\n    top_k_indices = np.argsort(gating_probs, axis=1)[:, -top_k:]\n    top_k_values = np.take_along_axis(gating_probs, top_k_indices, axis=1)\n    top_k_values /= np.sum(top_k_values, axis=1, keepdims=True)\n    (batch_size, input_dim) = x.shape\n    expert_outputs = np.zeros((batch_size, top_k, input_dim, We.shape[1]))\n    for i in range(batch_size):\n        for (j, expert_idx) in enumerate(top_k_indices[i]):\n            expert_outputs[i, j] = x[i] @ We[expert_idx]\n    weighted_expert_outputs = expert_outputs * top_k_values[:, :, np.newaxis, np.newaxis]\n    final_output = np.sum(weighted_expert_outputs, axis=1)\n    final_output = final_output.reshape(batch_size, -1)\n    final_output = np.round(final_output, 4)\n    return final_output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X: Input tensor of shape (B, C, H, W)\n    - gamma: Learnable scale parameter of shape (1, C, 1, 1)\n    - beta: Learnable shift parameter of shape (1, C, 1, 1)\n    - num_groups: Number of groups to separate the channels into\n    - epsilon: Small constant for numerical stability\n\n    Returns:\n    - Normalized tensor as a Python list rounded to the nearest 4th decimal\n    \"\"\"\n    (B, C, H, W) = X.shape\n    assert C % num_groups == 0, 'Number of channels must be divisible by num_groups'\n    X_reshaped = X.reshape(B, num_groups, C // num_groups, H, W)\n    mean = np.mean(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    var = np.var(X_reshaped, axis=(2, 3, 4), keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    Y = gamma * X_normalized + beta\n    result = np.round(Y, 4).tolist()\n    return result"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n\n    def f(x):\n        return x ** 4 - 3 * x ** 3 + 2\n\n    def df(x):\n        return 4 * x ** 3 - 9 * x ** 2\n    x = start_x\n    for i in range(max_iters):\n        gradient = df(x)\n        new_x = x - learning_rate * gradient\n        if abs(new_x - x) < tolerance:\n            break\n        x = new_x\n    return round(x, 4)\nstart_x = np.random.rand() * 10"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Implement the Dynamic Tanh (DyT) function.\n    \n    Parameters:\n    - x: Input numpy array.\n    - alpha: Scaling parameter.\n    - gamma: Shift parameter.\n    - beta: Exponential scaling parameter.\n    \n    Returns:\n    - A list of floats representing the transformed values, rounded to the nearest 4th decimal.\n    \"\"\"\n    transformed = alpha * np.tanh(gamma + beta * x)\n    rounded_transformed = np.round(transformed, 4)\n    result_list = rounded_transformed.tolist()\n    return result_list"}
