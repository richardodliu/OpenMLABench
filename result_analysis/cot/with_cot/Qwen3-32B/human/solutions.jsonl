{"task_id": 1, "completion_id": 0, "solution": ""}
{"task_id": 2, "completion_id": 0, "solution": ""}
{"task_id": 3, "completion_id": 0, "solution": ""}
{"task_id": 4, "completion_id": 0, "solution": "def calculate_matrix_mean(matrix: list[list[float]], mode: str) -> list[float]:\n    \"\"\"\n    Calculate the mean of a matrix either by row or by column.\n\n    Parameters:\n    - matrix (list[list[float]]): A 2D list of floats representing the matrix.\n    - mode (str): Either 'row' or 'column' to specify the direction of the mean calculation.\n\n    Returns:\n    - list[float]: A list of mean values for each row or column, depending on the mode.\n    \"\"\"\n    if mode == 'row':\n        return [sum(row) / len(row) for row in matrix]\n    elif mode == 'column':\n        transposed = zip(*matrix)\n        return [sum(col) / len(col) for col in transposed]\n    else:\n        raise ValueError(\"Invalid mode. Use 'row' or 'column'.\")"}
{"task_id": 5, "completion_id": 0, "solution": "def scalar_multiply(matrix: list[list[int | float]], scalar: int | float) -> list[list[int | float]]:\n    \"\"\"\n    Multiplies each element of the matrix by the given scalar.\n\n    Parameters:\n    matrix (list[list[int | float]]): A 2D list representing the matrix.\n    scalar (int | float): The scalar value to multiply with each element.\n\n    Returns:\n    list[list[int | float]]: A new matrix with each element multiplied by the scalar.\n    \"\"\"\n    return [[element * scalar for element in row] for row in matrix]"}
{"task_id": 6, "completion_id": 0, "solution": "import math\ndef calculate_eigenvalues(matrix: list[list[float | int]]) -> list[float]:\n    a = matrix[0][0]\n    b = matrix[0][1]\n    c = matrix[1][0]\n    d = matrix[1][1]\n    trace = a + d\n    determinant = a * d - b * c\n    discriminant = trace ** 2 - 4 * determinant\n    sqrt_discriminant = math.sqrt(discriminant)\n    eigen1 = (trace + sqrt_discriminant) / 2\n    eigen2 = (trace - sqrt_discriminant) / 2\n    return [eigen1, eigen2]"}
{"task_id": 7, "completion_id": 0, "solution": "import numpy as np\ndef transform_matrix(A: list[list[int | float]], T: list[list[int | float]], S: list[list[int | float]]) -> list[list[float]]:\n    \"\"\"\n    Transforms matrix A using the operation T^{-1} * A * S.\n    \n    Parameters:\n        A (list[list[int | float]]): The input matrix to be transformed.\n        T (list[list[int | float]]): An invertible square matrix.\n        S (list[list[int | float]]): An invertible square matrix.\n    \n    Returns:\n        list[list[float]]: The transformed matrix rounded to 4 decimal places.\n        -1: If T or S is not invertible, or if the dimensions of A are incompatible.\n    \"\"\"\n    A_np = np.array(A)\n    T_np = np.array(T)\n    S_np = np.array(S)\n    try:\n        T_inv = np.linalg.inv(T_np)\n    except np.linalg.LinAlgError:\n        return -1\n    try:\n        S_inv = np.linalg.inv(S_np)\n    except np.linalg.LinAlgError:\n        return -1\n    n_T = T_np.shape[0]\n    m_S = S_np.shape[0]\n    if A_np.shape != (n_T, m_S):\n        return -1\n    result = T_inv @ A_np @ S_np\n    result_rounded = np.round(result, 4)\n    return result_rounded.tolist()"}
{"task_id": 8, "completion_id": 0, "solution": "def inverse_2x2(matrix: list[list[float]]) -> list[list[float]]:\n    (a, b) = (matrix[0][0], matrix[0][1])\n    (c, d) = (matrix[1][0], matrix[1][1])\n    determinant = a * d - b * c\n    if determinant == 0:\n        return None\n    inv_det = 1.0 / determinant\n    return [[d * inv_det, -b * inv_det], [-c * inv_det, a * inv_det]]"}
{"task_id": 9, "completion_id": 0, "solution": "def matrixmul(a: list[list[int | float]], b: list[list[int | float]]) -> list[list[int | float]]:\n    if not a or not b:\n        return -1\n    if len(a[0]) != len(b):\n        return -1\n    transposed_b = list(zip(*b))\n    result = []\n    for row in a:\n        new_row = []\n        for col in transposed_b:\n            dot_product = sum((row[i] * col[i] for i in range(len(row))))\n            new_row.append(dot_product)\n        result.append(new_row)\n    return result"}
{"task_id": 10, "completion_id": 0, "solution": "def calculate_covariance_matrix(vectors: list[list[float]]) -> list[list[float]]:\n    n = len(vectors[0])\n    d = len(vectors)\n    means = [sum(vec) / n for vec in vectors]\n    cov_matrix = [[0.0 for _ in range(d)] for _ in range(d)]\n    for i in range(d):\n        for j in range(d):\n            covariance = 0.0\n            for k in range(n):\n                covariance += (vectors[i][k] - means[i]) * (vectors[j][k] - means[j])\n            covariance /= n - 1\n            cov_matrix[i][j] = covariance\n    return cov_matrix"}
{"task_id": 11, "completion_id": 0, "solution": "import numpy as np\ndef solve_jacobi(A: np.ndarray, b: np.ndarray, n: int) -> list:\n    \"\"\"\n    Solves the system of linear equations Ax = b using the Jacobi iterative method.\n    \n    Parameters:\n    - A (np.ndarray): Coefficient matrix (n x n).\n    - b (np.ndarray): Right-hand side vector (n x 1).\n    - n (int): Number of iterations to perform.\n    \n    Returns:\n    - list: Approximate solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    x = np.zeros_like(b, dtype=np.float64)\n    for _ in range(n):\n        new_x = np.zeros_like(x)\n        for i in range(len(b)):\n            dot_product = np.dot(A[i], x)\n            new_x[i] = (b[i] - (dot_product - A[i, i] * x[i])) / A[i, i]\n        x = np.round(new_x, 4)\n    return x.tolist()"}
{"task_id": 12, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef svd_2x2_singular_values(A: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the singular values of a 2x2 matrix using the Jacobi method.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 real matrix.\n    \n    Returns:\n        tuple: A tuple of two floats, the singular values of A, rounded to 4 decimal places.\n    \"\"\"\n    B = np.dot(A.T, A)\n    b11 = B[0, 0]\n    b12 = B[0, 1]\n    b22 = B[1, 1]\n    if abs(b12) < 1e-12:\n        lambda1 = b11\n        lambda2 = b22\n    else:\n        delta = b11 - b22\n        epsilon = b12\n        numerator = -delta + math.sqrt(delta ** 2 + 4 * epsilon ** 2)\n        denominator = 2 * epsilon\n        t = numerator / denominator\n        denom = math.sqrt(1 + t ** 2)\n        c = 1.0 / denom\n        s = t / denom\n        new_b11 = c ** 2 * b11 + 2 * c * s * b12 + s ** 2 * b22\n        new_b22 = b11 + b22 - new_b11\n        lambda1 = new_b11\n        lambda2 = new_b22\n    lambda1 = max(lambda1, 0)\n    lambda2 = max(lambda2, 0)\n    sigma1 = math.sqrt(lambda1)\n    sigma2 = math.sqrt(lambda2)\n    if sigma1 < sigma2:\n        (sigma1, sigma2) = (sigma2, sigma1)\n    sigma1 = round(sigma1, 4)\n    sigma2 = round(sigma2, 4)\n    return (sigma1, sigma2)"}
{"task_id": 13, "completion_id": 0, "solution": "def determinant_4x4(matrix: list[list[int | float]]) -> float:\n\n    def determinant_3x3(m3: list[list[int | float]]) -> float:\n\n        def determinant_2x2(m2: list[list[int | float]]) -> float:\n            return m2[0][0] * m2[1][1] - m2[0][1] * m2[1][0]\n        det = 0.0\n        for j in range(3):\n            sign = (-1) ** (0 + j)\n            minor = []\n            for row in m3[1:]:\n                new_row = row[:j] + row[j + 1:]\n                minor.append(new_row)\n            det += m3[0][j] * sign * determinant_2x2(minor)\n        return det\n    det_total = 0.0\n    for col in range(4):\n        element = matrix[0][col]\n        sign = (-1) ** (0 + col)\n        minor_matrix = []\n        for row in matrix[1:]:\n            new_row = row[:col] + row[col + 1:]\n            minor_matrix.append(new_row)\n        minor_det = determinant_3x3(minor_matrix)\n        det_total += element * sign * minor_det\n    return det_total"}
{"task_id": 14, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_normal_equation(X: list[list[float]], y: list[float]) -> list[float]:\n    X_np = np.array(X)\n    y_np = np.array(y)\n    theta = np.linalg.inv(X_np.T @ X_np) @ X_np.T @ y_np\n    theta_rounded = np.round(theta, 4)\n    return theta_rounded.tolist()"}
{"task_id": 15, "completion_id": 0, "solution": "import numpy as np\ndef linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n    m = len(y)\n    theta = np.zeros(X.shape[1])\n    for _ in range(iterations):\n        error = X @ theta - y\n        gradient = X.T @ error / m\n        theta = theta - alpha * gradient\n    return np.round(theta, 4).tolist()"}
{"task_id": 16, "completion_id": 0, "solution": "import numpy as np\ndef feature_scaling(data: np.ndarray) -> (list[list[float]], list[list[float]]):\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    standardized_data = (data - means) / stds\n    mins = np.min(data, axis=0)\n    maxs = np.max(data, axis=0)\n    data_range = maxs - mins\n    minmax_data = (data - mins) / data_range\n    standardized_rounded = np.round(standardized_data, 4)\n    minmax_rounded = np.round(minmax_data, 4)\n    standardized_list = standardized_rounded.tolist()\n    minmax_list = minmax_rounded.tolist()\n    return (standardized_list, minmax_list)"}
{"task_id": 17, "completion_id": 0, "solution": "import numpy as np\ndef k_means_clustering(points: list[tuple[float, float]], k: int, initial_centroids: list[tuple[float, float]], max_iterations: int) -> list[tuple[float, float]]:\n    points_np = np.array(points, dtype=float)\n    centroids = np.array(initial_centroids, dtype=float)\n    for _ in range(max_iterations):\n        diffs = points_np[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n        distances_sq = np.sum(diffs ** 2, axis=2)\n        assignments = np.argmin(distances_sq, axis=1)\n        new_centroids = np.zeros_like(centroids)\n        for i in range(k):\n            cluster_points = points_np[assignments == i]\n            if cluster_points.size == 0:\n                new_centroids[i] = centroids[i]\n            else:\n                new_centroids[i] = np.mean(cluster_points, axis=0)\n        centroids = new_centroids\n    rounded_centroids = np.round(centroids, 4)\n    return [tuple(row) for row in rounded_centroids]"}
{"task_id": 18, "completion_id": 0, "solution": "import numpy as np\ndef k_fold_cross_validation(X: np.ndarray, y: np.ndarray, k=5, shuffle=True, random_seed=None):\n    \"\"\"\n    Generate train and test index splits for K-Fold Cross-Validation.\n\n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix of shape (n_samples, n_features)\n    y : np.ndarray\n        Target vector of shape (n_samples,)\n    k : int, optional (default=5)\n        Number of folds\n    shuffle : bool, optional (default=True)\n        Whether to shuffle the data before splitting\n    random_seed : int or None, optional (default=None)\n        Seed for random number generator for reproducibility\n\n    Returns:\n    --------\n    list of tuples\n        A list of k tuples, where each tuple contains:\n        (train_indices, test_indices)\n        Each is a numpy array of indices\n    \"\"\"\n    n_samples = X.shape[0]\n    indices = np.arange(n_samples)\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    fold_size = n_samples // k\n    remainder = n_samples % k\n    folds = []\n    for i in range(k):\n        if i < remainder:\n            start = i * (fold_size + 1)\n            end = start + fold_size + 1\n        else:\n            start = i * fold_size + remainder\n            end = start + fold_size\n        test_indices = indices[start:end]\n        train_indices = np.concatenate([indices[:start], indices[end:]])\n        folds.append((train_indices, test_indices))\n    return folds"}
{"task_id": 19, "completion_id": 0, "solution": "import numpy as np\ndef pca(data: np.ndarray, k: int) -> list[list[float]]:\n    X = data\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    X_centered = (X - X_mean) / X_std\n    cov_matrix = np.cov(X_centered, rowvars=False)\n    (eigenvalues, eigenvectors) = np.linalg.eigh(cov_matrix)\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n    principal_components = sorted_eigenvectors[:, :k]\n    result = []\n    for i in range(principal_components.shape[1]):\n        vec = np.round(principal_components[:, i].real, 4).tolist()\n        result.append(vec)\n    return result"}
{"task_id": 20, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef learn_decision_tree(examples: list[dict], attributes: list[str], target_attr: str) -> dict:\n\n    def entropy(examples_subset):\n        if not examples_subset:\n            return 0.0\n        counts = Counter((ex[target_attr] for ex in examples_subset))\n        total = len(examples_subset)\n        e = 0.0\n        for count in counts.values():\n            prob = count / total\n            e -= prob * math.log(prob, 2)\n        return e\n\n    def information_gain(attribute):\n        entropy_before = entropy(examples)\n        values = set((ex[attribute] for ex in examples))\n        weighted_entropy = 0.0\n        for value in values:\n            subset = [ex for ex in examples if ex[attribute] == value]\n            prob = len(subset) / len(examples)\n            weighted_entropy += prob * entropy(subset)\n        return entropy_before - weighted_entropy\n\n    def majority_value(examples_subset):\n        counts = Counter((ex[target_attr] for ex in examples_subset))\n        return counts.most_common(1)[0][0]\n\n    def select_best_attribute():\n        best_attr = None\n        max_gain = -1\n        for attr in attributes:\n            if attr == target_attr:\n                continue\n            gain = information_gain(attr)\n            if gain > max_gain:\n                max_gain = gain\n                best_attr = attr\n        return best_attr\n\n    def get_possible_values(attribute):\n        return set((ex[attribute] for ex in examples))\n    target_values = [ex[target_attr] for ex in examples]\n    if all((tv == target_values[0] for tv in target_values)):\n        return target_values[0]\n    if not attributes:\n        return majority_value(examples)\n    best_attr = select_best_attribute()\n    if best_attr is None:\n        return majority_value(examples)\n    tree = {best_attr: {}}\n    for value in get_possible_values(best_attr):\n        subset = [ex for ex in examples if ex[best_attr] == value]\n        remaining_attributes = [a for a in attributes if a != best_attr]\n        subtree = learn_decision_tree(subset, remaining_attributes, target_attr)\n        tree[best_attr][value] = subtree\n    return tree"}
{"task_id": 21, "completion_id": 0, "solution": "import numpy as np\ndef pegasos_kernel_svm(data: np.ndarray, labels: np.ndarray, kernel='linear', lambda_val=0.01, iterations=100, sigma=1.0):\n    \"\"\"\n    Implements a deterministic version of the Pegasos algorithm for training a kernel SVM.\n\n    Parameters:\n    - data: 2D NumPy array of shape (n_samples, n_features)\n    - labels: 1D NumPy array of shape (n_samples,) with values in {+1, -1}\n    - kernel: 'linear' or 'RBF'\n    - lambda_val: Regularization parameter\n    - iterations: Number of training iterations\n    - sigma: Kernel width for RBF kernel\n\n    Returns:\n    - alpha: List of alpha coefficients (rounded to 4 decimals)\n    - bias: Bias term (rounded to 4 decimals)\n    \"\"\"\n    (n, d) = data.shape\n    if kernel == 'linear':\n        K = data @ data.T\n    elif kernel == 'RBF':\n        sq_norms = np.sum(data ** 2, axis=1)\n        sq_dist = sq_norms.reshape(-1, 1) + sq_norms.reshape(1, -1) - 2 * data @ data.T\n        K = np.exp(-sq_dist / (2 * sigma ** 2))\n    else:\n        raise ValueError(\"Invalid kernel type. Use 'linear' or 'RBF'.\")\n    alpha = np.zeros(n)\n    b = 0.0\n    for t in range(1, iterations + 1):\n        eta = 1.0 / (lambda_val * t)\n        scores_kernel = K @ (alpha * labels)\n        scores = scores_kernel + b\n        margins = labels * scores\n        I = (margins < 1).astype(float)\n        alpha = (1 - eta * lambda_val) * alpha + eta / n * I\n        sum_I_y = np.sum(I * labels)\n        b = b + eta / n * sum_I_y\n    alpha_rounded = np.round(alpha, 4)\n    b_rounded = round(b, 4)\n    return (alpha_rounded.tolist(), b_rounded)"}
{"task_id": 22, "completion_id": 0, "solution": "import math\ndef sigmoid(z: float) -> float:\n    \"\"\"\n    Computes the output of the sigmoid activation function for a given input z.\n    \n    The sigmoid function is defined as:\n        \u03c3(z) = 1 / (1 + e^(-z))\n    \n    The result is rounded to four decimal places.\n    \n    Parameters:\n        z (float): The input value.\n    \n    Returns:\n        float: The output of the sigmoid function rounded to four decimal places.\n    \"\"\"\n    result = 1 / (1 + math.exp(-z))\n    return round(result, 4)"}
{"task_id": 23, "completion_id": 0, "solution": "import math\ndef softmax(scores: list[float]) -> list[float]:\n    max_score = max(scores)\n    exp_scores = [math.exp(x - max_score) for x in scores]\n    sum_exp = sum(exp_scores)\n    softmax_list = [x / sum_exp for x in exp_scores]\n    return [round(val, 4) for val in softmax_list]"}
{"task_id": 24, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef single_neuron_model(features: list[list[float]], labels: list[int], weights: list[float], bias: float) -> (list[float], float):\n    probs = []\n    for feature in features:\n        z = sum((f * w for (f, w) in zip(feature, weights))) + bias\n        prob = 1 / (1 + math.exp(-z))\n        probs.append(prob)\n    n = len(labels)\n    mse = sum(((p - l) ** 2 for (p, l) in zip(probs, labels))) / n\n    probs_array = np.array(probs)\n    rounded_probs = probs_array.round(4).tolist()\n    rounded_mse = round(mse, 4)\n    return (rounded_probs, rounded_mse)"}
{"task_id": 25, "completion_id": 0, "solution": "import numpy as np\ndef train_neuron(features: np.ndarray, labels: np.ndarray, initial_weights: np.ndarray, initial_bias: float, learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n    m = len(labels)\n    weights = initial_weights.copy()\n    bias = initial_bias\n    mse_list = []\n    for _ in range(epochs):\n        z = np.dot(features, weights) + bias\n        y_pred = 1 / (1 + np.exp(-z))\n        mse = np.mean((labels - y_pred) ** 2)\n        error = y_pred - labels\n        dL_dz = 2 * error * y_pred * (1 - y_pred) / m\n        grad_weights = np.dot(features.T, dL_dz)\n        grad_bias = np.sum(dL_dz)\n        weights -= learning_rate * grad_weights\n        bias -= learning_rate * grad_bias\n        mse_list.append(round(mse, 4))\n    rounded_weights = np.round(weights, 4)\n    rounded_bias = round(bias, 4)\n    return (rounded_weights.tolist(), rounded_bias, mse_list)"}
{"task_id": 26, "completion_id": 0, "solution": "class Value:\n\n    def __init__(self, data, _children=(), _op=''):\n        self.data = data\n        self.grad = 0\n        self._backward = lambda : None\n        self._prev = set(_children)\n        self._op = _op\n\n    def __add__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data + other.data, _children=(self, other), _op='ADD')\n\n        def _backward():\n            self.grad += out.grad\n            other.grad += out.grad\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Value) else Value(other)\n        out = Value(self.data * other.data, _children=(self, other), _op='MUL')\n\n        def _backward():\n            self.grad += other.data * out.grad\n            other.grad += self.data * out.grad\n        out._backward = _backward\n        return out\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self * other\n\n    def relu(self):\n        out = Value(self.data if self.data > 0 else 0, _children=(self,), _op='ReLU')\n\n        def _backward():\n            self.grad += (out.data > 0) * out.grad\n        out._backward = _backward\n        return out\n\n    def backward(self):\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if v not in visited:\n                visited.add(v)\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n        build_topo(self)\n        self.grad = 1.0\n        for node in reversed(topo):\n            node._backward()\n\n    def __repr__(self):\n        return f\"Value(data={self.data}, grad={self.grad}, _op='{self._op}')\""}
{"task_id": 27, "completion_id": 0, "solution": "import numpy as np\ndef transform_basis(B: list[list[int]], C: list[list[int]]) -> list[list[float]]:\n    \"\"\"\n    Computes the transformation matrix P from basis B to basis C in R^3.\n\n    The transformation matrix P satisfies [v]_C = P [v]_B, where [v]_B and [v]_C\n    are the coordinates of a vector v in bases B and C, respectively.\n\n    Parameters:\n    - B: A list of 3 vectors (each a list of 3 integers), representing basis B.\n    - C: A list of 3 vectors (each a list of 3 integers), representing basis C.\n\n    Returns:\n    - A 3x3 list of lists of floats, representing the transformation matrix P,\n      rounded to 4 decimal places.\n    \"\"\"\n    B_matrix = np.array(B).T\n    C_matrix = np.array(C).T\n    C_inv = np.linalg.inv(C_matrix)\n    P = C_inv @ B_matrix\n    P_rounded = np.round(P, 4)\n    return P_rounded.tolist()"}
{"task_id": 28, "completion_id": 0, "solution": "import numpy as np\ndef svd_2x2(A: np.ndarray) -> tuple:\n    \"\"\"\n    Compute the SVD of a 2x2 matrix using eigendecomposition of A^T A.\n    \n    Parameters:\n        A (np.ndarray): A 2x2 input matrix.\n    \n    Returns:\n        tuple: A tuple of three lists representing U, S, and V such that A = U * S * V^T.\n    \"\"\"\n    A_T_A = A.T @ A\n    (eigenvalues, eigenvectors) = np.linalg.eig(A_T_A)\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    sigma1 = np.sqrt(eigenvalues[0])\n    sigma2 = np.sqrt(eigenvalues[1])\n    if np.isclose(sigma1, 0):\n        U = np.eye(2)\n        S = np.zeros((2, 2))\n        V = eigenvectors\n    else:\n        v1 = eigenvectors[:, 0]\n        u1 = A @ v1 / sigma1\n        if np.isclose(sigma2, 0):\n            u2 = np.array([u1[1], -u1[0]])\n        else:\n            v2 = eigenvectors[:, 1]\n            u2 = A @ v2 / sigma2\n        U = np.column_stack((u1, u2))\n        S = np.diag([sigma1, sigma2])\n        V = eigenvectors\n    U_rounded = np.round(U, 4)\n    S_rounded = np.round(S, 4)\n    V_rounded = np.round(V, 4)\n    return (U_rounded.tolist(), S_rounded.tolist(), V_rounded.tolist())"}
{"task_id": 29, "completion_id": 0, "solution": "import numpy as np\ndef shuffle_data(X, y, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    permutation = np.random.permutation(len(X))\n    X_shuffled = X[permutation]\n    y_shuffled = y[permutation]\n    return (X_shuffled.tolist(), y_shuffled.tolist())"}
{"task_id": 30, "completion_id": 0, "solution": "import numpy as np\ndef batch_iterator(X, y=None, batch_size=64):\n    \"\"\"\n    A generator function that yields batches of data from a NumPy array X,\n    and optionally from a corresponding NumPy array y.\n\n    Parameters:\n    - X (np.ndarray): The input data array (samples x features).\n    - y (np.ndarray, optional): The corresponding label array (samples x ...).\n    - batch_size (int): The number of samples per batch.\n\n    Yields:\n    - If y is provided: A tuple (X_batch, y_batch), where each is a Python list.\n    - If y is not provided: A Python list containing the X_batch.\n    \"\"\"\n    for start in range(0, len(X), batch_size):\n        end = min(start + batch_size, len(X))\n        X_batch = X[start:end]\n        if y is not None:\n            y_batch = y[start:end]\n            yield (X_batch.tolist(), y_batch.tolist())\n        else:\n            yield X_batch.tolist()"}
{"task_id": 31, "completion_id": 0, "solution": "import numpy as np\ndef divide_on_feature(X, feature_i, threshold):\n    \"\"\"\n    Divides the dataset X into two subsets based on whether the value of a specified feature\n    is greater than or equal to a given threshold.\n\n    Parameters:\n    - X: numpy.ndarray, shape (n_samples, n_features)\n    - feature_i: int, index of the feature to evaluate\n    - threshold: float or int, the threshold value\n\n    Returns:\n    - left_subset: list of samples where feature_i >= threshold\n    - right_subset: list of samples where feature_i < threshold\n    \"\"\"\n    mask = X[:, feature_i] >= threshold\n    left_subset = X[mask].tolist()\n    right_subset = X[~mask].tolist()\n    return (left_subset, right_subset)"}
{"task_id": 32, "completion_id": 0, "solution": "import numpy as np\nfrom itertools import combinations_with_replacement\ndef polynomial_features(X, degree):\n    \"\"\"\n    Generate polynomial features up to the specified degree for a 2D input array.\n\n    Parameters:\n    - X (np.ndarray): A 2D NumPy array of shape (n_samples, n_features).\n    - degree (int): The maximum degree of the polynomial features to generate.\n\n    Returns:\n    - list: A 2D list of shape (n_samples, n_poly_features), where each row contains\n            the polynomial features for the corresponding sample in X.\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    all_indices = []\n    for d in range(0, degree + 1):\n        if d == 0:\n            all_indices.append(())\n        else:\n            for indices in combinations_with_replacement(range(n_features), d):\n                all_indices.append(indices)\n    columns = []\n    for indices in all_indices:\n        if not indices:\n            columns.append(np.ones(n_samples))\n        else:\n            product = np.prod(X[:, list(indices)], axis=1)\n            columns.append(product)\n    poly_X = np.column_stack(columns)\n    return poly_X.tolist()"}
{"task_id": 33, "completion_id": 0, "solution": "import numpy as np\ndef get_random_subsets(X, y, n_subsets, replacements=True, seed=42):\n    \"\"\"\n    Generate random subsets of a dataset.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - y: 1D numpy array of shape (n_samples,)\n    - n_subsets: Number of random subsets to generate\n    - replacements: Boolean indicating whether to sample with replacement\n    - seed: Random seed for reproducibility\n\n    Returns:\n    - List of tuples, where each tuple is (X_subset, y_subset) as Python lists\n    \"\"\"\n    np.random.seed(seed)\n    n_samples = X.shape[0]\n    subsets = []\n    for _ in range(n_subsets):\n        indices = np.random.choice(n_samples, size=n_samples, replace=replacements)\n        X_subset = X[indices]\n        y_subset = y[indices]\n        subsets.append((X_subset.tolist(), y_subset.tolist()))\n    return subsets"}
{"task_id": 34, "completion_id": 0, "solution": "import numpy as np\ndef to_categorical(x, n_col=None):\n    if n_col is None:\n        n_col = np.max(x) + 1\n    m = len(x)\n    result = np.zeros((m, n_col), dtype=int)\n    result[np.arange(m), x] = 1\n    return result.tolist()"}
{"task_id": 35, "completion_id": 0, "solution": "import numpy as np\ndef make_diagonal(x):\n    \"\"\"\n    Converts a 1D numpy array into a diagonal matrix and returns it as a list.\n    \n    Parameters:\n        x (np.ndarray): A 1D numpy array.\n    \n    Returns:\n        list: A 2D list representing the diagonal matrix.\n    \"\"\"\n    diagonal_matrix = np.diag(x)\n    return diagonal_matrix.tolist()"}
{"task_id": 36, "completion_id": 0, "solution": "import numpy as np\ndef accuracy_score(y_true, y_pred):\n    correct = np.sum(y_true == y_pred)\n    total = len(y_true)\n    accuracy = correct / total\n    return round(accuracy, 4)"}
{"task_id": 37, "completion_id": 0, "solution": "import numpy as np\ndef calculate_correlation_matrix(X, Y=None):\n    \"\"\"\n    Calculate the correlation matrix between the columns of X and Y (or X with itself if Y is None).\n    \n    Parameters:\n    - X (np.ndarray): A 2D numpy array of shape (n_samples, n_features).\n    - Y (np.ndarray, optional): A 2D numpy array of shape (n_samples, n_features). \n                                If None, the correlation matrix of X with itself is computed.\n    \n    Returns:\n    - list: A 2D list representing the correlation matrix, with all values rounded to 4 decimal places.\n    \"\"\"\n    if Y is None:\n        Y = X\n    X_centered = X - np.mean(X, axis=0)\n    Y_centered = Y - np.mean(Y, axis=0)\n    X_std = np.std(X, axis=0, ddof=1)\n    Y_std = np.std(Y, axis=0, ddof=1)\n    Z_X = X_centered / X_std\n    Z_Y = Y_centered / Y_std\n    n_samples = X.shape[0]\n    correlation_matrix = Z_X.T @ Z_Y / (n_samples - 1)\n    correlation_matrix = np.round(correlation_matrix, 4)\n    return correlation_matrix.tolist()"}
{"task_id": 38, "completion_id": 0, "solution": "import numpy as np\nimport math\ndef adaboost_fit(X, y, n_clf):\n    \"\"\"\n    Trains an AdaBoost classifier using decision stumps as weak learners.\n\n    Parameters:\n    - X: 2D numpy array of shape (n_samples, n_features)\n    - y: 1D numpy array of shape (n_samples,) with labels in {+1, -1}\n    - n_clf: Number of weak classifiers to train\n\n    Returns:\n    - List of classifiers, each represented as a list [feature_index, threshold, polarity, alpha]\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    D = np.full(n_samples, 1 / n_samples)\n    classifiers = []\n    for _ in range(n_clf):\n        best_error = 1.0\n        best_feature = 0\n        best_threshold = 0.0\n        best_polarity = 1\n        for j in range(n_features):\n            unique_vals = np.unique(X[:, j])\n            sorted_vals = np.sort(unique_vals)\n            possible_thresholds = []\n            if len(sorted_vals) == 0:\n                continue\n            possible_thresholds.append(sorted_vals[0] - 1e-05)\n            for i in range(len(sorted_vals) - 1):\n                possible_thresholds.append((sorted_vals[i] + sorted_vals[i + 1]) / 2)\n            possible_thresholds.append(sorted_vals[-1] + 1e-05)\n            for t in possible_thresholds:\n                mask = X[:, j] <= t\n                pred1 = np.where(mask, 1, -1)\n                error1 = np.sum(D * (pred1 != y))\n                error2 = 1.0 - error1\n                error = min(error1, error2)\n                polarity = 1 if error1 < error2 else -1\n                if error < best_error:\n                    best_error = error\n                    best_feature = j\n                    best_threshold = t\n                    best_polarity = polarity\n        if best_error >= 0.5:\n            break\n        epsilon = max(best_error, 1e-10)\n        alpha = 0.5 * math.log((1 - epsilon) / epsilon)\n        alpha = round(alpha, 4)\n        mask = X[:, best_feature] <= best_threshold\n        pred = np.where(mask, best_polarity, -best_polarity)\n        misclassified = (pred != y).astype(float)\n        D = D * np.exp(-alpha * (1 - 2 * misclassified))\n        D = D / np.sum(D)\n        rounded_threshold = round(best_threshold, 4)\n        classifiers.append([best_feature, rounded_threshold, best_polarity, alpha])\n    return classifiers"}
{"task_id": 39, "completion_id": 0, "solution": "import numpy as np\ndef log_softmax(scores: list):\n    scores_np = np.array(scores)\n    scores_max = np.max(scores_np)\n    x_centered = scores_np - scores_max\n    sum_exp = np.sum(np.exp(x_centered))\n    log_sum_exp = np.log(sum_exp)\n    log_softmax_result = x_centered - log_sum_exp\n    rounded_result = np.round(log_softmax_result, 4)\n    return rounded_result.tolist()"}
{"task_id": 40, "completion_id": 0, "solution": "import numpy as np\nimport copy\nimport math\nclass Layer(object):\n\n    def set_input_shape(self, shape):\n        self.input_shape = shape\n\n    def layer_name(self):\n        return self.__class__.__name__\n\n    def parameters(self):\n        return 0\n\n    def forward_pass(self, X, training):\n        raise NotImplementedError()\n\n    def backward_pass(self, accum_grad):\n        raise NotImplementedError()\n\n    def output_shape(self):\n        raise NotImplementedError()\nclass Dense(Layer):\n\n    def __init__(self, n_units, input_shape=None):\n        self.layer_input = None\n        self.input_shape = input_shape\n        self.n_units = n_units\n        self.trainable = True\n        self.W = None\n        self.w0 = None\n        self.W_opt = None\n        self.w0_opt = None\n\n    def initialize(self, optimizer):\n        limit = 1.0 / math.sqrt(self.input_shape[0])\n        self.W = np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n        self.w0 = np.zeros(self.n_units)\n        self.W_opt = optimizer\n        self.w0_opt = optimizer\n\n    def parameters(self):\n        return self.input_shape[0] * self.n_units + self.n_units\n\n    def forward_pass(self, X, training):\n        self.layer_input = X\n        output = np.dot(X, self.W) + self.w0\n        return np.round(output, 4).tolist()\n\n    def backward_pass(self, accum_grad):\n        dX = np.dot(accum_grad, self.W.T)\n        if self.trainable:\n            dW = np.dot(self.layer_input.T, accum_grad)\n            dw0 = np.sum(accum_grad, axis=0)\n            self.W = self.W_opt.update(self.W, dW)\n            self.w0 = self.w0_opt.update(self.w0, dw0)\n        return np.round(dX, 4).tolist()\n\n    def output_shape(self):\n        return (self.n_units,)"}
{"task_id": 41, "completion_id": 0, "solution": "import numpy as np\ndef simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n    (input_h, input_w) = input_matrix.shape\n    (kernel_h, kernel_w) = kernel.shape\n    padded = np.pad(input_matrix, pad_width=padding, mode='constant', constant_values=0)\n    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n    output = np.zeros((output_h, output_w))\n    for i in range(output_h):\n        for j in range(output_w):\n            start_h = i * stride\n            start_w = j * stride\n            region = padded[start_h:start_h + kernel_h, start_w:start_w + kernel_w]\n            conv_val = np.sum(region * kernel)\n            output[i, j] = conv_val\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 42, "completion_id": 0, "solution": ""}
{"task_id": 43, "completion_id": 0, "solution": "import numpy as np\ndef ridge_loss(X: np.ndarray, w: np.ndarray, y_true: np.ndarray, alpha: float) -> float:\n    y_pred = X.dot(w)\n    mse = np.mean((y_true - y_pred) ** 2)\n    reg_term = alpha * np.sum(w ** 2)\n    loss = mse + reg_term\n    return round(float(loss), 4)"}
{"task_id": 44, "completion_id": 0, "solution": "def leaky_relu(z: float, alpha: float=0.01) -> float | int:\n    if z > 0:\n        return z\n    else:\n        return alpha * z"}
{"task_id": 45, "completion_id": 0, "solution": "import numpy as np\ndef kernel_function(x1, x2):\n    \"\"\"\n    Compute the linear kernel between two input vectors x1 and x2.\n    \n    Parameters:\n    x1 (array-like): First input vector.\n    x2 (array-like): Second input vector.\n    \n    Returns:\n    float: The dot product (linear kernel) of x1 and x2.\n    \"\"\"\n    return np.dot(x1, x2)"}
{"task_id": 46, "completion_id": 0, "solution": "import numpy as np\ndef precision(y_true, y_pred):\n    \"\"\"\n    Calculate the precision metric for binary classification.\n\n    Precision is defined as the ratio of true positives (TP) to the sum of true positives\n    and false positives (FP): precision = TP / (TP + FP)\n\n    Parameters:\n    y_true (np.ndarray): True binary labels (0 or 1).\n    y_pred (np.ndarray): Predicted binary labels (0 or 1).\n\n    Returns:\n    float: The precision score. Returns 0.0 if there are no positive predictions (i.e., TP + FP == 0).\n    \"\"\"\n    true_positives = np.sum((y_true == 1) & (y_pred == 1))\n    false_positives = np.sum((y_true == 0) & (y_pred == 1))\n    if true_positives + false_positives == 0:\n        return 0.0\n    return true_positives / (true_positives + false_positives)"}
{"task_id": 47, "completion_id": 0, "solution": "import numpy as np\ndef gradient_descent(X, y, weights, learning_rate, n_iterations, batch_size=1, method='batch'):\n    \"\"\"\n    Perform gradient descent optimization using one of the following methods:\n    - 'batch': Full-batch gradient descent\n    - 'sgd': Stochastic gradient descent\n    - 'mini-batch': Mini-batch gradient descent\n\n    Parameters:\n    - X: numpy.ndarray of shape (n_samples, n_features)\n    - y: numpy.ndarray of shape (n_samples,)\n    - weights: numpy.ndarray of shape (n_features,)\n    - learning_rate: float\n    - n_iterations: int\n    - batch_size: int (used only for 'mini-batch')\n    - method: str, one of ['batch', 'sgd', 'mini-batch']\n\n    Returns:\n    - weights: list of floats, rounded to 4 decimal places\n    \"\"\"\n    weights = weights.copy()\n    n_samples = X.shape[0]\n    for _ in range(n_iterations):\n        if method == 'batch':\n            actual_batch_size = n_samples\n        elif method == 'sgd':\n            actual_batch_size = 1\n        else:\n            actual_batch_size = batch_size\n        indices = np.random.choice(n_samples, actual_batch_size, replace=False)\n        X_batch = X[indices]\n        y_batch = y[indices]\n        predictions = X_batch @ weights\n        error = predictions - y_batch\n        gradient = 2 * X_batch.T @ error / actual_batch_size\n        weights -= learning_rate * gradient\n    return np.round(weights, 4).tolist()"}
{"task_id": 48, "completion_id": 0, "solution": "import numpy as np\ndef rref(matrix):\n    matrix = np.array(matrix, dtype=float)\n    (rows, cols) = matrix.shape\n    pivot_row = 0\n    for col in range(cols):\n        possible_pivots = matrix[pivot_row:, col]\n        non_zero = np.nonzero(possible_pivots)[0]\n        if len(non_zero) == 0:\n            continue\n        swap_row = pivot_row + non_zero[0]\n        matrix[[pivot_row, swap_row], :] = matrix[[swap_row, pivot_row], :]\n        lead = matrix[pivot_row, col]\n        matrix[pivot_row, :] /= lead\n        for r in range(rows):\n            if r != pivot_row:\n                factor = matrix[r, col]\n                matrix[r, :] -= factor * matrix[pivot_row, :]\n        pivot_row += 1\n        if pivot_row >= rows:\n            break\n    return matrix.tolist()"}
{"task_id": 49, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, num_iterations=10):\n    x = np.array(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    for t in range(1, num_iterations + 1):\n        grad_t = grad(x)\n        m = beta1 * m + (1 - beta1) * grad_t\n        v = beta2 * v + (1 - beta2) * grad_t ** 2\n        m_hat = m / (1 - beta1 ** t)\n        v_hat = v / (1 - beta2 ** t)\n        x -= learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    return np.round(x, 4).tolist()"}
{"task_id": 50, "completion_id": 0, "solution": "import numpy as np\ndef l1_regularization_gradient_descent(X: np.array, y: np.array, alpha: float=0.1, learning_rate: float=0.01, max_iter: int=1000, tol: float=0.0001) -> tuple:\n    \"\"\"\n    Implements Lasso Regression using Gradient Descent with L1 Regularization.\n\n    Parameters:\n    - X (np.array): Input feature matrix of shape (n_samples, n_features)\n    - y (np.array): Target vector of shape (n_samples,)\n    - alpha (float): L1 regularization strength\n    - learning_rate (float): Step size for gradient descent\n    - max_iter (int): Maximum number of iterations\n    - tol (float): Tolerance for early stopping based on parameter change\n\n    Returns:\n    - tuple: (w, b) where w is a list of weights and b is the bias, both rounded to 4 decimal places\n    \"\"\"\n    (n_samples, n_features) = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    for _ in range(max_iter):\n        y_pred = np.dot(X, w) + b\n        residuals = y_pred - y\n        gradient_w = np.dot(X.T, residuals) / n_samples + alpha * np.sign(w)\n        gradient_b = np.sum(residuals) / n_samples\n        prev_w = w.copy()\n        prev_b = b\n        w -= learning_rate * gradient_w\n        b -= learning_rate * gradient_b\n        w_change = np.linalg.norm(w - prev_w)\n        b_change = abs(b - prev_b)\n        if w_change < tol and b_change < tol:\n            break\n    w_rounded = np.round(w, 4)\n    b_rounded = round(b, 4)\n    return (w_rounded.tolist(), b_rounded)"}
{"task_id": 51, "completion_id": 0, "solution": "import numpy as np\ndef OSA(source: str, target: str) -> int:\n    \"\"\"\n    Computes the Optimal String Alignment Distance between two strings.\n    \n    Parameters:\n    - source (str): The original string.\n    - target (str): The target string to transform into.\n    \n    Returns:\n    - int: The minimum number of edit operations (insert, delete, substitute, transpose) needed.\n    \"\"\"\n    m = len(source)\n    n = len(target)\n    dp = np.zeros((m + 1, n + 1), dtype=int)\n    for i in range(m + 1):\n        dp[i][0] = i\n    for j in range(n + 1):\n        dp[0][j] = j\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if source[i - 1] == target[j - 1]:\n                dp[i][j] = dp[i - 1][j - 1]\n            else:\n                insert = dp[i][j - 1] + 1\n                delete = dp[i - 1][j] + 1\n                substitute = dp[i - 1][j - 1] + 1\n                min_val = min(insert, delete, substitute)\n                if i >= 2 and j >= 2:\n                    if source[i - 1] == target[j - 2] and source[i - 2] == target[j - 1]:\n                        transpose = dp[i - 2][j - 2] + 1\n                        min_val = min(min_val, transpose)\n                dp[i][j] = min_val\n    return dp[m][n]"}
{"task_id": 52, "completion_id": 0, "solution": "def recall(y_true, y_pred):\n    \"\"\"\n    Calculate the recall metric for binary classification.\n\n    Parameters:\n    - y_true (list): List of true binary labels (0 or 1).\n    - y_pred (list): List of predicted binary labels (0 or 1).\n\n    Returns:\n    - float: Recall value rounded to three decimal places.\n    \"\"\"\n    tp = sum((t == 1 and p == 1 for (t, p) in zip(y_true, y_pred)))\n    fn = sum((t == 1 and p == 0 for (t, p) in zip(y_true, y_pred)))\n    denominator = tp + fn\n    if denominator == 0:\n        return 0.0\n    return round(tp / denominator, 3)"}
{"task_id": 53, "completion_id": 0, "solution": "import numpy as np\ndef self_attention(X, W_q, W_k, W_v):\n    \"\"\"\n    Computes the self-attention mechanism output.\n\n    Parameters:\n    - X: Input sequence (numpy array of shape (n, d))\n    - W_q: Query weight matrix (numpy array of shape (d, d_k))\n    - W_k: Key weight matrix (numpy array of shape (d, d_k))\n    - W_v: Value weight matrix (numpy array of shape (d, d_v))\n\n    Returns:\n    - Output of self-attention as a Python list (after rounding to 4 decimal places)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    d_k = K.shape[1]\n    scores = Q @ K.T / np.sqrt(d_k)\n    scores_exp = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = scores_exp / np.sum(scores_exp, axis=1, keepdims=True)\n    output = attention_weights @ V\n    return np.round(output, 4).tolist()"}
{"task_id": 54, "completion_id": 0, "solution": "import numpy as np\ndef rnn_forward(input_sequence: list[list[float]], initial_hidden_state: list[float], Wx: list[list[float]], Wh: list[list[float]], b: list[float]) -> list[float]:\n    h_prev = np.array(initial_hidden_state)\n    Wx = np.array(Wx)\n    Wh = np.array(Wh)\n    b = np.array(b)\n    for x_t in input_sequence:\n        x_t = np.array(x_t)\n        pre_activation = np.dot(Wx, x_t) + np.dot(Wh, h_prev) + b\n        h_prev = np.tanh(pre_activation)\n    return np.round(h_prev, 4).tolist()"}
{"task_id": 55, "completion_id": 0, "solution": "import numpy as np\ndef translate_object(points, tx, ty):\n    \"\"\"\n    Applies a 2D translation to a list of [x, y] points using a translation matrix.\n\n    Parameters:\n    - points: List of [x, y] coordinates.\n    - tx: Translation distance in the x-direction.\n    - ty: Translation distance in the y-direction.\n\n    Returns:\n    - A new list of translated [x, y] coordinates.\n    \"\"\"\n    points_np = np.array(points)\n    homogeneous = np.column_stack((points_np, np.ones(len(points))))\n    T = np.array([[1, 0, 0], [0, 1, 0], [tx, ty, 1]])\n    translated_homogeneous = homogeneous @ T\n    translated_points = translated_homogeneous[:, :2]\n    return translated_points.tolist()"}
{"task_id": 56, "completion_id": 0, "solution": "import numpy as np\ndef kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n    \"\"\"\n    Compute the Kullback-Leibler divergence between two univariate normal distributions:\n    P ~ N(mu_p, sigma_p^2) and Q ~ N(mu_q, sigma_q^2)\n\n    Parameters:\n    - mu_p (float): Mean of distribution P\n    - sigma_p (float): Standard deviation of distribution P\n    - mu_q (float): Mean of distribution Q\n    - sigma_q (float): Standard deviation of distribution Q\n\n    Returns:\n    - kl (float): KL divergence D_KL(P || Q)\n    \"\"\"\n    mu_diff = mu_p - mu_q\n    mu_diff_sq = mu_diff ** 2\n    sigma_p_sq = sigma_p ** 2\n    sigma_q_sq = sigma_q ** 2\n    kl = (mu_diff_sq + sigma_p_sq) / (2 * sigma_q_sq) - 0.5 + np.log(sigma_q / sigma_p)\n    return kl"}
{"task_id": 57, "completion_id": 0, "solution": "import numpy as np\ndef gauss_seidel(A, b, n, x_ini=None):\n    \"\"\"\n    Solves a system of linear equations Ax = b using the Gauss-Seidel iterative method.\n\n    Parameters:\n    - A (list of lists or numpy array): Square matrix of coefficients.\n    - b (list or numpy array): Right-hand side vector.\n    - n (int): Number of iterations to perform.\n    - x_ini (list or numpy array, optional): Initial guess for the solution vector x. \n      If not provided, a vector of zeros is used.\n\n    Returns:\n    - list: Approximated solution vector x after n iterations, rounded to 4 decimal places.\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    m = len(A)\n    if x_ini is None:\n        x = np.zeros(m)\n    else:\n        x = np.array(x_ini, dtype=float)\n    for _ in range(n):\n        for i in range(m):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n    return np.around(x, 4).tolist()"}
{"task_id": 58, "completion_id": 0, "solution": "import numpy as np\ndef gaussian_elimination(A, b):\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    n = len(A)\n    aug = np.hstack((A, b.reshape(n, 1)))\n    for i in range(n):\n        max_row = i + np.argmax(np.abs(aug[i:, i]))\n        aug[[i, max_row]] = aug[[max_row, i]]\n        for r in range(i + 1, n):\n            factor = aug[r, i] / aug[i, i]\n            aug[r, i:] = aug[r, i:] - factor * aug[i, i:]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        sum_val = 0.0\n        for j in range(i + 1, n):\n            sum_val += aug[i, j] * x[j]\n        x[i] = (aug[i, -1] - sum_val) / aug[i, i]\n    x = np.round(x, 4)\n    return x.tolist()"}
{"task_id": 59, "completion_id": 0, "solution": "import numpy as np\nclass LSTM:\n\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)\n        self.bf = np.zeros((hidden_size, 1))\n        self.bi = np.zeros((hidden_size, 1))\n        self.bc = np.zeros((hidden_size, 1))\n        self.bo = np.zeros((hidden_size, 1))\n\n    def forward(self, x, initial_hidden_state, initial_cell_state):\n        \"\"\"\n        Processes a sequence of inputs and returns the hidden states at each time step,\n        as well as the final hidden state and final cell state.\n\n        Parameters:\n        - x: numpy array of shape (T, input_size), where T is the sequence length\n        - initial_hidden_state: numpy array of shape (hidden_size, 1)\n        - initial_cell_state: numpy array of shape (hidden_size, 1)\n\n        Returns:\n        - hidden_states: list of shape (T, hidden_size)\n        - final_hidden_state: list of shape (hidden_size, 1)\n        - final_cell_state: list of shape (hidden_size, 1)\n        \"\"\"\n        x = np.array(x)\n        T = x.shape[0]\n        input_size = self.input_size\n        hidden_size = self.hidden_size\n        h_prev = initial_hidden_state\n        c_prev = initial_cell_state\n        hidden_states = []\n        for t in range(T):\n            x_t = x[t].reshape(input_size, 1)\n            concat = np.concatenate((h_prev, x_t), axis=0)\n            fg = self.Wf @ concat + self.bf\n            f_t = 1 / (1 + np.exp(-fg))\n            ig = self.Wi @ concat + self.bi\n            i_t = 1 / (1 + np.exp(-ig))\n            c_tilda_g = self.Wc @ concat + self.bc\n            c_tilda = np.tanh(c_tilda_g)\n            og = self.Wo @ concat + self.bo\n            o_t = 1 / (1 + np.exp(-og))\n            c_curr = f_t * c_prev + i_t * c_tilda\n            h_curr = o_t * np.tanh(c_curr)\n            hidden_states.append(h_curr.flatten())\n            h_prev = h_curr\n            c_prev = c_curr\n        hidden_states_array = np.array(hidden_states)\n        final_hidden = h_prev\n        final_cell = c_prev\n        hidden_states_list = np.round(hidden_states_array, 4).tolist()\n        final_hidden_list = np.round(final_hidden, 4).tolist()\n        final_cell_list = np.round(final_cell, 4).tolist()\n        return (hidden_states_list, final_hidden_list, final_cell_list)"}
{"task_id": 60, "completion_id": 0, "solution": "import numpy as np\ndef compute_tf_idf(corpus, query):\n    \"\"\"\n    Computes the TF-IDF scores for each query word in each document of the corpus.\n\n    Parameters:\n    - corpus (list of list of str): A list of documents, where each document is a list of words.\n    - query (list of str): A list of words for which to compute TF-IDF scores.\n\n    Returns:\n    - list of list of float: A list of lists where each sublist contains the TF-IDF scores\n      for the query words in the corresponding document, rounded to 4 decimal places.\n    \"\"\"\n    if not corpus:\n        return []\n    N = len(corpus)\n    unique_terms_per_doc = [set(doc) for doc in corpus]\n    df = {}\n    for term in query:\n        count = 0\n        for doc_set in unique_terms_per_doc:\n            if term in doc_set:\n                count += 1\n        df[term] = count\n    idf_values = {}\n    for term in query:\n        idf = np.log((N + 1) / (df[term] + 1))\n        idf_values[term] = idf\n    result = []\n    for doc in corpus:\n        doc_length = len(doc)\n        doc_scores = []\n        if doc_length == 0:\n            doc_scores = [0.0] * len(query)\n        else:\n            for term in query:\n                count = doc.count(term)\n                tf = count / doc_length\n                idf = idf_values[term]\n                doc_scores.append(tf * idf)\n        result.append(doc_scores)\n    return np.round(np.array(result), 4).tolist()"}
{"task_id": 61, "completion_id": 0, "solution": "import numpy as np\ndef f_score(y_true, y_pred, beta):\n    \"\"\"\n    Calculate F-Score for a binary classification task.\n\n    The F-Score is a weighted harmonic mean of Precision and Recall,\n    where the weight is determined by the beta parameter. When beta = 1,\n    it becomes the F1-Score, which equally weights Precision and Recall.\n\n    :param y_true: Numpy array of true binary labels (0 or 1)\n    :param y_pred: Numpy array of predicted binary labels (0 or 1)\n    :param beta: A float value that adjusts the weight of Precision in the harmonic mean\n    :return: F-Score rounded to three decimal places\n    \"\"\"\n    TP = np.sum((y_true == 1) & (y_pred == 1))\n    FP = np.sum((y_true == 0) & (y_pred == 1))\n    FN = np.sum((y_true == 1) & (y_pred == 0))\n    if TP + FP == 0:\n        precision = 0.0\n    else:\n        precision = TP / (TP + FP)\n    if TP + FN == 0:\n        recall = 0.0\n    else:\n        recall = TP / (TP + FN)\n    beta_sq = beta ** 2\n    numerator = (1 + beta_sq) * precision * recall\n    denominator = beta_sq * precision + recall\n    if denominator == 0:\n        return 0.0\n    else:\n        return round(numerator / denominator, 3)"}
{"task_id": 62, "completion_id": 0, "solution": "import numpy as np\nclass SimpleRNN:\n\n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"\n        Initializes the RNN with random weights and zero biases.\n        Weights are initialized with small random values (scaled by 0.01),\n        and biases are initialized to zero.\n        \"\"\"\n        self.hidden_size = hidden_size\n        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01\n        self.b_h = np.zeros((hidden_size, 1))\n        self.b_y = np.zeros((output_size, 1))\n\n    def forward(self, input_sequence):\n        \"\"\"\n        Performs a forward pass through the RNN for a given sequence of inputs.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1)\n        \n        Returns:\n        - outputs: A list of output vectors, each of shape (output_size, 1)\n        - inputs: A list of input vectors (same as input_sequence)\n        - hiddens: A list of hidden states, including the initial zero state\n        \"\"\"\n        h_prev = np.zeros((self.hidden_size, 1))\n        hiddens = [h_prev]\n        inputs = []\n        outputs = []\n        for x in input_sequence:\n            h = np.tanh(self.W_xh @ x + self.W_hh @ h_prev + self.b_h)\n            y = self.W_hy @ h + self.b_y\n            hiddens.append(h)\n            inputs.append(x)\n            outputs.append(y)\n            h_prev = h\n        return (outputs, inputs, hiddens)\n\n    def backward(self, input_sequence, expected_output, outputs, last_inputs, last_hiddens, learning_rate):\n        \"\"\"\n        Performs backpropagation through time (BPTT) to update the RNN's parameters.\n        \n        Parameters:\n        - input_sequence: A list of input vectors, each of shape (input_size, 1)\n        - expected_output: A list of target vectors, each of shape (output_size, 1)\n        - outputs: A list of output vectors from the forward pass\n        - last_inputs: A list of input vectors (same as input_sequence)\n        - last_hiddens: A list of hidden states from the forward pass\n        - learning_rate: The learning rate for gradient descent\n        \"\"\"\n        T = len(input_sequence)\n        dW_xh = np.zeros_like(self.W_xh)\n        dW_hh = np.zeros_like(self.W_hh)\n        dW_hy = np.zeros_like(self.W_hy)\n        db_h = np.zeros_like(self.b_h)\n        db_y = np.zeros_like(self.b_y)\n        dh_next = np.zeros_like(last_hiddens[0])\n        for t in reversed(range(T)):\n            x_t = last_inputs[t]\n            h_prev = last_hiddens[t]\n            h_current = last_hiddens[t + 1]\n            dy = outputs[t] - expected_output[t]\n            dW_hy += dy @ h_current.T\n            db_y += dy\n            dh = self.W_hy.T @ dy + dh_next\n            dh_raw = (1 - h_current ** 2) * dh\n            dW_xh += dh_raw @ x_t.T\n            dW_hh += dh_raw @ h_prev.T\n            db_h += dh_raw\n            dh_next = self.W_hh.T @ dh_raw\n        self.W_xh -= learning_rate * dW_xh\n        self.W_hh -= learning_rate * dW_hh\n        self.W_hy -= learning_rate * dW_hy\n        self.b_h -= learning_rate * db_h\n        self.b_y -= learning_rate * db_y"}
{"task_id": 63, "completion_id": 0, "solution": "import numpy as np\ndef conjugate_gradient(A: np.array, b: np.array, n: int, x0: np.array=None, tol=1e-08):\n    \"\"\"\n    Solve the system Ax = b using the Conjugate Gradient method.\n\n    :param A: Symmetric positive-definite matrix (2D numpy array)\n    :param b: Right-hand side vector (1D numpy array)\n    :param n: Maximum number of iterations (int)\n    :param x0: Initial guess for solution (1D numpy array, optional)\n    :param tol: Convergence tolerance (float)\n    :return: Solution vector x (rounded to 8 decimal places and converted to list)\n    \"\"\"\n    if x0 is None:\n        x = np.zeros_like(b)\n    else:\n        x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    for _ in range(n):\n        Ap = A @ p\n        r_dot_r = np.dot(r, r)\n        p_dot_Ap = np.dot(p, Ap)\n        alpha = r_dot_r / p_dot_Ap\n        x = x + alpha * p\n        r_new = r - alpha * Ap\n        if np.linalg.norm(r_new) < tol:\n            break\n        beta = np.dot(r_new, r_new) / r_dot_r\n        p = r_new + beta * p\n        r = r_new\n    return np.round(x, 8).tolist()"}
{"task_id": 64, "completion_id": 0, "solution": "import numpy as np\ndef gini_impurity(y: list[int]) -> float:\n    \"\"\"\n    Calculate Gini Impurity for a list of class labels.\n\n    :param y: List of class labels\n    :return: Gini Impurity rounded to three decimal places\n    \"\"\"\n    (unique_classes, counts) = np.unique(y, return_counts=True)\n    total = len(y)\n    probabilities = counts / total\n    sum_squares = np.sum(probabilities ** 2)\n    gini = 1 - sum_squares\n    return round(gini, 3)"}
{"task_id": 65, "completion_id": 0, "solution": "def compressed_row_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix to its Compressed Row Sparse (CSR) representation.\n\n    :param dense_matrix: 2D list representing a dense matrix\n    :return: A tuple containing (values array, column indices array, row pointer array)\n    \"\"\"\n    values = []\n    column_indices = []\n    row_counts = []\n    for row in dense_matrix:\n        row_nonzeros = []\n        for (col_idx, val) in enumerate(row):\n            if val != 0:\n                row_nonzeros.append((val, col_idx))\n        row_counts.append(len(row_nonzeros))\n        for (val, col) in row_nonzeros:\n            values.append(val)\n            column_indices.append(col)\n    row_pointer = []\n    cumulative = 0\n    for count in row_counts:\n        row_pointer.append(cumulative)\n        cumulative += count\n    row_pointer.append(cumulative)\n    return (values, column_indices, row_pointer)"}
{"task_id": 66, "completion_id": 0, "solution": "def orthogonal_projection(v, L):\n    \"\"\"\n    Compute the orthogonal projection of vector v onto line L.\n\n    The orthogonal projection of a vector v onto a line defined by vector L\n    is the vector on L that is closest to v. This is calculated using the formula:\n\n        proj_L(v) = ( (v \u00b7 L) / (L \u00b7 L) ) * L\n\n    :param v: The vector to be projected (list of floats or integers)\n    :param L: The line vector defining the direction of projection (list of floats or integers)\n    :return: List representing the projection of v onto L, rounded to three decimal places\n    \"\"\"\n    dot_vL = sum((x * y for (x, y) in zip(v, L)))\n    dot_LL = sum((x * x for x in L))\n    t = dot_vL / dot_LL\n    projection = [round(t * x, 3) for x in L]\n    return projection"}
{"task_id": 67, "completion_id": 0, "solution": "def compressed_col_sparse_matrix(dense_matrix):\n    \"\"\"\n    Convert a dense matrix into its Compressed Column Sparse (CSC) representation.\n\n    :param dense_matrix: List of lists representing the dense matrix\n    :return: Tuple of (values, row indices, column pointer)\n    \"\"\"\n    values = []\n    row_indices = []\n    column_pointer = []\n    current_pos = 0\n    num_rows = len(dense_matrix)\n    if num_rows == 0:\n        column_pointer.append(0)\n        return (values, row_indices, column_pointer)\n    num_cols = len(dense_matrix[0])\n    for c in range(num_cols):\n        column_pointer.append(current_pos)\n        for r in range(num_rows):\n            val = dense_matrix[r][c]\n            if val != 0:\n                values.append(val)\n                row_indices.append(r)\n                current_pos += 1\n    column_pointer.append(current_pos)\n    return (values, row_indices, column_pointer)"}
{"task_id": 68, "completion_id": 0, "solution": "import numpy as np\ndef matrix_image(A):\n    \"\"\"\n    Computes the basis for the column space (image) of a matrix A.\n    \n    Parameters:\n        A (list of lists or 2D array): The input matrix.\n    \n    Returns:\n        list of lists: A list of basis vectors (rounded to 8 decimal places).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    (q, r, p) = np.linalg.qr(A, mode='reduced', pivoting=True)\n    rank = np.linalg.matrix_rank(A)\n    basis_cols = A[:, p[:rank]]\n    return np.round(basis_cols, 8).tolist()"}
{"task_id": 69, "completion_id": 0, "solution": "import numpy as np\ndef r_squared(y_true, y_pred):\n    y_mean = np.mean(y_true)\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_mean) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    return round(r2, 3)"}
{"task_id": 70, "completion_id": 0, "solution": "def calculate_brightness(img):\n    \"\"\"\n    Calculates the average brightness of a grayscale image represented as a 2D matrix.\n    \n    Parameters:\n        img (List[List[int]]): A 2D matrix where each element is a pixel value between 0 and 255.\n    \n    Returns:\n        float: The average brightness rounded to two decimal places.\n        int: -1 if any of the edge cases are encountered.\n    \n    Edge Cases Handled:\n        - Empty image matrix\n        - Inconsistent row lengths\n        - Pixel values outside the valid range (0-255)\n        - Zero columns in a non-empty matrix\n    \"\"\"\n    if not img:\n        return -1\n    rows = len(img)\n    if rows == 0:\n        return -1\n    cols = len(img[0])\n    if cols == 0:\n        return -1\n    for row in img:\n        if len(row) != cols:\n            return -1\n    total = 0\n    for row in img:\n        for pixel in row:\n            if not 0 <= pixel <= 255:\n                return -1\n            total += pixel\n    total_pixels = rows * cols\n    average = total / total_pixels\n    return round(average, 2)"}
{"task_id": 71, "completion_id": 0, "solution": "import numpy as np\ndef rmse(y_true, y_pred):\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between two arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth (correct) target values.\n    y_pred (array-like): Estimated target values.\n\n    Returns:\n    float: The RMSE value rounded to three decimal places.\n\n    Raises:\n    ValueError: If the input arrays are empty or have mismatched shapes.\n    TypeError: If the input arrays are not numeric.\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    if not (np.issubdtype(y_true.dtype, np.number) and np.issubdtype(y_pred.dtype, np.number)):\n        raise TypeError('Both y_true and y_pred must be numeric arrays.')\n    if y_true.shape != y_pred.shape:\n        raise ValueError('Shapes of y_true and y_pred do not match.')\n    if y_true.size == 0:\n        raise ValueError('Input arrays are empty.')\n    squared_diff = (y_true - y_pred) ** 2\n    mean_squared_diff = np.mean(squared_diff)\n    rmse_value = np.sqrt(mean_squared_diff)\n    return round(rmse_value, 3)"}
{"task_id": 72, "completion_id": 0, "solution": "import numpy as np\ndef jaccard_index(y_true, y_pred):\n    \"\"\"\n    Calculate the Jaccard Index between two binary arrays.\n\n    The Jaccard Index is defined as the size of the intersection\n    divided by the size of the union of the two sets.\n\n    Parameters:\n    y_true (array-like): Ground truth binary labels.\n    y_pred (array-like): Predicted binary labels.\n\n    Returns:\n    float: Jaccard Index rounded to three decimal places.\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    intersection = np.sum(y_true * y_pred)\n    union = np.sum(y_true + y_pred - y_true * y_pred)\n    if union == 0:\n        return 0.0\n    jaccard = intersection / union\n    return round(jaccard, 3)"}
{"task_id": 73, "completion_id": 0, "solution": "import numpy as np\ndef dice_score(y_true, y_pred):\n    \"\"\"\n    Calculate the Dice Score (S\u00f8rensen-Dice coefficient) between two binary arrays.\n\n    Parameters:\n    y_true (array-like): Ground truth binary array.\n    y_pred (array-like): Predicted binary array.\n\n    Returns:\n    float: Dice Score rounded to 3 decimal places.\n    \"\"\"\n    intersection = np.sum(np.array(y_true) * np.array(y_pred))\n    sum_true = np.sum(y_true)\n    sum_pred = np.sum(y_pred)\n    if sum_true == 0 and sum_pred == 0:\n        return 1.0\n    dice = 2.0 * intersection / (sum_true + sum_pred)\n    return round(dice, 3)"}
{"task_id": 74, "completion_id": 0, "solution": "import numpy as np\ndef create_row_hv(row, dim, random_seeds):\n    \"\"\"\n    Generate a composite hypervector for a dataset row using Hyperdimensional Computing (HDC).\n    \n    Parameters:\n    - row (dict): A dictionary representing a dataset row with feature names as keys and values as values.\n    - dim (int): The dimensionality of the hypervectors.\n    - random_seeds (dict): A dictionary mapping feature names to random seeds for reproducibility.\n    \n    Returns:\n    - list: A composite hypervector representing the entire row.\n    \"\"\"\n    composite = np.zeros(dim)\n    for (feature_name, feature_value) in row.items():\n        np.random.seed(hash(feature_name))\n        name_hv = 2 * (np.random.rand(dim) < 0.5) - 1\n        value_seed = random_seeds[feature_name]\n        np.random.seed(value_seed)\n        value_hv = 2 * (np.random.rand(dim) < 0.5) - 1\n        bound_hv = name_hv * value_hv\n        composite += bound_hv\n    return composite.tolist()"}
{"task_id": 75, "completion_id": 0, "solution": "from collections import Counter\ndef confusion_matrix(data):\n    \"\"\"\n    Generate a 2x2 confusion matrix for a binary classification problem.\n\n    Parameters:\n    - data: A list of lists, where each inner list is [y_true, y_pred]\n            representing the actual and predicted labels for one observation.\n\n    Returns:\n    - A 2x2 list of lists representing the confusion matrix:\n      [\n        [True Negatives, False Positives],\n        [False Negatives, True Positives]\n      ]\n    \"\"\"\n    cm = [[0, 0], [0, 0]]\n    for (y_true, y_pred) in data:\n        cm[y_true][y_pred] += 1\n    return cm"}
{"task_id": 76, "completion_id": 0, "solution": "import numpy as np\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    similarity = dot_product / (norm_v1 * norm_v2)\n    return round(similarity, 3)"}
{"task_id": 77, "completion_id": 0, "solution": "from collections import Counter\ndef performance_metrics(actual: list[int], predicted: list[int]) -> tuple:\n    \"\"\"\n    Calculate performance metrics for a binary classification model.\n\n    Parameters:\n    - actual (list[int]): List of actual class labels (0 or 1).\n    - predicted (list[int]): List of predicted class labels (0 or 1).\n\n    Returns:\n    - tuple: A tuple containing:\n        - confusion_matrix (list[list[int]]): 2x2 matrix with [TN, FP], [FN, TP]\n        - accuracy (float): Accuracy of the model, rounded to 3 decimal places.\n        - f1_score (float): F1 score of the model, rounded to 3 decimal places.\n        - specificity (float): Specificity of the model, rounded to 3 decimal places.\n        - negative_predictive_value (float): NPV of the model, rounded to 3 decimal places.\n    \"\"\"\n    TP = 0\n    TN = 0\n    FP = 0\n    FN = 0\n    for (a, p) in zip(actual, predicted):\n        if a == 1 and p == 1:\n            TP += 1\n        elif a == 0 and p == 0:\n            TN += 1\n        elif a == 0 and p == 1:\n            FP += 1\n        elif a == 1 and p == 0:\n            FN += 1\n    confusion_matrix = [[TN, FP], [FN, TP]]\n    total = len(actual)\n    accuracy = (TP + TN) / total if total != 0 else 0.0\n    accuracy = round(accuracy, 3)\n    precision = TP / (TP + FP) if TP + FP != 0 else 1.0\n    recall = TP / (TP + FN) if TP + FN != 0 else 1.0\n    if precision + recall == 0:\n        f1_score = 0.0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n    f1_score = round(f1_score, 3)\n    if TN + FP != 0:\n        specificity = TN / (TN + FP)\n    else:\n        specificity = 1.0\n    specificity = round(specificity, 3)\n    if TN + FN != 0:\n        npv = TN / (TN + FN)\n    else:\n        npv = 1.0\n    npv = round(npv, 3)\n    return (confusion_matrix, accuracy, f1_score, specificity, npv)"}
{"task_id": 78, "completion_id": 0, "solution": "import numpy as np\ndef descriptive_statistics(data):\n    data = np.array(data)\n    mean_val = np.mean(data)\n    median_val = np.median(data)\n    (values, counts) = np.unique(data, return_counts=True)\n    max_count = np.max(counts)\n    modes = values[counts == max_count]\n    mode_val = modes[0]\n    variance_val = np.var(data)\n    std_val = np.std(data)\n    p25 = np.percentile(data, 25)\n    p50 = np.percentile(data, 50)\n    p75 = np.percentile(data, 75)\n    iqr_val = p75 - p25\n    result = {'mean': round(mean_val, 4), 'median': round(median_val, 4), 'mode': mode_val, 'variance': round(variance_val, 4), 'standard_deviation': round(std_val, 4), '25th_percentile': round(p25, 4), '50th_percentile': round(p50, 4), '75th_percentile': round(p75, 4), 'interquartile_range': round(iqr_val, 4)}\n    return result"}
{"task_id": 79, "completion_id": 0, "solution": "import math\ndef binomial_probability(n, k, p):\n    \"\"\"\n    Calculate the probability of achieving exactly k successes in n independent Bernoulli trials,\n    each with probability p of success, using the Binomial distribution formula.\n    :param n: Total number of trials\n    :param k: Number of successes\n    :param p: Probability of success on each trial\n    :return: Probability of k successes in n trials\n    \"\"\"\n    combination = math.comb(n, k)\n    probability = combination * p ** k * (1 - p) ** (n - k)\n    return round(probability, 5)"}
{"task_id": 80, "completion_id": 0, "solution": "import math\ndef normal_pdf(x, mean, std_dev):\n    \"\"\"\n    Calculate the probability density function (PDF) of the normal distribution.\n    :param x: The value at which the PDF is evaluated.\n    :param mean: The mean (\u03bc) of the distribution.\n    :param std_dev: The standard deviation (\u03c3) of the distribution.\n    \"\"\"\n    exponent = -(x - mean) ** 2 / (2 * std_dev ** 2)\n    coefficient = 1 / (std_dev * math.sqrt(2 * math.pi))\n    pdf_value = coefficient * math.exp(exponent)\n    return round(pdf_value, 5)"}
{"task_id": 81, "completion_id": 0, "solution": "import math\ndef poisson_probability(k, lam):\n    \"\"\"\n    Calculate the probability of observing exactly k events in a fixed interval,\n    given the mean rate of events lam, using the Poisson distribution formula.\n    :param k: Number of events (non-negative integer)\n    :param lam: The average rate (mean) of occurrences in a fixed interval\n    \"\"\"\n    numerator = lam ** k * math.exp(-lam)\n    denominator = math.factorial(k)\n    probability = numerator / denominator\n    return round(probability, 5)"}
{"task_id": 82, "completion_id": 0, "solution": "import numpy as np\ndef calculate_contrast(img):\n    \"\"\"\n    Calculate the contrast of a grayscale image.\n    Args:\n        img (numpy.ndarray): 2D array representing a grayscale image with pixel values between 0 and 255.\n    \"\"\"\n    return np.max(img) - np.min(img)"}
{"task_id": 83, "completion_id": 0, "solution": "import numpy as np\ndef calculate_dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    \n    Args:\n        vec1 (numpy.ndarray): 1D array representing the first vector.\n        vec2 (numpy.ndarray): 1D array representing the second vector.\n    \n    Returns:\n        float: The dot product of the two input vectors.\n    \"\"\"\n    return np.dot(vec1, vec2)"}
{"task_id": 84, "completion_id": 0, "solution": "import numpy as np\ndef phi_transform(data: list[float], degree: int):\n    \"\"\"\n    Perform a Phi Transformation to map input features into a higher-dimensional space by generating polynomial features.\n\n    Args:\n        data (list[float]): A list of numerical values to transform.\n        degree (int): The degree of the polynomial expansion.\n\n    Returns:\n        list[list[float]]: A nested list where each inner list contains the polynomial features of the corresponding data point.\n                           The output is rounded to 8 decimal places.\n    \"\"\"\n    if degree < 0:\n        return []\n    result = []\n    for x in data:\n        features = []\n        for i in range(degree + 1):\n            if i == 0:\n                features.append(1.0)\n            else:\n                features.append(x ** i)\n        rounded_features = [round(f, 8) for f in features]\n        result.append(rounded_features)\n    return result"}
{"task_id": 85, "completion_id": 0, "solution": "import numpy as np\ndef pos_encoding(position: int, d_model: int):\n    if position == 0 or d_model <= 0:\n        return -1\n    j = np.arange(d_model)\n    i = j // 2\n    exponent = 2 * i / d_model\n    denominator = 10000 ** exponent\n    angle = position / denominator\n    pe = np.where(j % 2 == 0, np.sin(angle), np.cos(angle))\n    return pe.astype(np.float16).tolist()"}
{"task_id": 86, "completion_id": 0, "solution": "def model_fit_quality(training_accuracy, test_accuracy):\n    \"\"\"\n    Determine if the model is overfitting, underfitting, or a good fit based on training and test accuracy.\n    :param training_accuracy: float, training accuracy of the model (0 <= training_accuracy <= 1)\n    :param test_accuracy: float, test accuracy of the model (0 <= test_accuracy <= 1)\n    :return: int, one of 1 (Overfitting), -1 (Underfitting), or 0 (Good fit).\n    \"\"\"\n    if training_accuracy - test_accuracy > 0.2:\n        return 1\n    elif training_accuracy < 0.7 and test_accuracy < 0.7:\n        return -1\n    else:\n        return 0"}
{"task_id": 87, "completion_id": 0, "solution": "import numpy as np\ndef adam_optimizer(parameter, grad, m, v, t, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n    \"\"\"\n    Update parameters using the Adam optimizer.\n    Adjusts the learning rate based on the moving averages of the gradient and squared gradient.\n    \n    :param parameter: Current parameter value (scalar or numpy array)\n    :param grad: Current gradient (same shape as parameter)\n    :param m: First moment estimate (same shape as parameter)\n    :param v: Second moment estimate (same shape as parameter)\n    :param t: Current timestep (int, t >= 1)\n    :param learning_rate: Learning rate (default=0.001)\n    :param beta1: First moment decay rate (default=0.9)\n    :param beta2: Second moment decay rate (default=0.999)\n    :param epsilon: Small constant for numerical stability (default=1e-8)\n    :return: tuple: (updated_parameter, updated_m, updated_v)\n             All values are rounded to 5 decimal places and converted to lists using tolist()\n    \"\"\"\n    updated_m = beta1 * m + (1 - beta1) * grad\n    updated_v = beta2 * v + (1 - beta2) * grad ** 2\n    m_hat = updated_m / (1 - beta1 ** t)\n    v_hat = updated_v / (1 - beta2 ** t)\n    parameter_updated = parameter - learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n    parameter_rounded = np.round(parameter_updated, 5)\n    m_rounded = np.round(updated_m, 5)\n    v_rounded = np.round(updated_v, 5)\n    return (parameter_rounded.tolist(), m_rounded.tolist(), v_rounded.tolist())"}
{"task_id": 88, "completion_id": 0, "solution": "import numpy as np\ndef load_encoder_hparams_and_params(model_size: str='124M', models_dir: str='models'):\n\n    class DummyBPE:\n\n        def __init__(self):\n            self.encoder_dict = {'hello': 1, 'world': 2, '<UNK>': 0}\n\n        def encode(self, text: str):\n            tokens = text.strip().split()\n            return [self.encoder_dict.get(token, self.encoder_dict['<UNK>']) for token in tokens]\n\n        def decode(self, token_ids: list):\n            reversed_dict = {v: k for (k, v) in self.encoder_dict.items()}\n            return ' '.join([reversed_dict.get(tok_id, '<UNK>') for tok_id in token_ids])\n    hparams = {'n_ctx': 1024, 'n_head': 12}\n    params = {'wte': np.random.rand(3, 10), 'wpe': np.random.rand(1024, 10), 'blocks': [], 'ln_f': {'g': np.ones(10), 'b': np.zeros(10)}}\n    encoder = DummyBPE()\n    return (encoder, hparams, params)\ndef gen_text(prompt: str, n_tokens_to_generate: int=40):\n    (encoder, hparams, params) = load_encoder_hparams_and_params()\n    n_embd = params['wte'].shape[1]\n    block = {'ln_1': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'attn': {'c_attn': {'w': np.random.randn(n_embd, 3 * n_embd), 'b': np.zeros(3 * n_embd)}, 'c_proj': {'w': np.random.randn(n_embd, n_embd), 'b': np.zeros(n_embd)}}, 'ln_2': {'g': np.ones(n_embd), 'b': np.zeros(n_embd)}, 'ffn': {'c_in': {'w': np.random.randn(n_embd, 4 * n_embd), 'b': np.zeros(4 * n_embd)}, 'c_out': {'w': np.random.randn(4 * n_embd, n_embd), 'b': np.zeros(n_embd)}}}\n    params['blocks'] = [block]\n    token_ids = encoder.encode(prompt)\n    generated = token_ids.copy()\n\n    def layer_norm(x, gamma, beta, eps=1e-08):\n        mean = np.mean(x, axis=-1, keepdims=True)\n        var = np.var(x, axis=-1, keepdims=True)\n        x_normalized = (x - mean) / np.sqrt(var + eps)\n        return gamma * x_normalized + beta\n\n    def softmax(x, axis=-1):\n        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return e_x / e_x.sum(axis=axis, keepdims=True)\n    for _ in range(n_tokens_to_generate):\n        T = len(generated)\n        if T > hparams['n_ctx']:\n            raise ValueError('Context length exceeded')\n        token_embeddings = params['wte'][generated, :]\n        positional_embeddings = params['wpe'][:T, :]\n        x = token_embeddings + positional_embeddings\n        for block in params['blocks']:\n            ln1_g = block['ln_1']['g']\n            ln1_b = block['ln_1']['b']\n            x = layer_norm(x, ln1_g, ln1_b)\n            c_attn_w = block['attn']['c_attn']['w']\n            c_attn_b = block['attn']['c_attn']['b']\n            qkv = x @ c_attn_w + c_attn_b\n            (q, k, v) = np.split(qkv, 3, axis=1)\n            attn_scores = q @ k.T / np.sqrt(n_embd)\n            attn_weights = softmax(attn_scores, axis=1)\n            attn_output = attn_weights @ v\n            c_proj_w = block['attn']['c_proj']['w']\n            c_proj_b = block['attn']['c_proj']['b']\n            attn_output = attn_output @ c_proj_w + c_proj_b\n            x = x + attn_output\n            ln2_g = block['ln_2']['g']\n            ln2_b = block['ln_2']['b']\n            x = layer_norm(x, ln2_g, ln2_b)\n            c_in_w = block['ffn']['c_in']['w']\n            c_in_b = block['ffn']['c_in']['b']\n            ffn_output = x @ c_in_w + c_in_b\n            c_out_w = block['ffn']['c_out']['w']\n            c_out_b = block['ffn']['c_out']['b']\n            ffn_output = ffn_output @ c_out_w + c_out_b\n            x = x + ffn_output\n        ln_f_g = params['ln_f']['g']\n        ln_f_b = params['ln_f']['b']\n        x = layer_norm(x, ln_f_g, ln_f_b)\n        logits = x[-1, :]\n        probs = softmax(logits)\n        next_token_id = int(np.argmax(probs))\n        generated.append(next_token_id)\n    return encoder.decode(generated)"}
{"task_id": 89, "completion_id": 0, "solution": "import numpy as np\ndef pattern_weaver(n, crystal_values, dimension):\n\n    def softmax(values):\n        \"\"\"\n        Computes the softmax of a 1D array for numerical stability.\n        \"\"\"\n        exps = np.exp(values - np.max(values))\n        return exps / exps.sum()\n    values = np.array(crystal_values)\n    sqrt_d = np.sqrt(dimension)\n    attention_scores = sqrt_d * values.reshape(-1, 1) @ values.reshape(1, -1)\n    attention_weights = np.zeros_like(attention_scores)\n    for i in range(n):\n        attention_weights[i, :] = softmax(attention_scores[i, :])\n    output = np.zeros(n)\n    for i in range(n):\n        output[i] = np.dot(attention_weights[i, :], values)\n    return [round(x, 4) for x in output]"}
{"task_id": 90, "completion_id": 0, "solution": "import math\nfrom collections import Counter\ndef calculate_bm25_scores(corpus, query, k1=1.5, b=0.75):\n    \"\"\"\n    Calculate BM25 scores for each document in the corpus with respect to the given query.\n\n    Parameters:\n    - corpus (list of str): A list of documents (strings).\n    - query (str): The query string.\n    - k1 (float): Term frequency saturation parameter (default: 1.5).\n    - b (float): Document length normalization parameter (default: 0.75).\n\n    Returns:\n    - list of float: A list of BM25 scores for each document, rounded to three decimal places.\n    \"\"\"\n    N = len(corpus)\n    if N == 0:\n        return []\n    term_doc_freq = Counter()\n    for doc in corpus:\n        terms = doc.lower().split()\n        unique_terms = set(terms)\n        for term in unique_terms:\n            term_doc_freq[term] += 1\n    query_terms = query.lower().split()\n    unique_query_terms = list(set(query_terms))\n    doc_lengths = [len(doc.lower().split()) for doc in corpus]\n    avgdl = sum(doc_lengths) / N if N > 0 else 0\n    scores = []\n    for doc in corpus:\n        terms = doc.lower().split()\n        dl = len(terms)\n        score = 0.0\n        for term in unique_query_terms:\n            tf_t_d = terms.count(term)\n            df_t = term_doc_freq.get(term, 0)\n            idf = math.log((N - df_t + 0.5) / (df_t + 0.5))\n            if avgdl == 0:\n                length_ratio = 1.0\n            else:\n                length_ratio = dl / avgdl\n            denominator = tf_t_d + k1 * (1 - b + b * length_ratio)\n            numerator = (k1 + 1) * tf_t_d\n            if denominator != 0:\n                score += idf * (numerator / denominator)\n        scores.append(round(score, 3))\n    return scores"}
{"task_id": 91, "completion_id": 0, "solution": "def calculate_f1_score(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score based on true and predicted labels.\n\n    Args:\n        y_true (list): True labels (ground truth).\n        y_pred (list): Predicted labels.\n\n    Returns:\n        float: The F1 score rounded to three decimal places.\n    \"\"\"\n    tp = 0\n    fp = 0\n    fn = 0\n    for (true, pred) in zip(y_true, y_pred):\n        if true == 1 and pred == 1:\n            tp += 1\n        elif true == 0 and pred == 1:\n            fp += 1\n        elif true == 1 and pred == 0:\n            fn += 1\n    denominator_precision = tp + fp\n    precision = tp / denominator_precision if denominator_precision != 0 else 0.0\n    denominator_recall = tp + fn\n    recall = tp / denominator_recall if denominator_recall != 0 else 0.0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    return round(f1, 3)"}
{"task_id": 92, "completion_id": 0, "solution": "import math\nPI = 3.14159\ndef power_grid_forecast(consumption_data):\n    \"\"\"\n    Predicts the energy consumption for day 15 after removing a known daily fluctuation,\n    fitting a linear regression model to the detrended data, and applying a 5% safety margin.\n    \n    Parameters:\n        consumption_data (list of float): A list of 10 daily energy consumption measurements.\n    \n    Returns:\n        int: The final predicted energy consumption for day 15, rounded up after applying a 5% safety margin.\n    \"\"\"\n    detrended = []\n    for i in range(10):\n        day = i + 1\n        fluctuation = 10 * math.sin(2 * PI * day / 10)\n        detrended_y = consumption_data[i] - fluctuation\n        detrended.append(detrended_y)\n    N = 10\n    sum_x = sum((i + 1 for i in range(N)))\n    sum_y = sum(detrended)\n    sum_xy = sum(((i + 1) * detrended[i] for i in range(N)))\n    sum_x2 = sum(((i + 1) ** 2 for i in range(N)))\n    m_numerator = N * sum_xy - sum_x * sum_y\n    m_denominator = N * sum_x2 - sum_x ** 2\n    m = m_numerator / m_denominator\n    b = (sum_y - m * sum_x) / N\n    predicted_base = m * 15 + b\n    total_pred = predicted_base\n    rounded_val = round(total_pred)\n    safety_margin = rounded_val * 1.05\n    final_val = math.ceil(safety_margin)\n    return int(final_val)"}
{"task_id": 93, "completion_id": 0, "solution": "import numpy as np\ndef mae(y_true, y_pred):\n    \"\"\"\n    Calculate Mean Absolute Error between two arrays.\n\n    Parameters:\n    y_true (numpy.ndarray): Array of true values\n    y_pred (numpy.ndarray): Array of predicted values\n\n    Returns:\n    float: Mean Absolute Error rounded to 3 decimal places\n    \"\"\"\n    absolute_errors = np.abs(y_true - y_pred)\n    mean_error = np.mean(absolute_errors)\n    return round(mean_error, 3)"}
{"task_id": 94, "completion_id": 0, "solution": "import numpy as np\ndef compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray) -> tuple:\n    \"\"\"\n    Computes the Query (Q), Key (K), and Value (V) matrices by projecting the input X\n    using the respective weight matrices W_q, W_k, and W_v.\n    \n    Parameters:\n        X (np.ndarray): Input matrix of shape (seq_len, d_model)\n        W_q (np.ndarray): Query weight matrix of shape (d_model, d_q)\n        W_k (np.ndarray): Key weight matrix of shape (d_model, d_k)\n        W_v (np.ndarray): Value weight matrix of shape (d_model, d_v)\n    \n    Returns:\n        tuple: (Q, K, V) where each is a matrix of shape (seq_len, d_q), (seq_len, d_k), (seq_len, d_v)\n    \"\"\"\n    Q = X @ W_q\n    K = X @ W_k\n    V = X @ W_v\n    return (Q, K, V)\ndef self_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the self-attention for a single attention head.\n    \n    Parameters:\n        Q (np.ndarray): Query matrix of shape (seq_len, d_k)\n        K (np.ndarray): Key matrix of shape (seq_len, d_k)\n        V (np.ndarray): Value matrix of shape (seq_len, d_v)\n    \n    Returns:\n        np.ndarray: Output of self-attention of shape (seq_len, d_v)\n    \"\"\"\n    d_k = K.shape[1]\n    scores = Q @ K.T\n    scores = scores / np.sqrt(d_k)\n    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n    attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    output = attention_weights @ V\n    return output\ndef multi_head_attention(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray, n_heads: int) -> list:\n    \"\"\"\n    Computes the multi-head attention mechanism.\n    \n    Parameters:\n        X (np.ndarray): Input matrix of shape (seq_len, d_model)\n        W_q (np.ndarray): Query weight matrix of shape (d_model, d_q)\n        W_k (np.ndarray): Key weight matrix of shape (d_model, d_k)\n        W_v (np.ndarray): Value weight matrix of shape (d_model, d_v)\n        n_heads (int): Number of attention heads\n    \n    Returns:\n        list: Multi-head attention output as a Python list, with values rounded to 4 decimal places\n    \"\"\"\n    (Q, K, V) = compute_qkv(X, W_q, W_k, W_v)\n    Q_split = np.split(Q, n_heads, axis=1)\n    K_split = np.split(K, n_heads, axis=1)\n    V_split = np.split(V, n_heads, axis=1)\n    head_outputs = []\n    for i in range(n_heads):\n        head_Q = Q_split[i]\n        head_K = K_split[i]\n        head_V = V_split[i]\n        head_out = self_attention(head_Q, head_K, head_V)\n        head_outputs.append(head_out)\n    concatenated = np.concatenate(head_outputs, axis=1)\n    rounded = np.round(concatenated, 4)\n    return rounded.tolist()"}
{"task_id": 95, "completion_id": 0, "solution": "def phi_corr(x: list[int], y: list[int]) -> float:\n    \"\"\"\n    Calculate the Phi coefficient between two binary variables.\n\n    Args:\n    x (list[int]): A list of binary values (0 or 1).\n    y (list[int]): A list of binary values (0 or 1).\n\n    Returns:\n    float: The Phi coefficient rounded to 4 decimal places.\n    \"\"\"\n    a = 0\n    b = 0\n    c = 0\n    d = 0\n    for (xi, yi) in zip(x, y):\n        if xi == 1 and yi == 1:\n            a += 1\n        elif xi == 1 and yi == 0:\n            b += 1\n        elif xi == 0 and yi == 1:\n            c += 1\n        else:\n            d += 1\n    numerator = a * d - b * c\n    denominator = ((a + b) * (a + c) * (b + d) * (c + d)) ** 0.5\n    if denominator == 0:\n        return 0.0\n    else:\n        phi = numerator / denominator\n        return round(phi, 4)"}
{"task_id": 96, "completion_id": 0, "solution": "def hard_sigmoid(x: float) -> float:\n    \"\"\"\n    Implements the Hard Sigmoid activation function.\n\n    The Hard Sigmoid is a computationally efficient approximation of the standard sigmoid function.\n    It is defined as:\n    - 0 if x <= -2.5\n    - 1 if x >= 2.5\n    - (x + 2.5) / 5 otherwise\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Hard Sigmoid of the input\n    \"\"\"\n    if x <= -2.5:\n        return 0.0\n    elif x >= 2.5:\n        return 1.0\n    else:\n        return (x + 2.5) / 5.0"}
{"task_id": 97, "completion_id": 0, "solution": "import math\ndef elu(x: float, alpha: float=1.0) -> float:\n    \"\"\"\n    Compute the ELU activation function.\n\n    Args:\n        x (float): Input value\n        alpha (float): ELU parameter for negative values (default: 1.0)\n\n    Returns:\n        float: ELU activation value\n    \"\"\"\n    if x > 0:\n        result = x\n    else:\n        result = alpha * (math.exp(x) - 1)\n    return round(result, 4)"}
{"task_id": 98, "completion_id": 0, "solution": "def prelu(x: float, alpha: float=0.25) -> float:\n    \"\"\"\n    Implements the PReLU (Parametric ReLU) activation function.\n\n    Args:\n        x: Input value\n        alpha: Slope parameter for negative values (default: 0.25)\n\n    Returns:\n        float: PReLU activation value\n    \"\"\"\n    if x > 0:\n        return x\n    else:\n        return alpha * x"}
{"task_id": 99, "completion_id": 0, "solution": "import math\ndef softplus(x: float) -> float:\n    \"\"\"\n    Compute the softplus activation function.\n\n    The softplus function is defined as:\n        softplus(x) = log(1 + exp(x))\n\n    It is a smooth approximation of the ReLU function and is numerically stable\n    for both large positive and large negative values of x.\n\n    Args:\n        x: Input value (float)\n\n    Returns:\n        The softplus value, rounded to 4 decimal places (float)\n    \"\"\"\n    if x >= 0:\n        res = x + math.log(1 + math.exp(-x))\n    else:\n        res = math.log(1 + math.exp(x))\n    return round(res, 4)"}
{"task_id": 100, "completion_id": 0, "solution": "def softsign(x: float) -> float:\n    \"\"\"\n    Implements the Softsign activation function.\n\n    Args:\n        x (float): Input value\n\n    Returns:\n        float: The Softsign of the input, rounded to 4 decimal places\n    \"\"\"\n    return round(x / (1 + abs(x)), 4)"}
{"task_id": 101, "completion_id": 0, "solution": "import numpy as np\ndef grpo_objective(rhos, A, pi_theta_old, pi_theta_ref, epsilon=0.2, beta=0.01) -> float:\n    \"\"\"\n    Compute the GRPO objective function.\n\n    Args:\n        rhos: List of likelihood ratios (p_i) = pi_theta(o_i | q) / pi_theta_old(o_i | q).\n        A: List of advantage estimates (A_i).\n        pi_theta_old: List representing the old policy probabilities pi_theta_old(o_i | q).\n        pi_theta_ref: List representing the reference policy probabilities pi_ref(o_i | q).\n        epsilon: Clipping parameter (eps).\n        beta: KL divergence penalty coefficient (beta).\n\n    Returns:\n        The computed GRPO objective value.\n    \"\"\"\n    rhos_arr = np.array(rhos)\n    A_arr = np.array(A)\n    pi_old_arr = np.array(pi_theta_old)\n    pi_ref_arr = np.array(pi_theta_ref)\n    clipped_rhos = np.clip(rhos_arr, 1 - epsilon, 1 + epsilon)\n    original_surrogate = rhos_arr * A_arr\n    clipped_surrogate = clipped_rhos * A_arr\n    surrogate = np.where(A_arr > 0, np.minimum(original_surrogate, clipped_surrogate), np.maximum(original_surrogate, clipped_surrogate))\n    surrogate_part = np.sum(surrogate)\n    pi_theta = rhos_arr * pi_old_arr\n    log_ratio = np.log(pi_theta / pi_ref_arr)\n    kl_term_per_sample = pi_theta * log_ratio\n    average_kl = np.mean(kl_term_per_sample)\n    kl_penalty = beta * average_kl\n    objective = surrogate_part - kl_penalty\n    return round(objective, 6)"}
{"task_id": 102, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef swish(x: float) -> float:\n    \"\"\"\n    Implements the Swish activation function.\n\n    Args:\n        x: Input value\n\n    Returns:\n        The Swish activation value rounded to 4 decimal places\n    \"\"\"\n    sigmoid = 1 / (1 + math.exp(-x))\n    result = x * sigmoid\n    return round(result, 4)"}
{"task_id": 103, "completion_id": 0, "solution": "import math\nimport numpy as np\ndef selu(x: float) -> float:\n    \"\"\"\n    Implements the SELU (Scaled Exponential Linear Unit) activation function.\n\n    SELU is defined as:\n        - For x > 0: SELU(x) = scale * x\n        - For x <= 0: SELU(x) = scale * alpha * (exp(x) - 1)\n\n    To ensure numerical stability, especially for very small or negative x,\n    we use `math.expm1(x)` instead of `math.exp(x) - 1`, which provides better\n    precision for small values of x.\n\n    Args:\n        x: Input value (float)\n\n    Returns:\n        SELU activation value, rounded to the nearest 4th decimal\n    \"\"\"\n    alpha = 1.6732632423543772\n    scale = 1.0507009873554805\n    if x > 0:\n        result = scale * x\n    else:\n        result = scale * alpha * math.expm1(x)\n    return round(result, 4)"}
{"task_id": 104, "completion_id": 0, "solution": "import numpy as np\ndef predict_logistic(X: np.ndarray, weights: np.ndarray, bias: float) -> list:\n    \"\"\"\n    Implements binary classification prediction using Logistic Regression.\n\n    Args:\n        X: Input feature matrix (shape: N \u00d7 D)\n        weights: Model weights (shape: D)\n        bias: Model bias\n\n    Returns:\n        Binary predictions (0 or 1) as a Python list\n    \"\"\"\n    z = X.dot(weights) + bias\n    probabilities = 1 / (1 + np.exp(-z))\n    predictions = (probabilities >= 0.5).astype(int)\n    return predictions.tolist()"}
{"task_id": 105, "completion_id": 0, "solution": "import numpy as np\ndef train_softmaxreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list, list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for Softmax regression, optimizing parameters with Cross Entropy loss.\n\n    Returns:\n        B : list, CxM updated parameter vector rounded to 4 floating points\n        losses : list[float], collected values of a Cross Entropy rounded to 4 floating points\n    \"\"\"\n    X = np.hstack([np.ones((X.shape[0], 1)), X])\n    unique_classes = np.unique(y)\n    C = len(unique_classes)\n    (N, M) = X.shape\n    y_encoded = np.searchsorted(unique_classes, y)\n    y_one_hot = (y_encoded[:, None] == np.arange(C)).astype(float)\n    B = np.zeros((C, M))\n    losses = []\n    for _ in range(iterations):\n        logits = X @ B.T\n        logits_max = np.max(logits, axis=1, keepdims=True)\n        logits_stable = logits - logits_max\n        exps = np.exp(logits_stable)\n        probabilities = exps / np.sum(exps, axis=1, keepdims=True)\n        log_prob = np.log(probabilities)\n        loss = -np.sum(y_one_hot * log_prob) / N\n        losses.append(loss)\n        gradient = (probabilities - y_one_hot).T @ X / N\n        B -= learning_rate * gradient\n    B_rounded = np.round(B, 4)\n    losses_rounded = [round(loss, 4) for loss in losses]\n    return (B_rounded.tolist(), losses_rounded)"}
{"task_id": 106, "completion_id": 0, "solution": "import numpy as np\ndef train_logreg(X: np.ndarray, y: np.ndarray, learning_rate: float, iterations: int) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Gradient-descent training algorithm for logistic regression, optimizing parameters with Binary Cross Entropy loss.\n\n    Parameters:\n    - X (np.ndarray): Input feature matrix of shape (m, n)\n    - y (np.ndarray): Target vector of shape (m,)\n    - learning_rate (float): Step size for gradient descent\n    - iterations (int): Number of iterations to run gradient descent\n\n    Returns:\n    - tuple[list[float], list[float]]: A tuple containing:\n        - list of optimized coefficients (including intercept)\n        - list of loss values over each iteration (rounded to 4 decimals)\n    \"\"\"\n    X = np.c_[np.ones(X.shape[0]), X]\n    (m, n) = X.shape\n    theta = np.zeros(n)\n    losses = []\n    for _ in range(iterations):\n        z = X @ theta\n        h = 1 / (1 + np.exp(-z))\n        gradient = X.T @ (h - y) / m\n        theta -= learning_rate * gradient\n        h_clipped = np.clip(h, 1e-15, 1 - 1e-15)\n        loss = -(np.dot(y, np.log(h_clipped)) + np.dot(1 - y, np.log(1 - h_clipped))) / m\n        losses.append(round(loss, 4))\n    theta_rounded = np.round(theta, 4)\n    theta_list = theta_rounded.tolist()\n    return (theta_list, losses)"}
{"task_id": 107, "completion_id": 0, "solution": "import numpy as np\ndef masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute masked self-attention.\n    \n    Parameters:\n        Q, K, V (np.ndarray): Query, Key, and Value matrices of shape (seq_len, d_k)\n        mask (np.ndarray): Attention mask of shape (seq_len, seq_len), where 1 indicates positions to be masked\n    \n    Returns:\n        np.ndarray: Output of masked self-attention of shape (seq_len, d_k)\n    \"\"\"\n    attention_scores = np.dot(Q, K.T)\n    attention_scores = attention_scores - mask * 1000000000.0\n    exp_scores = np.exp(attention_scores - np.max(attention_scores, axis=-1, keepdims=True))\n    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n    attention_weights = exp_scores / sum_exp\n    output = np.dot(attention_weights, V)\n    return output.tolist()"}
{"task_id": 108, "completion_id": 0, "solution": "def disorder(apples: list) -> float:\n    \"\"\"\n    Calculates a measure of disorder in a basket of apples based on their colors.\n    \"\"\"\n    import math\n    from collections import Counter\n    total = len(apples)\n    if total == 0:\n        return 0.0\n    counts = Counter(apples)\n    entropy = 0.0\n    for count in counts.values():\n        p = count / total\n        entropy -= p * math.log2(p)\n    return round(entropy, 4)"}
{"task_id": 109, "completion_id": 0, "solution": "import numpy as np\ndef layer_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform Layer Normalization on a 3D input tensor.\n\n    Parameters:\n    - X: np.ndarray of shape (batch_size, sequence_length, feature_dim)\n    - gamma: np.ndarray of shape (feature_dim,) - scaling parameters\n    - beta: np.ndarray of shape (feature_dim,) - shifting parameters\n    - epsilon: float, small value to avoid division by zero\n\n    Returns:\n    - output: list of normalized and transformed data, rounded to 5 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=-1, keepdims=True)\n    var = np.var(X, axis=-1, keepdims=True)\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = X_normalized * gamma + beta\n    return np.round(output, 5).tolist()"}
{"task_id": 110, "completion_id": 0, "solution": "import numpy as np\nfrom collections import Counter\ndef meteor_score(reference, candidate, alpha=0.9, beta=3, gamma=0.5):\n    reference_tokens = reference.split()\n    candidate_tokens = candidate.split()\n    ref_counter = Counter(reference_tokens)\n    cand_counter = Counter(candidate_tokens)\n    matches = 0\n    for word in ref_counter:\n        matches += min(ref_counter[word], cand_counter.get(word, 0))\n    if matches == 0:\n        return 0.0\n    len_ref = len(reference_tokens)\n    len_cand = len(candidate_tokens)\n    precision = matches / len_cand\n    recall = matches / len_ref\n    F_mean = (alpha + beta) * precision * recall / (beta * precision + alpha * recall)\n    cand_counter_copy = cand_counter.copy()\n    matched = []\n    for token in reference_tokens:\n        if cand_counter_copy[token] > 0:\n            matched.append(True)\n            cand_counter_copy[token] -= 1\n        else:\n            matched.append(False)\n    chunks = 0\n    prev = False\n    for flag in matched:\n        if flag:\n            if not prev:\n                chunks += 1\n                prev = True\n        else:\n            prev = False\n    penalty = chunks - 1\n    penalty_rate = penalty / len_cand\n    score = F_mean * (1 - penalty_rate * gamma)\n    return round(score, 3)"}
{"task_id": 111, "completion_id": 0, "solution": "import numpy as np\ndef compute_pmi(joint_counts, total_counts_x, total_counts_y, total_samples):\n    \"\"\"\n    Compute the Pointwise Mutual Information (PMI) between two events.\n\n    PMI is defined as:\n        PMI(x, y) = log( (P(x,y)) / (P(x) * P(y)) )\n\n    Where:\n        P(x,y) = joint_counts / total_samples\n        P(x) = total_counts_x / total_samples\n        P(y) = total_counts_y / total_samples\n\n    This simplifies to:\n        PMI(x, y) = log( (joint_counts * total_samples) / (total_counts_x * total_counts_y) )\n\n    Parameters:\n        joint_counts (int): Number of times both events x and y co-occur.\n        total_counts_x (int): Total number of times event x occurs.\n        total_counts_y (int): Total number of times event y occurs.\n        total_samples (int): Total number of samples or observations.\n\n    Returns:\n        float: PMI value rounded to 3 decimal places.\n    \"\"\"\n    ratio = joint_counts * total_samples / (total_counts_x * total_counts_y)\n    pmi = np.log(ratio)\n    return round(pmi, 3)"}
{"task_id": 112, "completion_id": 0, "solution": "def min_max(x: list[int]) -> list[float]:\n    min_x = min(x)\n    max_x = max(x)\n    if max_x == min_x:\n        return [0.0 for _ in x]\n    denominator = max_x - min_x\n    normalized = [(num - min_x) / denominator for num in x]\n    return [round(num, 4) for num in normalized]"}
{"task_id": 113, "completion_id": 0, "solution": "import numpy as np\ndef residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray):\n    hidden = x.dot(w1)\n    hidden = np.maximum(0, hidden)\n    hidden = hidden.dot(w2)\n    output = hidden + x\n    output = np.maximum(0, output)\n    return np.round(output, 4).tolist()"}
{"task_id": 114, "completion_id": 0, "solution": "import numpy as np\ndef global_avg_pool(x: np.ndarray):\n    \"\"\"\n    Perform Global Average Pooling on a 3D NumPy array.\n\n    Parameters:\n    x (np.ndarray): A 3D array of shape (height, width, channels) representing\n                    feature maps from a convolutional layer.\n\n    Returns:\n    np.ndarray: A 1D array of shape (channels,) where each element is the\n                average of all values in the corresponding feature map.\n    \"\"\"\n    return np.mean(x, axis=(0, 1))"}
{"task_id": 115, "completion_id": 0, "solution": "import numpy as np\ndef batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float=1e-05):\n    \"\"\"\n    Perform batch normalization on a 4D input array in BCHW format.\n\n    Parameters:\n    - X: Input array of shape (batch, channels, height, width)\n    - gamma: Scale parameter of shape (channels,)\n    - beta: Shift parameter of shape (channels,)\n    - epsilon: Small value for numerical stability\n\n    Returns:\n    - Normalized output as a list, rounded to 4 decimal places\n    \"\"\"\n    mean = np.mean(X, axis=(0, 2, 3))\n    var = np.var(X, axis=(0, 2, 3))\n    X_normalized = (X - mean) / np.sqrt(var + epsilon)\n    output = X_normalized * gamma + beta\n    return np.round(output, 4).tolist()"}
{"task_id": 116, "completion_id": 0, "solution": "def poly_term_derivative(c: float, x: float, n: float) -> float:\n    if abs(x) < 1e-09:\n        x = 0.0\n    if x == 0.0:\n        if abs(n) < 1e-09:\n            return round(0.0, 4)\n        elif abs(n - 1.0) < 1e-09:\n            return round(c * n, 4)\n        else:\n            return round(0.0, 4)\n    else:\n        derivative = c * n * x ** (n - 1)\n        return round(derivative, 4)"}
{"task_id": 117, "completion_id": 0, "solution": "import numpy as np\ndef orthonormal_basis(vectors: list[list[float]], tol: float=1e-10):\n    \"\"\"\n    Compute an orthonormal basis for the subspace spanned by a list of 2D vectors\n    using the Gram-Schmidt process.\n\n    Parameters:\n    - vectors: List of 2D vectors (each is a list of two floats)\n    - tol: Tolerance to determine linear independence\n\n    Returns:\n    - List of orthonormal vectors (each is a list of two floats, rounded to 4 decimals)\n    \"\"\"\n    orthogonal = []\n    orthonormal = []\n    for vector in vectors:\n        v = np.array(vector, dtype=float)\n        for u in orthogonal:\n            projection = np.dot(v, u) / np.dot(u, u) * u\n            v = v - projection\n        norm = np.linalg.norm(v)\n        if norm > tol:\n            v_normalized = v / norm\n            orthogonal.append(v.copy())\n            orthonormal.append(v_normalized)\n    return [np.round(vec, 4).tolist() for vec in orthonormal]"}
{"task_id": 118, "completion_id": 0, "solution": "import numpy as np\ndef cross_product(a, b):\n    cross = np.cross(a, b)\n    rounded = np.round(cross, 4)\n    return rounded.tolist()"}
{"task_id": 119, "completion_id": 0, "solution": "import numpy as np\ndef cramers_rule(A, b):\n    \"\"\"\n    Solves a system of linear equations Ax = b using Cramer's Rule.\n\n    Parameters:\n    A (list of lists or np.ndarray): A square coefficient matrix.\n    b (list or np.ndarray): A constant vector.\n\n    Returns:\n    list: A list of solution values rounded to 4 decimal places.\n    int: -1 if the system has no unique solution (i.e., determinant of A is zero).\n    \"\"\"\n    A = np.array(A, dtype=float)\n    b = np.array(b, dtype=float)\n    if A.shape[0] != A.shape[1]:\n        raise ValueError('Matrix A must be square.')\n    if A.shape[0] != b.shape[0]:\n        raise ValueError('Matrix A and vector b must have the same number of rows.')\n    det_A = np.linalg.det(A)\n    if abs(det_A) < 1e-09:\n        return -1\n    n = A.shape[0]\n    x = []\n    for i in range(n):\n        A_i = A.copy()\n        A_i[:, i] = b\n        det_A_i = np.linalg.det(A_i)\n        x_i = det_A_i / det_A\n        x.append(x_i)\n    rounded_x = [round(val, 4) for val in x]\n    return np.array(rounded_x).tolist()"}
{"task_id": 120, "completion_id": 0, "solution": "import numpy as np\ndef bhattacharyya_distance(p: list[float], q: list[float]) -> float:\n    \"\"\"\n    Calculate the Bhattacharyya distance between two discrete probability distributions.\n\n    The Bhattacharyya distance is defined as:\n        D_B(p, q) = -ln( sum( sqrt(p_i * q_i) ) )\n\n    Parameters:\n    - p (list[float]): First probability distribution.\n    - q (list[float]): Second probability distribution.\n\n    Returns:\n    - float: The Bhattacharyya distance rounded to 4 decimal places.\n             Returns 0.0 if the inputs are of different lengths or are empty.\n    \"\"\"\n    if not p or not q or len(p) != len(q):\n        return 0.0\n    p_arr = np.array(p)\n    q_arr = np.array(q)\n    sum_sqrt = np.sum(np.sqrt(p_arr * q_arr))\n    distance = -np.log(sum_sqrt)\n    return round(distance, 4)"}
{"task_id": 121, "completion_id": 0, "solution": "from typing import list\ndef vector_sum(a: list[int | float], b: list[int | float]) -> list[int | float]:\n    if len(a) != len(b):\n        return -1\n    return [x + y for (x, y) in zip(a, b)]"}
{"task_id": 122, "completion_id": 0, "solution": "import numpy as np\ndef compute_policy_gradient(theta: np.ndarray, episodes: list[list[tuple[int, int, float]]]):\n    (num_states, num_actions) = theta.shape\n    total_gradient = np.zeros_like(theta, dtype=np.float64)\n    total_steps = 0\n    for episode in episodes:\n        (states, actions, rewards) = zip(*episode)\n        rewards_array = np.array(rewards, dtype=np.float64)\n        returns = np.cumsum(rewards_array[::-1])[::-1]\n        for t in range(len(states)):\n            s = states[t]\n            a = actions[t]\n            G_t = returns[t]\n            logits = theta[s, :]\n            exps = np.exp(logits)\n            pi = exps / np.sum(exps)\n            one_hot = np.zeros(num_actions)\n            one_hot[a] = 1.0\n            delta = one_hot - pi\n            total_gradient[s, :] += delta * G_t\n        total_steps += len(episode)\n    average_gradient = total_gradient / total_steps\n    rounded_gradient = np.round(average_gradient, 4)\n    return rounded_gradient.tolist()"}
{"task_id": 123, "completion_id": 0, "solution": "def compute_efficiency(n_experts, k_active, d_in, d_out):\n    \"\"\"\n    Calculate the computational efficiency of an MoE layer compared to a dense layer.\n\n    Parameters:\n    - n_experts (int): Number of experts in the MoE layer.\n    - k_active (int): Number of active experts per input.\n    - d_in (int): Input dimension.\n    - d_out (int): Output dimension.\n\n    Returns:\n    - dict: A dictionary with keys:\n        - 'flops_dense': FLOPs for the dense layer.\n        - 'flops_moe': FLOPs for the MoE layer.\n        - 'savings_percent': Percentage of FLOPs saved using MoE.\n    \"\"\"\n    flops_dense = 2 * d_in * d_out\n    flops_moe = k_active / n_experts * flops_dense\n    savings_percent = (1 - k_active / n_experts) * 100\n    return {'flops_dense': round(flops_dense, 1), 'flops_moe': round(flops_moe, 1), 'savings_percent': round(savings_percent, 1)}"}
{"task_id": 124, "completion_id": 0, "solution": "import numpy as np\ndef noisy_topk_gating(X: np.ndarray, W_g: np.ndarray, W_noise: np.ndarray, N: np.ndarray, k: int):\n    \"\"\"\n    Implements the Noisy Top-K Gating Function used in Mixture-of-Experts (MoE) models.\n\n    Parameters:\n    - X (np.ndarray): Input matrix of shape (batch_size, input_dim)\n    - W_g (np.ndarray): Gate weight matrix of shape (input_dim, num_experts)\n    - W_noise (np.ndarray): Noise weight matrix of shape (input_dim, num_experts)\n    - N (np.ndarray): Pre-sampled noise matrix of shape (batch_size, num_experts)\n    - k (int): Sparsity constraint (number of top experts to select per example)\n\n    Returns:\n    - List[List[float]]: Final gating probabilities matrix, rounded to 4 decimal places\n    \"\"\"\n    (batch_size, input_dim) = X.shape\n    num_experts = W_g.shape[1]\n    original_logits = X @ W_g.T\n    noise_logits_part = X @ W_noise.T\n    noisy_logits = original_logits + noise_logits_part + N\n    topk_indices = np.argpartition(-noisy_logits, kth=k - 1, axis=1)[:, :k]\n    selected_logits = original_logits[np.arange(batch_size)[:, None], topk_indices]\n    max_vals = np.max(selected_logits, axis=1, keepdims=True)\n    exp_selected = np.exp(selected_logits - max_vals)\n    sum_exp = np.sum(exp_selected, axis=1, keepdims=True)\n    softmax_selected = exp_selected / sum_exp\n    probs = np.zeros((batch_size, num_experts))\n    probs[np.arange(batch_size)[:, None], topk_indices] = softmax_selected\n    probs = np.round(probs, 4)\n    return probs.tolist()"}
{"task_id": 125, "completion_id": 0, "solution": "import numpy as np\ndef moe(x: np.ndarray, We: np.ndarray, Wg: np.ndarray, n_experts: int, top_k: int):\n    \"\"\"\n    Computes the output of a Mixture of Experts (MoE) layer using softmax gating and top-k routing.\n\n    Parameters:\n    - x: Input tensor of shape (batch_size, input_dim)\n    - We: Expert weight matrices of shape (n_experts, input_dim, output_dim)\n    - Wg: Gating weight matrix of shape (input_dim, n_experts)\n    - n_experts: Number of experts\n    - top_k: Number of top experts to select per token\n\n    Returns:\n    - Python list of shape (batch_size, output_dim), rounded to 4 decimal places\n    \"\"\"\n    gate_logits = x @ Wg\n    gate_logits_max = np.max(gate_logits, axis=1, keepdims=True)\n    exp_gate = np.exp(gate_logits - gate_logits_max)\n    gate_probs = exp_gate / np.sum(exp_gate, axis=1, keepdims=True)\n    topk_indices = np.argsort(gate_probs, axis=1)[:, -top_k:]\n    topk_indices = np.flip(topk_indices, axis=1)\n    batch_size = x.shape[0]\n    row_indices = np.arange(batch_size)\n    topk_values = gate_probs[row_indices[:, None], topk_indices]\n    Z = np.sum(topk_values, axis=1, keepdims=True)\n    normalized_weights = topk_values / Z\n    We_topk = We[topk_indices]\n    expert_outputs = np.einsum('bi, biko -> bko', x, We_topk)\n    weighted_outputs = expert_outputs * normalized_weights[:, :, np.newaxis]\n    output = np.sum(weighted_outputs, axis=1)\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 126, "completion_id": 0, "solution": "import numpy as np\ndef group_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, num_groups: int, epsilon: float=1e-05):\n    \"\"\"\n    Perform Group Normalization on a 4D input tensor.\n\n    Parameters:\n    - X (np.ndarray): Input tensor of shape (B, C, H, W)\n    - gamma (np.ndarray): Learnable scale parameters of shape (C,)\n    - beta (np.ndarray): Learnable shift parameters of shape (C,)\n    - num_groups (int): Number of groups to divide the channels into\n    - epsilon (float): Small value to avoid division by zero\n\n    Returns:\n    - list: Normalized output as a Python list, rounded to 4 decimal places\n    \"\"\"\n    (B, C, H, W) = X.shape\n    c_g = C // num_groups\n    X_reshaped = X.reshape(B, num_groups, c_g, H, W)\n    mean = np.mean(X_reshaped, axis=2, keepdims=True)\n    var = np.var(X_reshaped, axis=2, keepdims=True)\n    X_normalized = (X_reshaped - mean) / np.sqrt(var + epsilon)\n    X_normalized = X_normalized.reshape(B, C, H, W)\n    output = gamma * X_normalized + beta\n    output = np.round(output, 4)\n    return output.tolist()"}
{"task_id": 127, "completion_id": 0, "solution": "import numpy as np\ndef find_treasure(start_x: float, learning_rate: float=0.1, tolerance: float=1e-06, max_iters: int=10000) -> float:\n    current_x = start_x\n    for _ in range(max_iters):\n        gradient = 4 * current_x ** 3 - 9 * current_x ** 2\n        next_x = current_x - learning_rate * gradient\n        if abs(next_x - current_x) < tolerance:\n            return round(next_x, 4)\n        current_x = next_x\n    return round(current_x, 4)"}
{"task_id": 128, "completion_id": 0, "solution": "import numpy as np\ndef dynamic_tanh(x: np.ndarray, alpha: float, gamma: float, beta: float) -> list[float]:\n    \"\"\"\n    Applies the Dynamic Tanh (DyT) transformation to the input array.\n    \n    The Dynamic Tanh function is defined as:\n    DyT(x) = alpha * tanh(gamma * x + beta)\n    \n    This function replaces layer normalization in Transformer architectures\n    by combining scaling, shifting, and squashing behavior in a single operation.\n    \n    Parameters:\n        x (np.ndarray): Input array of any shape.\n        alpha (float): Scaling factor applied after the tanh activation.\n        gamma (float): Scaling factor applied to the input before tanh.\n        beta (float): Shifting factor applied to the input before tanh.\n    \n    Returns:\n        list[float]: Output array transformed by DyT, rounded to 4 decimal places\n                     and converted to a Python list using numpy's tolist().\n    \"\"\"\n    transformed = alpha * np.tanh(gamma * x + beta)\n    rounded = np.round(transformed, 4)\n    return rounded.tolist()"}
